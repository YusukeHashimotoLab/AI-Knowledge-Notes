<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3 Chapterï¼šDaskã«ã‚ˆã‚‹ä¸¦åˆ—è¨ˆç®— - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
            opacity: 0.9;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/large-scale-data-processing-introduction/index.html">Large Scale Data Processing</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3 Chapterï¼šDaskã«ã‚ˆã‚‹ä¸¦åˆ—è¨ˆç®—</h1>
            <p class="subtitle">Pythonã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 25-30 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 10</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Daskã®åŸºæœ¬æ¦‚å¿µã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Dask Arrayã¨Dask DataFrameã‚’ä½¿ã„ã“ãªã™</li>
<li>âœ… é…å»¶è©•ä¾¡ã¨ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Dask-MLã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªMachine Learningã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ä¸¦åˆ—è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©åˆ‡ã«ä½¿ã„ minutesã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… Daskã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç®¡ç†ãƒ»æœ€é©åŒ–ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 Daskã®æ¦‚è¦</h2>

<h3>Daskã¨ã¯</h3>
<p><strong>Dask</strong>ã¯ã€Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ãªä¸¦åˆ—è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€NumPyã‚„Pandasã®APIã¨äº’æ›æ€§ã‚’æŒã¡ãªãŒã‚‰ã€ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã§ãã¾ã™ã€‚</p>

<blockquote>
<p>ã€ŒDask = Pandas + ä¸¦åˆ—å‡¦ç† + ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€- æ—¢å­˜ã®Pythonã‚³ãƒ¼ãƒ‰ã‚’å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«æ‹¡å¼µ</p>
</blockquote>

<h3>Daskã®ä¸»è¦ãªç‰¹å¾´</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>èª¬æ˜</th>
<th>åˆ©ç‚¹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Pandas/NumPyäº’æ›</strong></td>
<td>æ—¢å­˜ã®APIã‚’ãã®ã¾ã¾åˆ©ç”¨</td>
<td>å­¦ç¿’ã‚³ã‚¹ãƒˆãŒä½ã„</td>
</tr>
<tr>
<td><strong>é…å»¶è©•ä¾¡</strong></td>
<td>è¨ˆç®—ã¯å¿…è¦ã«ãªã‚‹ã¾ã§å®Ÿè¡Œã•ã‚Œãªã„</td>
<td>æœ€é©åŒ–ã®ä½™åœ°</td>
</tr>
<tr>
<td><strong> distributedå‡¦ç†</strong></td>
<td>è¤‡æ•°ãƒã‚·ãƒ³ã§ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½</td>
<td>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</td>
</tr>
<tr>
<td><strong>å‹•çš„ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</strong></td>
<td>åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨</td>
<td>é«˜é€Ÿãªå‡¦ç†</td>
</tr>
</tbody>
</table>

<h3>Daskã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<div class="mermaid">
graph TB
    A[Daskã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³] --> B[ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•]
    B --> C[ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©]
    C --> D[ãƒ¯ãƒ¼ã‚«ãƒ¼1]
    C --> E[ãƒ¯ãƒ¼ã‚«ãƒ¼2]
    C --> F[ãƒ¯ãƒ¼ã‚«ãƒ¼3]
    D --> G[çµæœ]
    E --> G
    F --> G

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#e8f5e9
    style F fill:#e8f5e9
    style G fill:#c8e6c9
</div>

<h3>ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import dask
import dask.array as da
import dask.dataframe as dd
from dask.distributed import Client
import matplotlib.pyplot as plt

# Daskãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
print(f"Dask version: {dask.__version__}")

# ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿ã®èµ·å‹•
client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')
print(client)

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰URLï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã‘ã‚‹ï¼‰
print(f"\nDaskãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {client.dashboard_link}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>Dask version: 2023.10.1

&lt;Client: 'tcp://127.0.0.1:8786' processes=4 threads=8, memory=8.00 GB&gt;

Daskãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: http://127.0.0.1:8787/status
</code></pre>

<h3>Pandas vs Dask DataFrame</h3>

<pre><code class="language-python">import pandas as pd
import dask.dataframe as dd
import numpy as np

# Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
df_pandas = pd.DataFrame({
    'x': np.random.random(10000),
    'y': np.random.random(10000),
    'z': np.random.choice(['A', 'B', 'C'], 10000)
})

# Dask DataFrameã«å¤‰æ›ï¼ˆ4ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã« minuteså‰²ï¼‰
df_dask = dd.from_pandas(df_pandas, npartitions=4)

print("=== Pandas DataFrame ===")
print(f"å‹: {type(df_pandas)}")
print(f"å½¢çŠ¶: {df_pandas.shape}")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df_pandas.memory_usage(deep=True).sum() / 1024:.2f} KB")

print("\n=== Dask DataFrame ===")
print(f"å‹: {type(df_dask)}")
print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {df_dask.npartitions}")
print(f"åˆ—: {df_dask.columns.tolist()}")

# Daskã¯é…å»¶è©•ä¾¡ï¼šcompute()ã§å®Ÿéš›ã«å®Ÿè¡Œ
print("\nå¹³å‡å€¤ã®è¨ˆç®—:")
print(f"Pandas: {df_pandas['x'].mean():.6f}")
print(f"Dask: {df_dask['x'].mean().compute():.6f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Pandas DataFrame ===
å‹: &lt;class 'pandas.core.frame.DataFrame'&gt;
å½¢çŠ¶: (10000, 3)
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 235.47 KB

=== Dask DataFrame ===
å‹: &lt;class 'dask.dataframe.core.DataFrame'&gt;
ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 4
åˆ—: ['x', 'y', 'z']

å¹³å‡å€¤ã®è¨ˆç®—:
Pandas: 0.499845
Dask: 0.499845
</code></pre>

<h3>ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import dask.array as da

# ç°¡å˜ãªè¨ˆç®—ã®ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–
x = da.random.random((1000, 1000), chunks=(100, 100))
y = x + x.T
z = y.mean(axis=0)

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®è¡¨ç¤º
z.visualize(filename='dask_task_graph.png', optimize_graph=True)
print("ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚’ dask_task_graph.png ã«ä¿å­˜ã—ã¾ã—ãŸ")

# ã‚¿ã‚¹ã‚¯æ•°ã®ç¢ºèª
print(f"\nã‚¿ã‚¹ã‚¯æ•°: {len(z.__dask_graph__())}")
print(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {x.npartitions}")
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: Daskã¯è¨ˆç®—ã‚’è‡ªå‹•çš„ã«æœ€é©åŒ–ã—ã€ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>3.2 Dask Arrays & DataFrames</h2>

<h3>Dask Arrayï¼šå¤§è¦æ¨¡NumPyé…åˆ—</h3>

<p>Dask Arrayã¯ã€NumPyé…åˆ—ã‚’ãƒãƒ£ãƒ³ã‚¯ã« minuteså‰²ã—ã€ä¸¦åˆ—å‡¦ç†ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚</p>

<h4>åŸºæœ¬çš„ãªæ“ä½œ</h4>

<pre><code class="language-python">import dask.array as da
import numpy as np

# å¤§è¦æ¨¡ãªé…åˆ—ã®ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„ã‚µã‚¤ã‚ºï¼‰
# 10GBç›¸å½“ã®é…åˆ—ï¼ˆ10000 x 10000 x 100ã® float64ï¼‰
x = da.random.random((10000, 10000, 100), chunks=(1000, 1000, 10))

print("=== Dask Array ===")
print(f"å½¢çŠ¶: {x.shape}")
print(f"ãƒ‡ãƒ¼ã‚¿å‹: {x.dtype}")
print(f"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {x.chunks}")
print(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {x.npartitions}")
print(f"æ¨å®šã‚µã‚¤ã‚º: {x.nbytes / 1e9:.2f} GB")

# NumPyäº’æ›ã®æ“ä½œ
mean_value = x.mean()
std_value = x.std()
max_value = x.max()

# é…å»¶è©•ä¾¡ã®ãŸã‚ã€ã¾ã è¨ˆç®—ã•ã‚Œã¦ã„ãªã„
print(f"\nå¹³å‡å€¤ï¼ˆé…å»¶ï¼‰: {mean_value}")

# compute()ã§å®Ÿéš›ã«è¨ˆç®—ã‚’å®Ÿè¡Œ
print(f"å¹³å‡å€¤ï¼ˆå®Ÿè¡Œï¼‰: {mean_value.compute():.6f}")
print(f"æ¨™æº–åå·®: {std_value.compute():.6f}")
print(f"æœ€å¤§å€¤: {max_value.compute():.6f}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Dask Array ===
å½¢çŠ¶: (10000, 10000, 100)
ãƒ‡ãƒ¼ã‚¿å‹: float64
ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: ((1000, 1000, ...), (1000, 1000, ...), (10, 10, ...))
ãƒãƒ£ãƒ³ã‚¯æ•°: 1000
æ¨å®šã‚µã‚¤ã‚º: 80.00 GB

å¹³å‡å€¤ï¼ˆé…å»¶ï¼‰: dask.array&lt;mean_agg-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray&gt;

å¹³å‡å€¤ï¼ˆå®Ÿè¡Œï¼‰: 0.500021
æ¨™æº–åå·®: 0.288668
æœ€å¤§å€¤: 0.999999
</code></pre>

<h4>ç·šå½¢ä»£æ•°æ“ä½œ</h4>

<pre><code class="language-python">import dask.array as da

# å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—
A = da.random.random((5000, 5000), chunks=(1000, 1000))
B = da.random.random((5000, 5000), chunks=(1000, 1000))

# è¡Œåˆ—ç©ï¼ˆä¸¦åˆ—è¨ˆç®—ï¼‰
C = da.matmul(A, B)

print("=== è¡Œåˆ—æ¼”ç®— ===")
print(f"Aå½¢çŠ¶: {A.shape}, ãƒãƒ£ãƒ³ã‚¯æ•°: {A.npartitions}")
print(f"Bå½¢çŠ¶: {B.shape}, ãƒãƒ£ãƒ³ã‚¯æ•°: {B.npartitions}")
print(f"Cå½¢çŠ¶: {C.shape}, ãƒãƒ£ãƒ³ã‚¯æ•°: {C.npartitions}")

# SVDï¼ˆç‰¹ç•°å€¤ minutesè§£ï¼‰
U, s, V = da.linalg.svd_compressed(A, k=50)

print(f"\nç‰¹ç•°å€¤ minutesè§£:")
print(f"Uå½¢çŠ¶: {U.shape}")
print(f"ç‰¹ç•°å€¤æ•°: {len(s)}")
print(f"Vå½¢çŠ¶: {V.shape}")

# ä¸Šä½5ã¤ã®ç‰¹ç•°å€¤ã‚’è¨ˆç®—
top_5_singular_values = s[:5].compute()
print(f"\nä¸Šä½5ç‰¹ç•°å€¤: {top_5_singular_values}")
</code></pre>

<h3>Dask DataFrameï¼šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ </h3>

<h4>CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿</h4>

<pre><code class="language-python">import dask.dataframe as dd
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
for i in range(5):
    df = pd.DataFrame({
        'id': range(i * 1000000, (i + 1) * 1000000),
        'value': np.random.randn(1000000),
        'category': np.random.choice(['A', 'B', 'C', 'D'], 1000000),
        'timestamp': pd.date_range('2024-01-01', periods=1000000, freq='s')
    })
    df.to_csv(f'data_part_{i}.csv', index=False)

# Daskã§è¤‡æ•°CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—èª­ã¿è¾¼ã¿
ddf = dd.read_csv('data_part_*.csv', parse_dates=['timestamp'])

print("=== Dask DataFrame ===")
print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf.npartitions}")
print(f"åˆ—: {ddf.columns.tolist()}")
print(f"æ¨å®šè¡Œæ•°: ~{len(ddf)}è¡Œ")  # compute()ãªã—ã§æ¨å®šå¯èƒ½

# ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
print(f"\nãƒ‡ãƒ¼ã‚¿å‹:")
print(ddf.dtypes)

# æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤ºï¼ˆä¸€éƒ¨ã®ã¿è¨ˆç®—ï¼‰
print(f"\næœ€åˆã®5è¡Œ:")
print(ddf.head())
</code></pre>

<h4>DataFrameæ“ä½œ</h4>

<pre><code class="language-python">import dask.dataframe as dd

# ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ
category_stats = ddf.groupby('category')['value'].agg(['mean', 'std', 'count'])

print("=== ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆï¼ˆé…å»¶è©•ä¾¡ï¼‰===")
print(category_stats)

# compute()ã§å®Ÿè¡Œ
print("\n=== ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆï¼ˆå®Ÿè¡Œçµæœï¼‰===")
result = category_stats.compute()
print(result)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨å¤‰æ›
filtered = ddf[ddf['value'] > 0]
filtered['value_squared'] = filtered['value'] ** 2

# æ™‚ç³»åˆ—é›†è¨ˆ
daily_stats = ddf.set_index('timestamp').resample('D')['value'].mean()

print("\n=== æ—¥æ¬¡å¹³å‡ï¼ˆæœ€åˆã®5æ—¥ï¼‰===")
print(daily_stats.head())

# è¤‡æ•°ã®è¨ˆç®—ã‚’ä¸€åº¦ã«å®Ÿè¡Œï¼ˆåŠ¹ç‡çš„ï¼‰
mean_val, std_val, filtered_count = dask.compute(
    ddf['value'].mean(),
    ddf['value'].std(),
    len(filtered)
)

print(f"\nå…¨ä½“çµ±è¨ˆ:")
print(f"å¹³å‡: {mean_val:.6f}")
print(f"æ¨™æº–åå·®: {std_val:.6f}")
print(f"æ­£ã®å€¤ã®æ•°: {filtered_count:,}")
</code></pre>

<h4>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–</h4>

<pre><code class="language-python">import dask.dataframe as dd

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒªãƒãƒ©ãƒ³ã‚¹
print(f"å…ƒã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf.npartitions}")

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’èª¿æ•´ï¼ˆãƒ¡ãƒ¢ãƒªã¨CPUã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
ddf_optimized = ddf.repartition(npartitions=20)
print(f"æœ€é©åŒ–å¾Œã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf_optimized.npartitions}")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ minuteså‰²
ddf_indexed = ddf.set_index('category', sorted=True)
print(f"\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šå¾Œ:")
print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf_indexed.npartitions}")
print(f"æ—¢çŸ¥ã®ãƒ‡ã‚£ãƒ“ã‚¸ãƒ§ãƒ³: {ddf_indexed.known_divisions}")

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ç¢ºèª
partition_sizes = ddf.map_partitions(len).compute()
print(f"\nå„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã‚µã‚¤ã‚º: {partition_sizes.tolist()[:10]}")  # æœ€åˆã®10
</code></pre>

<blockquote>
<p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¯100MB-1GBç¨‹åº¦ãŒç†æƒ³çš„ã§ã™ã€‚</p>
</blockquote>

<hr>

<h2>3.3 Dask-MLï¼šã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«Machine Learning</h2>

<h3>Dask-MLã¨ã¯</h3>

<p><strong>Dask-ML</strong>ã¯ã€scikit-learnã®APIã‚’æ‹¡å¼µã—ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®Machine Learningã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚</p>

<h3>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</h3>

<pre><code class="language-python">import dask.dataframe as dd
import dask.array as da
from dask_ml.preprocessing import StandardScaler, LabelEncoder
from dask_ml.model_selection import train_test_split

# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
ddf = dd.read_csv('data_part_*.csv', parse_dates=['timestamp'])

# ç‰¹å¾´é‡ã®æŠ½å‡º
ddf['hour'] = ddf['timestamp'].dt.hour
ddf['day_of_week'] = ddf['timestamp'].dt.dayofweek

# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
le = LabelEncoder()
ddf['category_encoded'] = le.fit_transform(ddf['category'])

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã® minutesé›¢
X = ddf[['value', 'hour', 'day_of_week', 'category_encoded']].to_dask_array(lengths=True)
y = (ddf['value'] > 0).astype(int).to_dask_array(lengths=True)

print("=== ç‰¹å¾´é‡ ===")
print(f"Xå½¢çŠ¶: {X.shape}")
print(f"yå½¢çŠ¶: {y.shape}")

# ãƒ‡ãƒ¼ã‚¿ minuteså‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0].compute():,}è¡Œ")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0].compute():,}è¡Œ")

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\næ¨™æº–åŒ–å¾Œã®ãƒ‡ãƒ¼ã‚¿å‹: {type(X_train_scaled)}")
</code></pre>

<h3>å¢— minuteså­¦ç¿’ï¼ˆIncremental Learningï¼‰</h3>

<pre><code class="language-python">from dask_ml.linear_model import LogisticRegression
from dask_ml.metrics import accuracy_score, log_loss

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆå¢— minuteså­¦ç¿’ï¼‰
clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=42)

# ä¸¦åˆ—å­¦ç¿’
clf.fit(X_train_scaled, y_train)

# äºˆæ¸¬
y_pred = clf.predict(X_test_scaled)
y_pred_proba = clf.predict_proba(X_test_scaled)

# è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
loss = log_loss(y_test, y_pred_proba)

print("=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
print(f"ç²¾åº¦: {accuracy.compute():.4f}")
print(f"å¯¾æ•°æå¤±: {loss.compute():.4f}")

# ä¿‚æ•°ã®ç¢ºèª
print(f"\nãƒ¢ãƒ‡ãƒ«ä¿‚æ•°: {clf.coef_}")
print(f"åˆ‡ç‰‡: {clf.intercept_}")
</code></pre>

<h3>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h3>

<pre><code class="language-python">from dask_ml.model_selection import GridSearchCV
from dask_ml.linear_model import LogisticRegression
import numpy as np

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['saga']
}

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
clf = LogisticRegression(max_iter=100, random_state=42)
grid_search = GridSearchCV(
    clf,
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1
)

print("=== ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒé–‹å§‹ ===")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›æ•°: {len(param_grid['C']) * len(param_grid['penalty'])}")

# å­¦ç¿’ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œï¼‰
X_train_sample = X_train_scaled[:100000].compute()
y_train_sample = y_train[:100000].compute()

grid_search.fit(X_train_sample, y_train_sample)

print("\n=== ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœ ===")
print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
print(f"æœ€è‰¯ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")

# çµæœã®è©³ç´°
results_df = pd.DataFrame(grid_search.cv_results_)
print("\nTop 3 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›:")
print(results_df[['params', 'mean_test_score', 'std_test_score']].nlargest(3, 'mean_test_score'))
</code></pre>

<h3>Random Forest with Dask-ML</h3>

<pre><code class="language-python">from dask_ml.ensemble import RandomForestClassifier
from dask_ml.metrics import accuracy_score, classification_report

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
rf_clf = RandomForestClassifier(
    n_estimators=10,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

print("=== ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå­¦ç¿’ ===")
rf_clf.fit(X_train_scaled, y_train)

# äºˆæ¸¬
y_pred_rf = rf_clf.predict(X_test_scaled)

# è©•ä¾¡
accuracy_rf = accuracy_score(y_test, y_pred_rf)

print(f"ç²¾åº¦: {accuracy_rf.compute():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = rf_clf.feature_importances_
feature_names = ['value', 'hour', 'day_of_week', 'category_encoded']

print("\nç‰¹å¾´é‡é‡è¦åº¦:")
for name, importance in zip(feature_names, feature_importance):
    print(f"  {name}: {importance:.4f}")
</code></pre>

<h3>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰</h3>

<pre><code class="language-python">from dask_ml.compose import ColumnTransformer
from dask_ml.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline

# å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
numeric_features = ['value', 'hour', 'day_of_week']
categorical_features = ['category_encoded']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ]
)

# å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=100, random_state=42))
])

print("=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
print(pipeline)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å­¦ç¿’ã¨è©•ä¾¡
pipeline.fit(X_train, y_train)
y_pred_pipeline = pipeline.predict(X_test)
accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)

print(f"\nãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç²¾åº¦: {accuracy_pipeline.compute():.4f}")
</code></pre>

<hr>

<h2>3.4 ä¸¦åˆ—è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³</h2>

<h3>dask.delayedï¼šé–¢æ•°ã®é…å»¶å®Ÿè¡Œ</h3>

<p><code>dask.delayed</code>ã¯ã€ä»»æ„ã®Pythoné–¢æ•°ã‚’é…å»¶è©•ä¾¡ã«å¤‰æ›ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">import dask
from dask import delayed
import time

# é€šå¸¸ã®é–¢æ•°
def process_data(x):
    time.sleep(1)  # å‡¦ç†ã« hoursãŒã‹ã‹ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    return x ** 2

def aggregate(results):
    return sum(results)

# é€æ¬¡å®Ÿè¡Œ
print("=== é€æ¬¡å®Ÿè¡Œ ===")
start = time.time()
results = []
for i in range(8):
    results.append(process_data(i))
total = aggregate(results)
print(f"çµæœ: {total}")
print(f"å®Ÿè¡Œ hours: {time.time() - start:.2f}ç§’")

# ä¸¦åˆ—å®Ÿè¡Œï¼ˆdask.delayedï¼‰
print("\n=== ä¸¦åˆ—å®Ÿè¡Œï¼ˆdask.delayedï¼‰===")
start = time.time()
results_delayed = []
for i in range(8):
    result = delayed(process_data)(i)
    results_delayed.append(result)

total_delayed = delayed(aggregate)(results_delayed)
total = total_delayed.compute()

print(f"çµæœ: {total}")
print(f"å®Ÿè¡Œ hours: {time.time() - start:.2f}ç§’")

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–
total_delayed.visualize(filename='delayed_task_graph.png')
print("\nã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚’ delayed_task_graph.png ã«ä¿å­˜ã—ã¾ã—ãŸ")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== é€æ¬¡å®Ÿè¡Œ ===
çµæœ: 140
å®Ÿè¡Œ hours: 8.02ç§’

=== ä¸¦åˆ—å®Ÿè¡Œï¼ˆdask.delayedï¼‰===
çµæœ: 140
å®Ÿè¡Œ hours: 2.03ç§’
</code></pre>

<blockquote>
<p><strong>æ³¨ç›®</strong>: 4ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦åˆ—å®Ÿè¡Œã—ãŸãŸã‚ã€ç´„4å€é«˜é€ŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚</p>
</blockquote>

<h3>dask.bagï¼šéæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å‡¦ç†</h3>

<pre><code class="language-python">import dask.bag as db
import json

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
logs = [
    {'timestamp': '2024-01-01 10:00:00', 'level': 'INFO', 'message': 'User login'},
    {'timestamp': '2024-01-01 10:01:00', 'level': 'ERROR', 'message': 'Connection failed'},
    {'timestamp': '2024-01-01 10:02:00', 'level': 'INFO', 'message': 'User logout'},
    {'timestamp': '2024-01-01 10:03:00', 'level': 'WARNING', 'message': 'Slow query'},
] * 1000

with open('logs.json', 'w') as f:
    for log in logs:
        f.write(json.dumps(log) + '\n')

# Dask Bagã§ã®èª­ã¿è¾¼ã¿
bag = db.read_text('logs.json').map(json.loads)

print("=== Dask Bag ===")
print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {bag.npartitions}")

# å„ãƒ­ã‚°Levelã®é›†è¨ˆ
level_counts = bag.pluck('level').frequencies()
print(f"\nãƒ­ã‚°Levelåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ:")
print(level_counts.compute())

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
errors = bag.filter(lambda x: x['level'] == 'ERROR')
print(f"\nã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æ•°: {errors.count().compute():,}")

# ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†
def extract_hour(log):
    timestamp = log['timestamp']
    return timestamp.split()[1].split(':')[0]

hourly_distribution = bag.map(extract_hour).frequencies()
print(f"\n hoursåˆ¥ minuteså¸ƒ:")
print(hourly_distribution.compute())
</code></pre>

<h3>ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•</h3>

<pre><code class="language-python">import dask
from dask.threaded import get

# ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®å®šç¾©
# DAG (Directed Acyclic Graph) å½¢å¼
task_graph = {
    'x': 1,
    'y': 2,
    'z': (lambda a, b: a + b, 'x', 'y'),
    'w': (lambda a: a * 2, 'z'),
    'result': (lambda a, b: a ** b, 'w', 'y')
}

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œ
result = get(task_graph, 'result')
print(f"=== ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ• ===")
print(f"çµæœ: {result}")

# è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•
def load_data(source):
    print(f"Loading from {source}")
    return f"data_{source}"

def process(data):
    print(f"Processing {data}")
    return f"processed_{data}"

def merge(data1, data2):
    print(f"Merging {data1} and {data2}")
    return f"merged_{data1}_{data2}"

complex_graph = {
    'load_a': (load_data, 'source_A'),
    'load_b': (load_data, 'source_B'),
    'process_a': (process, 'load_a'),
    'process_b': (process, 'load_b'),
    'final': (merge, 'process_a', 'process_b')
}

final_result = get(complex_graph, 'final')
print(f"\næœ€çµ‚çµæœ: {final_result}")
</code></pre>

<h3>ä¸¦åˆ—map/apply</h3>

<pre><code class="language-python">import dask.dataframe as dd
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
ddf = dd.from_pandas(
    pd.DataFrame({
        'A': np.random.randn(10000),
        'B': np.random.randn(10000),
        'C': np.random.choice(['X', 'Y', 'Z'], 10000)
    }),
    npartitions=4
)

# map_partitions: å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«é–¢æ•°ã‚’é©ç”¨
def custom_processing(partition):
    # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã”ã¨ã®ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†
    partition['A_squared'] = partition['A'] ** 2
    partition['B_log'] = np.log1p(np.abs(partition['B']))
    return partition

ddf_processed = ddf.map_partitions(custom_processing)

print("=== map_partitions ===")
print(ddf_processed.head())

# apply: å„è¡Œã«é–¢æ•°ã‚’é©ç”¨
def row_function(row):
    return row['A'] * row['B']

ddf['A_times_B'] = ddf.apply(row_function, axis=1, meta=('A_times_B', 'f8'))

print("\n=== apply ===")
print(ddf.head())

# é›†è¨ˆé–¢æ•°ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
def custom_agg(partition):
    return pd.Series({
        'mean': partition['A'].mean(),
        'std': partition['A'].std(),
        'min': partition['A'].min(),
        'max': partition['A'].max()
    })

stats = ddf.map_partitions(custom_agg).compute()
print("\n=== ã‚«ã‚¹ã‚¿ãƒ é›†è¨ˆ ===")
print(stats)
</code></pre>

<hr>

<h2>3.5 Daskã‚¯ãƒ©ã‚¹ã‚¿ç®¡ç†</h2>

<h3>LocalClusterï¼šãƒ­ãƒ¼ã‚«ãƒ«ä¸¦åˆ—å‡¦ç†</h3>

<pre><code class="language-python">from dask.distributed import Client, LocalCluster
import dask.array as da

# LocalClusterã®è©³ç´°è¨­å®š
cluster = LocalCluster(
    n_workers=4,
    threads_per_worker=2,
    memory_limit='2GB',
    dashboard_address=':8787'
)

client = Client(cluster)

print("=== LocalCluster æƒ…å ± ===")
print(f"ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {len(client.scheduler_info()['workers'])}")
print(f"ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}")
print(f"ãƒ¡ãƒ¢ãƒªåˆ¶é™: {cluster.worker_spec[0]['options']['memory_limit']}")
print(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {client.dashboard_link}")

# ãƒ¯ãƒ¼ã‚«ãƒ¼æƒ…å ±ã®è©³ç´°
for worker_id, info in client.scheduler_info()['workers'].items():
    print(f"\nãƒ¯ãƒ¼ã‚«ãƒ¼ {worker_id}:")
    print(f"  ã‚¹ãƒ¬ãƒƒãƒ‰: {info['nthreads']}")
    print(f"  ãƒ¡ãƒ¢ãƒª: {info['memory_limit'] / 1e9:.2f} GB")

# è¨ˆç®—ã®å®Ÿè¡Œ
x = da.random.random((10000, 10000), chunks=(1000, 1000))
result = (x + x.T).mean().compute()

print(f"\nè¨ˆç®—çµæœ: {result:.6f}")

# ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚¯ãƒ­ãƒ¼ã‚º
client.close()
cluster.close()
</code></pre>

<h3> distributedã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</h3>

<pre><code class="language-python">from dask.distributed import Client, progress
import dask.array as da
import time

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®èµ·å‹•
client = Client(n_workers=4, threads_per_worker=2)

# å¤§è¦æ¨¡è¨ˆç®—ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
x = da.random.random((50000, 50000), chunks=(5000, 5000))
y = da.random.random((50000, 50000), chunks=(5000, 5000))

# è¤‡æ•°ã®è¨ˆç®—ã‚’åŒæ™‚ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
results = []
for i in range(5):
    result = (x + y * i).sum()
    results.append(result)

# é€²æ—çŠ¶æ³ã®è¡¨ç¤º
futures = client.compute(results)
progress(futures)

# çµæœã®å–å¾—
final_results = client.gather(futures)

print("\n=== è¨ˆç®—çµæœ ===")
for i, result in enumerate(final_results):
    print(f"è¨ˆç®— {i + 1}: {result:.2f}")

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®çµ±è¨ˆ
print("\n=== ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©çµ±è¨ˆ ===")
print(f"å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯æ•°: {client.scheduler_info()['total_occupancy']}")
print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ã‚«ãƒ¼: {len(client.scheduler_info()['workers'])}")
</code></pre>

<h3>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</h3>

<pre><code class="language-python">from dask.distributed import Client, performance_report
import dask.dataframe as dd
import dask.array as da

client = Client(n_workers=4)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
with performance_report(filename="dask_performance.html"):
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ
    ddf = dd.read_csv('data_part_*.csv')
    result1 = ddf.groupby('category')['value'].mean().compute()

    # é…åˆ—æ“ä½œ
    x = da.random.random((10000, 10000), chunks=(1000, 1000))
    result2 = (x + x.T).mean().compute()

    print("è¨ˆç®—å®Œäº†")

print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ dask_performance.html ã«ä¿å­˜ã—ã¾ã—ãŸ")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª
memory_info = client.run(lambda: {
    'used': sum(v['memory'] for v in client.scheduler_info()['workers'].values()),
    'limit': sum(v['memory_limit'] for v in client.scheduler_info()['workers'].values())
})

print("\n=== ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ ===")
for worker, info in memory_info.items():
    print(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ {worker}: ä½¿ç”¨ç‡ N/A")

# ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµ±è¨ˆ
print("\n=== ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµ±è¨ˆ ===")
print(f"å‡¦ç†ã•ã‚ŒãŸãƒã‚¤ãƒˆæ•°: {client.scheduler_info().get('total_occupancy', 'N/A')}")
</code></pre>

<h3>ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>

<pre><code class="language-python">from dask.distributed import Client
from dask_kubernetes import KubeCluster

# Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã®è¨­å®šï¼ˆä¾‹ï¼‰
"""
cluster = KubeCluster(
    name='dask-cluster',
    namespace='default',
    image='daskdev/dask:latest',
    n_workers=10,
    resources={
        'requests': {'memory': '4Gi', 'cpu': '2'},
        'limits': {'memory': '8Gi', 'cpu': '4'}
    }
)

client = Client(cluster)

# å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
cluster.adapt(minimum=2, maximum=20)

print(f"ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±: {cluster}")
"""

# ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
from dask.distributed import Client, LocalCluster

cluster = LocalCluster()
client = Client(cluster)

# ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å‹•çš„ã«èª¿æ•´
cluster.adapt(minimum=2, maximum=8)

print("=== ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° ===")
print(f"æœ€å°ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: 2")
print(f"æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: 8")
print(f"ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {len(client.scheduler_info()['workers'])}")

# è² è·ã‚’ã‹ã‘ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç¢ºèª
import dask.array as da
x = da.random.random((50000, 50000), chunks=(1000, 1000))
result = x.sum().compute()

print(f"\nè¨ˆç®—å¾Œã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {len(client.scheduler_info()['workers'])}")
</code></pre>

<h3>ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°</h3>

<pre><code class="language-python">from dask.distributed import Client
import dask.array as da

client = Client(n_workers=4)

# ã‚¿ã‚¹ã‚¯ã®ç›£è¦–
x = da.random.random((10000, 10000), chunks=(1000, 1000))
future = client.compute(x.sum())

# ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ç¢ºèª
print("=== ã‚¿ã‚¹ã‚¯çŠ¶æ…‹ ===")
print(f"çŠ¶æ…‹: {future.status}")
print(f"ã‚­ãƒ¼: {future.key}")

# çµæœã‚’å¾…ã¤
result = future.result()
print(f"çµæœ: {result:.6f}")

# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚°ã®å–å¾—
logs = client.get_worker_logs()
print("\n=== ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚°ï¼ˆæœ€åˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰===")
first_worker = list(logs.keys())[0]
print(f"ãƒ¯ãƒ¼ã‚«ãƒ¼: {first_worker}")
print(logs[first_worker][:500])  # æœ€åˆã®500æ–‡å­—

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®çµ±è¨ˆ
print("\n=== ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•çµ±è¨ˆ ===")
print(f"ã‚¿ã‚¹ã‚¯æ•°: {len(x.__dask_graph__())}")
print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(x.__dask_layers__())}")
</code></pre>

<hr>

<h2>3.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>Daskã®åŸºæœ¬</strong></p>
<ul>
<li>Pandas/NumPyäº’æ›ã®API</li>
<li>é…å»¶è©•ä¾¡ã¨ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•</li>
<li> distributedä¸¦åˆ—å‡¦ç†ã®ä»•çµ„ã¿</li>
</ul></li>

<li><p><strong>Dask ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³</strong></p>
<ul>
<li>Dask Array: å¤§è¦æ¨¡NumPyé…åˆ—</li>
<li>Dask DataFrame: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ </li>
<li>Dask Bag: éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å‡¦ç†</li>
</ul></li>

<li><p><strong>Dask-ML</strong></p>
<ul>
<li>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªMachine Learning</li>
<li>å¢— minuteså­¦ç¿’ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</li>
<li>å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
</ul></li>

<li><p><strong>ä¸¦åˆ—è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³</strong></p>
<ul>
<li>dask.delayed: é–¢æ•°ã®é…å»¶å®Ÿè¡Œ</li>
<li>dask.bag: éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿</li>
<li>ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•</li>
<li>map_partitions/apply</li>
</ul></li>

<li><p><strong>ã‚¯ãƒ©ã‚¹ã‚¿ç®¡ç†</strong></p>
<ul>
<li>LocalCluster: ãƒ­ãƒ¼ã‚«ãƒ«ä¸¦åˆ—å‡¦ç†</li>
<li> distributedã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</li>
<li>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</li>
<li>å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</li>
</ul></li>
</ol>

<h3>Daskã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Recommendedäº‹é …</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º</strong></td>
<td>100MB-1GBç¨‹åº¦ãŒç†æƒ³çš„</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°</strong></td>
<td>ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®2-4å€</td>
</tr>
<tr>
<td><strong>compute()ã®ä½¿ç”¨</strong></td>
<td>è¤‡æ•°ã®è¨ˆç®—ã‚’ä¸€åº¦ã«compute()ã§å®Ÿè¡Œ</td>
</tr>
<tr>
<td><strong>persist()ã®æ´»ç”¨</strong></td>
<td>å†åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¡ãƒ¢ãƒªã«ä¿æŒ</td>
</tr>
<tr>
<td><strong>ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š</strong></td>
<td>sorted=Trueã§é«˜é€Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</td>
</tr>
</tbody>
</table>

<h3>Spark vs Dask æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Spark</th>
<th>Dask</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>è¨€èª</strong></td>
<td>Scala/Java/Python</td>
<td>Pythonå°‚ç”¨</td>
</tr>
<tr>
<td><strong>API</strong></td>
<td>ç‹¬è‡ªAPI</td>
<td>Pandas/NumPyäº’æ›</td>
</tr>
<tr>
<td><strong>å­¦ç¿’æ›²ç·š</strong></td>
<td>æ€¥</td>
<td>ç·©ã‚„ã‹</td>
</tr>
<tr>
<td><strong>ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ </strong></td>
<td>å¤§è¦æ¨¡ãƒ»æˆç†Ÿ</td>
<td>å°è¦æ¨¡ãƒ»æˆé•·ä¸­</td>
</tr>
<tr>
<td><strong>é©ç”¨å ´é¢</strong></td>
<td>è¶…å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†</td>
<td>ä¸­è¦æ¨¡ãƒ»å¯¾è©±çš„å‡¦ç†</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªç®¡ç†</strong></td>
<td>JVMãƒ™ãƒ¼ã‚¹</td>
<td>Pythonãƒã‚¤ãƒ†ã‚£ãƒ–</td>
</tr>
</tbody>
</table>

<h3>Next Chapterã¸</h3>

<p>Chapter 4 Chapterã§ã¯ã€<strong>ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Parquet/ORCãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ</li>
<li>ã‚«ãƒ©ãƒ ãƒŠãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸</li>
<li>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</li>
<li>ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>Daskã¨Pandasã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã€ãã‚Œãã‚Œã®ç‰¹å¾´ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>å®Ÿè¡Œãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>Pandas: å³åº§ã«å®Ÿè¡Œï¼ˆEager Evaluationï¼‰</li>
<li>Dask: é…å»¶è©•ä¾¡ï¼ˆLazy Evaluationï¼‰- compute()ã§å®Ÿè¡Œ</li>
</ul></li>

<li><p><strong>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£</strong></p>
<ul>
<li>Pandas: ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿</li>
<li>Dask: ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿ã‚‚å‡¦ç†å¯èƒ½</li>
</ul></li>

<li><p><strong>ä¸¦åˆ—å‡¦ç†</strong></p>
<ul>
<li>Pandas: å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹</li>
<li>Dask: è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦åˆ—å‡¦ç†å¯èƒ½</li>
</ul></li>
</ol>

<p><strong>ä½¿ã„ minutesã‘</strong>ï¼š</p>
<ul>
<li>å°ã€œä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ< æ•°GBï¼‰: Pandas</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ> 10GBï¼‰: Dask</li>
<li>è¤‡é›‘ãªé›†è¨ˆãƒ»å¤‰æ›: Pandasï¼ˆé«˜é€Ÿï¼‰</li>
<li>ä¸¦åˆ—å‡¦ç†ãŒå¿…è¦: Dask</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€é…å»¶è©•ä¾¡ã®ä»•çµ„ã¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãªãœ2ã¤ã®å‡ºåŠ›ãŒç•°ãªã‚‹ã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import dask.array as da

x = da.random.random((1000, 1000), chunks=(100, 100))
y = x + 1
z = y * 2

print("1.", z)
print("2.", z.compute())
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import dask.array as da

x = da.random.random((1000, 1000), chunks=(100, 100))
y = x + 1
z = y * 2

print("1.", z)
print("2.", z.compute())
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>1. dask.array&lt;mul, shape=(1000, 1000), dtype=float64, chunksize=(100, 100), chunktype=numpy.ndarray&gt;
2. [[1.234 2.567 ...] [3.890 1.456 ...] ...]
</code></pre>

<p><strong>èª¬æ˜</strong>ï¼š</p>

<ol>
<li><p><strong>1ã¤ç›®ã®å‡ºåŠ›ï¼ˆé…å»¶ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰</strong>ï¼š</p>
<ul>
<li><code>z</code>ã¯é…å»¶è©•ä¾¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€è¨ˆç®—ã¯ã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„</li>
<li>ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®ã¿ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹çŠ¶æ…‹</li>
<li>ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå½¢çŠ¶ã€ãƒ‡ãƒ¼ã‚¿å‹ã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼‰ã®ã¿è¡¨ç¤º</li>
</ul></li>

<li><p><strong>2ã¤ç›®ã®å‡ºåŠ›ï¼ˆè¨ˆç®—çµæœï¼‰</strong>ï¼š</p>
<ul>
<li><code>compute()</code>ã‚’å‘¼ã¶ã“ã¨ã§å®Ÿéš›ã«è¨ˆç®—ãŒå®Ÿè¡Œã•ã‚Œã‚‹</li>
<li>ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ãŒå®Ÿè¡Œã•ã‚Œã€çµæœãŒNumPyé…åˆ—ã¨ã—ã¦è¿”ã•ã‚Œã‚‹</li>
</ul></li>
</ol>

<p><strong>é…å»¶è©•ä¾¡ã®åˆ©ç‚¹</strong>ï¼š</p>
<ul>
<li>è¨ˆç®—ã®æœ€é©åŒ–ï¼ˆä¸è¦ãªä¸­é–“çµæœã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰</li>
<li>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼ˆå¿…è¦ãªéƒ¨ minutesã®ã¿è¨ˆç®—ï¼‰</li>
<li>ä¸¦åˆ—å®Ÿè¡Œã®ä½™åœ°ï¼ˆã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’è¦‹ã¦æœ€é©åŒ–ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>Dask DataFrameã§1å„„è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹éš›ã®é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒç´„100MBã«ãªã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚1è¡Œã‚ãŸã‚Š50ãƒã‚¤ãƒˆã¨ä»®å®šã—ã¾ã™ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<pre><code class="language-python"># ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±
total_rows = 100_000_000  # 1å„„è¡Œ
bytes_per_row = 50  # 1è¡Œã‚ãŸã‚Š50ãƒã‚¤ãƒˆ
target_partition_size_mb = 100  # ç›®æ¨™ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºï¼ˆMBï¼‰

# è¨ˆç®—
total_size_bytes = total_rows * bytes_per_row
total_size_mb = total_size_bytes / (1024 ** 2)

partition_count = total_size_mb / target_partition_size_mb

print("=== ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®è¨ˆç®— ===")
print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_size_mb:.2f} MB ({total_size_bytes / 1e9:.2f} GB)")
print(f"ç›®æ¨™ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: {target_partition_size_mb} MB")
print(f"å¿…è¦ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {partition_count:.0f}")
print(f"å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è¡Œæ•°: {total_rows / partition_count:,.0f}è¡Œ")

# Dask DataFrameã§ã®å®Ÿè£…ä¾‹
import dask.dataframe as dd
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯1å„„è¡Œï¼‰
df = pd.DataFrame({
    'id': range(1000000),
    'value': np.random.randn(1000000)
})

# è¨ˆç®—ã—ãŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã§ minuteså‰²
npartitions = int(partition_count)
ddf = dd.from_pandas(df, npartitions=npartitions)

print(f"\nDask DataFrame:")
print(f"  ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf.npartitions}")
print(f"  å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æ¨å®šã‚µã‚¤ã‚º: {total_size_mb / npartitions:.2f} MB")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®è¨ˆç®— ===
ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 4768.37 MB (5.00 GB)
ç›®æ¨™ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: 100 MB
å¿…è¦ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 48
å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è¡Œæ•°: 2,083,333è¡Œ

Dask DataFrame:
  ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 48
  å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æ¨å®šã‚µã‚¤ã‚º: 99.34 MB
</code></pre>

<p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>ï¼š</p>
<ul>
<li>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: 100MB-1GB</li>
<li>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®2-4å€</li>
<li>ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ã‚µã‚¤ã‚ºã«èª¿æ•´</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>dask.delayedã‚’ä½¿ã£ã¦ã€ä»¥ä¸‹ã®ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚‚å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚</p>
<ul>
<li>ã‚¿ã‚¹ã‚¯A, Bã¯ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½</li>
<li>ã‚¿ã‚¹ã‚¯Cã¯A, Bã®çµæœã‚’ä½¿ç”¨</li>
<li>ã‚¿ã‚¹ã‚¯Dã¯Cã®çµæœã‚’ä½¿ç”¨</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import dask
from dask import delayed
import time

# ã‚¿ã‚¹ã‚¯å®šç¾©
@delayed
def task_a():
    time.sleep(2)
    print("ã‚¿ã‚¹ã‚¯Aå®Œäº†")
    return 10

@delayed
def task_b():
    time.sleep(2)
    print("ã‚¿ã‚¹ã‚¯Bå®Œäº†")
    return 20

@delayed
def task_c(a_result, b_result):
    time.sleep(1)
    print("ã‚¿ã‚¹ã‚¯Cå®Œäº†")
    return a_result + b_result

@delayed
def task_d(c_result):
    time.sleep(1)
    print("ã‚¿ã‚¹ã‚¯Då®Œäº†")
    return c_result * 2

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
print("=== ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ===")
a = task_a()
b = task_b()
c = task_c(a, b)
d = task_d(c)

print("ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†ï¼ˆã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")

# ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–
d.visualize(filename='task_dependency_graph.png')
print("ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚’ task_dependency_graph.png ã«ä¿å­˜ã—ã¾ã—ãŸ")

# å®Ÿè¡Œ
print("\n=== ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé–‹å§‹ ===")
start_time = time.time()
result = d.compute()
end_time = time.time()

print(f"\n=== çµæœ ===")
print(f"æœ€çµ‚çµæœ: {result}")
print(f"å®Ÿè¡Œ hours: {end_time - start_time:.2f}ç§’")

# æœŸå¾…ã•ã‚Œã‚‹å®Ÿè¡Œ hours
print(f"\næœŸå¾…ã•ã‚Œã‚‹å®Ÿè¡Œ hours:")
print(f"  é€æ¬¡å®Ÿè¡Œ: 2 + 2 + 1 + 1 = 6ç§’")
print(f"  ä¸¦åˆ—å®Ÿè¡Œ: max(2, 2) + 1 + 1 = 4ç§’")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ===
ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†ï¼ˆã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰
ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã‚’ task_dependency_graph.png ã«ä¿å­˜ã—ã¾ã—ãŸ

=== ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé–‹å§‹ ===
ã‚¿ã‚¹ã‚¯Aå®Œäº†
ã‚¿ã‚¹ã‚¯Bå®Œäº†
ã‚¿ã‚¹ã‚¯Cå®Œäº†
ã‚¿ã‚¹ã‚¯Då®Œäº†

=== çµæœ ===
æœ€çµ‚çµæœ: 60
å®Ÿè¡Œ hours: 4.02ç§’

æœŸå¾…ã•ã‚Œã‚‹å®Ÿè¡Œ hours:
  é€æ¬¡å®Ÿè¡Œ: 2 + 2 + 1 + 1 = 6ç§’
  ä¸¦åˆ—å®Ÿè¡Œ: max(2, 2) + 1 + 1 = 4ç§’
</code></pre>

<p><strong>ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ã®èª¬æ˜</strong>ï¼š</p>
<ul>
<li>Aã¨Bã¯ä¾å­˜é–¢ä¿‚ãŒãªã„ãŸã‚ä¸¦åˆ—å®Ÿè¡Œ</li>
<li>Cã¯A, Bã®å®Œäº†ã‚’å¾…ã¤</li>
<li>Dã¯Cã®å®Œäº†ã‚’å¾…ã¤</li>
<li>å…¨ä½“ã§ç´„4ç§’ï¼ˆä¸¦åˆ—åŒ–ã«ã‚ˆã‚Š33%é«˜é€ŸåŒ–ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Dask DataFrameã§å¤§è¦æ¨¡ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li>æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤</li>
<li>ç‰¹å®šã®ã‚«ãƒ©ãƒ ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å¹³å‡ã‚’è¨ˆç®—</li>
<li>çµæœã‚’Parquetãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜</li>
<li>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹</li>
</ol>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import dask.dataframe as dd
import pandas as pd
import numpy as np
import time
from dask.distributed import Client, performance_report

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
print("=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ ===")
for i in range(10):
    df = pd.DataFrame({
        'id': range(i * 100000, (i + 1) * 100000),
        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 100000),
        'value1': np.random.randn(100000),
        'value2': np.random.randn(100000),
        'timestamp': pd.date_range('2024-01-01', periods=100000, freq='s')
    })
    # æ„å›³çš„ã«æ¬ æå€¤ã‚’è¿½åŠ 
    df.loc[np.random.choice(100000, 1000, replace=False), 'value1'] = np.nan
    df.to_csv(f'large_data_{i}.csv', index=False)

print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†")

# Daskã‚¯ãƒ©ã‚¹ã‚¿ã®èµ·å‹•
client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')
print(f"\nDaskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: {client}")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä»˜ãã§å‡¦ç†
with performance_report(filename="processing_performance.html"):

    print("\n=== Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===")
    start = time.time()

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¦åˆ—èª­ã¿è¾¼ã¿
    ddf = dd.read_csv(
        'large_data_*.csv',
        parse_dates=['timestamp'],
        assume_missing=True
    )

    print(f"èª­ã¿è¾¼ã¿å®Œäº†: {time.time() - start:.2f}ç§’")
    print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf.npartitions}")
    print(f"æ¨å®šè¡Œæ•°: ~{len(ddf):,}è¡Œ")

    print("\n=== Step 2: æ¬ æå€¤å‰Šé™¤ ===")
    start = time.time()

    # æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
    ddf_clean = ddf.dropna()

    print(f"æ¬ æå€¤å‰Šé™¤å®Œäº†: {time.time() - start:.2f}ç§’")

    print("\n=== Step 3: ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ ===")
    start = time.time()

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¹³å‡è¨ˆç®—
    result = ddf_clean.groupby('category').agg({
        'value1': ['mean', 'std', 'count'],
        'value2': ['mean', 'std', 'count']
    })

    # è¨ˆç®—ã®å®Ÿè¡Œ
    result_computed = result.compute()

    print(f"é›†è¨ˆå®Œäº†: {time.time() - start:.2f}ç§’")
    print("\né›†è¨ˆçµæœ:")
    print(result_computed)

    print("\n=== Step 4: Parquetä¿å­˜ ===")
    start = time.time()

    # Parquetãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ minuteså‰²ï¼‰
    ddf_clean.to_parquet(
        'output_data.parquet',
        engine='pyarrow',
        partition_on=['category'],
        compression='snappy'
    )

    print(f"ä¿å­˜å®Œäº†: {time.time() - start:.2f}ç§’")

print("\n=== æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ ===")
print("1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’èª¿æ•´ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ï¼‰")
print("2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨­å®šï¼ˆé«˜é€Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰")
print("3. persist()ã§ä¸­é–“çµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ")
print("4. Parquetã§ä¿å­˜ï¼ˆã‚«ãƒ©ãƒ ãƒŠãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰")
print("5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã§ minutesæ")

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–ã®ä¾‹
print("\n=== ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ– ===")

# å…ƒã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
print(f"å…ƒã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf.npartitions}")

# æœ€é©åŒ–ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®2-4å€ãŒRecommendedï¼‰
n_workers = len(client.scheduler_info()['workers'])
optimal_partitions = n_workers * 3

ddf_optimized = ddf.repartition(npartitions=optimal_partitions)
print(f"æœ€é©åŒ–å¾Œã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {ddf_optimized.npartitions}")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šã«ã‚ˆã‚‹é«˜é€ŸåŒ–
ddf_indexed = ddf_clean.set_index('category', sorted=True)
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šå¾Œ: {ddf_indexed.npartitions}ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³")

# ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚¯ãƒ­ãƒ¼ã‚º
client.close()

print("\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: processing_performance.html")
print("å‡¦ç†å®Œäº†!")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ ===
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†

Daskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: &lt;Client: 'tcp://127.0.0.1:xxxxx' processes=4 threads=8&gt;

=== Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
èª­ã¿è¾¼ã¿å®Œäº†: 0.15ç§’
ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 10
æ¨å®šè¡Œæ•°: ~1,000,000è¡Œ

=== Step 2: æ¬ æå€¤å‰Šé™¤ ===
æ¬ æå€¤å‰Šé™¤å®Œäº†: 0.01ç§’

=== Step 3: ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ ===
é›†è¨ˆå®Œäº†: 1.23ç§’

é›†è¨ˆçµæœ:
          value1                    value2
            mean       std  count      mean       std  count
category
A        0.0012  0.999845 200145  -0.0008  1.000234 200145
B       -0.0023  1.001234 199876   0.0015  0.999876 199876
C        0.0034  0.998765 200234  -0.0021  1.001345 200234
D       -0.0011  1.000987 199987   0.0028  0.998654 199987
E        0.0019  0.999543 199758  -0.0013  1.000789 199758

=== Step 4: Parquetä¿å­˜ ===
ä¿å­˜å®Œäº†: 2.45ç§’

=== æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ ===
1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’èª¿æ•´ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ï¼‰
2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨­å®šï¼ˆé«˜é€Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
3. persist()ã§ä¸­é–“çµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
4. Parquetã§ä¿å­˜ï¼ˆã‚«ãƒ©ãƒ ãƒŠãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰
5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã§ minutesæ

=== ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ– ===
å…ƒã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 10
æœ€é©åŒ–å¾Œã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 12
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šå¾Œ: 5ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: processing_performance.html
å‡¦ç†å®Œäº†!
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Dask Development Team. (2024). <em>Dask: Scalable analytics in Python</em>. <a href="https://docs.dask.org/">https://docs.dask.org/</a></li>
<li>Rocklin, M. (2015). <em>Dask: Parallel Computation with Blocked algorithms and Task Scheduling</em>. Proceedings of the 14th Python in Science Conference.</li>
<li>McKinney, W. (2017). <em>Python for Data Analysis</em> (2nd ed.). O'Reilly Media.</li>
<li>VanderPlas, J. (2016). <em>Python Data Science Handbook</em>. O'Reilly Media.</li>
<li>Dask-ML Documentation. <a href="https://ml.dask.org/">https://ml.dask.org/</a></li>
</ol>

<div class="navigation">
    <a href="chapter2-spark.html" class="nav-button">â† Previous Chapter: Apache Spark</a>
    <a href="chapter4-storage.html" class="nav-button">Next Chapter: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ– â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
