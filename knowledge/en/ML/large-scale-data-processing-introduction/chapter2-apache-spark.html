<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Apache Sparkã«ã‚ˆã‚‹ distributedMachine Learning - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/large-scale-data-processing-introduction/index.html">Large Scale Data Processing</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Apache Sparkã«ã‚ˆã‚‹ distributedMachine Learning</h1>
            <p class="subtitle">ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿MLã®å®Ÿè·µåŸºç›¤ - Sparkã§åŠ é€Ÿã™ã‚‹Large-Scale Data Processing</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 35-40 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 10</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Sparkã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ distributedå‡¦ç†ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… RDDã€DataFrameã€Datasetã®APIã‚’ä½¿ã„ minutesã‘ã‚‰ã‚Œã‚‹</li>
<li>âœ… Spark SQLã§åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ“ä½œãŒã§ãã‚‹</li>
<li>âœ… Spark MLlibã§ distributedMachine Learningã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ‰‹æ³•ã‚’é©ç”¨ã§ãã‚‹</li>
<li>âœ… å®Ÿãƒ‡ãƒ¼ã‚¿ã§å¤§è¦æ¨¡MLå‡¦ç†ã‚’å®Ÿè¡Œã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 Sparkã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>Apache Sparkã¨ã¯</h3>
<p><strong>Apache Spark</strong>ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿ distributedå‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚MapReduceã®100å€ä»¥ä¸Šã®é€Ÿåº¦ã‚’å®Ÿç¾ã—ã€Machine Learningã€ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã€ã‚°ãƒ©ãƒ•å‡¦ç†ã«å¯¾å¿œã—ã¾ã™ã€‚</p>

<blockquote>
<p>ã€ŒMapReduceã®å¾Œç¶™ learnerã€- ãƒ¡ãƒ¢ãƒªå†…å‡¦ç†ã«ã‚ˆã‚Šã€åå¾©å‡¦ç†ãŒåœ§å€’çš„ã«é«˜é€ŸåŒ–ã—ã¾ã™ã€‚</p>
</blockquote>

<h3>Sparkã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>ç”¨é€”</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Spark Core</strong></td>
<td>åŸºæœ¬å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³</td>
<td>RDDã€ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°</td>
</tr>
<tr>
<td><strong>Spark SQL</strong></td>
<td>æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å‡¦ç†</td>
<td>DataFrameã€SQL ã‚¯ã‚¨ãƒª</td>
</tr>
<tr>
<td><strong>Spark MLlib</strong></td>
<td>Machine Learning</td>
<td> distributedMLã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</td>
</tr>
<tr>
<td><strong>Spark Streaming</strong></td>
<td>ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†</td>
<td>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†</td>
</tr>
<tr>
<td><strong>GraphX</strong></td>
<td>ã‚°ãƒ©ãƒ•å‡¦ç†</td>
<td>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ</td>
</tr>
</tbody>
</table>

<h3>Driver ã¨ Executor ã®é–¢ä¿‚</h3>

<div class="mermaid">
graph TB
    subgraph "Driver Program"
        A[SparkContext]
        B[DAG Scheduler]
        C[Task Scheduler]
    end

    subgraph "Cluster Manager"
        D[YARN / Mesos / K8s]
    end

    subgraph "Worker Node 1"
        E1[Executor 1]
        E2[Task]
        E3[Cache]
    end

    subgraph "Worker Node 2"
        F1[Executor 2]
        F2[Task]
        F3[Cache]
    end

    subgraph "Worker Node N"
        G1[Executor N]
        G2[Task]
        G3[Cache]
    end

    A --> B
    B --> C
    C --> D
    D --> E1
    D --> F1
    D --> G1

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style E1 fill:#e8f5e9
    style F1 fill:#e8f5e9
    style G1 fill:#e8f5e9
</div>

<h3>Lazy Evaluationï¼ˆé…å»¶è©•ä¾¡ï¼‰</h3>

<p>Sparkã¯<strong>Transformation</strong>ï¼ˆå¤‰æ›ï¼‰ã¨<strong>Action</strong>ï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã‚’åŒºåˆ¥ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ã‚¿ã‚¤ãƒ—</th>
<th>èª¬æ˜</th>
<th>ä¾‹</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Transformation</strong></td>
<td>æ–°ã—ã„RDD/DataFrameã‚’è¿”ã™<br>é…å»¶è©•ä¾¡ï¼ˆè¨ˆç®—ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰</td>
<td><code>map()</code>, <code>filter()</code>, <code>groupBy()</code></td>
</tr>
<tr>
<td><strong>Action</strong></td>
<td>çµæœã‚’è¿”ã™/ä¿å­˜ã™ã‚‹<br>å³æ™‚è©•ä¾¡ï¼ˆå®Ÿéš›ã®è¨ˆç®—ã‚’å®Ÿè¡Œï¼‰</td>
<td><code>count()</code>, <code>collect()</code>, <code>save()</code></td>
</tr>
</tbody>
</table>

<h3>DAGå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«</h3>

<div class="mermaid">
graph LR
    A[ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿] --> B[filter]
    B --> C[map]
    C --> D[reduceByKey]
    D --> E[collect]

    style A fill:#e3f2fd
    style E fill:#ffebee
    style B fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5

    classDef transformation fill:#f3e5f5
    classDef action fill:#ffebee
</div>

<p><strong>Transformation</strong>ã¯å®Ÿè¡Œè¨ˆç”»ï¼ˆDAGï¼‰ã‚’æ§‹ç¯‰ã—ã€<strong>Action</strong>ãŒå‘¼ã°ã‚ŒãŸæ™‚ã«æœ€é©åŒ–ã•ã‚ŒãŸè¨ˆç®—ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚</p>

<h3>Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–</h3>

<pre><code class="language-python">from pyspark.sql import SparkSession

# Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
spark = SparkSession.builder \
    .appName("SparkMLExample") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

print(f"Spark Version: {spark.version}")
print(f"Spark Master: {spark.sparkContext.master}")
print(f"App Name: {spark.sparkContext.appName}")

# Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®šç¢ºèª
spark.sparkContext.getConf().getAll()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>Spark Version: 3.5.0
Spark Master: local[*]
App Name: SparkMLExample
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: <code>local[*]</code>ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§å…¨CPUã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§ã¯<code>yarn</code>ã‚„<code>k8s://</code>ã‚’æŒ‡å®šã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>2.2 RDDï¼ˆResilient Distributed Datasetsï¼‰</h2>

<h3>RDDã¨ã¯</h3>

<p><strong>RDDï¼ˆResilient Distributed Datasetï¼‰</strong>ã¯ã€Sparkã®åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æŠ½è±¡åŒ–ã§ã€ distributedã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸å¤‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚</p>

<h4>RDDã®3ã¤ã®ç‰¹æ€§</h4>

<ol>
<li><strong>Resilientï¼ˆè€éšœå®³æ€§ï¼‰</strong>: Lineageï¼ˆç³»è­œï¼‰ã«ã‚ˆã‚Šã€éšœå®³æ™‚ã«è‡ªå‹•å¾©æ—§</li>
<li><strong>Distributedï¼ˆ distributedï¼‰</strong>: ãƒ‡ãƒ¼ã‚¿ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã« distributed</li>
<li><strong>Datasetï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰</strong>: ãƒ¡ãƒ¢ãƒªå†…ã®ä¸å¤‰ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³</li>
</ol>

<h3>RDDã®åŸºæœ¬æ“ä½œ</h3>

<h4>RDDä½œæˆ</h4>

<pre><code class="language-python">from pyspark import SparkContext

# SparkContextã®å–å¾—ï¼ˆSparkSessionã‹ã‚‰ï¼‰
sc = spark.sparkContext

# æ–¹æ³•1: Pythonãƒªã‚¹ãƒˆã‹ã‚‰ä½œæˆ
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(data, numSlices=4)  # 4ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã« minuteså‰²

print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {rdd.getNumPartitions()}")
print(f"æœ€åˆã®5è¦ç´ : {rdd.take(5)}")

# æ–¹æ³•2: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½œæˆ
# text_rdd = sc.textFile("hdfs://path/to/file.txt")

# æ–¹æ³•3: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½œæˆ
# multi_rdd = sc.wholeTextFiles("hdfs://path/to/directory/")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: 4
æœ€åˆã®5è¦ç´ : [1, 2, 3, 4, 5]
</code></pre>

<h3>Transformationsï¼ˆå¤‰æ›ï¼‰</h3>

<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿æº–å‚™
numbers = sc.parallelize(range(1, 11))

# map: å„è¦ç´ ã«é–¢æ•°ã‚’é©ç”¨
squares = numbers.map(lambda x: x ** 2)
print(f"äºŒä¹—: {squares.collect()}")

# filter: æ¡ä»¶ã«åˆã†è¦ç´ ã®ã¿æŠ½å‡º
evens = numbers.filter(lambda x: x % 2 == 0)
print(f"å¶æ•°: {evens.collect()}")

# flatMap: å„è¦ç´ ã‚’è¤‡æ•°è¦ç´ ã«å±•é–‹
words = sc.parallelize(["Hello World", "Apache Spark"])
all_words = words.flatMap(lambda line: line.split(" "))
print(f"å˜èª: {all_words.collect()}")

# union: 2ã¤ã®RDDã‚’çµåˆ
rdd1 = sc.parallelize([1, 2, 3])
rdd2 = sc.parallelize([4, 5, 6])
combined = rdd1.union(rdd2)
print(f"çµåˆ: {combined.collect()}")

# distinct: é‡è¤‡ã‚’å‰Šé™¤
duplicates = sc.parallelize([1, 2, 2, 3, 3, 3, 4])
unique = duplicates.distinct()
print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯: {unique.collect()}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>äºŒä¹—: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
å¶æ•°: [2, 4, 6, 8, 10]
å˜èª: ['Hello', 'World', 'Apache', 'Spark']
çµåˆ: [1, 2, 3, 4, 5, 6]
ãƒ¦ãƒ‹ãƒ¼ã‚¯: [1, 2, 3, 4]
</code></pre>

<h3>Key-Value RDDæ“ä½œ</h3>

<pre><code class="language-python"># ãƒšã‚¢RDDã®ä½œæˆ
pairs = sc.parallelize([("apple", 3), ("banana", 2), ("apple", 5), ("orange", 1)])

# reduceByKey: ã‚­ãƒ¼ã”ã¨ã«å€¤ã‚’é›†ç´„
total_by_key = pairs.reduceByKey(lambda a, b: a + b)
print(f"ã‚­ãƒ¼åˆ¥åˆè¨ˆ: {total_by_key.collect()}")

# groupByKey: ã‚­ãƒ¼ã”ã¨ã«å€¤ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
grouped = pairs.groupByKey()
print(f"ã‚°ãƒ«ãƒ¼ãƒ—åŒ–: {[(k, list(v)) for k, v in grouped.collect()]}")

# mapValues: å€¤ã®ã¿ã«é–¢æ•°ã‚’é©ç”¨
doubled_values = pairs.mapValues(lambda x: x * 2)
print(f"å€¤ã‚’2å€: {doubled_values.collect()}")

# sortByKey: ã‚­ãƒ¼ã§ã‚½ãƒ¼ãƒˆ
sorted_pairs = pairs.sortByKey()
print(f"ã‚½ãƒ¼ãƒˆ: {sorted_pairs.collect()}")

# join: 2ã¤ã®ãƒšã‚¢RDDã‚’çµåˆ
prices = sc.parallelize([("apple", 100), ("banana", 80), ("orange", 60)])
joined = pairs.join(prices)
print(f"çµåˆ: {joined.collect()}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ã‚­ãƒ¼åˆ¥åˆè¨ˆ: [('apple', 8), ('banana', 2), ('orange', 1)]
ã‚°ãƒ«ãƒ¼ãƒ—åŒ–: [('apple', [3, 5]), ('banana', [2]), ('orange', [1])]
å€¤ã‚’2å€: [('apple', 6), ('banana', 4), ('apple', 10), ('orange', 2)]
ã‚½ãƒ¼ãƒˆ: [('apple', 3), ('apple', 5), ('banana', 2), ('orange', 1)]
çµåˆ: [('apple', (3, 100)), ('apple', (5, 100)), ('banana', (2, 80)), ('orange', (1, 60))]
</code></pre>

<h3>Actionsï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰</h3>

<pre><code class="language-python">numbers = sc.parallelize(range(1, 11))

# count: è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
print(f"è¦ç´ æ•°: {numbers.count()}")

# collect: ã™ã¹ã¦ã®è¦ç´ ã‚’å–å¾—ï¼ˆæ³¨æ„: ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ã‚µã‚¤ã‚ºã®ã¿ï¼‰
print(f"å…¨è¦ç´ : {numbers.collect()}")

# take: æœ€åˆã®nè¦ç´ ã‚’å–å¾—
print(f"æœ€åˆã®3è¦ç´ : {numbers.take(3)}")

# first: æœ€åˆã®è¦ç´ ã‚’å–å¾—
print(f"æœ€åˆã®è¦ç´ : {numbers.first()}")

# reduce: å…¨è¦ç´ ã‚’é›†ç´„
sum_all = numbers.reduce(lambda a, b: a + b)
print(f"åˆè¨ˆ: {sum_all}")

# foreach: å„è¦ç´ ã«å‰¯ä½œç”¨ã®ã‚ã‚‹å‡¦ç†ã‚’å®Ÿè¡Œ
numbers.foreach(lambda x: print(f"å‡¦ç†ä¸­: {x}"))

# saveAsTextFile: ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
# numbers.saveAsTextFile("output/numbers")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>è¦ç´ æ•°: 10
å…¨è¦ç´ : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
æœ€åˆã®3è¦ç´ : [1, 2, 3]
æœ€åˆã®è¦ç´ : 1
åˆè¨ˆ: 55
</code></pre>

<h3>Lineageï¼ˆç³»è­œï¼‰ã¨è€éšœå®³æ€§</h3>

<pre><code class="language-python"># RDDã®ç³»è­œã‚’ç¢ºèª
numbers = sc.parallelize(range(1, 101))
squares = numbers.map(lambda x: x ** 2)
evens = squares.filter(lambda x: x % 2 == 0)

# ãƒ‡ãƒãƒƒã‚°æ–‡å­—åˆ—ã§ç³»è­œã‚’è¡¨ç¤º
print("RDDç³»è­œ:")
print(evens.toDebugString().decode('utf-8'))
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>RDDç³»è­œ:
(4) PythonRDD[10] at RDD at PythonRDD.scala:53 []
 |  MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:145 []
 |  MapPartitionsRDD[8] at mapPartitions at PythonRDD.scala:145 []
 |  ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:195 []
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: Sparkã¯ç³»è­œã‚’è¨˜éŒ²ã—ã¦ãŠã‚Šã€ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã«ã¯ã“ã®ç³»è­œã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>2.3 Spark DataFrame ã¨ SQL</h2>

<h3>DataFrameã¨ã¯</h3>

<p><strong>DataFrame</strong>ã¯ã€åå‰ä»˜ãåˆ—ã‚’æŒã¤ distributedãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€RDDã‚ˆã‚Šé«˜é€Ÿã§ä½¿ã„ã‚„ã™ã„APIã§ã™ã€‚</p>

<h4>DataFrameã®åˆ©ç‚¹</h4>

<ul>
<li><strong>Catalyst Optimizer</strong>: ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€ŸåŒ–</li>
<li><strong>Tungstenå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³</strong>: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®å‘ä¸Š</li>
<li><strong>ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±</strong>: å‹å®‰å…¨æ€§ã¨æœ€é©åŒ–</li>
<li><strong>SQLäº’æ›æ€§</strong>: SQLã‚¯ã‚¨ãƒªãŒä½¿ãˆã‚‹</li>
</ul>

<h3>DataFrameä½œæˆ</h3>

<pre><code class="language-python">from pyspark.sql import Row
import pandas as pd

# æ–¹æ³•1: Pythonãƒªã‚¹ãƒˆã‹ã‚‰ä½œæˆ
data = [
    ("Alice", 25, "Engineer"),
    ("Bob", 30, "Data Scientist"),
    ("Charlie", 35, "Manager"),
    ("Diana", 28, "Analyst")
]
columns = ["name", "age", "job"]
df = spark.createDataFrame(data, columns)

# ãƒ‡ãƒ¼ã‚¿ç¢ºèª
df.show()
df.printSchema()

# æ–¹æ³•2: Rowã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ä½œæˆ
rows = [
    Row(name="Eve", age=32, job="Developer"),
    Row(name="Frank", age=29, job="Designer")
]
df2 = spark.createDataFrame(rows)

# æ–¹æ³•3: Pandas DataFrameã‹ã‚‰ä½œæˆ
pandas_df = pd.DataFrame({
    'name': ['Grace', 'Henry'],
    'age': [27, 31],
    'job': ['Researcher', 'Architect']
})
df3 = spark.createDataFrame(pandas_df)

# æ–¹æ³•4: CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
# df_csv = spark.read.csv("data.csv", header=True, inferSchema=True)
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>+-------+---+---------------+
|   name|age|            job|
+-------+---+---------------+
|  Alice| 25|       Engineer|
|    Bob| 30|Data Scientist|
|Charlie| 35|        Manager|
|  Diana| 28|        Analyst|
+-------+---+---------------+

root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
 |-- job: string (nullable = true)
</code></pre>

<h3>DataFrameæ“ä½œ</h3>

<h4>é¸æŠã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</h4>

<pre><code class="language-python"># åˆ—ã®é¸æŠ
df.select("name", "age").show()

# æ¡ä»¶ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
df.filter(df.age > 28).show()

# whereï¼ˆfilterã®åˆ¥åï¼‰
df.where(df.job == "Engineer").show()

# è¤‡æ•°æ¡ä»¶
df.filter((df.age > 25) & (df.age < 32)).show()

# æ–°ã—ã„åˆ—ã®è¿½åŠ 
from pyspark.sql.functions import col, lit

df_with_salary = df.withColumn("salary", col("age") * 1000)
df_with_salary.show()

# åˆ—åã®å¤‰æ›´
df_renamed = df.withColumnRenamed("job", "position")
df_renamed.show()

# åˆ—ã®å‰Šé™¤
df_dropped = df.drop("job")
df_dropped.show()
</code></pre>

<h3>é›†ç´„ã¨ã‚°ãƒ«ãƒ¼ãƒ—åŒ–</h3>

<pre><code class="language-python">from pyspark.sql.functions import avg, count, max, min, sum

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
sales_data = [
    ("Alice", "2024-01", 100),
    ("Alice", "2024-02", 150),
    ("Bob", "2024-01", 200),
    ("Bob", "2024-02", 180),
    ("Charlie", "2024-01", 120),
    ("Charlie", "2024-02", 140)
]
sales_df = spark.createDataFrame(sales_data, ["name", "month", "sales"])

# ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¨é›†ç´„
sales_summary = sales_df.groupBy("name").agg(
    sum("sales").alias("total_sales"),
    avg("sales").alias("avg_sales"),
    count("sales").alias("num_months")
)
sales_summary.show()

# è¤‡æ•°åˆ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
monthly_stats = sales_df.groupBy("name", "month").agg(
    max("sales").alias("max_sales"),
    min("sales").alias("min_sales")
)
monthly_stats.show()

# ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
pivot_df = sales_df.groupBy("name").pivot("month").sum("sales")
pivot_df.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>+-------+-----------+---------+----------+
|   name|total_sales|avg_sales|num_months|
+-------+-----------+---------+----------+
|  Alice|        250|    125.0|         2|
|    Bob|        380|    190.0|         2|
|Charlie|        260|    130.0|         2|
+-------+-----------+---------+----------+
</code></pre>

<h3>Spark SQLã®åˆ©ç”¨</h3>

<pre><code class="language-python"># DataFrameã‚’ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ“ãƒ¥ãƒ¼ã¨ã—ã¦ç™»éŒ²
df.createOrReplaceTempView("employees")

# SQLã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ
sql_result = spark.sql("""
    SELECT
        job,
        COUNT(*) as num_employees,
        AVG(age) as avg_age,
        MAX(age) as max_age,
        MIN(age) as min_age
    FROM employees
    GROUP BY job
    ORDER BY avg_age DESC
""")

sql_result.show()

# è¤‡é›‘ãªSQLã‚¯ã‚¨ãƒª
advanced_query = spark.sql("""
    SELECT
        name,
        age,
        job,
        CASE
            WHEN age < 28 THEN 'Junior'
            WHEN age >= 28 AND age < 32 THEN 'Mid-level'
            ELSE 'Senior'
        END as level
    FROM employees
    WHERE age > 25
    ORDER BY age
""")

advanced_query.show()
</code></pre>

<h3>çµåˆæ“ä½œ</h3>

<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿æº–å‚™
employees = spark.createDataFrame([
    (1, "Alice", "Engineering"),
    (2, "Bob", "Data Science"),
    (3, "Charlie", "Management")
], ["id", "name", "department"])

salaries = spark.createDataFrame([
    (1, 80000),
    (2, 95000),
    (4, 70000)  # id=4ã¯ç¤¾å“¡ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã—ãªã„
], ["id", "salary"])

# Inner Joinï¼ˆå†…éƒ¨çµåˆï¼‰
inner_join = employees.join(salaries, "id", "inner")
print("Inner Join:")
inner_join.show()

# Left Outer Joinï¼ˆå·¦å¤–éƒ¨çµåˆï¼‰
left_join = employees.join(salaries, "id", "left")
print("Left Outer Join:")
left_join.show()

# Right Outer Joinï¼ˆå³å¤–éƒ¨çµåˆï¼‰
right_join = employees.join(salaries, "id", "right")
print("Right Outer Join:")
right_join.show()

# Full Outer Joinï¼ˆå®Œå…¨å¤–éƒ¨çµåˆï¼‰
full_join = employees.join(salaries, "id", "outer")
print("Full Outer Join:")
full_join.show()
</code></pre>

<p><strong>å‡ºåŠ›ï¼ˆInner Joinï¼‰</strong>ï¼š</p>
<pre><code>+---+-----+-------------+------+
| id| name|   department|salary|
+---+-----+-------------+------+
|  1|Alice|  Engineering| 80000|
|  2|  Bob| Data Science| 95000|
+---+-----+-------------+------+
</code></pre>

<h3>Catalyst Optimizerã®åŠ¹æœ</h3>

<pre><code class="language-python"># ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œè¨ˆç”»ã‚’ç¢ºèª
df_filtered = df.filter(df.age > 25).select("name", "age")

# ç‰©ç†å®Ÿè¡Œè¨ˆç”»
print("Physical Plan:")
df_filtered.explain(mode="formatted")

# æœ€é©åŒ–å‰ã®è«–ç†ãƒ—ãƒ©ãƒ³
print("\nLogical Plan:")
df_filtered.explain(mode="extended")
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: Catalystã¯è¿°èªãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã€ã‚«ãƒ©ãƒ ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã€å®šæ•°ç•³ã¿è¾¼ã¿ãªã©ã®æœ€é©åŒ–ã‚’è‡ªå‹•çš„ã«é©ç”¨ã—ã¾ã™ã€‚</p>
</blockquote>

<hr>

<h2>2.4 Spark MLlibï¼ˆMachine Learningï¼‰</h2>

<h3>MLlibã¨ã¯</h3>

<p><strong>Spark MLlib</strong>ã¯ã€Sparkã® distributedMachine Learningãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å­¦ç¿’ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã—ã¾ã™ã€‚</p>

<h4>MLlibã®ä¸»è¦æ©Ÿèƒ½</h4>

<ul>
<li><strong> classification</strong>: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€æ±ºå®šæœ¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€GBT</li>
<li><strong>å›å¸°</strong>: ç·šå½¢å›å¸°ã€å›å¸°æœ¨ã€ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«</li>
<li><strong>ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</strong>: K-Meansã€GMMã€LDA</li>
<li><strong>å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</strong>: ALSï¼ˆäº¤äº’æœ€å°äºŒä¹—æ³•ï¼‰</li>
<li><strong>æ¬¡å…ƒå‰Šæ¸›</strong>: PCAã€SVD</li>
<li><strong>ç‰¹å¾´é‡å¤‰æ›</strong>: VectorAssemblerã€StringIndexerã€OneHotEncoder</li>
</ul>

<h3>ML Pipelineã®åŸºæœ¬</h3>

<div class="mermaid">
graph LR
    A[ç”Ÿãƒ‡ãƒ¼ã‚¿] --> B[StringIndexer]
    B --> C[VectorAssembler]
    C --> D[StandardScaler]
    D --> E[ classificationå™¨]
    E --> F[äºˆæ¸¬çµæœ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
</div>

<h3> classificationã‚¿ã‚¹ã‚¯ã®å®Ÿè£…</h3>

<pre><code class="language-python">from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
from pyspark.sql.functions import rand, when

# Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ˆã†ãªæ§‹é€ ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
data = spark.range(0, 1000).select(
    (rand() * 3 + 4).alias("sepal_length"),
    (rand() * 2 + 2).alias("sepal_width"),
    (rand() * 3 + 1).alias("petal_length"),
    (rand() * 2 + 0.1).alias("petal_width")
)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
data = data.withColumn(
    "species",
    when((data.petal_length < 2), "setosa")
    .when((data.petal_length >= 2) & (data.petal_length < 4), "versicolor")
    .otherwise("virginica")
)

# ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
data.show(10)
data.groupBy("species").count().show()

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ minuteså‰²
train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_data.count()}è¡Œ")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_data.count()}è¡Œ")
</code></pre>

<h3>ç‰¹å¾´é‡å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h3>

<pre><code class="language-python"># ã‚¹ãƒ†ãƒ¼ã‚¸1: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
label_indexer = StringIndexer(
    inputCol="species",
    outputCol="label"
)

# ã‚¹ãƒ†ãƒ¼ã‚¸2: ç‰¹å¾´é‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«çµåˆ
feature_columns = ["sepal_length", "sepal_width", "petal_length", "petal_width"]
vector_assembler = VectorAssembler(
    inputCols=feature_columns,
    outputCol="features"
)

# ã‚¹ãƒ†ãƒ¼ã‚¸3: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
lr = LogisticRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=100,
    regParam=0.01
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
pipeline = Pipeline(stages=[label_indexer, vector_assembler, lr])

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
print("ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹...")
model = pipeline.fit(train_data)
print("è¨“ç·´å®Œäº†")

# äºˆæ¸¬
predictions = model.transform(test_data)

# äºˆæ¸¬çµæœã®ç¢ºèª
predictions.select("species", "label", "features", "prediction", "probability").show(10, truncate=False)
</code></pre>

<h3>ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡</h3>

<pre><code class="language-python"># å¤šã‚¯ãƒ©ã‚¹ classificationã®è©•ä¾¡
multi_evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction"
)

# ç²¾åº¦ï¼ˆAccuracyï¼‰
accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "accuracy"})
print(f"ç²¾åº¦: {accuracy:.4f}")

# F1ã‚¹ã‚³ã‚¢
f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "f1"})
print(f"F1ã‚¹ã‚³ã‚¢: {f1:.4f}")

# åŠ é‡ç²¾åº¦
weighted_precision = multi_evaluator.evaluate(
    predictions,
    {multi_evaluator.metricName: "weightedPrecision"}
)
print(f"åŠ é‡ç²¾åº¦: {weighted_precision:.4f}")

# åŠ é‡å†ç¾ç‡
weighted_recall = multi_evaluator.evaluate(
    predictions,
    {multi_evaluator.metricName: "weightedRecall"}
)
print(f"åŠ é‡å†ç¾ç‡: {weighted_recall:.4f}")

# æ··åŒè¡Œåˆ—ã®è¨ˆç®—
from pyspark.ml.evaluation import MulticlassMetrics
prediction_and_labels = predictions.select("prediction", "label").rdd
metrics = MulticlassMetrics(prediction_and_labels)

print("\næ··åŒè¡Œåˆ—:")
print(metrics.confusionMatrix().toArray())
</code></pre>

<h3>å›å¸°ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…</h3>

<pre><code class="language-python">from pyspark.ml.regression import LinearRegression, RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# å›å¸°ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
regression_data = spark.range(0, 1000).select(
    (rand() * 100).alias("feature1"),
    (rand() * 50).alias("feature2"),
    (rand() * 30).alias("feature3")
)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚ºï¼‰
from pyspark.sql.functions import col
regression_data = regression_data.withColumn(
    "target",
    col("feature1") * 2 + col("feature2") * 1.5 - col("feature3") * 0.5 + (rand() * 10 - 5)
)

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆ minuteså‰²
train_reg, test_reg = regression_data.randomSplit([0.8, 0.2], seed=42)

# ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆ
feature_cols = ["feature1", "feature2", "feature3"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
lr_regressor = LinearRegression(
    featuresCol="features",
    labelCol="target",
    maxIter=100,
    regParam=0.1,
    elasticNetParam=0.5  # L1/L2æ­£å‰‡åŒ–ã®æ··åˆæ¯”
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
regression_pipeline = Pipeline(stages=[assembler, lr_regressor])

# è¨“ç·´
regression_model = regression_pipeline.fit(train_reg)

# äºˆæ¸¬
regression_predictions = regression_model.transform(test_reg)

# è©•ä¾¡
reg_evaluator = RegressionEvaluator(
    labelCol="target",
    predictionCol="prediction"
)

rmse = reg_evaluator.evaluate(regression_predictions, {reg_evaluator.metricName: "rmse"})
mae = reg_evaluator.evaluate(regression_predictions, {reg_evaluator.metricName: "mae"})
r2 = reg_evaluator.evaluate(regression_predictions, {reg_evaluator.metricName: "r2"})

print(f"\n=== å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ ===")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ²: {r2:.4f}")

# ãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°
lr_model = regression_model.stages[-1]
print(f"\nä¿‚æ•°: {lr_model.coefficients}")
print(f"åˆ‡ç‰‡: {lr_model.intercept:.4f}")
</code></pre>

<h3>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹ classification</h3>

<pre><code class="language-python">from pyspark.ml.classification import RandomForestClassifier

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
rf = RandomForestClassifier(
    featuresCol="features",
    labelCol="label",
    numTrees=100,
    maxDepth=10,
    seed=42
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆç‰¹å¾´é‡å¤‰æ› + RFï¼‰
rf_pipeline = Pipeline(stages=[label_indexer, vector_assembler, rf])

# è¨“ç·´
print("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®è¨“ç·´é–‹å§‹...")
rf_model = rf_pipeline.fit(train_data)
print("è¨“ç·´å®Œäº†")

# äºˆæ¸¬
rf_predictions = rf_model.transform(test_data)

# è©•ä¾¡
rf_accuracy = multi_evaluator.evaluate(
    rf_predictions,
    {multi_evaluator.metricName: "accuracy"}
)
rf_f1 = multi_evaluator.evaluate(
    rf_predictions,
    {multi_evaluator.metricName: "f1"}
)

print(f"\n=== ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®è©•ä¾¡ ===")
print(f"ç²¾åº¦: {rf_accuracy:.4f}")
print(f"F1ã‚¹ã‚³ã‚¢: {rf_f1:.4f}")

# ç‰¹å¾´é‡ã®é‡è¦åº¦
rf_classifier = rf_model.stages[-1]
feature_importances = rf_classifier.featureImportances

print("\nç‰¹å¾´é‡ã®é‡è¦åº¦:")
for idx, importance in enumerate(feature_importances):
    print(f"{feature_columns[idx]}: {importance:.4f}")
</code></pre>

<h3>ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</h3>

<pre><code class="language-python">from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã®æ§‹ç¯‰
param_grid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.001, 0.01, 0.1]) \
    .addGrid(lr.maxIter, [50, 100, 150]) \
    .build()

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š
cv = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=param_grid,
    evaluator=multi_evaluator,
    numFolds=3,
    seed=42
)

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
print("ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")
cv_model = cv.fit(train_data)
print("å®Œäº†")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
cv_predictions = cv_model.transform(test_data)

# æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª
best_model = cv_model.bestModel
print("\næœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(best_model.stages[-1].extractParamMap())

# è©•ä¾¡
cv_accuracy = multi_evaluator.evaluate(
    cv_predictions,
    {multi_evaluator.metricName: "accuracy"}
)
print(f"\nCVå¾Œã®ç²¾åº¦: {cv_accuracy:.4f}")
</code></pre>

<hr>

<h2>2.5 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</h2>

<h3>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</h3>

<p>é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¯ã€Sparkã®æ€§èƒ½ã‚’å¤§ããå·¦å³ã—ã¾ã™ã€‚</p>

<h4>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®æ±ºå®š</h4>

<pre><code class="language-python"># ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {spark.sparkContext.defaultParallelism}")

# RDDã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ç¢ºèª
rdd = sc.parallelize(range(1000))
print(f"RDDãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {rdd.getNumPartitions()}")

# DataFrameã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ç¢ºèª
df = spark.range(10000)
print(f"DataFrameãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {df.rdd.getNumPartitions()}")

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®å†è¨­å®š
rdd_repartitioned = rdd.repartition(8)
print(f"å†ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¾Œ: {rdd_repartitioned.getNumPartitions()}")

# coalesce: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’æ¸›ã‚‰ã™ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰
rdd_coalesced = rdd.coalesce(4)
print(f"Coalesceå¾Œ: {rdd_coalesced.getNumPartitions()}")
</code></pre>

<blockquote>
<p><strong>Recommended</strong>: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¯ï¼ˆCPUã‚³ã‚¢æ•° Ã— 2ã€œ3ï¼‰ãŒç›®å®‰ã§ã™ã€‚</p>
</blockquote>

<h4>ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒŠãƒ¼</h4>

<pre><code class="language-python"># Key-Valueãƒšã‚¢ã§ã®ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
pairs = sc.parallelize([("A", 1), ("B", 2), ("A", 3), ("C", 4), ("B", 5)])

# ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
hash_partitioned = pairs.partitionBy(4)
print(f"ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {hash_partitioned.getNumPartitions()}")

# å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å†…å®¹ã‚’ç¢ºèª
def show_partition_contents(index, iterator):
    yield f"Partition {index}: {list(iterator)}"

partition_contents = hash_partitioned.mapPartitionsWithIndex(show_partition_contents)
for content in partition_contents.collect():
    print(content)
</code></pre>

<h3>ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¨æ°¸ç¶šåŒ–</h3>

<h4>ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°</h4>

<pre><code class="language-python"># DataFrameã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
df_large = spark.range(0, 10000000)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ¡ãƒ¢ãƒªã®ã¿ï¼‰
df_large.cache()

# åˆå›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒä½œæˆã•ã‚Œã‚‹
count1 = df_large.count()
print(f"åˆå›ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆï¼‰: {count1}")

# 2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆé«˜é€Ÿï¼‰
count2 = df_large.count()
print(f"2å›ç›®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰: {count2}")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è§£æ”¾
df_large.unpersist()
</code></pre>

<h4>æ°¸ç¶šåŒ–Levelã®é¸æŠ</h4>

<pre><code class="language-python">from pyspark import StorageLevel

# RDDã®æ°¸ç¶šåŒ–Level
rdd = sc.parallelize(range(1000000))

# ãƒ¡ãƒ¢ãƒªã¨ãƒ‡ã‚£ã‚¹ã‚¯ã®ä¸¡æ–¹ã‚’ä½¿ç”¨
rdd.persist(StorageLevel.MEMORY_AND_DISK)

# ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¦ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šï¼‰
rdd.persist(StorageLevel.MEMORY_ONLY_SER)

# ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè€éšœå®³æ€§å‘ä¸Šï¼‰
rdd.persist(StorageLevel.MEMORY_AND_DISK_2)

print(f"ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸Level: {rdd.getStorageLevel()}")
</code></pre>

<table>
<thead>
<tr>
<th>ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸Level</th>
<th>èª¬æ˜</th>
<th>ç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MEMORY_ONLY</code></td>
<td>ãƒ¡ãƒ¢ãƒªã®ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰</td>
<td>å minutesãªãƒ¡ãƒ¢ãƒªãŒã‚ã‚‹å ´åˆ</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK</code></td>
<td>ãƒ¡ãƒ¢ãƒª â†’ ãƒ‡ã‚£ã‚¹ã‚¯ã«ã‚¹ãƒ”ãƒ«</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_SER</code></td>
<td>ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¦ãƒ¡ãƒ¢ãƒªä¿å­˜</td>
<td>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–</td>
</tr>
<tr>
<td><code>DISK_ONLY</code></td>
<td>ãƒ‡ã‚£ã‚¹ã‚¯ã®ã¿</td>
<td>ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚</td>
</tr>
<tr>
<td><code>OFF_HEAP</code></td>
<td>ã‚ªãƒ•ãƒ’ãƒ¼ãƒ—ãƒ¡ãƒ¢ãƒª</td>
<td>GCå›é¿</td>
</tr>
</tbody>
</table>

<h3>ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°</h3>

<pre><code class="language-python"># å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¨ãƒãƒ¼ãƒ‰ã«é…å¸ƒ
lookup_table = {"A": 100, "B": 200, "C": 300, "D": 400}

# ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
broadcast_lookup = sc.broadcast(lookup_table)

# RDDã§ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°ã‚’ä½¿ç”¨
data = sc.parallelize([("A", 1), ("B", 2), ("C", 3), ("A", 4)])

def enrich_data(pair):
    key, value = pair
    # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°ã‚’å‚ç…§
    multiplier = broadcast_lookup.value.get(key, 1)
    return (key, value * multiplier)

enriched = data.map(enrich_data)
print(enriched.collect())

# ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°ã®è§£æ”¾
broadcast_lookup.unpersist()
</code></pre>

<blockquote>
<p><strong>é‡è¦</strong>: ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°ã¯çµåˆæ“ä½œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ï¼ˆç‰¹ã«å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®çµåˆï¼‰ã€‚</p>
</blockquote>

<h3>ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h3>

<h4>Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š</h4>

<pre><code class="language-python"># ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®è¨­å®š
spark_optimized = SparkSession.builder \
    .appName("OptimizedSparkApp") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "4") \
    .config("spark.default.parallelism", "100") \
    .config("spark.sql.shuffle.partitions", "100") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# è¨­å®šã®ç¢ºèª
for conf in spark_optimized.sparkContext.getConf().getAll():
    print(f"{conf[0]}: {conf[1]}")
</code></pre>

<h4>ä¸»è¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h4>

<table>
<thead>
<tr>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
<th>èª¬æ˜</th>
<th>Recommendedå€¤</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>spark.executor.memory</code></td>
<td>Executorã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º</td>
<td>åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã®70%</td>
</tr>
<tr>
<td><code>spark.executor.cores</code></td>
<td>Executorå½“ãŸã‚Šã®ã‚³ã‚¢æ•°</td>
<td>4-6ã‚³ã‚¢</td>
</tr>
<tr>
<td><code>spark.default.parallelism</code></td>
<td>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸¦åˆ—åº¦</td>
<td>ã‚³ã‚¢æ•° Ã— 2-3</td>
</tr>
<tr>
<td><code>spark.sql.shuffle.partitions</code></td>
<td>ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°</td>
<td>100-200ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºä¾å­˜ï¼‰</td>
</tr>
<tr>
<td><code>spark.sql.adaptive.enabled</code></td>
<td>é©å¿œçš„ã‚¯ã‚¨ãƒªå®Ÿè¡Œ</td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>spark.serializer</code></td>
<td>ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶</td>
<td><code>KryoSerializer</code></td>
</tr>
</tbody>
</table>

<h3>å®Ÿè¡Œè¨ˆç”»ã®æœ€é©åŒ–</h3>

<pre><code class="language-python"># DataFrameã®æœ€é©åŒ–ä¾‹
large_df = spark.range(0, 10000000)
small_df = spark.range(0, 100)

# æœ€é©åŒ–å‰: å¤§ãã„ãƒ†ãƒ¼ãƒ–ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿ â†’ çµåˆ
result_unoptimized = large_df.filter(large_df.id % 2 == 0).join(small_df, "id")

# æœ€é©åŒ–å¾Œ: çµåˆ â†’ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆè¿°èªãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ï¼‰
result_optimized = large_df.join(small_df, "id").filter(large_df.id % 2 == 0)

# å®Ÿè¡Œè¨ˆç”»ã®æ¯”è¼ƒ
print("æœ€é©åŒ–å‰:")
result_unoptimized.explain()

print("\næœ€é©åŒ–å¾Œ:")
result_optimized.explain()

# CatalystãŒè‡ªå‹•çš„ã«æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€å®Ÿéš›ã«ã¯ä¸¡æ–¹ã¨ã‚‚åŒã˜å®Ÿè¡Œè¨ˆç”»ã«ãªã‚‹
</code></pre>

<hr>

<h2>2.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>Sparkã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong></p>
<ul>
<li>Driver-Executor ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ distributedå‡¦ç†</li>
<li>Lazy Evaluationã¨DAGå®Ÿè¡Œ</li>
<li>ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆYARNã€Mesosã€K8sï¼‰</li>
<li>Transformationã¨Actionã®åŒºåˆ¥</li>
</ul></li>

<li><p><strong>RDDï¼ˆResilient Distributed Datasetsï¼‰</strong></p>
<ul>
<li>ä¸å¤‰ãƒ» distributedãƒ»è€éšœå®³æ€§ã®ã‚ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³</li>
<li>Lineageã«ã‚ˆã‚‹è‡ªå‹•å¾©æ—§</li>
<li>mapã€filterã€reduceByKeyãªã©ã®æ“ä½œ</li>
<li>Key-Value ãƒšã‚¢ã®å‡¦ç†</li>
</ul></li>

<li><p><strong>Spark DataFrame ã¨ SQL</strong></p>
<ul>
<li>Catalyst Optimizerã«ã‚ˆã‚‹é«˜é€ŸåŒ–</li>
<li>ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã«ã‚ˆã‚‹å‹å®‰å…¨æ€§</li>
<li>SQLã‚¯ã‚¨ãƒªã¨DataFrame APIã®çµ±åˆ</li>
<li>çµåˆã€é›†ç´„ã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®åŠ¹ç‡çš„ãªå‡¦ç†</li>
</ul></li>

<li><p><strong>Spark MLlib</strong></p>
<ul>
<li> distributedMachine Learningã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</li>
<li>ç‰¹å¾´é‡å¤‰æ›ã¨å‰å‡¦ç†</li>
<li> classificationã€å›å¸°ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°</li>
<li>ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´</li>
</ul></li>

<li><p><strong>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–</strong></p>
<ul>
<li>é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</li>
<li>ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¨æ°¸ç¶šåŒ–Level</li>
<li>ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¤‰æ•°ã«ã‚ˆã‚‹çµåˆæœ€é©åŒ–</li>
<li>ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š</li>
</ul></li>
</ol>

<h3>Sparkæ´»ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>Recommendedäº‹é …</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>APIé¸æŠ</strong></td>
<td>DataFrame/Dataset > RDDï¼ˆæœ€é©åŒ–ã®æ©æµï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³</strong></td>
<td>é©åˆ‡ãªæ•°ï¼ˆã‚³ã‚¢æ•° Ã— 2-3ï¼‰ã€å‡ç­‰ãª distributed</td>
</tr>
<tr>
<td><strong>ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°</strong></td>
<td>å†åˆ©ç”¨ã™ã‚‹ä¸­é–“çµæœã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥</td>
</tr>
<tr>
<td><strong>ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›</strong></td>
<td>ä¸è¦ãªgroupByKeyå›é¿ã€reduceByKeyä½¿ç”¨</td>
</tr>
<tr>
<td><strong>ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ</strong></td>
<td>å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®çµåˆã«æ´»ç”¨</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªç®¡ç†</strong></td>
<td>Executor ãƒ¡ãƒ¢ãƒªã®é©åˆ‡ãªè¨­å®š</td>
</tr>
</tbody>
</table>

<h3>Next Chapterã¸</h3>

<p>Chapter 3 Chapterã§ã¯ã€<strong> distributedæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Horovod ã«ã‚ˆã‚‹ distributedå­¦ç¿’</li>
<li>TensorFlow ã¨ PyTorch ã® distributedæˆ¦ç•¥</li>
<li>Ray ã«ã‚ˆã‚‹è¶…ä¸¦åˆ—å‡¦ç†</li>
<li>MLflow ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†</li>
<li> distributedãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>Sparkã«ãŠã‘ã‚‹Transformationã¨Actionã®é•ã„ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã®ä¾‹ã‚’3ã¤ãšã¤æŒ™ã’ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Transformationï¼ˆå¤‰æ›ï¼‰</strong>ï¼š</p>
<ul>
<li>å®šç¾©: æ–°ã—ã„RDD/DataFrameã‚’è¿”ã™æ“ä½œã§ã€é…å»¶è©•ä¾¡ã•ã‚Œã‚‹</li>
<li>ç‰¹å¾´: å®Ÿéš›ã®è¨ˆç®—ã¯å®Ÿè¡Œã•ã‚Œãšã€å®Ÿè¡Œè¨ˆç”»ï¼ˆDAGï¼‰ãŒæ§‹ç¯‰ã•ã‚Œã‚‹</li>
<li>ä¾‹:
<ol>
<li><code>map()</code> - å„è¦ç´ ã«é–¢æ•°ã‚’é©ç”¨</li>
<li><code>filter()</code> - æ¡ä»¶ã«åˆã†è¦ç´ ã®ã¿æŠ½å‡º</li>
<li><code>groupBy()</code> - ã‚­ãƒ¼ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–</li>
</ol></li>
</ul>

<p><strong>Actionï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰</strong>ï¼š</p>
<ul>
<li>å®šç¾©: çµæœã‚’è¿”ã™/ä¿å­˜ã™ã‚‹æ“ä½œã§ã€å³æ™‚è©•ä¾¡ã•ã‚Œã‚‹</li>
<li>ç‰¹å¾´: å®Ÿéš›ã®è¨ˆç®—ãŒå®Ÿè¡Œã•ã‚Œã€Driverã¾ãŸã¯ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ãƒ‡ãƒ¼ã‚¿ãŒè¿”ã•ã‚Œã‚‹</li>
<li>ä¾‹:
<ol>
<li><code>count()</code> - è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ</li>
<li><code>collect()</code> - ã™ã¹ã¦ã®è¦ç´ ã‚’å–å¾—</li>
<li><code>saveAsTextFile()</code> - ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜</li>
</ol></li>
</ul>

<p><strong>é•ã„ã®é‡è¦æ€§</strong>ï¼š</p>
<p>Transformationã¯DAGã‚’æ§‹ç¯‰ã™ã‚‹ã ã‘ãªã®ã§é«˜é€Ÿã§ã™ã€‚ActionãŒå‘¼ã°ã‚ŒãŸæ™‚ã«ã€Sparkã¯æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè¡Œè¨ˆç”»ã§è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä½•ãŒå•é¡Œã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã¾ãŸã€ã©ã®ã‚ˆã†ã«ä¿®æ­£ã™ã¹ãã§ã™ã‹ï¼Ÿ</p>

<pre><code class="language-python">rdd = sc.parallelize(range(1, 1000000))
result = rdd.map(lambda x: x ** 2).collect()
print(result)
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å•é¡Œç‚¹</strong>ï¼š</p>
<ol>
<li><strong>ãƒ¡ãƒ¢ãƒªä¸è¶³</strong>: <code>collect()</code>ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Driverãƒ¡ãƒ¢ãƒªã«é›†ã‚ã‚‹ãŸã‚ã€100ä¸‡è¦ç´ ã ã¨ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹å¯èƒ½æ€§</li>
<li><strong>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹</strong>:  distributedå‡¦ç†ã®åˆ©ç‚¹ãŒå¤±ã‚ã‚Œã‚‹</li>
<li><strong>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è² è·</strong>: å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’Executorã‹ã‚‰Driverã«è»¢é€</li>
</ol>

<p><strong>ä¿®æ­£æ–¹æ³•</strong>ï¼š</p>

<pre><code class="language-python"># æ–¹æ³•1: å¿…è¦ãªè¦ç´ ã®ã¿å–å¾—
rdd = sc.parallelize(range(1, 1000000))
result = rdd.map(lambda x: x ** 2).take(10)  # æœ€åˆã®10è¦ç´ ã®ã¿
print(result)

# æ–¹æ³•2: ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
rdd.map(lambda x: x ** 2).saveAsTextFile("output/squares")

# æ–¹æ³•3: é›†ç´„æ“ä½œã‚’ä½¿ç”¨
total = rdd.map(lambda x: x ** 2).sum()
print(f"åˆè¨ˆ: {total}")

# æ–¹æ³•4: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
sample = rdd.map(lambda x: x ** 2).sample(False, 0.01).collect()
print(f"ã‚µãƒ³ãƒ—ãƒ«: {sample[:10]}")
</code></pre>

<p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>ï¼š</p>
<ul>
<li><code>collect()</code>ã¯å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ•°åƒè¡Œä»¥ä¸‹ï¼‰ã®ã¿ã«ä½¿ç”¨</li>
<li>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¯<code>take(n)</code>ã€<code>sample()</code>ã€<code>saveAsTextFile()</code>ã‚’ä½¿ç”¨</li>
</ul>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>Spark DataFrameã§ä»¥ä¸‹ã®SQLã‚¯ã‚¨ãƒªã¨åŒç­‰ã®å‡¦ç†ã‚’DataFrame APIã§å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-sql">SELECT
    department,
    AVG(salary) as avg_salary,
    MAX(salary) as max_salary,
    COUNT(*) as num_employees
FROM employees
WHERE age > 25
GROUP BY department
HAVING COUNT(*) > 5
ORDER BY avg_salary DESC
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from pyspark.sql.functions import avg, max, count, col

# DataFrame APIç‰ˆ
result = employees \
    .filter(col("age") > 25) \
    .groupBy("department") \
    .agg(
        avg("salary").alias("avg_salary"),
        max("salary").alias("max_salary"),
        count("*").alias("num_employees")
    ) \
    .filter(col("num_employees") > 5) \
    .orderBy(col("avg_salary").desc())

result.show()

# åˆ¥ã®æ›¸ãæ–¹ï¼ˆãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³ï¼‰
result_alt = (employees
    .where("age > 25")
    .groupBy("department")
    .agg(
        {"salary": "avg", "salary": "max", "*": "count"}
    )
    .withColumnRenamed("avg(salary)", "avg_salary")
    .withColumnRenamed("max(salary)", "max_salary")
    .withColumnRenamed("count(1)", "num_employees")
    .filter("num_employees > 5")
    .sort(col("avg_salary").desc())
)
</code></pre>

<p><strong>èª¬æ˜</strong>ï¼š</p>
<ul>
<li><code>filter()</code> / <code>where()</code>: WHEREå¥</li>
<li><code>groupBy()</code>: GROUP BYå¥</li>
<li><code>agg()</code>: é›†ç´„é–¢æ•°ï¼ˆAVGã€MAXã€COUNTï¼‰</li>
<li><code>filter()</code>ï¼ˆ2å›ç›®ï¼‰: HAVINGå¥</li>
<li><code>orderBy()</code> / <code>sort()</code>: ORDER BYå¥</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ1å„„è¡Œï¼‰ã§Key-Valueãƒšã‚¢ã®çµåˆã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ã€ä»¥ä¸‹ã®3ã¤ã®ã‚·ãƒŠãƒªã‚ªã§èª¬æ˜ã—ã¦ãã ã•ã„ï¼š</p>
<ol>
<li>ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã„å ´åˆ</li>
<li>ç‰‡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã„ï¼ˆãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ï¼‰å ´åˆ</li>
<li>ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®å ´åˆ</li>
</ol>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<h4>ã‚·ãƒŠãƒªã‚ª1: ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã„å ´åˆ</h4>

<pre><code class="language-python"># æ¨™æº–çš„ãªjoinï¼ˆã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸çµåˆã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥çµåˆï¼‰
large_df1 = spark.read.parquet("large_dataset1.parquet")
large_df2 = spark.read.parquet("large_dataset2.parquet")

# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’æœ€é©åŒ–
large_df1 = large_df1.repartition(200, "join_key")
large_df2 = large_df2.repartition(200, "join_key")

# çµåˆ
result = large_df1.join(large_df2, "join_key", "inner")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå†åˆ©ç”¨ã™ã‚‹å ´åˆï¼‰
result.cache()
result.count()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Ÿä½“åŒ–
</code></pre>

<p><strong>æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li>é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´ï¼‰</li>
<li>çµåˆã‚­ãƒ¼ã§äº‹å‰ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–</li>
<li>é©å¿œçš„ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆAQEï¼‰ã‚’æœ‰åŠ¹åŒ–</li>
</ul>

<h4>ã‚·ãƒŠãƒªã‚ª2: ç‰‡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã„å ´åˆ</h4>

<pre><code class="language-python">from pyspark.sql.functions import broadcast

large_df = spark.read.parquet("large_dataset.parquet")
small_df = spark.read.parquet("small_dataset.parquet")

# ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆçµåˆï¼ˆå°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…¨ãƒãƒ¼ãƒ‰ã«é…å¸ƒï¼‰
result = large_df.join(broadcast(small_df), "join_key", "inner")

# ã¾ãŸã¯ã€è‡ªå‹•ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆé–¾å€¤ã‚’è¨­å®š
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10485760)  # 10MB
</code></pre>

<p><strong>æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li>å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ<10MBï¼‰ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ</li>
<li>ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãŒä¸è¦ã«ãªã‚Šã€å¤§å¹…ãªé«˜é€ŸåŒ–</li>
<li>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æ³¨æ„ï¼ˆå…¨Executorã«é…å¸ƒã•ã‚Œã‚‹ï¼‰</li>
</ul>

<h4>ã‚·ãƒŠãƒªã‚ª3: ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®å ´åˆ</h4>

<pre><code class="language-python"># ãƒ‡ãƒ¼ã‚¿ãŒçµåˆã‚­ãƒ¼ã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ»ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–æ¸ˆã¿ã®å ´åˆ
sorted_df1 = spark.read.parquet("sorted_dataset1.parquet")
sorted_df2 = spark.read.parquet("sorted_dataset2.parquet")

# ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸çµåˆã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨
result = sorted_df1.join(
    sorted_df2,
    sorted_df1["join_key"] == sorted_df2["join_key"],
    "inner"
)

# ãƒ’ãƒ³ãƒˆã‚’ä½¿ã£ã¦ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸çµåˆã‚’å¼·åˆ¶
from pyspark.sql.functions import expr
result = sorted_df1.hint("merge").join(sorted_df2, "join_key")
</code></pre>

<p><strong>æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š</p>
<ul>
<li>æ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãªã‚‰ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãŒå‰Šæ¸›ã•ã‚Œã‚‹</li>
<li>ãƒã‚±ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆBucketingï¼‰ã‚’ä½¿ã£ã¦äº‹å‰ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–</li>
<li>Parquetå½¢å¼ã§ä¿å­˜æ™‚ã«ã‚½ãƒ¼ãƒˆé †ã‚’ç¶­æŒ</li>
</ul>

<h4>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ</h4>

<table>
<thead>
<tr>
<th>ã‚·ãƒŠãƒªã‚ª</th>
<th>çµåˆã‚¿ã‚¤ãƒ—</th>
<th>ã‚·ãƒ£ãƒƒãƒ•ãƒ«</th>
<th>é€Ÿåº¦</th>
</tr>
</thead>
<tbody>
<tr>
<td>ä¸¡æ–¹å¤§ãã„</td>
<td>ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸/ãƒãƒƒã‚·ãƒ¥</td>
<td>ã‚ã‚Š</td>
<td>ä¸­</td>
</tr>
<tr>
<td>ç‰‡æ–¹å°ã•ã„</td>
<td>ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ</td>
<td>ãªã—</td>
<td>é€Ÿã„</td>
</tr>
<tr>
<td>ã‚½ãƒ¼ãƒˆæ¸ˆã¿</td>
<td>ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸</td>
<td>éƒ¨ minutesçš„</td>
<td>é€Ÿã„</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Spark MLlibã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆ classificationã‚¿ã‚¹ã‚¯ï¼ˆã‚¹ãƒ‘ãƒ æ¤œå‡ºï¼‰ã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š</p>
<ul>
<li>ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ï¼‰</li>
<li>TF-IDFç‰¹å¾´é‡ã®ä½œæˆ</li>
<li>ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´</li>
<li>ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹è©•ä¾¡</li>
</ul>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
data = spark.createDataFrame([
    (0, "Free money now click here"),
    (0, "Congratulations you won a prize"),
    (1, "Meeting scheduled for tomorrow"),
    (1, "Please review the attached document"),
    (0, "Claim your free gift today"),
    (1, "Project update for next week"),
    (0, "Urgent account verification required"),
    (1, "Thanks for your help yesterday"),
    (0, "You have been selected winner"),
    (1, "Let's discuss the proposal")
] * 100, ["label", "text"])  # ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™

print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {data.count()}")
data.show(5)

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆ minuteså‰²
train, test = data.randomSplit([0.8, 0.2], seed=42)

# ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
tokenizer = Tokenizer(inputCol="text", outputCol="words")

# ã‚¹ãƒ†ãƒ¼ã‚¸2: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")

# ã‚¹ãƒ†ãƒ¼ã‚¸3: ãƒãƒƒã‚·ãƒ³ã‚° TF
hashingTF = HashingTF(
    inputCol="filtered_words",
    outputCol="raw_features",
    numFeatures=1000
)

# ã‚¹ãƒ†ãƒ¼ã‚¸4: IDF
idf = IDF(inputCol="raw_features", outputCol="features")

# ã‚¹ãƒ†ãƒ¼ã‚¸5: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
lr = LogisticRegression(maxIter=100, regParam=0.01)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
paramGrid = ParamGridBuilder() \
    .addGrid(hashingTF.numFeatures, [500, 1000, 2000]) \
    .addGrid(lr.regParam, [0.001, 0.01, 0.1]) \
    .addGrid(lr.maxIter, [50, 100]) \
    .build()

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
cv = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=BinaryClassificationEvaluator(),
    numFolds=3,
    seed=42
)

# è¨“ç·´
print("\nã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")
cv_model = cv.fit(train)
print("è¨“ç·´å®Œäº†")

# äºˆæ¸¬
predictions = cv_model.transform(test)

# äºˆæ¸¬çµæœã®ç¢ºèª
predictions.select("text", "label", "prediction", "probability").show(10, truncate=False)

# è©•ä¾¡
binary_evaluator = BinaryClassificationEvaluator()
multi_evaluator = MulticlassClassificationEvaluator()

auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: "areaUnderROC"})
accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "accuracy"})
f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "f1"})

print("\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")
print(f"AUC: {auc:.4f}")
print(f"ç²¾åº¦: {accuracy:.4f}")
print(f"F1ã‚¹ã‚³ã‚¢: {f1:.4f}")

# æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
best_model = cv_model.bestModel
print("\næœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"numFeatures: {best_model.stages[2].getNumFeatures()}")
print(f"regParam: {best_model.stages[-1].getRegParam()}")
print(f"maxIter: {best_model.stages[-1].getMaxIter()}")

# æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã§äºˆæ¸¬
new_data = spark.createDataFrame([
    (0, "Free lottery winner claim now"),
    (1, "Project deadline next Monday")
], ["id", "text"])

new_predictions = cv_model.transform(new_data)
new_predictions.select("text", "prediction", "probability").show(truncate=False)
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===
AUC: 0.9850
ç²¾åº¦: 0.9500
F1ã‚¹ã‚³ã‚¢: 0.9495
</code></pre>

<p><strong>æ‹¡å¼µã‚¢ã‚¤ãƒ‡ã‚¢</strong>ï¼š</p>
<ul>
<li>Word2Vec ã‚„GloVeåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨</li>
<li>N-gramç‰¹å¾´é‡ã‚’è¿½åŠ </li>
<li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚„GBTã‚’è©¦ã™</li>
<li>ã‚«ã‚¹ã‚¿ãƒ ç‰¹å¾´é‡ï¼ˆæ–‡ã®é•·ã•ã€å¤§æ–‡å­—ã®å‰²åˆãªã©ï¼‰ã‚’è¿½åŠ </li>
</ul>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Zaharia, M., et al. (2016). <em>Apache Spark: A Unified Engine for Big Data Processing</em>. Communications of the ACM, 59(11), 56-65.</li>
<li>Karau, H., Konwinski, A., Wendell, P., & Zaharia, M. (2015). <em>Learning Spark: Lightning-Fast Big Data Analysis</em>. O'Reilly Media.</li>
<li>Chambers, B., & Zaharia, M. (2018). <em>Spark: The Definitive Guide</em>. O'Reilly Media.</li>
<li>Meng, X., et al. (2016). <em>MLlib: Machine Learning in Apache Spark</em>. Journal of Machine Learning Research, 17(1), 1235-1241.</li>
<li>Apache Spark Documentation. (2024). <em>Spark SQL, DataFrames and Datasets Guide</em>. URL: https://spark.apache.org/docs/latest/sql-programming-guide.html</li>
<li>Databricks. (2024). <em>Apache Spark Performance Tuning Guide</em>. URL: https://www.databricks.com/blog/performance-tuning</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">â† Series Contents</a>
    <a href="chapter3-distributed-deep-learning.html" class="nav-button">Next Chapter:  distributedæ·±å±¤å­¦ç¿’ â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
