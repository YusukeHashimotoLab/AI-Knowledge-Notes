<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Neural Networks Introduction Series v1.0 - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Neural Networks Introduction Series - Complete Guide" name="description"/>
<title>Neural Networks Introduction Series v1.0 - AI Terakoya</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Mermaid for diagrams -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Neural Networks</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/neural-networks-introduction/index.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>Neural Networks Introduction Series v1.0</h1>
<div class="meta">
<span>üìñ Total Learning Time: 120-140 minutes</span>
<span>üìä Level: Beginner to Intermediate</span>
</div>
</div>
</header>
<main class="container">
<p><strong>Learn Deep Learning Fundamentals and Practice from Zero</strong></p>
<h2 id="overview">Series Overview</h2>
<p>This series is a practical educational content with 5 chapters that teaches Neural Networks progressively from the basics.</p>
<p><strong>Neural Networks</strong> are machine learning models that mimic the neurons of the human brain. Starting from simple perceptrons and becoming multilayered, they learn complex patterns and achieve remarkable results in diverse fields including image recognition, natural language processing, and speech recognition.</p>
<p><strong>Features:</strong></p>
<ul>
<li>‚úÖ <strong>From Basics to Practice</strong>: Systematic learning from perceptrons to latest deep learning frameworks</li>
<li>‚úÖ <strong>Implementation Focused</strong>: Over 60 executable Python code examples, 5 practical projects</li>
<li>‚úÖ <strong>Balance of Math and Intuition</strong>: Emphasizing intuitive understanding beyond just formulas</li>
<li>‚úÖ <strong>Latest Technology</strong>: State-of-the-art implementation methods using PyTorch and TensorFlow</li>
<li>‚úÖ <strong>Practical Projects</strong>: Real-world image classification with MNIST and CIFAR-10</li>
</ul>
<p><strong>Total Learning Time</strong>: 120-140 minutes (including code execution and exercises)</p>
<h2 id="learning">How to Learn</h2>
<h3>Recommended Learning Path</h3>
<div class="mermaid">
graph TD
    A[Chapter 1: Perceptron Basics] --&gt; B[Chapter 2: Multilayer Perceptron and Backpropagation]
    B --&gt; C[Chapter 3: Activation Functions and Optimization]
    C --&gt; D[Chapter 4: PyTorch and TensorFlow Practice]
    D --&gt; E[Chapter 5: Image Classification Projects]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>
<p><strong>For Complete Beginners (No ML Knowledge):</strong><br/>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (All chapters recommended)<br/>
        - Duration: 120-140 minutes</p>
<p><strong>For Intermediate Learners (ML Experience):</strong><br/>
        - Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br/>
        - Duration: 90-110 minutes</p>
<p><strong>Practical Skill Enhancement (Implementation over Theory):</strong><br/>
        - Chapter 4 (Intensive) ‚Üí Chapter 5<br/>
        - Duration: 50-60 minutes</p>
<h2 id="chapters">Chapter Details</h2>
<h3><a href="#">Chapter 1: Perceptron Basics</a></h3>
<p><strong>Difficulty</strong>: Introductory<br/>
<strong>Reading Time</strong>: 20-25 minutes<br/>
<strong>Code Examples</strong>: 9</p>
<h4>Learning Content</h4>
<ol>
<li><strong>What is a Perceptron</strong> - The simplest neural network</li>
<li><strong>Logic Gate Implementation</strong> - AND, OR, NAND gates</li>
<li><strong>Weights and Bias</strong> - Meaning and role of parameters</li>
<li><strong>Linear Separability</strong> - Limitations of perceptrons</li>
<li><strong>XOR Problem</strong> - Why multilayer networks are needed</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand perceptron structure and operation principles</li>
<li>‚úÖ Can implement logic gates in Python</li>
<li>‚úÖ Can explain the role of weights and bias</li>
<li>‚úÖ Understand the concept of linear separability</li>
<li>‚úÖ Know the limitations of single-layer perceptrons</li>
</ul>
<p><strong><a href="#">Read Chapter 1 ‚Üí</a></strong></p>
<hr/>
<h3><a href="#">Chapter 2: Multilayer Perceptron and Backpropagation</a></h3>
<p><strong>Difficulty</strong>: Beginner to Intermediate<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 15</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Multilayer Perceptron (MLP) Structure</strong> - Input, hidden, and output layers</li>
<li><strong>Backpropagation</strong> - Core of the learning algorithm</li>
<li><strong>Gradient Descent</strong> - Parameter update method</li>
<li><strong>Chain Rule</strong> - Basics of differentiation</li>
<li><strong>Complete Implementation</strong> - Scratch implementation with NumPy</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand and diagram MLP structure</li>
<li>‚úÖ Can explain backpropagation mechanism</li>
<li>‚úÖ Can implement MLP with NumPy</li>
<li>‚úÖ Understand mathematical background of gradient descent</li>
<li>‚úÖ Can solve the XOR problem</li>
</ul>
<p><strong><a href="#">Read Chapter 2 ‚Üí</a></strong></p>
<hr/>
<h3><a href="#">Chapter 3: Activation Functions and Optimization</a></h3>
<p><strong>Difficulty</strong>: Intermediate<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 12</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Types of Activation Functions</strong> - Sigmoid, ReLU, Leaky ReLU, ELU, Swish</li>
<li><strong>Vanishing Gradient Problem</strong> - Challenges of deep networks</li>
<li><strong>Optimization Algorithms</strong> - SGD, Momentum, AdaGrad, Adam, RMSprop</li>
<li><strong>Learning Rate Adjustment</strong> - Learning Rate Scheduling</li>
<li><strong>Initialization Strategies</strong> - Xavier, He initialization</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand characteristics and usage of each activation function</li>
<li>‚úÖ Can explain vanishing gradient problem and countermeasures</li>
<li>‚úÖ Can select appropriate optimization algorithms</li>
<li>‚úÖ Can implement learning rate scheduling</li>
<li>‚úÖ Understand importance of initialization</li>
</ul>
<p><strong><a href="#">Read Chapter 3 ‚Üí</a></strong></p>
<hr/>
<h3><a href="#">Chapter 4: PyTorch and TensorFlow Practice</a></h3>
<p><strong>Difficulty</strong>: Intermediate<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 14</p>
<h4>Learning Content</h4>
<ol>
<li><strong>PyTorch Basics</strong> - Tensor, Autograd, nn.Module</li>
<li><strong>TensorFlow/Keras Basics</strong> - Sequential API, Functional API</li>
<li><strong>Model Building</strong> - Custom layers, model definition</li>
<li><strong>Training Loop</strong> - Training, validation, testing</li>
<li><strong>GPU Utilization</strong> - CUDA, acceleration techniques</li>
<li><strong>Model Save/Load</strong> - Checkpoint management</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand differences between PyTorch and TensorFlow</li>
<li>‚úÖ Can create custom models with nn.Module</li>
<li>‚úÖ Can implement complete training loops</li>
<li>‚úÖ Can accelerate training with GPU</li>
<li>‚úÖ Can save and reuse models</li>
</ul>
<p><strong><a href="#">Read Chapter 4 ‚Üí</a></strong></p>
<hr/>
<h3><a href="#">Chapter 5: Image Classification Projects</a></h3>
<p><strong>Difficulty</strong>: Intermediate to Advanced<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 13</p>
<h4>Learning Content</h4>
<ol>
<li><strong>MNIST Project</strong> - Complete implementation of handwritten digit recognition</li>
<li><strong>Data Preprocessing</strong> - Normalization, data augmentation</li>
<li><strong>CIFAR-10 Project</strong> - Color image classification</li>
<li><strong>Regularization Techniques</strong> - Dropout, Batch Normalization, Weight Decay</li>
<li><strong>Hyperparameter Tuning</strong> - Grid Search, Random Search</li>
<li><strong>Model Evaluation</strong> - Confusion Matrix, Accuracy, Recall, F1 Score</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Can achieve 98%+ accuracy on MNIST</li>
<li>‚úÖ Can implement MLP on CIFAR-10</li>
<li>‚úÖ Can improve generalization with data augmentation</li>
<li>‚úÖ Can appropriately use regularization techniques</li>
<li>‚úÖ Can evaluate model performance from multiple perspectives</li>
</ul>
<p><strong><a href="#">Read Chapter 5 ‚Üí</a></strong></p>
<hr/>
<h2 id="outcomes">Overall Learning Outcomes</h2>
<p>Upon completing this series, you will acquire the following skills and knowledge:</p>
<h3>Knowledge Level (Understanding)</h3>
<ul>
<li>‚úÖ Can explain the history and basic principles of neural networks</li>
<li>‚úÖ Understand mechanisms of perceptron, MLP, and backpropagation</li>
<li>‚úÖ Can differentiate and use activation functions and optimization algorithms</li>
<li>‚úÖ Can explain vanishing gradient problem and countermeasures</li>
<li>‚úÖ Understand differences between PyTorch and TensorFlow</li>
</ul>
<h3>Practical Skills (Doing)</h3>
<ul>
<li>‚úÖ Can implement neural networks from scratch with NumPy</li>
<li>‚úÖ Can build custom models with PyTorch</li>
<li>‚úÖ Can implement complete training loops</li>
<li>‚úÖ Can achieve 98%+ accuracy on MNIST</li>
<li>‚úÖ Can apply data augmentation and regularization</li>
<li>‚úÖ Can tune hyperparameters</li>
</ul>
<h3>Application Ability (Applying)</h3>
<ul>
<li>‚úÖ Can design appropriate architectures for new problems</li>
<li>‚úÖ Can handle overfitting and training stagnation</li>
<li>‚úÖ Can evaluate model performance from multiple perspectives</li>
<li>‚úÖ Can advance to sophisticated architectures like CNN and RNN</li>
</ul>
<hr/>
<h2 id="start">Let's Get Started!</h2>
<p>Are you ready? Start with Chapter 1 and begin your journey into the world of neural networks!</p>
<p><strong><a href="#">Chapter 1: Perceptron Basics ‚Üí</a></strong></p>
<hr/>
<p><strong>Update History</strong></p>
<ul>
<li><strong>2025-10-20</strong>: v1.0 Initial Release</li>
</ul>
<hr/>
<p><strong>Your neural network learning journey starts here!</strong></p>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, data, tools, or libraries provided by third parties.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0), which typically include a no-warranty clause.</li>
</ul>
</section>
<footer>
<div class="container">
<p>¬© 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
