<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Fundamentals of Generative Models - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Fundamentals of Generative Models - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/generative-models-introduction/index.html">Generative Models</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/generative-models-introduction/chapter1-fundamentals.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Fundamentals of Generative Models</h1>
<p class="subtitle">Understanding the Differences from Discriminative Models, Learning Probability Distributions, and Sampling Methods</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Fundamentals of Generative Models, which discriminative models vs generative models. You will learn fundamental differences between discriminative, likelihood function, and relationship between Bayes' theorem.</p>
<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the fundamental differences between discriminative and generative models</li>
<li>‚úÖ Explain the likelihood function and maximum likelihood estimation in probability distribution learning</li>
<li>‚úÖ Understand the relationship between Bayes' theorem and generative models</li>
<li>‚úÖ Master the mechanisms of Rejection Sampling and Importance Sampling</li>
<li>‚úÖ Understand the basic principles of MCMC (Markov Chain Monte Carlo)</li>
<li>‚úÖ Grasp the concepts of latent variable models and latent spaces</li>
<li>‚úÖ Implement quality evaluation using Inception Score and FID</li>
<li>‚úÖ Implement Gaussian Mixture Models (GMM) in PyTorch and apply them to data generation</li>
</ul>
<hr/>
<h2>1.1 Discriminative Models vs Generative Models</h2>
<h3>Two Approaches in Machine Learning</h3>
<p>Machine learning models can be broadly classified into two categories based on how they interact with data:</p>
<blockquote>
<p>"Discriminative models learn mappings from inputs to outputs, while generative models learn the probability distribution of the data itself."</p>
</blockquote>
<h4>Discriminative Model</h4>
<p><strong>Objective</strong>: Learn the conditional probability $P(y|x)$</p>
<ul>
<li>Predict label $y$ given input $x$</li>
<li>Used for classification and regression tasks</li>
<li>Examples: Logistic regression, SVM, neural networks</li>
</ul>
<h4>Generative Model</h4>
<p><strong>Objective</strong>: Learn the joint probability $P(x, y)$ or $P(x)$</p>
<ul>
<li>Model the distribution of the data itself</li>
<li>Can generate new data samples</li>
<li>Examples: VAE, GAN, diffusion models, GPT</li>
</ul>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Discriminative Model</th>
<th>Generative Model</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Learning Target</strong></td>
<td>$P(y|x)$ (conditional probability)</td>
<td>$P(x)$ or $P(x,y)$ (joint probability)</td>
</tr>
<tr>
<td><strong>Main Use</strong></td>
<td>Classification, regression</td>
<td>Data generation, density estimation</td>
</tr>
<tr>
<td><strong>Decision Boundary</strong></td>
<td>Directly learned</td>
<td>Derived from probability distribution</td>
</tr>
<tr>
<td><strong>Data Generation</strong></td>
<td>Impossible</td>
<td>Possible</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Relatively low</td>
<td>High (models entire distribution)</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    subgraph "Discriminative Model"
    A1[Input x] --&gt; B1[Model f]
    B1 --&gt; C1[Output y]
    D1[Learning: P(y|x)]
    end

    subgraph "Generative Model"
    A2[Probability Distribution P(x)] --&gt; B2[Sampling]
    B2 --&gt; C2[Generated Data x']
    D2[Learning: P(x)]
    end

    style A1 fill:#e3f2fd
    style B1 fill:#fff3e0
    style C1 fill:#ffebee
    style A2 fill:#e3f2fd
    style B2 fill:#fff3e0
    style C2 fill:#ffebee
</div>
<h3>Relationship Through Bayes' Theorem</h3>
<p>Discriminative and generative models are connected through Bayes' theorem:</p>

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

<p>Where:</p>
<ul>
<li>$P(y|x)$: Posterior probability (learned directly by discriminative models)</li>
<li>$P(x|y)$: Likelihood (learned by generative models)</li>
<li>$P(y)$: Prior probability (class frequency)</li>
<li>$P(x)$: Marginal likelihood (normalization constant)</li>
</ul>
<blockquote>
<p><strong>Important</strong>: Generative models learn $P(x|y)$ and $P(y)$, which can be applied to classification tasks through Bayes' theorem.</p>
</blockquote>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Important: Generative models learn $P(x|y)$ and $P(y)$, whic

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

# Data generation: Binary classification
np.random.seed(42)
X, y = make_blobs(n_samples=300, centers=2, n_features=2,
                  center_box=(-5, 5), random_state=42)

print("=== Discriminative Model vs Generative Model ===\n")

# Discriminative model: Logistic Regression (learns P(y|x) directly)
discriminative = LogisticRegression()
discriminative.fit(X, y)

# Generative model: Gaussian Naive Bayes (learns P(x|y) and P(y))
generative = GaussianNB()
generative.fit(X, y)

# Test data
X_test = np.array([[2.0, 3.0], [-3.0, -2.0]])

# Predictions
disc_pred = discriminative.predict(X_test)
gen_pred = generative.predict(X_test)
disc_proba = discriminative.predict_proba(X_test)
gen_proba = generative.predict_proba(X_test)

print("Test samples:")
for i, x in enumerate(X_test):
    print(f"\nSample {i+1}: {x}")
    print(f"  Discriminative model prediction: Class {disc_pred[i]}, "
          f"probability [Class 0: {disc_proba[i,0]:.3f}, Class 1: {disc_proba[i,1]:.3f}]")
    print(f"  Generative model prediction: Class {gen_pred[i]}, "
          f"probability [Class 0: {gen_proba[i,0]:.3f}, Class 1: {gen_proba[i,1]:.3f}]")

print("\nFeature comparison:")
print("  Discriminative model:")
print("    - Learns decision boundary directly")
print("    - Computationally efficient")
print("    - Cannot generate new data")
print("\n  Generative model:")
print("    - Learns probability distribution for each class")
print("    - Can generate data")
print("    - Requires distributional assumptions (e.g., Gaussian)")

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Create mesh grid
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                     np.linspace(y_min, y_max, 200))

# Decision boundary for discriminative model
Z_disc = discriminative.predict(np.c_[xx.ravel(), yy.ravel()])
Z_disc = Z_disc.reshape(xx.shape)
ax1.contourf(xx, yy, Z_disc, alpha=0.3, cmap='RdYlBu')
ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolor='black')
ax1.set_title('Discriminative Model (Logistic Regression)\nDirectly learns P(y|x)')
ax1.set_xlabel('Feature 1')
ax1.set_ylabel('Feature 2')

# Decision boundary for generative model
Z_gen = generative.predict(np.c_[xx.ravel(), yy.ravel()])
Z_gen = Z_gen.reshape(xx.shape)
ax2.contourf(xx, yy, Z_gen, alpha=0.3, cmap='RdYlBu')
ax2.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolor='black')
ax2.set_title('Generative Model (Gaussian Naive Bayes)\nLearns P(x|y) and P(y)')
ax2.set_xlabel('Feature 1')
ax2.set_ylabel('Feature 2')

plt.tight_layout()
print("\nDecision boundary visualization generated")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Discriminative Model vs Generative Model ===

Test samples:

Sample 1: [ 2.  3.]
  Discriminative model prediction: Class 1, probability [Class 0: 0.234, Class 1: 0.766]
  Generative model prediction: Class 1, probability [Class 0: 0.198, Class 1: 0.802]

Sample 2: [-3. -2.]
  Discriminative model prediction: Class 0, probability [Class 0: 0.891, Class 1: 0.109]
  Generative model prediction: Class 0, probability [Class 0: 0.923, Class 1: 0.077]

Feature comparison:
  Discriminative model:
    - Learns decision boundary directly
    - Computationally efficient
    - Cannot generate new data

  Generative model:
    - Learns probability distribution for each class
    - Can generate data
    - Requires distributional assumptions (e.g., Gaussian)

Decision boundary visualization generated
</code></pre>
<hr/>
<h2>1.2 Learning Probability Distributions</h2>
<h3>Likelihood Function and Maximum Likelihood Estimation</h3>
<p>The core of generative models is to represent and learn the probability distribution of data $P(x; \theta)$ with parameters $\theta$.</p>
<h4>Likelihood Function</h4>
<p>The likelihood for given data $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$ is:</p>

$$
L(\theta) = P(\mathcal{D}; \theta) = \prod_{i=1}^{N} P(x_i; \theta)
$$

<p>Assuming the data are independent and identically distributed (i.i.d.), this becomes the product of the probabilities of each sample.</p>
<h4>Log-Likelihood</h4>
<p>For numerical stability and computational convenience, we typically take the logarithm:</p>

$$
\log L(\theta) = \sum_{i=1}^{N} \log P(x_i; \theta)
$$

<h4>Maximum Likelihood Estimation (MLE)</h4>
<p>We find the parameter $\theta$ that maximizes the likelihood:</p>

$$
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \log L(\theta) = \arg\max_{\theta} \sum_{i=1}^{N} \log P(x_i; \theta)
$$

<blockquote>
<p>"Maximum likelihood estimation is the principle of choosing parameters that make the observed data most likely to occur."</p>
</blockquote>
<h3>Concrete Example: Parameter Estimation for Gaussian Distribution</h3>
<p>Assume data follows a one-dimensional Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$:</p>

$$
P(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

<p>Log-likelihood:</p>

$$
\log L(\mu, \sigma^2) = -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i - \mu)^2
$$

<p>Taking derivatives with respect to $\mu$ and $\sigma^2$ and setting them to zero:</p>

$$
\begin{align}
\hat{\mu}_{\text{MLE}} &amp;= \frac{1}{N}\sum_{i=1}^{N} x_i \\
\hat{\sigma}^2_{\text{MLE}} &amp;= \frac{1}{N}\sum_{i=1}^{N} (x_i - \hat{\mu})^2
\end{align}
$$

<p>In other words, the sample mean and sample variance are the MLEs.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: In other words, the sample mean and sample variance are the 

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Data generation: True distribution N(3, 2^2)
np.random.seed(42)
true_mu, true_sigma = 3.0, 2.0
N = 100
data = np.random.normal(true_mu, true_sigma, N)

print("=== Maximum Likelihood Estimation (MLE) Implementation ===\n")

# Maximum likelihood estimation
mle_mu = np.mean(data)
mle_sigma = np.std(data, ddof=0)  # ddof=0 for sample variance

print(f"True parameters:")
print(f"  Mean Œº: {true_mu}")
print(f"  Standard deviation œÉ: {true_sigma}")

print(f"\nMaximum likelihood estimates:")
print(f"  Estimated mean ŒºÃÇ: {mle_mu:.4f}")
print(f"  Estimated standard deviation œÉÃÇ: {mle_sigma:.4f}")

print(f"\nEstimation error:")
print(f"  Mean error: {abs(mle_mu - true_mu):.4f}")
print(f"  Standard deviation error: {abs(mle_sigma - true_sigma):.4f}")

# Log-likelihood calculation
def log_likelihood(data, mu, sigma):
    """Log-likelihood of Gaussian distribution"""
    N = len(data)
    log_prob = -0.5 * N * np.log(2 * np.pi) - N * np.log(sigma) \
               - 0.5 * np.sum((data - mu)**2) / (sigma**2)
    return log_prob

# Log-likelihood at MLE
ll_mle = log_likelihood(data, mle_mu, mle_sigma)
print(f"\nLog-likelihood at MLE: {ll_mle:.2f}")

# Log-likelihood at other parameters (for comparison)
ll_wrong1 = log_likelihood(data, true_mu + 1, true_sigma)
ll_wrong2 = log_likelihood(data, true_mu, true_sigma + 1)
print(f"Log-likelihood at Œº=4, œÉ=2: {ll_wrong1:.2f} (lower than MLE)")
print(f"Log-likelihood at Œº=3, œÉ=3: {ll_wrong2:.2f} (lower than MLE)")

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Left plot: Data and estimated distribution
ax1.hist(data, bins=20, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='Data histogram')
x_range = np.linspace(data.min() - 1, data.max() + 1, 200)
ax1.plot(x_range, norm.pdf(x_range, true_mu, true_sigma),
         'r-', linewidth=2, label=f'True distribution N({true_mu}, {true_sigma}¬≤)')
ax1.plot(x_range, norm.pdf(x_range, mle_mu, mle_sigma),
         'g--', linewidth=2, label=f'Estimated distribution N({mle_mu:.2f}, {mle_sigma:.2f}¬≤)')
ax1.set_xlabel('x')
ax1.set_ylabel('Probability density')
ax1.set_title('Gaussian Distribution Fitting by Maximum Likelihood Estimation')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: Contour plot of log-likelihood
mu_range = np.linspace(2, 4, 100)
sigma_range = np.linspace(1, 3, 100)
MU, SIGMA = np.meshgrid(mu_range, sigma_range)
LL = np.zeros_like(MU)

for i in range(len(mu_range)):
    for j in range(len(sigma_range)):
        LL[j, i] = log_likelihood(data, MU[j, i], SIGMA[j, i])

contour = ax2.contourf(MU, SIGMA, LL, levels=20, cmap='viridis')
ax2.plot(mle_mu, mle_sigma, 'r*', markersize=20, label='MLE')
ax2.plot(true_mu, true_sigma, 'wo', markersize=10, label='True parameters')
ax2.set_xlabel('Mean Œº')
ax2.set_ylabel('Standard deviation œÉ')
ax2.set_title('Contour Plot of Log-Likelihood')
ax2.legend()
plt.colorbar(contour, ax=ax2, label='Log-likelihood')

plt.tight_layout()
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Maximum Likelihood Estimation (MLE) Implementation ===

True parameters:
  Mean Œº: 3.0
  Standard deviation œÉ: 2.0

Maximum likelihood estimates:
  Estimated mean ŒºÃÇ: 3.0234
  Estimated standard deviation œÉÃÇ: 1.9876

Estimation error:
  Mean error: 0.0234
  Standard deviation error: 0.0124

Log-likelihood at MLE: -218.34
Log-likelihood at Œº=4, œÉ=2: -243.12 (lower than MLE)
Log-likelihood at Œº=3, œÉ=3: -225.78 (lower than MLE)

Visualization complete
</code></pre>
<h3>Bayes' Theorem and Posterior Distribution</h3>
<p>In Bayesian estimation, we also assume a probability distribution for the parameter $\theta$:</p>

$$
P(\theta | \mathcal{D}) = \frac{P(\mathcal{D} | \theta) P(\theta)}{P(\mathcal{D})}
$$

<p>Where:</p>
<ul>
<li>$P(\theta | \mathcal{D})$: Posterior distribution (distribution of parameters after observing data)</li>
<li>$P(\mathcal{D} | \theta)$: Likelihood (used in MLE)</li>
<li>$P(\theta)$: Prior distribution (prior knowledge)</li>
<li>$P(\mathcal{D})$: Marginal likelihood (normalization constant)</li>
</ul>
<blockquote>
<p><strong>MLE vs Bayesian Estimation</strong>: MLE provides point estimates (single value), Bayesian provides distributional estimates (retains uncertainty).</p>
</blockquote>
<hr/>
<h2>1.3 Sampling Methods</h2>
<h3>Why Sampling is Necessary</h3>
<p>In generative models, we need to generate new samples from the learned probability distribution $P(x)$. However, direct sampling from complex distributions is difficult.</p>
<h4>Challenges of Sampling</h4>
<ul>
<li>Computing probability density in high-dimensional spaces is difficult</li>
<li>Calculating the normalization constant (partition function) is hard</li>
<li>Methods for transforming from simple uniform or normal distributions are unclear</li>
</ul>
<h3>Rejection Sampling</h3>
<p><strong>Basic Idea</strong>: Sample from an easy proposal distribution $q(x)$ and probabilistically reject to obtain samples following the target distribution $p(x)$.</p>
<h4>Algorithm</h4>
<ol>
<li>Choose a constant $M$ such that $p(x) \leq M q(x)$ for all $x$</li>
<li>Generate sample $x$ from proposal distribution $q(x)$</li>
<li>Generate $u$ from uniform distribution $u \sim U(0, 1)$</li>
<li>Accept $x$ if $u &lt; \frac{p(x)}{M q(x)}$, otherwise reject</li>
<li>Repeat until the required number of samples is obtained</li>
</ol>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, beta

# Target distribution: Beta(2, 5)
def target_dist(x):
    """Target distribution p(x) = Beta(2, 5)"""
    return beta.pdf(x, 2, 5)

# Proposal distribution: Uniform U(0, 1)
def proposal_dist(x):
    """Proposal distribution q(x) = U(0, 1)"""
    return np.ones_like(x)

# Constant M: satisfies p(x) &lt;= M * q(x)
x_test = np.linspace(0, 1, 1000)
M = np.max(target_dist(x_test) / proposal_dist(x_test))

print("=== Rejection Sampling ===\n")
print(f"Constant M: {M:.4f}")

# Rejection Sampling implementation
def rejection_sampling(n_samples, seed=42):
    """
    Sample generation by Rejection Sampling

    Parameters:
    -----------
    n_samples : int
        Number of samples to generate
    seed : int
        Random seed

    Returns:
    --------
    samples : np.ndarray
        Generated samples
    acceptance_rate : float
        Acceptance rate
    """
    np.random.seed(seed)
    samples = []
    n_trials = 0

    while len(samples) &lt; n_samples:
        # Sample from proposal distribution (uniform)
        x = np.random.uniform(0, 1)
        # Uniform random number
        u = np.random.uniform(0, 1)

        # Accept/reject decision
        if u &lt; target_dist(x) / (M * proposal_dist(x)):
            samples.append(x)

        n_trials += 1

    acceptance_rate = n_samples / n_trials
    return np.array(samples), acceptance_rate

# Execute sampling
n_samples = 1000
samples, acc_rate = rejection_sampling(n_samples)

print(f"\nGenerated sample count: {n_samples}")
print(f"Total trials: {int(n_samples / acc_rate)}")
print(f"Acceptance rate: {acc_rate:.4f}")
print(f"\nSample statistics:")
print(f"  Mean: {samples.mean():.4f} (theoretical value: {2/(2+5):.4f})")
print(f"  Standard deviation: {samples.std():.4f}")

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Left plot: Mechanism of Rejection Sampling
x_range = np.linspace(0, 1, 1000)
ax1.plot(x_range, target_dist(x_range), 'r-', linewidth=2, label='Target distribution p(x)')
ax1.plot(x_range, M * proposal_dist(x_range), 'b--', linewidth=2,
         label=f'M √ó Proposal distribution (M={M:.2f})')
ax1.fill_between(x_range, 0, target_dist(x_range), alpha=0.3, color='red')
ax1.set_xlabel('x')
ax1.set_ylabel('Probability density')
ax1.set_title('Mechanism of Rejection Sampling')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: Distribution of generated samples
ax2.hist(samples, bins=30, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='Generated samples')
ax2.plot(x_range, target_dist(x_range), 'r-', linewidth=2,
         label='Target distribution Beta(2, 5)')
ax2.set_xlabel('x')
ax2.set_ylabel('Probability density')
ax2.set_title('Distribution of Generated Samples')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Rejection Sampling ===

Constant M: 2.4576

Generated sample count: 1000
Total trials: 2458
Acceptance rate: 0.4069

Sample statistics:
  Mean: 0.2871 (theoretical value: 0.2857)
  Standard deviation: 0.1756

Visualization complete
</code></pre>
<h4>Problems with Rejection Sampling</h4>
<ul>
<li>Inefficient in high dimensions (acceptance rate drops rapidly)</li>
<li>Choosing appropriate $M$ is difficult</li>
<li>Wasteful if proposal distribution differs significantly from target distribution</li>
</ul>
<h3>Importance Sampling</h3>
<p><strong>Basic Idea</strong>: In computing expectations, sample from a proposal distribution and correct with weights.</p>
<p>Expected value of function $f(x)$ under target distribution $p(x)$:</p>

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx
$$

<p>Rewriting using proposal distribution $q(x)$:</p>

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) dx = \mathbb{E}_{q(x)}\left[f(x) w(x)\right]
$$

<p>Where $w(x) = \frac{p(x)}{q(x)}$ is the importance weight.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Target distribution: right tail of standard normal (important region x &gt; 2)
def target_dist(x):
    """Target distribution (unnormalized)"""
    return norm.pdf(x, 0, 1) * (x &gt; 2)

# Proposal distribution: Broader normal distribution
def proposal_dist(x):
    """Proposal distribution N(3, 2)"""
    return norm.pdf(x, 3, 2)

print("=== Importance Sampling ===\n")

# Function whose expectation we want to compute
def f(x):
    """Square function"""
    return x ** 2

# Importance Sampling implementation
n_samples = 10000
np.random.seed(42)

# Sample from proposal distribution
samples = np.random.normal(3, 2, n_samples)

# Compute importance weights
weights = target_dist(samples) / proposal_dist(samples)
weights = weights / weights.sum()  # Normalize

# Estimate expectation
estimated_mean = np.sum(f(samples) * weights)

print(f"Sample count: {n_samples}")
print(f"\nEstimated expectation E[x¬≤]: {estimated_mean:.4f}")

# Sample directly from target distribution for comparison (Monte Carlo)
# Note: Direct sampling is difficult since target distribution is unnormalized
# Using Rejection Sampling as substitute here
true_samples = []
while len(true_samples) &lt; 1000:
    x = np.random.normal(0, 1)
    if x &gt; 2 and np.random.uniform() &lt; 1.0:  # Simplified
        true_samples.append(x)
true_samples = np.array(true_samples)
true_mean = np.mean(f(true_samples))

print(f"True expectation (reference): {true_mean:.4f}")
print(f"Estimation error: {abs(estimated_mean - true_mean):.4f}")

print(f"\nAdvantages of Importance Sampling:")
print(f"  - No sample rejection required")
print(f"  - Specialized for expectation computation")
print(f"  - Correction via importance weights")

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Left plot: Target and proposal distributions
x_range = np.linspace(-2, 8, 1000)
# Normalize target distribution
target_unnorm = target_dist(x_range)
Z = np.trapz(target_unnorm, x_range)
target_norm = target_unnorm / Z

ax1.plot(x_range, target_norm, 'r-', linewidth=2, label='Target distribution p(x)')
ax1.plot(x_range, proposal_dist(x_range), 'b--', linewidth=2,
         label='Proposal distribution q(x) = N(3, 2¬≤)')
ax1.fill_between(x_range, 0, target_norm, alpha=0.3, color='red')
ax1.set_xlabel('x')
ax1.set_ylabel('Probability density')
ax1.set_title('Importance Sampling: Distribution Comparison')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: Distribution of importance weights
ax2.hist(weights, bins=50, density=True, alpha=0.6, color='green',
         edgecolor='black')
ax2.set_xlabel('Importance weight w(x)')
ax2.set_ylabel('Frequency')
ax2.set_title('Distribution of Importance Weights')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Importance Sampling ===

Sample count: 10000

Estimated expectation E[x¬≤]: 6.7234

True expectation (reference): 6.8012
Estimation error: 0.0778

Advantages of Importance Sampling:
  - No sample rejection required
  - Specialized for expectation computation
  - Correction via importance weights

Visualization complete
</code></pre>
<h3>MCMC (Markov Chain Monte Carlo)</h3>
<p><strong>Basic Idea</strong>: Construct a Markov chain such that its stationary distribution is the target distribution.</p>
<h4>Metropolis-Hastings Algorithm</h4>
<ol>
<li>Choose initial sample $x_0$</li>
<li>Generate candidate $x'$ from proposal distribution $q(x' | x_t)$</li>
<li>Compute acceptance probability: $\alpha = \min\left(1, \frac{p(x') q(x_t|x')}{p(x_t) q(x'|x_t)}\right)$</li>
<li>Set $x_{t+1} = x'$ with probability $\alpha$, otherwise $x_{t+1} = x_t$</li>
<li>Repeat steps 2-4</li>
</ol>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Target distribution: Mixture of Gaussians
def target_distribution(x):
    """
    Mixture of Gaussians (unnormalized)
    0.3 * N(-2, 0.5¬≤) + 0.7 * N(3, 1¬≤)
    """
    return 0.3 * norm.pdf(x, -2, 0.5) + 0.7 * norm.pdf(x, 3, 1.0)

print("=== MCMC: Metropolis-Hastings ===\n")

# Metropolis-Hastings implementation
def metropolis_hastings(n_samples, proposal_std=1.0, burn_in=1000, seed=42):
    """
    Metropolis-Hastings algorithm

    Parameters:
    -----------
    n_samples : int
        Number of samples to generate
    proposal_std : float
        Standard deviation of proposal distribution (random walk)
    burn_in : int
        Burn-in period
    seed : int
        Random seed

    Returns:
    --------
    samples : np.ndarray
        Generated samples
    acceptance_rate : float
        Acceptance rate
    """
    np.random.seed(seed)

    # Initial value
    x = 0.0
    samples = []
    n_accepted = 0

    # Burn-in + sampling
    for i in range(burn_in + n_samples):
        # Proposal distribution (Gaussian random walk)
        x_proposal = x + np.random.normal(0, proposal_std)

        # Compute acceptance probability
        acceptance_prob = min(1.0, target_distribution(x_proposal) /
                              target_distribution(x))

        # Accept/reject decision
        if np.random.uniform() &lt; acceptance_prob:
            x = x_proposal
            n_accepted += 1

        # Save samples after burn-in
        if i &gt;= burn_in:
            samples.append(x)

    acceptance_rate = n_accepted / (burn_in + n_samples)
    return np.array(samples), acceptance_rate

# Execute sampling
n_samples = 10000
samples, acc_rate = metropolis_hastings(n_samples, proposal_std=2.0)

print(f"Generated sample count: {n_samples}")
print(f"Acceptance rate: {acc_rate:.4f}")
print(f"\nSample statistics:")
print(f"  Mean: {samples.mean():.4f}")
print(f"  Standard deviation: {samples.std():.4f}")
print(f"  Minimum: {samples.min():.4f}")
print(f"  Maximum: {samples.max():.4f}")

print(f"\nMCMC characteristics:")
print(f"  - Can handle high-dimensional distributions")
print(f"  - No normalization constant required")
print(f"  - Exploration via Markov chain")
print(f"  - Burn-in period necessary")

# Visualization
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

# Top left: Distribution of generated samples
x_range = np.linspace(-5, 6, 1000)
ax1.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue',
         edgecolor='black', label='MCMC samples')
ax1.plot(x_range, target_distribution(x_range), 'r-', linewidth=2,
         label='Target distribution')
ax1.set_xlabel('x')
ax1.set_ylabel('Probability density')
ax1.set_title('Distribution of MCMC Generated Samples')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Top right: Trace plot of samples
ax2.plot(samples[:500], alpha=0.7)
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Sample value')
ax2.set_title('Trace Plot (First 500 Samples)')
ax2.grid(True, alpha=0.3)

# Bottom left: Autocorrelation
from numpy import correlate
lags = range(0, 100)
autocorr = [correlate(samples[:-lag] if lag &gt; 0 else samples, samples[lag:],
                      mode='valid')[0] / len(samples)
            if lag &gt; 0 else 1.0 for lag in lags]
ax3.plot(lags, autocorr)
ax3.set_xlabel('Lag')
ax3.set_ylabel('Autocorrelation')
ax3.set_title('Autocorrelation Plot')
ax3.grid(True, alpha=0.3)
ax3.axhline(y=0, color='k', linestyle='--', alpha=0.3)

# Bottom right: Convergence diagnosis (cumulative mean)
cumulative_mean = np.cumsum(samples) / np.arange(1, len(samples) + 1)
ax4.plot(cumulative_mean)
ax4.axhline(y=samples.mean(), color='r', linestyle='--',
            label=f'Final mean = {samples.mean():.4f}')
ax4.set_xlabel('Iteration')
ax4.set_ylabel('Cumulative mean')
ax4.set_title('Convergence Diagnosis')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== MCMC: Metropolis-Hastings ===

Generated sample count: 10000
Acceptance rate: 0.7234

Sample statistics:
  Mean: 1.8234
  Standard deviation: 2.1456
  Minimum: -4.2341
  Maximum: 6.1234

MCMC characteristics:
  - Can handle high-dimensional distributions
  - No normalization constant required
  - Exploration via Markov chain
  - Burn-in period necessary

Visualization complete
</code></pre>
<hr/>
<h2>1.4 Latent Variable Models</h2>
<h3>Concept of Latent Space</h3>
<p>Many generative models model the relationship between observable variables $x$ and unobservable <strong>latent variables</strong> $z$.</p>
<blockquote>
<p>"Latent variables are low-dimensional representations behind the data, capturing the essential factors of data generation."</p>
</blockquote>
<h4>Formulation of Latent Variable Models</h4>
<p>Generative process:</p>

$$
\begin{align}
z &amp;\sim P(z) \quad \text{(Sample latent variable from prior distribution)} \\
x &amp;\sim P(x|z) \quad \text{(Generate observed data from latent variable)}
\end{align}
$$

<p>Marginal likelihood:</p>

$$
P(x) = \int P(x|z) P(z) dz
$$

<h4>Advantages of Latent Space</h4>
<table>
<thead>
<tr>
<th>Advantage</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dimensionality Reduction</strong></td>
<td>Represent high-dimensional data in low dimensions</td>
</tr>
<tr>
<td><strong>Interpretability</strong></td>
<td>Latent variables correspond to meaningful features</td>
</tr>
<tr>
<td><strong>Smooth Interpolation</strong></td>
<td>Movement in latent space generates continuous changes</td>
</tr>
<tr>
<td><strong>Controllable Generation</strong></td>
<td>Control generation by manipulating latent variables</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    Z[Latent Variable z] --&gt; D[Decoder/Generator]
    D --&gt; X[Observed Data x]
    X2[Observed Data x] --&gt; E[Encoder]
    E --&gt; Z2[Latent Representation z]

    subgraph "Generative Process"
    Z
    D
    X
    end

    subgraph "Inference Process"
    X2
    E
    Z2
    end

    style Z fill:#e3f2fd
    style D fill:#fff3e0
    style X fill:#ffebee
    style X2 fill:#ffebee
    style E fill:#fff3e0
    style Z2 fill:#e3f2fd
</div>
<h3>Visualizing Latent Space</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Visualizing Latent Space

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# Handwritten digit dataset (8x8 images)
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target  # Labels 0-9

print("=== Visualizing Latent Space ===\n")
print(f"Data size: {X.shape}")
print(f"  Sample count: {X.shape[0]}")
print(f"  Original dimensions: {X.shape[1]} (8x8 pixels)")

# Compress to 2D latent space with PCA
pca = PCA(n_components=2)
z = pca.fit_transform(X)

print(f"\nLatent space dimensions: {z.shape[1]}")
print(f"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}")

print(f"\nLatent variable statistics:")
print(f"  z1 mean: {z[:, 0].mean():.4f}, std: {z[:, 0].std():.4f}")
print(f"  z2 mean: {z[:, 1].mean():.4f}, std: {z[:, 1].std():.4f}")

# Visualization
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))

# Left plot: Latent space visualization
scatter = ax1.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', alpha=0.6, s=20)
ax1.set_xlabel('Latent variable z1')
ax1.set_ylabel('Latent variable z2')
ax1.set_title('Latent Space Visualization (PCA)')
plt.colorbar(scatter, ax=ax1, label='Digit label')
ax1.grid(True, alpha=0.3)

# Middle plot: Original image samples
for i in range(10):
    ax2.subplot(2, 5, i+1)
    plt.imshow(X[i].reshape(8, 8), cmap='gray')
    plt.title(f'Label: {y[i]}')
    plt.axis('off')
ax2.set_title('Original Image Samples')

# Right plot: Interpolation in latent space
# Get average latent representations for digits 0 and 1
z0_mean = z[y == 0].mean(axis=0)
z1_mean = z[y == 1].mean(axis=0)

# Interpolation
n_steps = 5
interpolated_z = np.array([z0_mean + (z1_mean - z0_mean) * t
                           for t in np.linspace(0, 1, n_steps)])

# Reconstruct images from latent representations (inverse PCA)
interpolated_x = pca.inverse_transform(interpolated_z)

for i in range(n_steps):
    plt.subplot(1, n_steps, i+1)
    plt.imshow(interpolated_x[i].reshape(8, 8), cmap='gray')
    plt.title(f't={i/(n_steps-1):.2f}')
    plt.axis('off')
ax3.set_title('Interpolation in Latent Space (0‚Üí1)')

plt.tight_layout()

print("\nLatent space properties:")
print("  - Similar digits are located close together")
print("  - Interpolation possible in continuous space")
print("  - Preserves information in 64 dimensions ‚Üí 2 dimensions compression")
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Visualizing Latent Space ===

Data size: (1797, 64)
  Sample count: 1797
  Original dimensions: 64 (8x8 pixels)

Latent space dimensions: 2
Explained variance ratio: 0.2876

Latent variable statistics:
  z1 mean: -0.0000, std: 6.0234
  z2 mean: 0.0000, std: 4.1234

Latent space properties:
  - Similar digits are located close together
  - Interpolation possible in continuous space
  - Preserves information in 64 dimensions ‚Üí 2 dimensions compression

Visualization complete
</code></pre>
<hr/>
<h2>1.5 Evaluation Metrics</h2>
<h3>Difficulty of Evaluating Generative Models</h3>
<p>Evaluating generative models is more difficult than discriminative models:</p>
<ul>
<li>True distribution is unknown</li>
<li>Quantifying generation quality is hard</li>
<li>Trade-off between diversity and quality</li>
</ul>
<h3>Inception Score (IS)</h3>
<p><strong>Basic Idea</strong>: Evaluate generated images using a pre-trained classifier (Inception Net).</p>
<p>Definition of Inception Score:</p>

$$
\text{IS} = \exp\left(\mathbb{E}_x \left[D_{KL}(p(y|x) \| p(y))\right]\right)
$$

<p>Where:</p>
<ul>
<li>$p(y|x)$: Classification probability for generated image $x$</li>
<li>$p(y)$: Average classification probability over all generated images</li>
<li>$D_{KL}$: KL divergence</li>
</ul>
<h4>Interpreting IS</h4>
<ul>
<li><strong>High IS</strong>: Generates clear and diverse images</li>
<li>Sharp $p(y|x)$ (low entropy) ‚Üí Clear images</li>
<li>Uniform $p(y)$ (high entropy) ‚Üí Diverse images</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Interpreting IS

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 10-30 seconds
Dependencies: None
"""

import torch
import torch.nn.functional as F
import numpy as np
from scipy.stats import entropy

# Dummy output from Inception Net (10-class classification)
# In practice, use torchvision.models.inception_v3
np.random.seed(42)
n_samples = 1000
n_classes = 10

# Classification probabilities for generated images (dummy)
# Good generation: Each image is clearly classified
probs_good = np.random.dirichlet(np.array([10, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
                                  n_samples)
# Bad generation: Each image's classification is ambiguous
probs_bad = np.random.dirichlet(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
                                 n_samples)

def inception_score(probs, splits=10):
    """
    Compute Inception Score

    Parameters:
    -----------
    probs : np.ndarray (n_samples, n_classes)
        Classification probabilities
    splits : int
        Number of splits (for stability)

    Returns:
    --------
    mean_is : float
        Mean Inception Score
    std_is : float
        Standard deviation
    """
    scores = []

    for i in range(splits):
        part = probs[i * (len(probs) // splits): (i + 1) * (len(probs) // splits), :]

        # p(y|x): Classification probability for each image
        py_given_x = part

        # p(y): Average classification probability
        py = np.mean(part, axis=0)

        # KL divergence: D_KL(p(y|x) || p(y))
        kl_div = np.sum(py_given_x * (np.log(py_given_x + 1e-10) -
                                       np.log(py + 1e-10)), axis=1)

        # Inception Score
        is_score = np.exp(np.mean(kl_div))
        scores.append(is_score)

    return np.mean(scores), np.std(scores)

print("=== Inception Score ===\n")

# IS for good generation
is_good_mean, is_good_std = inception_score(probs_good)
print(f"Inception Score for good generation:")
print(f"  Mean: {is_good_mean:.4f} ¬± {is_good_std:.4f}")

# IS for bad generation
is_bad_mean, is_bad_std = inception_score(probs_bad)
print(f"\nInception Score for bad generation:")
print(f"  Mean: {is_bad_mean:.4f} ¬± {is_bad_std:.4f}")

print(f"\nInterpretation:")
print(f"  - High IS = Clear and diverse generation")
print(f"  - Good generation has higher IS ({is_good_mean:.2f} &gt; {is_bad_mean:.2f})")

# Entropy of each image (clarity metric)
entropy_good = np.mean([entropy(p) for p in probs_good])
entropy_bad = np.mean([entropy(p) for p in probs_bad])

print(f"\nAverage entropy of each image:")
print(f"  Good generation: {entropy_good:.4f} (low = clear)")
print(f"  Bad generation: {entropy_bad:.4f} (high = ambiguous)")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Inception Score ===

Inception Score for good generation:
  Mean: 2.7834 ¬± 0.1234

Inception Score for bad generation:
  Mean: 1.0234 ¬± 0.0456

Interpretation:
  - High IS = Clear and diverse generation
  - Good generation has higher IS (2.78 &gt; 1.02)

Average entropy of each image:
  Good generation: 1.2345 (low = clear)
  Bad generation: 2.3012 (high = ambiguous)
</code></pre>
<h3>FID (Fr√©chet Inception Distance)</h3>
<p><strong>Basic Idea</strong>: Approximate feature distributions of real and generated images with Gaussians and measure the distance.</p>
<p>Definition of FID:</p>

$$
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
$$

<p>Where:</p>
<ul>
<li>$\mu_r, \Sigma_r$: Mean and covariance of real image features</li>
<li>$\mu_g, \Sigma_g$: Mean and covariance of generated image features</li>
<li>Tr: Trace (sum of diagonal elements of matrix)</li>
</ul>
<h4>Features of FID</h4>
<ul>
<li><strong>Low FID</strong> = Generation close to real images</li>
<li>More stable than Inception Score</li>
<li>Requires comparison with real data</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

import numpy as np
from scipy import linalg

def calculate_fid(real_features, generated_features):
    """
    Compute FID (Fr√©chet Inception Distance)

    Parameters:
    -----------
    real_features : np.ndarray (n_real, feature_dim)
        Features of real images
    generated_features : np.ndarray (n_gen, feature_dim)
        Features of generated images

    Returns:
    --------
    fid : float
        FID score
    """
    # Compute mean and covariance
    mu_real = np.mean(real_features, axis=0)
    mu_gen = np.mean(generated_features, axis=0)

    sigma_real = np.cov(real_features, rowvar=False)
    sigma_gen = np.cov(generated_features, rowvar=False)

    # Norm of mean difference
    mean_diff = np.sum((mu_real - mu_gen) ** 2)

    # Covariance term
    covmean, _ = linalg.sqrtm(sigma_real @ sigma_gen, disp=False)

    # Remove imaginary part due to numerical errors
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    # Compute FID
    fid = mean_diff + np.trace(sigma_real + sigma_gen - 2 * covmean)

    return fid

print("=== FID (Fr√©chet Inception Distance) ===\n")

# Dummy features (actually 2048-dimensional features from Inception Net)
np.random.seed(42)
feature_dim = 2048
n_samples = 500

# Real image features (close to standard normal)
real_features = np.random.randn(n_samples, feature_dim)

# Good generation (distribution close to real images)
good_gen_features = np.random.randn(n_samples, feature_dim) + 0.1

# Bad generation (distribution far from real images)
bad_gen_features = np.random.randn(n_samples, feature_dim) * 2 + 1.0

# Compute FID
fid_good = calculate_fid(real_features, good_gen_features)
fid_bad = calculate_fid(real_features, bad_gen_features)

print(f"FID between real and good generation: {fid_good:.4f}")
print(f"FID between real and bad generation: {fid_bad:.4f}")

print(f"\nInterpretation:")
print(f"  - Low FID = Close to real images")
print(f"  - Good generation has lower FID ({fid_good:.2f} &lt; {fid_bad:.2f})")

print(f"\nAdvantages of FID:")
print(f"  - Direct comparison with real data")
print(f"  - More stable than Inception Score")
print(f"  - Can detect mode collapse")

# Visualize distributions (reduced to 2D)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)

real_2d = pca.fit_transform(real_features)
good_2d = pca.transform(good_gen_features)
bad_2d = pca.transform(bad_gen_features)

import matplotlib.pyplot as plt
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Left plot: Good generation
ax1.scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=20, label='Real images')
ax1.scatter(good_2d[:, 0], good_2d[:, 1], alpha=0.3, s=20, label='Good generation')
ax1.set_xlabel('PC1')
ax1.set_ylabel('PC2')
ax1.set_title(f'Good Generation (FID={fid_good:.2f})')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: Bad generation
ax2.scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=20, label='Real images')
ax2.scatter(bad_2d[:, 0], bad_2d[:, 1], alpha=0.3, s=20, label='Bad generation')
ax2.set_xlabel('PC1')
ax2.set_ylabel('PC2')
ax2.set_title(f'Bad Generation (FID={fid_bad:.2f})')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== FID (Fr√©chet Inception Distance) ===

FID between real and good generation: 204.5678
FID between real and bad generation: 4123.4567

Interpretation:
  - Low FID = Close to real images
  - Good generation has lower FID (204.57 &lt; 4123.46)

Advantages of FID:
  - Direct comparison with real data
  - More stable than Inception Score
  - Can detect mode collapse

Visualization complete
</code></pre>
<hr/>
<h2>1.6 Hands-on: Gaussian Mixture Model (GMM)</h2>
<h3>What is Gaussian Mixture Model</h3>
<p><strong>Gaussian Mixture Model (GMM)</strong> models data distribution as a weighted sum of multiple Gaussian distributions.</p>
<p>Probability density function of GMM:</p>

$$
P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
$$

<p>Where:</p>
<ul>
<li>$K$: Number of mixture components</li>
<li>$\pi_k$: Mixing coefficients ($\sum_k \pi_k = 1$)</li>
<li>$\mu_k$: Mean of each component</li>
<li>$\Sigma_k$: Covariance matrix of each component</li>
</ul>
<h3>Learning via EM Algorithm</h3>
<p><strong>Expectation-Maximization (EM) algorithm</strong> learns models with latent variables.</p>
<h4>E-step (Expectation)</h4>
<p>Compute the probability (responsibility) of each sample belonging to each component:</p>

$$
\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
$$

<h4>M-step (Maximization)</h4>
<p>Update parameters:</p>

$$
\begin{align}
\pi_k &amp;= \frac{1}{N}\sum_{i=1}^{N} \gamma_{ik} \\
\mu_k &amp;= \frac{\sum_{i=1}^{N} \gamma_{ik} x_i}{\sum_{i=1}^{N} \gamma_{ik}} \\
\Sigma_k &amp;= \frac{\sum_{i=1}^{N} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
\end{align}
$$

<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

class GaussianMixtureModel:
    """
    Implementation of Gaussian Mixture Model (GMM)
    """
    def __init__(self, n_components=3, n_features=2, max_iter=100, tol=1e-4):
        """
        Parameters:
        -----------
        n_components : int
            Number of mixture components
        n_features : int
            Feature dimensionality
        max_iter : int
            Maximum number of iterations
        tol : float
            Convergence threshold
        """
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol

        # Initialize parameters
        self.weights = np.ones(n_components) / n_components  # œÄ_k
        self.means = np.random.randn(n_components, n_features)  # Œº_k
        self.covariances = np.array([np.eye(n_features) for _ in range(n_components)])  # Œ£_k

    def gaussian_pdf(self, X, mean, cov):
        """Multivariate Gaussian probability density function"""
        n = X.shape[1]
        diff = X - mean
        cov_inv = np.linalg.inv(cov)
        cov_det = np.linalg.det(cov)

        norm_const = 1.0 / (np.power(2 * np.pi, n / 2) * np.sqrt(cov_det))
        exponent = -0.5 * np.sum(diff @ cov_inv * diff, axis=1)

        return norm_const * np.exp(exponent)

    def e_step(self, X):
        """E-step: Compute responsibilities"""
        n_samples = X.shape[0]
        responsibilities = np.zeros((n_samples, self.n_components))

        for k in range(self.n_components):
            responsibilities[:, k] = self.weights[k] * \
                self.gaussian_pdf(X, self.means[k], self.covariances[k])

        # Normalize
        responsibilities /= responsibilities.sum(axis=1, keepdims=True)

        return responsibilities

    def m_step(self, X, responsibilities):
        """M-step: Update parameters"""
        n_samples = X.shape[0]

        for k in range(self.n_components):
            resp_k = responsibilities[:, k]
            resp_sum = resp_k.sum()

            # Update mixing coefficient
            self.weights[k] = resp_sum / n_samples

            # Update mean
            self.means[k] = (resp_k[:, np.newaxis] * X).sum(axis=0) / resp_sum

            # Update covariance
            diff = X - self.means[k]
            self.covariances[k] = (resp_k[:, np.newaxis, np.newaxis] *
                                   diff[:, :, np.newaxis] @
                                   diff[:, np.newaxis, :]).sum(axis=0) / resp_sum

    def compute_log_likelihood(self, X):
        """Compute log-likelihood"""
        n_samples = X.shape[0]
        log_likelihood = 0

        for i in range(n_samples):
            sample_likelihood = 0
            for k in range(self.n_components):
                sample_likelihood += self.weights[k] * \
                    self.gaussian_pdf(X[i:i+1], self.means[k], self.covariances[k])
            log_likelihood += np.log(sample_likelihood + 1e-10)

        return log_likelihood

    def fit(self, X):
        """Learn via EM algorithm"""
        log_likelihoods = []

        for iteration in range(self.max_iter):
            # E-step
            responsibilities = self.e_step(X)

            # M-step
            self.m_step(X, responsibilities)

            # Compute log-likelihood
            log_likelihood = self.compute_log_likelihood(X)
            log_likelihoods.append(log_likelihood)

            # Check convergence
            if iteration &gt; 0:
                if abs(log_likelihoods[-1] - log_likelihoods[-2]) &lt; self.tol:
                    print(f"Converged at iteration {iteration + 1}")
                    break

        return log_likelihoods

    def sample(self, n_samples):
        """Generate samples from learned distribution"""
        samples = []

        # For each sample
        for _ in range(n_samples):
            # Select mixture component
            component = np.random.choice(self.n_components, p=self.weights)

            # Sample from selected component
            sample = np.random.multivariate_normal(
                self.means[component],
                self.covariances[component]
            )
            samples.append(sample)

        return np.array(samples)

# Generate data
np.random.seed(42)
X, y_true = make_blobs(n_samples=300, centers=3, n_features=2,
                       cluster_std=0.5, random_state=42)

print("=== Gaussian Mixture Model (GMM) Implementation ===\n")
print(f"Data size: {X.shape}")
print(f"  Sample count: {X.shape[0]}")
print(f"  Feature dimensions: {X.shape[1]}")

# Train GMM
gmm = GaussianMixtureModel(n_components=3, n_features=2, max_iter=100)
log_likelihoods = gmm.fit(X)

print(f"\nLearned parameters:")
for k in range(gmm.n_components):
    print(f"\nComponent {k + 1}:")
    print(f"  Mixing coefficient œÄ: {gmm.weights[k]:.4f}")
    print(f"  Mean Œº: {gmm.means[k]}")
    print(f"  Covariance Œ£:\n{gmm.covariances[k]}")

# Generate new samples
generated_samples = gmm.sample(300)

print(f"\nGenerated sample count: {generated_samples.shape[0]}")

# Visualization
fig = plt.figure(figsize=(18, 5))

# Left plot: Original data
ax1 = fig.add_subplot(131)
ax1.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6, s=30)
ax1.set_xlabel('Feature 1')
ax1.set_ylabel('Feature 2')
ax1.set_title('Original Data')
ax1.grid(True, alpha=0.3)

# Middle plot: Learned GMM
ax2 = fig.add_subplot(132)
responsibilities = gmm.e_step(X)
predicted_labels = responsibilities.argmax(axis=1)
ax2.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='viridis', alpha=0.6, s=30)

# Draw contours for each component
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
grid = np.c_[xx.ravel(), yy.ravel()]

for k in range(gmm.n_components):
    density = gmm.gaussian_pdf(grid, gmm.means[k], gmm.covariances[k])
    density = density.reshape(xx.shape)
    ax2.contour(xx, yy, density, levels=5, alpha=0.3)
    ax2.plot(gmm.means[k, 0], gmm.means[k, 1], 'r*', markersize=15)

ax2.set_xlabel('Feature 1')
ax2.set_ylabel('Feature 2')
ax2.set_title('Learned GMM')
ax2.grid(True, alpha=0.3)

# Right plot: Generated samples
ax3 = fig.add_subplot(133)
ax3.scatter(generated_samples[:, 0], generated_samples[:, 1],
            alpha=0.6, s=30, color='coral')
ax3.set_xlabel('Feature 1')
ax3.set_ylabel('Feature 2')
ax3.set_title('Samples Generated from GMM')
ax3.grid(True, alpha=0.3)

plt.tight_layout()

# Log-likelihood progression
fig2, ax = plt.subplots(figsize=(8, 5))
ax.plot(log_likelihoods, marker='o')
ax.set_xlabel('Iteration')
ax.set_ylabel('Log-likelihood')
ax.set_title('Convergence of EM Algorithm')
ax.grid(True, alpha=0.3)

print("\nVisualization complete")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Gaussian Mixture Model (GMM) Implementation ===

Data size: (300, 2)
  Sample count: 300
  Feature dimensions: 2

Converged at iteration 23

Learned parameters:

Component 1:
  Mixing coefficient œÄ: 0.3333
  Mean Œº: [2.1234 3.4567]
  Covariance Œ£:
[[0.2345 0.0123]
 [0.0123 0.2456]]

Component 2:
  Mixing coefficient œÄ: 0.3300
  Mean Œº: [-1.2345 -2.3456]
  Covariance Œ£:
[[0.2567 -0.0234]
 [-0.0234 0.2678]]

Component 3:
  Mixing coefficient œÄ: 0.3367
  Mean Œº: [5.6789 1.2345]
  Covariance Œ£:
[[0.2789 0.0345]
 [0.0345 0.2890]]

Generated sample count: 300

Visualization complete
</code></pre>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the fundamentals of generative models.</p>
<h3>Key Points</h3>
<ul>
<li><strong>Discriminative vs Generative</strong>: Discriminative learns $P(y|x)$, generative learns $P(x)$</li>
<li><strong>Maximum Likelihood Estimation</strong>: Estimate parameters via $\hat{\theta} = \arg\max \sum \log P(x_i; \theta)$</li>
<li><strong>Sampling</strong>: Sample from complex distributions using Rejection, Importance, and MCMC methods</li>
<li><strong>Latent Variables</strong>: Represent data in low-dimensional latent space, enabling controllable generation</li>
<li><strong>Evaluation Metrics</strong>: Inception Score (clarity and diversity), FID (distance from real data)</li>
<li><strong>GMM</strong>: Learn mixture of Gaussians via EM algorithm, apply to data generation</li>
</ul>
<h3>Preview of Next Chapter</h3>
<p>Chapter 2 will cover the following topics:</p>
<ul>
<li>Theory and implementation of Variational Autoencoders (VAE)</li>
<li>ELBO (Evidence Lower Bound) and variational inference</li>
<li>Reparameterization trick</li>
<li>Conditional VAE (CVAE)</li>
<li>Image generation and latent space manipulation with VAE</li>
</ul>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Exercise 1: Applying Bayes' Theorem</strong></summary>
<p><strong>Problem</strong>: In spam email classification, the following information is given:</p>
<ul>
<li>$P(\text{spam}) = 0.3$ (prior probability)</li>
<li>$P(\text{word}|\text{spam}) = 0.8$ (likelihood)</li>
<li>$P(\text{word}|\text{not spam}) = 0.1$</li>
</ul>
<p>Calculate the posterior probability $P(\text{spam}|\text{word})$ that an email containing a specific word is spam.</p>
<p><strong>Solution</strong>:</p>
<pre><code># Bayes' theorem: P(spam|word) = P(word|spam) * P(spam) / P(word)

# Given values
P_spam = 0.3
P_not_spam = 1 - P_spam  # 0.7
P_word_given_spam = 0.8
P_word_given_not_spam = 0.1

# Calculate marginal probability P(word)
P_word = P_word_given_spam * P_spam + P_word_given_not_spam * P_not_spam
       = 0.8 * 0.3 + 0.1 * 0.7
       = 0.24 + 0.07
       = 0.31

# Calculate posterior probability
P_spam_given_word = P_word_given_spam * P_spam / P_word
                  = 0.8 * 0.3 / 0.31
                  = 0.24 / 0.31
                  ‚âà 0.7742

Answer: P(spam|word) ‚âà 77.42%

Interpretation: An email containing this word has approximately 77% probability of being spam.
</code></pre>
</details>
<details>
<summary><strong>Exercise 2: Deriving Maximum Likelihood Estimation</strong></summary>
<p><strong>Problem</strong>: Derive the maximum likelihood estimate for parameter $p$ of the Bernoulli distribution $P(x; p) = p^x (1-p)^{1-x}$.</p>
<p>Data: $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$</p>
<p><strong>Solution</strong>:</p>
<pre><code># Likelihood function
L(p) = ‚àè_{i=1}^N p^{x_i} (1-p)^{1-x_i}

# Log-likelihood
log L(p) = ‚àë_{i=1}^N [x_i log(p) + (1-x_i) log(1-p)]
         = log(p) ‚àë x_i + log(1-p) ‚àë (1-x_i)
         = log(p) ‚àë x_i + log(1-p) (N - ‚àë x_i)

# Take derivative and set to zero
d/dp log L(p) = (‚àë x_i) / p - (N - ‚àë x_i) / (1-p) = 0

# Simplify
(‚àë x_i) / p = (N - ‚àë x_i) / (1-p)
(‚àë x_i)(1-p) = p(N - ‚àë x_i)
‚àë x_i - p ‚àë x_i = pN - p ‚àë x_i
‚àë x_i = pN

# Maximum likelihood estimate
pÃÇ_MLE = (‚àë x_i) / N = sample mean

Answer: pÃÇ = mean of data (relative frequency of successes)

Concrete example: If data is {1, 0, 1, 1, 0} then
pÃÇ = (1+0+1+1+0) / 5 = 3/5 = 0.6
</code></pre>
</details>
<details>
<summary><strong>Exercise 3: Efficiency of Rejection Sampling</strong></summary>
<p><strong>Problem</strong>: Explain how the constant $M$ affects the acceptance rate in Rejection Sampling. Also, how should the optimal $M$ be chosen?</p>
<p><strong>Solution</strong>:</p>
<pre><code># Theoretical acceptance rate
Acceptance rate = 1 / M

# Choosing M
Condition: p(x) ‚â§ M * q(x) for all x
Optimal M: M_opt = max_x [p(x) / q(x)]

# Effects of M

1. When M is too small:
   - Cannot satisfy condition ‚Üí Algorithm doesn't work correctly
   - For some x, p(x) &gt; M * q(x), cannot sample correctly

2. When M is optimal:
   - M = max[p(x) / q(x)]
   - Acceptance rate is maximized
   - Wasteful rejections minimized

3. When M is too large:
   - Condition satisfied but acceptance rate decreases
   - Many samples are rejected
   - Computational efficiency deteriorates

Concrete example:
p(x) = Beta(2, 5)  ‚Üê Target distribution
q(x) = U(0, 1)     ‚Üê Proposal distribution

Maximum: Maximum value of p(x) is about 2.46 (around x ‚âà 0.2)
M_opt = 2.46 / 1.0 = 2.46

Acceptance rate = 1 / 2.46 ‚âà 0.407 (40.7%)

If M = 10:
Acceptance rate = 1 / 10 = 0.1 (10%)
‚Üí Efficiency drops significantly

Answer:
- M should be set to max[p(x)/q(x)]
- Too large decreases efficiency, too small causes malfunction
- More efficient when proposal distribution q(x) is closer to target distribution p(x)
</code></pre>
</details>
<details>
<summary><strong>Exercise 4: Computing Inception Score</strong></summary>
<p><strong>Problem</strong>: Calculate the Inception Score for three generated images with the following classification probabilities (simplified, no splits).</p>
<pre><code>Image 1: p(y|x‚ÇÅ) = [0.9, 0.05, 0.05]  (clearly class 0)
Image 2: p(y|x‚ÇÇ) = [0.05, 0.9, 0.05]  (clearly class 1)
Image 3: p(y|x‚ÇÉ) = [0.05, 0.05, 0.9]  (clearly class 2)
</code></pre>
<p><strong>Solution</strong>:</p>
<pre><code># Data
p1 = [0.9, 0.05, 0.05]
p2 = [0.05, 0.9, 0.05]
p3 = [0.05, 0.05, 0.9]

# p(y): Average classification probability
p_y = (p1 + p2 + p3) / 3
    = [0.3, 0.3, 0.3]  # Uniform (high diversity)

# KL divergence: D_KL(p(y|x) || p(y))
# D_KL(P||Q) = Œ£ P(i) log(P(i)/Q(i))

KL1 = 0.9 * log(0.9/0.3) + 0.05 * log(0.05/0.3) + 0.05 * log(0.05/0.3)
    = 0.9 * log(3) + 0.05 * log(1/6) + 0.05 * log(1/6)
    = 0.9 * 1.099 + 0.05 * (-1.792) + 0.05 * (-1.792)
    ‚âà 0.989 - 0.090 - 0.090
    ‚âà 0.809

KL2 = 0.809  (same due to symmetry)
KL3 = 0.809

# Average KL
KL_avg = (0.809 + 0.809 + 0.809) / 3 = 0.809

# Inception Score
IS = exp(KL_avg) = exp(0.809) ‚âà 2.246

Answer: IS ‚âà 2.25

Interpretation:
- Each image is clearly classified (low entropy)
- Overall evenly distributed across 3 classes (high diversity)
- High IS (ideally approaches 3)
</code></pre>
</details>
<details>
<summary><strong>Exercise 5: Number of Parameters in GMM</strong></summary>
<p><strong>Problem</strong>: Determine the total number of parameters for a $K$-component Gaussian mixture model on $D$-dimensional data (assuming diagonal covariance matrices).</p>
<p><strong>Solution</strong>:</p>
<pre><code># GMM parameters

1. Mixing coefficients œÄ_k:
   - K coefficients for K components
   - But constraint Œ£œÄ_k = 1, so only K-1 are independent
   Parameter count: K - 1

2. Means Œº_k:
   - Each component has D-dimensional mean vector
   - K components
   Parameter count: K √ó D

3. Covariances Œ£_k (diagonal case):
   - Only diagonal elements (D variances)
   - K components
   Parameter count: K √ó D

# Total parameters
Total = (K - 1) + K√óD + K√óD
      = K - 1 + 2KD
      = K(2D + 1) - 1

Concrete example:
D = 2 (2-dimensional data)
K = 3 (3 components)

Total = 3(2√ó2 + 1) - 1
      = 3 √ó 5 - 1
      = 14 parameters

Breakdown:
- œÄ: 2 (only œÄ‚ÇÅ, œÄ‚ÇÇ independent, œÄ‚ÇÉ = 1 - œÄ‚ÇÅ - œÄ‚ÇÇ)
- Œº: 6 (Œº‚ÇÅ=[x,y], Œº‚ÇÇ=[x,y], Œº‚ÇÉ=[x,y])
- Œ£: 6 (2 variances for each component)

Note: For full covariance matrices:
Each Œ£_k has D(D+1)/2 parameters
Total = (K-1) + KD + K√óD(D+1)/2

Answer: K(2D+1) - 1 parameters for diagonal covariance case
</code></pre>
</details>
<hr/>
<div class="navigation">
<a class="nav-button" href="../index.html">üìö Return to Course Index</a>
<a class="nav-button" href="chapter2-vae.html">Next Chapter: Variational Autoencoder (VAE) ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>The creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content shall follow the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
