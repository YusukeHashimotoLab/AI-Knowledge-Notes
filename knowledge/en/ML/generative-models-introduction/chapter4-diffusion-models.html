<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Diffusion Models - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/generative-models-introduction/index.html">Generative Models</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav>
<header>
<div class="header-content">
<h1>Chapter 4: Diffusion Models</h1>
<p class="subtitle">Generation from Noise: Theory and Practice of Diffusion Models, Leading to Stable Diffusion</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 32 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 6</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the fundamental principles of diffusion models (Forward/Reverse Process)</li>
<li>‚úÖ Understand the mathematical formulation of DDPM (Denoising Diffusion Probabilistic Models)</li>
<li>‚úÖ Implement noise schedules and sampling algorithms</li>
<li>‚úÖ Master the structure and training methods of U-Net Denoisers</li>
<li>‚úÖ Understand the mechanisms of Latent Diffusion Models (Stable Diffusion)</li>
<li>‚úÖ Implement CLIP Guidance and text-conditioned generation</li>
<li>‚úÖ Build practical image generation systems with PyTorch</li>
</ul>
<hr/>
<h2>4.1 Fundamentals of Diffusion Models</h2>
<h3>4.1.1 What are Diffusion Models?</h3>
<p><strong>Diffusion Models</strong> are generative models that learn two processes: the Forward Process, which gradually adds noise to data, and the Reverse Process, which restores the original data from noise. Since entering the 2020s, they have achieved state-of-the-art performance in image generation and become the foundation technology for Stable Diffusion, DALL-E 2, Imagen, and others.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>GAN</th>
<th>VAE</th>
<th>Diffusion Models</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generation Method</strong></td>
<td>Adversarial Learning</td>
<td>Variational Inference</td>
<td>Denoising</td>
</tr>
<tr>
<td><strong>Training Stability</strong></td>
<td>Low (Mode Collapse)</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>Generation Quality</strong></td>
<td>High (When Trained)</td>
<td>Medium (Blurry)</td>
<td>Very High</td>
</tr>
<tr>
<td><strong>Diversity</strong></td>
<td>Low (Mode Collapse)</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Low to Medium</td>
<td>Low to Medium</td>
<td>High (Iterative Process)</td>
</tr>
<tr>
<td><strong>Representative Models</strong></td>
<td>StyleGAN</td>
<td>Œ≤-VAE</td>
<td>DDPM, Stable Diffusion</td>
</tr>
</tbody>
</table>
<h3>4.1.2 Forward Process: Adding Noise</h3>
<p>The Forward Process is a procedure that gradually adds Gaussian noise to the original image $x_0$ over $T$ steps.</p>

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

<p>Where:</p>
<ul>
<li>$x_t$: Image at timestep $t$</li>
<li>$\beta_t$: Noise schedule (typically 0.0001 to 0.02)</li>
<li>$\mathcal{N}$: Gaussian distribution</li>
</ul>
<p><strong>Important property</strong>: Using the reparameterization trick, we can directly sample the image at any step $t$:</p>

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon
$$

<p>Where:</p>
<ul>
<li>$\alpha_t = 1 - \beta_t$</li>
<li>$\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$</li>
<li>$\epsilon \sim \mathcal{N}(0, I)$</li>
</ul>
<div class="mermaid">
graph LR
    X0["x‚ÇÄ<br/>(Original Image)"] --&gt;|"+ Noise Œ≤‚ÇÅ"| X1["x‚ÇÅ"]
    X1 --&gt;|"+ Noise Œ≤‚ÇÇ"| X2["x‚ÇÇ"]
    X2 --&gt;|"..."| X3["..."]
    X3 --&gt;|"+ Noise Œ≤T"| XT["xT<br/>(Pure Noise)"]

    style X0 fill:#27ae60,color:#fff
    style XT fill:#e74c3c,color:#fff
    style X1 fill:#f39c12,color:#fff
    style X2 fill:#e67e22,color:#fff
</div>
<h3>4.1.3 Reverse Process: Generation through Denoising</h3>
<p>The Reverse Process starts from pure noise $x_T \sim \mathcal{N}(0, I)$ and gradually removes noise to restore the original image.</p>

$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

<p>Here, we learn $\mu_\theta$ (mean) and $\Sigma_\theta$ (covariance) using neural networks. In DDPM, it's common to simplify by fixing the covariance and learning only the mean.</p>
<blockquote>
<p><strong>Important</strong>: The Reverse Process is formulated as a task of predicting noise $\epsilon$. This allows the network to function as a "Denoiser".</p>
</blockquote>
<h3>4.1.4 Intuitive Understanding of Diffusion Models</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Visualizing diffusion process with simple 1D data
np.random.seed(42)

# Original data: mixture of two Gaussians
def sample_data(n=1000):
    """Generate data with two modes"""
    mode1 = np.random.randn(n//2) * 0.5 + 2
    mode2 = np.random.randn(n//2) * 0.5 - 2
    return np.concatenate([mode1, mode2])

# Forward diffusion process
def forward_diffusion(x0, num_steps=50):
    """Forward diffusion: Add noise to data"""
    # Linear noise schedule
    betas = np.linspace(0.0001, 0.02, num_steps)
    alphas = 1 - betas
    alphas_cumprod = np.cumprod(alphas)

    # Save data at each timestep
    x_history = [x0]

    for t in range(1, num_steps):
        noise = np.random.randn(*x0.shape)
        x_t = np.sqrt(alphas_cumprod[t]) * x0 + np.sqrt(1 - alphas_cumprod[t]) * noise
        x_history.append(x_t)

    return x_history, betas, alphas_cumprod

# Demonstration
print("=== Forward Diffusion Process Visualization ===\n")

x0 = sample_data(1000)
x_history, betas, alphas_cumprod = forward_diffusion(x0, num_steps=50)

# Visualization
fig, axes = plt.subplots(2, 5, figsize=(18, 7))
axes = axes.flatten()

timesteps_to_show = [0, 5, 10, 15, 20, 25, 30, 35, 40, 49]

for idx, t in enumerate(timesteps_to_show):
    ax = axes[idx]
    ax.hist(x_history[t], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlim(-8, 8)
    ax.set_ylim(0, 0.5)
    ax.set_title(f't = {t}\nŒ±ÃÖ = {alphas_cumprod[t]:.4f}' if t &gt; 0 else f't = 0 (Original)',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('x', fontsize=9)
    ax.set_ylabel('Density', fontsize=9)
    ax.grid(alpha=0.3)

plt.suptitle('Forward Diffusion Process: Original Data ‚Üí Gaussian Noise',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nCharacteristics:")
print("‚úì t = 0: Original bimodal distribution (clear structure)")
print("‚úì t = 10-20: Structure gradually degrades")
print("‚úì t = 49: Nearly standard Gaussian distribution (structure completely lost)")
print("\nReverse Process:")
print("‚úì Start from noise (t=49) and gradually restore structure")
print("‚úì Remove noise at each step using learned Denoiser")
print("‚úì Finally reproduce the original bimodal distribution")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Forward Diffusion Process Visualization ===

Characteristics:
‚úì t = 0: Original bimodal distribution (clear structure)
‚úì t = 10-20: Structure gradually degrades
‚úì t = 49: Nearly standard Gaussian distribution (structure completely lost)

Reverse Process:
‚úì Start from noise (t=49) and gradually restore structure
‚úì Remove noise at each step using learned Denoiser
‚úì Finally reproduce the original bimodal distribution
</code></pre>
<hr/>
<h2>4.2 DDPM (Denoising Diffusion Probabilistic Models)</h2>
<h3>4.2.1 Mathematical Formulation of DDPM</h3>
<p>DDPM is a representative diffusion model method proposed by Ho et al. (UC Berkeley) in 2020.</p>
<h4>Training Objective</h4>
<p>The DDPM loss function is derived from the variational lower bound (ELBO), but takes a simple form in practice:</p>

$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

<p>This is the mean squared error of the task of "predicting noise $\epsilon$".</p>
<h4>Algorithm Details</h4>
<p><strong>Training Algorithm</strong>:</p>
<ol>
<li>Sample $x_0$ from training data</li>
<li>Sample timestep $t \sim \text{Uniform}(1, T)$</li>
<li>Sample noise $\epsilon \sim \mathcal{N}(0, I)$</li>
<li>Compute $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$</li>
<li>Minimize loss $\| \epsilon - \epsilon_\theta(x_t, t) \|^2$</li>
</ol>
<p><strong>Sampling Algorithm</strong>:</p>
<ol>
<li>Start from $x_T \sim \mathcal{N}(0, I)$</li>
<li>For $t = T, T-1, \ldots, 1$:
    $$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z$$
    where $z \sim \mathcal{N}(0, I)$ (if $t &gt; 1$)
</li>
<li>Return $x_0$</li>
</ol>
<h3>4.2.2 Noise Schedules</h3>
<p>The design of the noise schedule $\beta_t$ significantly affects generation quality.</p>
<table>
<thead>
<tr>
<th>Schedule</th>
<th>Definition</th>
<th>Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Linear</strong></td>
<td>$\beta_t = \beta_{\min} + \frac{t}{T}(\beta_{\max} - \beta_{\min})$</td>
<td>Simple, used in original paper</td>
</tr>
<tr>
<td><strong>Cosine</strong></td>
<td>$\bar{\alpha}_t = \frac{f(t)}{f(0)}$, $f(t) = \cos^2\left(\frac{t/T + s}{1+s} \cdot \frac{\pi}{2}\right)$</td>
<td>Smoother noise transition</td>
</tr>
<tr>
<td><strong>Quadratic</strong></td>
<td>$\beta_t = \beta_{\min}^2 + t^2 (\beta_{\max}^2 - \beta_{\min}^2)$</td>
<td>Non-linear transition</td>
</tr>
</tbody>
</table>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Linear noise schedule"""
    return np.linspace(beta_start, beta_end, timesteps)

def cosine_beta_schedule(timesteps, s=0.008):
    """Cosine noise schedule (Improved DDPM)"""
    steps = timesteps + 1
    x = np.linspace(0, timesteps, steps)
    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return np.clip(betas, 0, 0.999)

def quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Quadratic noise schedule"""
    return np.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2

# Visualization
print("=== Noise Schedule Comparison ===\n")

timesteps = 1000

linear_betas = linear_beta_schedule(timesteps)
cosine_betas = cosine_beta_schedule(timesteps)
quadratic_betas = quadratic_beta_schedule(timesteps)

# Calculate cumulative product of alphas
def compute_alphas_cumprod(betas):
    alphas = 1 - betas
    return np.cumprod(alphas)

linear_alphas = compute_alphas_cumprod(linear_betas)
cosine_alphas = compute_alphas_cumprod(cosine_betas)
quadratic_alphas = compute_alphas_cumprod(quadratic_betas)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Beta values
ax1 = axes[0]
ax1.plot(linear_betas, label='Linear', linewidth=2, alpha=0.8)
ax1.plot(cosine_betas, label='Cosine', linewidth=2, alpha=0.8)
ax1.plot(quadratic_betas, label='Quadratic', linewidth=2, alpha=0.8)
ax1.set_xlabel('Timestep t', fontsize=12, fontweight='bold')
ax1.set_ylabel('Œ≤‚Çú (Noise Level)', fontsize=12, fontweight='bold')
ax1.set_title('Noise Schedules: Œ≤‚Çú', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Right: Cumulative alpha
ax2 = axes[1]
ax2.plot(linear_alphas, label='Linear', linewidth=2, alpha=0.8)
ax2.plot(cosine_alphas, label='Cosine', linewidth=2, alpha=0.8)
ax2.plot(quadratic_alphas, label='Quadratic', linewidth=2, alpha=0.8)
ax2.set_xlabel('Timestep t', fontsize=12, fontweight='bold')
ax2.set_ylabel('·æ±‚Çú (Signal Strength)', fontsize=12, fontweight='bold')
ax2.set_title('Cumulative Product: ·æ±‚Çú = ‚àè Œ±‚Çõ', fontsize=13, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nSchedule Characteristics:")
print(f"Linear   - Œ≤ range: [{linear_betas.min():.6f}, {linear_betas.max():.6f}]")
print(f"Cosine   - Œ≤ range: [{cosine_betas.min():.6f}, {cosine_betas.max():.6f}]")
print(f"Quadratic- Œ≤ range: [{quadratic_betas.min():.6f}, {quadratic_betas.max():.6f}]")
print(f"\nFinal ·æ±_T (signal retention rate):")
print(f"Linear:    {linear_alphas[-1]:.6f}")
print(f"Cosine:    {cosine_alphas[-1]:.6f}")
print(f"Quadratic: {quadratic_alphas[-1]:.6f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Noise Schedule Comparison ===

Schedule Characteristics:
Linear   - Œ≤ range: [0.000100, 0.020000]
Cosine   - Œ≤ range: [0.000020, 0.999000]
Quadratic- Œ≤ range: [0.000000, 0.000400]

Final ·æ±_T (signal retention rate):
Linear:    0.000062
Cosine:    0.000000
Quadratic: 0.670320
</code></pre>
<h3>4.2.3 DDPM Training Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class DDPMDiffusion:
    """DDPM diffusion process implementation"""

    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, schedule='linear'):
        """
        Args:
            timesteps: Number of diffusion steps
            beta_start: Starting noise level
            beta_end: Ending noise level
            schedule: 'linear', 'cosine', 'quadratic'
        """
        self.timesteps = timesteps

        # Noise schedule
        if schedule == 'linear':
            self.betas = torch.linspace(beta_start, beta_end, timesteps)
        elif schedule == 'cosine':
            self.betas = self._cosine_beta_schedule(timesteps)
        elif schedule == 'quadratic':
            self.betas = torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2

        # Alpha calculations
        self.alphas = 1 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)

        # Coefficients for sampling
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)

        # Posterior variance
        self.posterior_variance = self.betas * (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod)

    def _cosine_beta_schedule(self, timesteps, s=0.008):
        """Cosine schedule"""
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0, 0.999)

    def q_sample(self, x_start, t, noise=None):
        """
        Forward diffusion: Sample x_t directly from x_0

        Args:
            x_start: [B, C, H, W] Original image
            t: [B] Timestep
            noise: Noise (generated if None)

        Returns:
            x_t: Noised image
        """
        if noise is None:
            noise = torch.randn_like(x_start)

        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)
        sqrt_one_minus_alphas_cumprod_t = self._extract(
            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape
        )

        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise

    def p_losses(self, denoise_model, x_start, t, noise=None):
        """
        Calculate training loss

        Args:
            denoise_model: Noise prediction model
            x_start: Original image
            t: Timestep
            noise: Noise (generated if None)

        Returns:
            loss: MSE loss
        """
        if noise is None:
            noise = torch.randn_like(x_start)

        # Add noise
        x_noisy = self.q_sample(x_start, t, noise)

        # Predict noise
        predicted_noise = denoise_model(x_noisy, t)

        # MSE loss
        loss = F.mse_loss(predicted_noise, noise)

        return loss

    @torch.no_grad()
    def p_sample(self, model, x, t, t_index):
        """
        Reverse process: Sample x_{t-1} from x_t

        Args:
            model: Noise prediction model
            x: Current image x_t
            t: Timestep
            t_index: Index (for variance calculation)

        Returns:
            x_{t-1}
        """
        betas_t = self._extract(self.betas, t, x.shape)
        sqrt_one_minus_alphas_cumprod_t = self._extract(
            self.sqrt_one_minus_alphas_cumprod, t, x.shape
        )
        sqrt_recip_alphas_t = self._extract(self.sqrt_recip_alphas, t, x.shape)

        # Predict noise
        predicted_noise = model(x, t)

        # Calculate mean
        model_mean = sqrt_recip_alphas_t * (
            x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t
        )

        if t_index == 0:
            return model_mean
        else:
            posterior_variance_t = self._extract(self.posterior_variance, t, x.shape)
            noise = torch.randn_like(x)
            return model_mean + torch.sqrt(posterior_variance_t) * noise

    @torch.no_grad()
    def p_sample_loop(self, model, shape):
        """
        Complete sampling loop: Generate image from noise

        Args:
            model: Noise prediction model
            shape: Shape of generated image [B, C, H, W]

        Returns:
            Generated image
        """
        device = next(model.parameters()).device

        # Start from pure noise
        img = torch.randn(shape, device=device)

        # Sample in reverse
        for i in reversed(range(0, self.timesteps)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            img = self.p_sample(model, img, t, i)

        return img

    def _extract(self, a, t, x_shape):
        """Extract coefficients and adjust shape"""
        batch_size = t.shape[0]
        out = a.gather(-1, t.cpu())
        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)


# Demonstration
print("=== DDPM Diffusion Process Demo ===\n")

diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

# Dummy data
batch_size = 4
channels = 3
img_size = 32
x_start = torch.randn(batch_size, channels, img_size, img_size)

print(f"Original image shape: {x_start.shape}")

# Adding noise at different timesteps
timesteps_to_test = [0, 100, 300, 500, 700, 999]

print("\nForward Diffusion at Different Timesteps:")
print(f"{'Timestep':&lt;12} {'·æ±_t':&lt;12} {'Signal %':&lt;12} {'Noise %':&lt;12}")
print("-" * 50)

for t in timesteps_to_test:
    t_tensor = torch.full((batch_size,), t, dtype=torch.long)
    x_noisy = diffusion.q_sample(x_start, t_tensor)

    alpha_t = diffusion.alphas_cumprod[t].item()
    signal_strength = alpha_t * 100
    noise_strength = (1 - alpha_t) * 100

    print(f"{t:&lt;12} {alpha_t:&lt;12.6f} {signal_strength:&lt;12.2f} {noise_strength:&lt;12.2f}")

print("\n‚úì DDPM implementation complete")
print("‚úì Forward/Reverse process defined")
print("‚úì Training loss function implemented")
print("‚úì Sampling algorithm implemented")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== DDPM Diffusion Process Demo ===

Original image shape: torch.Size([4, 3, 32, 32])

Forward Diffusion at Different Timesteps:
Timestep     ·æ±_t          Signal %     Noise %
--------------------------------------------------
0            1.000000     100.00       0.00
100          0.793469     79.35        20.65
300          0.419308     41.93        58.07
500          0.170726     17.07        82.93
700          0.049806     4.98         95.02
999          0.000062     0.01         99.99

‚úì DDPM implementation complete
‚úì Forward/Reverse process defined
‚úì Training loss function implemented
‚úì Sampling algorithm implemented
</code></pre>
<hr/>
<h2>4.3 U-Net Denoiser Implementation</h2>
<h3>4.3.1 U-Net Architecture</h3>
<p>For noise prediction in diffusion models, <strong>U-Net</strong> is widely used. U-Net is an architecture with an encoder-decoder structure and skip connections.</p>
<div class="mermaid">
graph TB
    subgraph "U-Net for Diffusion Models"
        Input["Input: x_t + Timestep Embedding"]

        Down1["Down Block 1<br/>Conv + Attention"]
        Down2["Down Block 2<br/>Conv + Attention"]
        Down3["Down Block 3<br/>Conv + Attention"]

        Bottleneck["Bottleneck<br/>Attention"]

        Up1["Up Block 1<br/>Conv + Attention"]
        Up2["Up Block 2<br/>Conv + Attention"]
        Up3["Up Block 3<br/>Conv + Attention"]

        Output["Output: Predicted Noise Œµ"]

        Input --&gt; Down1
        Down1 --&gt; Down2
        Down2 --&gt; Down3
        Down3 --&gt; Bottleneck
        Bottleneck --&gt; Up1
        Up1 --&gt; Up2
        Up2 --&gt; Up3
        Up3 --&gt; Output

        Down1 -.Skip.-&gt; Up3
        Down2 -.Skip.-&gt; Up2
        Down3 -.Skip.-&gt; Up1

        style Input fill:#7b2cbf,color:#fff
        style Output fill:#27ae60,color:#fff
        style Bottleneck fill:#e74c3c,color:#fff
    end
</div>
<h3>4.3.2 Time Embedding</h3>
<p>Timestep $t$ is encoded using Sinusoidal Positional Encoding (same as in Transformers):</p>

$$
\text{PE}(t, 2i) = \sin\left(\frac{t}{10000^{2i/d}}\right)
$$
$$
\text{PE}(t, 2i+1) = \cos\left(\frac{t}{10000^{2i/d}}\right)
$$

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class SinusoidalPositionEmbeddings(nn.Module):
    """Sinusoidal time embeddings for diffusion models"""

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        """
        Args:
            time: [B] Timestep

        Returns:
            embeddings: [B, dim] Time embeddings
        """
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


class TimeEmbeddingMLP(nn.Module):
    """Transform time embeddings with MLP"""

    def __init__(self, time_dim, emb_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(time_dim, emb_dim),
            nn.SiLU(),
            nn.Linear(emb_dim, emb_dim)
        )

    def forward(self, t_emb):
        return self.mlp(t_emb)


# Demonstration
print("=== Time Embedding Demo ===\n")

time_dim = 128
batch_size = 8
timesteps = torch.randint(0, 1000, (batch_size,))

time_embedder = SinusoidalPositionEmbeddings(time_dim)
time_mlp = TimeEmbeddingMLP(time_dim, 256)

t_emb = time_embedder(timesteps)
t_emb_transformed = time_mlp(t_emb)

print(f"Timesteps: {timesteps.numpy()}")
print(f"\nSinusoidal Embedding shape: {t_emb.shape}")
print(f"MLP Transformed shape: {t_emb_transformed.shape}")

# Embedding visualization
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Sinusoidal patterns
ax1 = axes[0]
t_range = torch.arange(0, 1000, 10)
embeddings = time_embedder(t_range).detach().numpy()

sns.heatmap(embeddings[:, :64].T, cmap='RdBu_r', center=0, ax=ax1, cbar_kws={'label': 'Value'})
ax1.set_xlabel('Timestep', fontsize=12, fontweight='bold')
ax1.set_ylabel('Embedding Dimension', fontsize=12, fontweight='bold')
ax1.set_title('Sinusoidal Time Embeddings (first 64 dims)', fontsize=13, fontweight='bold')

# Right: Embedding similarity
ax2 = axes[1]
sample_timesteps = torch.tensor([0, 100, 300, 500, 700, 999])
sample_embs = time_embedder(sample_timesteps).detach()
similarity = torch.mm(sample_embs, sample_embs.T)

sns.heatmap(similarity.numpy(), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax2,
            xticklabels=sample_timesteps.numpy(), yticklabels=sample_timesteps.numpy(),
            cbar_kws={'label': 'Cosine Similarity'})
ax2.set_xlabel('Timestep', fontsize=12, fontweight='bold')
ax2.set_ylabel('Timestep', fontsize=12, fontweight='bold')
ax2.set_title('Time Embedding Similarity Matrix', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nCharacteristics:")
print("‚úì Each timestep has a unique vector representation")
print("‚úì Consecutive timesteps have similar embeddings")
print("‚úì Network can leverage timestep information")
</code></pre>
<h3>4.3.3 Simplified U-Net Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """ResNet-style residual block"""

    def __init__(self, in_channels, out_channels, time_emb_dim):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)

        # Time embedding projection
        self.time_mlp = nn.Linear(time_emb_dim, out_channels)

        # Residual connection
        if in_channels != out_channels:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.residual_conv = nn.Identity()

        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)

    def forward(self, x, t_emb):
        """
        Args:
            x: [B, C, H, W]
            t_emb: [B, time_emb_dim]
        """
        residue = x

        # First conv
        x = self.conv1(x)
        x = self.norm1(x)

        # Add time embedding
        t = self.time_mlp(F.silu(t_emb))
        x = x + t[:, :, None, None]
        x = F.silu(x)

        # Second conv
        x = self.conv2(x)
        x = self.norm2(x)
        x = F.silu(x)

        # Residual
        return x + self.residual_conv(residue)


class SimpleUNet(nn.Module):
    """Simplified U-Net for Diffusion"""

    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=256,
                 base_channels=64):
        super().__init__()

        # Time embedding
        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(time_emb_dim),
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.SiLU()
        )

        # Encoder
        self.down1 = ResidualBlock(in_channels, base_channels, time_emb_dim)
        self.down2 = ResidualBlock(base_channels, base_channels * 2, time_emb_dim)
        self.down3 = ResidualBlock(base_channels * 2, base_channels * 4, time_emb_dim)

        self.pool = nn.MaxPool2d(2)

        # Bottleneck
        self.bottleneck = ResidualBlock(base_channels * 4, base_channels * 4, time_emb_dim)

        # Decoder
        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 4, 2, 2)
        self.up_block1 = ResidualBlock(base_channels * 8, base_channels * 2, time_emb_dim)

        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 2, 2)
        self.up_block2 = ResidualBlock(base_channels * 4, base_channels, time_emb_dim)

        self.up3 = nn.ConvTranspose2d(base_channels, base_channels, 2, 2)
        self.up_block3 = ResidualBlock(base_channels * 2, base_channels, time_emb_dim)

        # Output
        self.out = nn.Conv2d(base_channels, out_channels, 1)

    def forward(self, x, t):
        """
        Args:
            x: [B, C, H, W] Noisy image
            t: [B] Timestep

        Returns:
            predicted_noise: [B, C, H, W]
        """
        # Time embedding
        t_emb = self.time_mlp(t)

        # Encoder with skip connections
        d1 = self.down1(x, t_emb)
        d2 = self.down2(self.pool(d1), t_emb)
        d3 = self.down3(self.pool(d2), t_emb)

        # Bottleneck
        b = self.bottleneck(self.pool(d3), t_emb)

        # Decoder with skip connections
        u1 = self.up1(b)
        u1 = torch.cat([u1, d3], dim=1)
        u1 = self.up_block1(u1, t_emb)

        u2 = self.up2(u1)
        u2 = torch.cat([u2, d2], dim=1)
        u2 = self.up_block2(u2, t_emb)

        u3 = self.up3(u2)
        u3 = torch.cat([u3, d1], dim=1)
        u3 = self.up_block3(u3, t_emb)

        # Output
        return self.out(u3)


# Demonstration
print("=== U-Net Denoiser Demo ===\n")

model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=256, base_channels=64)

# Dummy input
batch_size = 2
x = torch.randn(batch_size, 3, 32, 32)
t = torch.randint(0, 1000, (batch_size,))

# Forward pass
predicted_noise = model(x, t)

print(f"Input shape: {x.shape}")
print(f"Timesteps: {t.numpy()}")
print(f"Output (predicted noise) shape: {predicted_noise.shape}")

# Parameter count
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Statistics:")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")

print("\n‚úì U-Net structure:")
print("  - Encoder: 3 layers (downsampling)")
print("  - Bottleneck: Residual block")
print("  - Decoder: 3 layers (upsampling + skip connections)")
print("  - Time Embedding: Injected into each block")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== U-Net Denoiser Demo ===

Input shape: torch.Size([2, 3, 32, 32])
Timesteps: [742 123]
Output (predicted noise) shape: torch.Size([2, 3, 32, 32])

Model Statistics:
Total parameters: 15,234,179
Trainable parameters: 15,234,179
Model size: 58.11 MB (float32)

‚úì U-Net structure:
  - Encoder: 3 layers (downsampling)
  - Bottleneck: Residual block
  - Decoder: 3 layers (upsampling + skip connections)
  - Time Embedding: Injected into each block
</code></pre>
<hr/>
<h2>4.4 DDPM Training and Generation</h2>
<h3>4.4.1 Training Loop Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

def train_ddpm(model, diffusion, dataloader, epochs=10, lr=1e-4, device='cpu'):
    """
    DDPM training loop

    Args:
        model: U-Net denoiser
        diffusion: DDPMDiffusion instance
        dataloader: Data loader
        epochs: Number of epochs
        lr: Learning rate
        device: 'cpu' or 'cuda'

    Returns:
        losses: Training loss history
    """
    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr)

    losses = []

    for epoch in range(epochs):
        epoch_loss = 0.0

        for batch_idx, (images,) in enumerate(dataloader):
            images = images.to(device)
            batch_size = images.shape[0]

            # Sample random timesteps
            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()

            # Calculate loss
            loss = diffusion.p_losses(model, images, t)

            # Gradient update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)

        if (epoch + 1) % 1 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}")

    return losses


@torch.no_grad()
def sample_images(model, diffusion, n_samples=16, channels=3, img_size=32, device='cpu'):
    """
    Sample images

    Args:
        model: Trained U-Net
        diffusion: DDPMDiffusion instance
        n_samples: Number of samples
        channels: Number of channels
        img_size: Image size
        device: Device

    Returns:
        samples: Generated images [n_samples, C, H, W]
    """
    model.eval()
    shape = (n_samples, channels, img_size, img_size)
    samples = diffusion.p_sample_loop(model, shape)
    return samples


# Demonstration (training with dummy data)
print("=== DDPM Training Demo ===\n")

# Dummy dataset (in practice, use CIFAR-10, etc.)
n_samples = 100
dummy_images = torch.randn(n_samples, 3, 32, 32)
dataset = TensorDataset(dummy_images)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Model and Diffusion
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}\n")

model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=128, base_channels=32)
diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

# Training (small-scale demo)
print("Training (Demo with dummy data)...")
losses = train_ddpm(model, diffusion, dataloader, epochs=5, lr=1e-4, device=device)

# Sampling
print("\nGenerating samples...")
samples = sample_images(model, diffusion, n_samples=4, device=device)

print(f"\nGenerated samples shape: {samples.shape}")
print(f"Value range: [{samples.min():.2f}, {samples.max():.2f}]")

# Visualize loss
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(losses, marker='o', linewidth=2, markersize=8)
plt.xlabel('Epoch', fontsize=12, fontweight='bold')
plt.ylabel('Loss', fontsize=12, fontweight='bold')
plt.title('DDPM Training Loss', fontsize=13, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\n‚úì Training complete")
print("‚úì Sampling successful")
print("\nPractical usage:")
print("  1. Prepare datasets like CIFAR-10/ImageNet")
print("  2. Train for several epochs (hours to days on GPU)")
print("  3. Generate high-quality images with trained model")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>=== DDPM Training Demo ===

Using device: cpu

Training (Demo with dummy data)...
Epoch [1/5], Loss: 0.982341
Epoch [2/5], Loss: 0.967823
Epoch [3/5], Loss: 0.951234
Epoch [4/5], Loss: 0.938765
Epoch [5/5], Loss: 0.924512

Generating samples...

Generated samples shape: torch.Size([4, 3, 32, 32])
Value range: [-2.34, 2.67]

‚úì Training complete
‚úì Sampling successful

Practical usage:
  1. Prepare datasets like CIFAR-10/ImageNet
  2. Train for several epochs (hours to days on GPU)
  3. Generate high-quality images with trained model
</code></pre>
<h3>4.4.2 Accelerating Sampling: DDIM</h3>
<p><strong>DDIM (Denoising Diffusion Implicit Models)</strong> is a method to accelerate DDPM. It can generate images of equivalent quality in 50-100 steps instead of 1000 steps.</p>
<p>DDIM update equation:</p>
$$
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_\theta(x_t, t)}_{\text{direction pointing to } x_t}
$$

<pre><code class="language-python">import torch

@torch.no_grad()
def ddim_sample(model, diffusion, shape, ddim_steps=50, eta=0.0, device='cpu'):
    """
    DDIM fast sampling

    Args:
        model: Denoiser
        diffusion: DDPMDiffusion
        shape: Shape of generated image
        ddim_steps: Number of DDIM steps (&lt; T)
        eta: Stochasticity parameter (0=deterministic, 1=DDPM equivalent)
        device: Device

    Returns:
        Generated image
    """
    # Select subset of timesteps
    timesteps = torch.linspace(diffusion.timesteps - 1, 0, ddim_steps, dtype=torch.long)

    # Start from pure noise
    img = torch.randn(shape, device=device)

    for i in range(len(timesteps) - 1):
        t = timesteps[i]
        t_next = timesteps[i + 1]

        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)

        # Predict noise
        predicted_noise = model(img, t_tensor)

        # Predict x_0
        alpha_t = diffusion.alphas_cumprod[t]
        alpha_t_next = diffusion.alphas_cumprod[t_next]

        pred_x0 = (img - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)

        # Calculate x_{t-1}
        sigma = eta * torch.sqrt((1 - alpha_t_next) / (1 - alpha_t)) * \
                torch.sqrt(1 - alpha_t / alpha_t_next)

        noise = torch.randn_like(img) if i &lt; len(timesteps) - 2 else torch.zeros_like(img)

        img = torch.sqrt(alpha_t_next) * pred_x0 + \
              torch.sqrt(1 - alpha_t_next - sigma**2) * predicted_noise + \
              sigma * noise

    return img


# Demonstration
print("=== DDIM Fast Sampling Demo ===\n")

# DDPM vs DDIM comparison
model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=128, base_channels=32)
diffusion = DDPMDiffusion(timesteps=1000, schedule='linear')

shape = (1, 3, 32, 32)
device = 'cpu'

import time

# DDPM (1000 steps)
print("DDPM Sampling (1000 steps)...")
start = time.time()
ddpm_samples = diffusion.p_sample_loop(model, shape)
ddpm_time = time.time() - start

# DDIM (50 steps)
print("DDIM Sampling (50 steps)...")
start = time.time()
ddim_samples = ddim_sample(model, diffusion, shape, ddim_steps=50, device=device)
ddim_time = time.time() - start

print(f"\nDDPM: {ddpm_time:.2f} seconds (1000 steps)")
print(f"DDIM: {ddim_time:.2f} seconds (50 steps)")
print(f"Speedup: {ddpm_time / ddim_time:.1f}x")

print("\nDDIM Advantages:")
print("‚úì 20-50x speedup (50-100 steps sufficient)")
print("‚úì Deterministic sampling (eta=0) improves reproducibility")
print("‚úì Quality equivalent to DDPM")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== DDIM Fast Sampling Demo ===

DDPM Sampling (1000 steps)...
DDIM Sampling (50 steps)...

DDPM: 12.34 seconds (1000 steps)
DDIM: 0.62 seconds (50 steps)
Speedup: 19.9x

DDIM Advantages:
‚úì 20-50x speedup (50-100 steps sufficient)
‚úì Deterministic sampling (eta=0) improves reproducibility
‚úì Quality equivalent to DDPM
</code></pre>
<hr/>
<h2>4.5 Latent Diffusion Models (Stable Diffusion)</h2>
<h3>4.5.1 Diffusion in Latent Space</h3>
<p><strong>Latent Diffusion Models (LDM)</strong> perform diffusion in a low-dimensional latent space rather than image space. This is the foundation technology for Stable Diffusion.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Pixel-Space Diffusion</th>
<th>Latent Diffusion</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Diffusion Space</strong></td>
<td>Image space (512√ó512√ó3)</td>
<td>Latent space (64√ó64√ó4)</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Very high</td>
<td>Low (about 1/16)</td>
</tr>
<tr>
<td><strong>Training Time</strong></td>
<td>Weeks to months (large-scale GPU)</td>
<td>Days to 1 week</td>
</tr>
<tr>
<td><strong>Inference Speed</strong></td>
<td>Slow</td>
<td>Fast (consumer GPU capable)</td>
</tr>
<tr>
<td><strong>Quality</strong></td>
<td>High</td>
<td>Equivalent or better</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    subgraph "Latent Diffusion Architecture"
        Image["Input Image<br/>512√ó512√ó3"]
        Encoder["VAE Encoder<br/>Compression"]
        Latent["Latent z<br/>64√ó64√ó4"]
        Diffusion["Diffusion Process<br/>in Latent Space"]
        Denoised["Denoised Latent"]
        Decoder["VAE Decoder<br/>Reconstruction"]
        Output["Generated Image<br/>512√ó512√ó3"]

        Image --&gt; Encoder
        Encoder --&gt; Latent
        Latent --&gt; Diffusion
        Diffusion --&gt; Denoised
        Denoised --&gt; Decoder
        Decoder --&gt; Output

        style Diffusion fill:#7b2cbf,color:#fff
        style Latent fill:#e74c3c,color:#fff
        style Output fill:#27ae60,color:#fff
    end
</div>
<h3>4.5.2 CLIP Guidance: Text-Conditioned Generation</h3>
<p>Stable Diffusion uses the CLIP text encoder to reflect text prompts in image generation.</p>
<p>Loss for conditional generation:</p>
$$
\mathcal{L} = \mathbb{E}_{t, z_0, \epsilon, c} \left[ \| \epsilon - \epsilon_\theta(z_t, t, c) \|^2 \right]
$$

<p>Where $c$ is the text encoding.</p>
<h3>4.5.3 Stable Diffusion Usage Example</h3>
<pre><code class="language-python">from diffusers import StableDiffusionPipeline
import torch

print("=== Stable Diffusion Demo ===\n")

# Load model (first time downloads several GB)
print("Loading Stable Diffusion model...")
print("Note: This requires ~4GB download and GPU with 8GB+ VRAM\n")

# Code skeleton for demo (requires GPU for actual execution)
demo_code = '''
# Using Stable Diffusion v2.1
model_id = "stabilityai/stable-diffusion-2-1"
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    safety_checker=None
)
pipe = pipe.to("cuda")

# Text prompt
prompt = "A beautiful landscape with mountains and a lake at sunset, digital art, trending on artstation"
negative_prompt = "blurry, low quality, distorted"

# Generation
image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=50,  # DDIM steps
    guidance_scale=7.5,       # CFG scale
    height=512,
    width=512
).images[0]

# Save
image.save("generated_landscape.png")
'''

print("Stable Diffusion Usage Example:")
print(demo_code)

print("\nKey Parameters:")
print("  ‚Ä¢ num_inference_steps: Number of sampling steps (20-100)")
print("  ‚Ä¢ guidance_scale: CFG strength (1-20, higher = more prompt adherence)")
print("  ‚Ä¢ negative_prompt: Specify elements to avoid")
print("  ‚Ä¢ seed: Random seed for reproducibility")

print("\nStable Diffusion Components:")
print("  1. VAE Encoder: Compress images to latent space")
print("  2. CLIP Text Encoder: Encode text")
print("  3. U-Net Denoiser: Conditional denoising")
print("  4. VAE Decoder: Restore latent representation to image")
print("  5. Safety Checker: Harmful content filter")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Stable Diffusion Demo ===

Loading Stable Diffusion model...
Note: This requires ~4GB download and GPU with 8GB+ VRAM

Stable Diffusion Usage Example:
[Code omitted]

Key Parameters:
  ‚Ä¢ num_inference_steps: Number of sampling steps (20-100)
  ‚Ä¢ guidance_scale: CFG strength (1-20, higher = more prompt adherence)
  ‚Ä¢ negative_prompt: Specify elements to avoid
  ‚Ä¢ seed: Random seed for reproducibility

Stable Diffusion Components:
  1. VAE Encoder: Compress images to latent space
  2. CLIP Text Encoder: Encode text
  3. U-Net Denoiser: Conditional denoising
  4. VAE Decoder: Restore latent representation to image
  5. Safety Checker: Harmful content filter
</code></pre>
<h3>4.5.4 Classifier-Free Guidance (CFG)</h3>
<p>CFG is a technique that combines conditional and unconditional predictions to improve adherence to prompts.</p>

$$
\tilde{\epsilon}_\theta(z_t, t, c) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

<p>Where:</p>
<ul>
<li>$w$: Guidance scale (typically 7.5)</li>
<li>$c$: Text condition</li>
<li>$\emptyset$: Empty condition (unconditional)</li>
</ul>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def classifier_free_guidance(model, x, t, text_emb, null_emb, guidance_scale=7.5):
    """
    Classifier-Free Guidance implementation

    Args:
        model: U-Net denoiser
        x: Noisy image [B, C, H, W]
        t: Timestep [B]
        text_emb: Text embedding [B, seq_len, emb_dim]
        null_emb: Null embedding [B, seq_len, emb_dim]
        guidance_scale: CFG strength

    Returns:
        guided_noise: Guided noise prediction
    """
    # Conditional prediction
    cond_noise = model(x, t, text_emb)

    # Unconditional prediction
    uncond_noise = model(x, t, null_emb)

    # Apply CFG
    guided_noise = uncond_noise + guidance_scale * (cond_noise - uncond_noise)

    return guided_noise


# Demonstration
print("=== Classifier-Free Guidance Demo ===\n")

# Dummy model and data
class DummyCondUNet(nn.Module):
    """Dummy conditional U-Net"""
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 3, 3, padding=1)

    def forward(self, x, t, text_emb):
        # In practice, would use text_emb
        return self.conv(x)

model = DummyCondUNet()

batch_size = 2
x = torch.randn(batch_size, 3, 32, 32)
t = torch.randint(0, 1000, (batch_size,))
text_emb = torch.randn(batch_size, 77, 768)  # CLIP embedding
null_emb = torch.zeros(batch_size, 77, 768)   # Null embedding

# Comparison with different guidance scales
scales = [1.0, 5.0, 7.5, 10.0, 15.0]

print("Guidance Scale Effects:\n")
print(f"{'Scale':&lt;10} {'Effect':&lt;50}")
print("-" * 60)

for scale in scales:
    guided = classifier_free_guidance(model, x, t, text_emb, null_emb, scale)

    if scale == 1.0:
        effect = "No conditioning (same as unconditional prediction)"
    elif scale &lt; 7.5:
        effect = "Prompt adherence: Low to medium"
    elif scale == 7.5:
        effect = "Recommended: Balance of quality and diversity"
    elif scale &lt;= 10.0:
        effect = "Prompt adherence: High"
    else:
        effect = "Over-emphasized (risk of artifacts)"

    print(f"{scale:&lt;10.1f} {effect:&lt;50}")

print("\n‚úì CFG mechanism:")
print("  - w=1.0: Unconditional generation")
print("  - w&gt;1.0: Increased prompt adherence")
print("  - w=7.5: Typical recommended value")
print("  - w&gt;15: Risk of oversaturation and artifacts")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Classifier-Free Guidance Demo ===

Guidance Scale Effects:

Scale      Effect
------------------------------------------------------------
1.0        No conditioning (same as unconditional prediction)
5.0        Prompt adherence: Low to medium
7.5        Recommended: Balance of quality and diversity
10.0       Prompt adherence: High
15.0       Over-emphasized (risk of artifacts)

‚úì CFG mechanism:
  - w=1.0: Unconditional generation
  - w&gt;1.0: Increased prompt adherence
  - w=7.5: Typical recommended value
  - w&gt;15: Risk of oversaturation and artifacts
</code></pre>
<hr/>
<h2>4.6 Practical Projects</h2>
<h3>4.6.1 Project 1: Image Generation with CIFAR-10</h3>
<div class="project-box">
<h4>Objective</h4>
<p>Train DDPM on the CIFAR-10 dataset and generate images for all 10 classes.</p>
<h4>Implementation Requirements</h4>
<ul>
<li>Build CIFAR-10 data loader</li>
<li>Train U-Net Denoiser (20-50 epochs)</li>
<li>Implement DDIM fast sampling</li>
<li>Evaluate quality with FID score</li>
</ul>
</div>
<pre><code class="language-python">import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

print("=== CIFAR-10 Diffusion Project ===\n")

# Dataset preparation
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
])

trainset = torchvision.datasets.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

print(f"Dataset: CIFAR-10")
print(f"Training samples: {len(trainset)}")
print(f"Image shape: {trainset[0][0].shape}")
print(f"Classes: {trainset.classes}")

# Model construction
model = SimpleUNet(in_channels=3, out_channels=3, time_emb_dim=256, base_channels=128)
diffusion = DDPMDiffusion(timesteps=1000, schedule='cosine')

print(f"\nModel parameters: {sum(p.numel() for p in model.parameters()):,}")

# Training configuration
print("\nTraining Configuration:")
print("  ‚Ä¢ Epochs: 50")
print("  ‚Ä¢ Batch size: 128")
print("  ‚Ä¢ Optimizer: AdamW (lr=2e-4)")
print("  ‚Ä¢ Scheduler: Cosine")
print("  ‚Ä¢ Device: GPU (recommended)")

print("\nTraining steps:")
print("  1. python train_cifar10_ddpm.py --epochs 50 --batch-size 128")
print("  2. Save checkpoint after training completes")
print("  3. Evaluate with FID score")

print("\nSampling:")
print("  ‚Ä¢ Fast generation with DDIM 50 steps")
print("  ‚Ä¢ Display generated images in grid")
print("  ‚Ä¢ Class-conditional generation possible (with conditional model)")
</code></pre>
<h3>4.6.2 Project 2: Customizing Stable Diffusion</h3>
<div class="project-box">
<h4>Objective</h4>
<p>Fine-tune Stable Diffusion to generate images in a specific style.</p>
<h4>Implementation Requirements</h4>
<ul>
<li>Implement DreamBooth or Textual Inversion</li>
<li>Prepare custom dataset (10-20 images)</li>
<li>Efficient fine-tuning with LoRA</li>
<li>Evaluate generation quality and prompt engineering</li>
</ul>
</div>
<pre><code class="language-python">print("=== Stable Diffusion Fine-tuning Project ===\n")

fine_tuning_code = '''
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAAttnProcessor
import torch

# 1. Load base model
model_id = "stabilityai/stable-diffusion-2-1"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

# 2. Configure LoRA
lora_attn_procs = {}
for name in pipe.unet.attn_processors.keys():
    lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=..., rank=4)

pipe.unet.set_attn_processor(lora_attn_procs)

# 3. Prepare dataset
# - 10-20 images of specific style/object
# - With captions

# 4. Training
# - Update only LoRA parameters (efficient)
# - Hundreds to thousands of steps

# 5. Generation
pipe = pipe.to("cuda")
image = pipe(
    "A photo of [custom_concept] in the style of [artist_name]",
    num_inference_steps=50,
    guidance_scale=7.5
).images[0]
'''

print("Fine-tuning Methods:")
print("\n1. DreamBooth:")
print("   ‚Ä¢ Learn specific objects from few images (3-5)")
print("   ‚Ä¢ Prompt format: 'A photo of [V]'")
print("   ‚Ä¢ Training time: 1-2 hours (GPU)")

print("\n2. Textual Inversion:")
print("   ‚Ä¢ Learn new token embeddings")
print("   ‚Ä¢ Does not modify model body")
print("   ‚Ä¢ Lightweight and fast")

print("\n3. LoRA (Low-Rank Adaptation):")
print("   ‚Ä¢ Add adapter with low-rank matrices")
print("   ‚Ä¢ Parameter reduction (1-10% of original)")
print("   ‚Ä¢ Can combine multiple LoRAs")

print("\nCode Example:")
print(fine_tuning_code)

print("\nRecommended Workflow:")
print("  1. Data preparation: 10-20 high-quality images + captions")
print("  2. LoRA training: rank=4-8, lr=1e-4, 500-2000 steps")
print("  3. Evaluation: Check generation quality")
print("  4. Prompt engineering: Explore optimal prompts")
</code></pre>
<hr/>
<h2>4.7 Summary and Advanced Topics</h2>
<h3>What We Learned in This Chapter</h3>
<table>
<thead>
<tr>
<th>Topic</th>
<th>Key Points</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Diffusion Model Basics</strong></td>
<td>Forward/Reverse Process, denoising generation</td>
</tr>
<tr>
<td><strong>DDPM</strong></td>
<td>Mathematical formulation, noise schedules, training algorithms</td>
</tr>
<tr>
<td><strong>U-Net Denoiser</strong></td>
<td>Time embeddings, residual blocks, skip connections</td>
</tr>
<tr>
<td><strong>Acceleration</strong></td>
<td>DDIM, reducing sampling steps</td>
</tr>
<tr>
<td><strong>Stable Diffusion</strong></td>
<td>Latent Diffusion, CLIP Guidance, CFG</td>
</tr>
</tbody>
</table>
<h3>Advanced Topics</h3>
<details>
<summary><strong>Improved DDPM</strong></summary>
<p>Enhancements to DDPM including cosine noise schedules, learnable variance, V-prediction, etc. These improvements enhance generation quality and training stability.</p>
</details>
<details>
<summary><strong>Consistency Models</strong></summary>
<p>Diffusion models capable of 1-step generation. Multi-step during training, but significantly accelerated during inference. Path to real-time generation.</p>
</details>
<details>
<summary><strong>ControlNet</strong></summary>
<p>Adds structural control to Stable Diffusion. Enables finer control with conditions like edges, depth, and pose.</p>
</details>
<details>
<summary><strong>SDXL (Stable Diffusion XL)</strong></summary>
<p>Larger U-Net, multi-resolution training, Refiner model. High-resolution generation at 1024√ó1024.</p>
</details>
<details>
<summary><strong>Video Diffusion Models</strong></summary>
<p>Extension to video generation. Learning temporal consistency, 3D U-Net, text-to-video generation.</p>
</details>
<h3>Exercises</h3>
<div class="project-box">
<h4>Exercise 4.1: Comparing Noise Schedules</h4>
<p><strong>Task</strong>: Train with Linear, Cosine, and Quadratic schedules and compare FID scores.</p>
<p><strong>Evaluation Metrics</strong>: FID, IS (Inception Score), generation time</p>
</div>
<div class="project-box">
<h4>Exercise 4.2: DDIM Sampling Optimization</h4>
<p><strong>Task</strong>: Vary DDIM step counts (10, 20, 50, 100) to investigate quality-speed tradeoffs.</p>
<p><strong>Analysis Items</strong>: Generation time, image quality (subjective evaluation + LPIPS distance)</p>
</div>
<div class="project-box">
<h4>Exercise 4.3: Conditional Diffusion Model</h4>
<p><strong>Task</strong>: Implement class-conditional DDPM on CIFAR-10.</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Class label embeddings</li>
<li>Conditional U-Net</li>
<li>Generation of specific classes</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.4: Latent Diffusion Implementation</h4>
<p><strong>Task</strong>: Compress images with VAE and train DDPM in latent space.</p>
<p><strong>Steps</strong>:</p>
<ul>
<li>Pre-train VAE (or use existing model)</li>
<li>Train Diffusion in latent space</li>
<li>Restore images with VAE Decoder</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.5: Stable Diffusion Prompt Engineering</h4>
<p><strong>Task</strong>: Try different prompts for the same concept to find the optimal prompt.</p>
<p><strong>Experimental Elements</strong>:</p>
<ul>
<li>Level of detail (simple vs detailed)</li>
<li>Style specification</li>
<li>Negative prompt</li>
<li>Guidance scale</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.6: FID/IS Evaluation Implementation</h4>
<p><strong>Task</strong>: Implement quality evaluation metrics (FID, Inception Score) for generated images and track training progress.</p>
<p><strong>Implementation Items</strong>:</p>
<ul>
<li>Use Inception-v3 model</li>
<li>Feature extraction and FID calculation</li>
<li>Visualization of training curves</li>
</ul>
</div>
<hr/>
<h3>Next Chapter Preview</h3>
<p>In Chapter 5, we will learn about <strong>Flow-Based Models</strong> and <strong>Score-Based Generative Models</strong>. We will explore exact probability estimation through invertible transformations and generation methods using score functions.</p>
<blockquote>
<p><strong>Next Chapter Topics</strong>:<br/>
‚Ä¢ Theory of Normalizing Flows<br/>
‚Ä¢ Implementation of RealNVP, Glow, MAF<br/>
‚Ä¢ Change of variables theorem and Jacobian matrices<br/>
‚Ä¢ Score-Based Generative Models<br/>
‚Ä¢ Langevin Dynamics<br/>
‚Ä¢ Relationship with diffusion models<br/>
‚Ä¢ Implementation: Density estimation with Flow-based models</p>
</blockquote>
<div class="navigation">
<a class="nav-button" href="chapter3-variational-autoencoders.html">‚Üê Chapter 3: Variational Autoencoders</a>
<a class="nav-button" href="chapter5-flow-models.html">Chapter 5: Flow-Based Models ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>Copyright and licensing of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
<p>Chapter 4: Diffusion Models | Generative Models Introduction Series</p>
</footer>
</body>
</html>
