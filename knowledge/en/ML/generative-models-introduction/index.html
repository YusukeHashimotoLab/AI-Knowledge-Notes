<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Generative Models Introduction Series v1.0 - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Generative Models Introduction Series - Complete guide to theory and implementation of VAE, GAN, and Diffusion Models" name="description"/>
<title>Generative Models Introduction Series v1.0 - AI Terakoya</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Mermaid for diagrams -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">â€º</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Generative Models</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">ğŸŒ EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/generative-models-introduction/index.html" class="locale-link">ğŸ‡¯ğŸ‡µ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>ğŸ¨ Generative Models Introduction Series v1.0</h1>
<p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">Theory and Implementation of VAE, GAN, and Diffusion Models</p>
<div class="meta">
<span>ğŸ“– Total Study Time: 120-150 minutes</span>
<span>ğŸ“Š Level: Advanced</span>
</div>
</div>
</header>
<main class="container">
<p><strong>Systematically master the core technologies of modern AI image generation from fundamentals</strong></p>
<h2 id="overview">Series Overview</h2>
<p>This series is a practical educational content consisting of 5 chapters that progressively teaches the theory and implementation of generative models from the basics.</p>
<p><strong>Generative Models</strong> are deep learning models that learn the probability distribution of data and generate new data. These technologies, including learning latent space representations with Variational Autoencoders (VAE), adversarial learning with Generative Adversarial Networks (GAN), and gradual denoising processes with Diffusion Models, form the core of creative AI applications such as image generation, speech synthesis, and video generation. You will understand and be able to implement the foundational technologies behind text-to-image generation systems like DALL-E, Stable Diffusion, and Midjourney. We provide systematic knowledge from probabilistic generative model fundamentals to cutting-edge Diffusion Models.</p>
<p><strong>Features:</strong></p>
<ul>
<li>âœ… <strong>From Theory to Implementation</strong>: Systematic learning from probabilistic foundations to the latest Stable Diffusion</li>
<li>âœ… <strong>Implementation-Focused</strong>: 35+ executable PyTorch code examples with practical techniques</li>
<li>âœ… <strong>Intuitive Understanding</strong>: Understand operating principles through visualization of generation processes and latent space exploration</li>
<li>âœ… <strong>Latest Technology Compliance</strong>: Implementation using Hugging Face Diffusers and Stable Diffusion</li>
<li>âœ… <strong>Practical Applications</strong>: Application to practical tasks such as image generation, text-to-image, and speech synthesis</li>
</ul>
<p><strong>Total Study Time</strong>: 120-150 minutes (including code execution and exercises)</p>
<h2 id="learning">How to Learn</h2>
<h3>Recommended Learning Order</h3>
<div class="mermaid">
graph TD
    A[Chapter 1: Generative Model Fundamentals] --&gt; B[Chapter 2: VAE]
    B --&gt; C[Chapter 3: GAN]
    C --&gt; D[Chapter 4: Diffusion Models]
    D --&gt; E[Chapter 5: Generative Model Applications]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>
<p><strong>For Beginners (completely new to generative models):</strong><br/>
        - Chapter 1 â†’ Chapter 2 â†’ Chapter 3 â†’ Chapter 4 â†’ Chapter 5 (all chapters recommended)<br/>
        - Duration: 120-150 minutes</p>
<p><strong>For Intermediate Learners (with autoencoder experience):</strong><br/>
        - Chapter 2 â†’ Chapter 3 â†’ Chapter 4 â†’ Chapter 5<br/>
        - Duration: 90-110 minutes</p>
<p><strong>For Specific Topic Enhancement:</strong><br/>
        - VAE Theory: Chapter 2 (focused study)<br/>
        - GAN Implementation: Chapter 3 (focused study)<br/>
        - Diffusion/Stable Diffusion: Chapter 4 (focused study)<br/>
        - Duration: 25-30 minutes/chapter</p>
<h2 id="chapters">Chapter Details</h2>
<h3><a href="chapter-1.html">Chapter 1: Generative Model Fundamentals</a></h3>
<p><strong>Difficulty</strong>: Advanced<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 7</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Discriminative Models vs Generative Models</strong> - P(y|x) vs P(x), differences in objectives and applications</li>
<li><strong>Probability Distribution Modeling</strong> - Likelihood maximization, KL divergence</li>
<li><strong>Latent Variable Models</strong> - Latent space, low-dimensional data representations</li>
<li><strong>Sampling Methods</strong> - Monte Carlo methods, MCMC, importance sampling</li>
<li><strong>Evaluation Metrics</strong> - Inception Score, FID, quantitative evaluation of generation quality</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>âœ… Understand fundamental concepts of generative models</li>
<li>âœ… Explain probability distribution modeling techniques</li>
<li>âœ… Understand the role of latent variable models</li>
<li>âœ… Implement sampling methods</li>
<li>âœ… Quantitatively evaluate generation quality</li>
</ul>
<p><strong><a href="chapter-1.html">Read Chapter 1 â†’</a></strong></p>
<hr/>
<h3><a href="./chapter2-vae.html">Chapter 2: VAE (Variational Autoencoder)</a></h3>
<p><strong>Difficulty</strong>: Advanced<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 8</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Autoencoder Review</strong> - Encoder-Decoder, reconstruction error</li>
<li><strong>Variational Inference Fundamentals</strong> - ELBO, variational lower bound, evidence lower bound</li>
<li><strong>Reparameterization Trick</strong> - Gradient propagation, making sampling differentiable</li>
<li><strong>KL Divergence</strong> - Regularization term, distribution similarity</li>
<li><strong>VAE Implementation and Visualization</strong> - PyTorch implementation, latent space exploration</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>âœ… Understand the principles of variational inference</li>
<li>âœ… Explain the derivation of ELBO</li>
<li>âœ… Understand the necessity of the Reparameterization Trick</li>
<li>âœ… Explain the role of KL divergence</li>
<li>âœ… Implement VAE in PyTorch</li>
</ul>
<p><strong><a href="./chapter2-vae.html">Read Chapter 2 â†’</a></strong></p>
<hr/>
<h3><a href="./chapter3-gan.html">Chapter 3: GAN (Generative Adversarial Network)</a></h3>
<p><strong>Difficulty</strong>: Advanced<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 8</p>
<h4>Learning Content</h4>
<ol>
<li><strong>GAN Principles</strong> - Generator and Discriminator, adversarial learning</li>
<li><strong>Minimax Game</strong> - Nash equilibrium, objective function</li>
<li><strong>DCGAN</strong> - Convolutional GAN, stable training techniques</li>
<li><strong>StyleGAN</strong> - Style-based generation, AdaIN, high-quality image generation</li>
<li><strong>Training Stabilization</strong> - Mode collapse countermeasures, Spectral Normalization</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>âœ… Understand GAN's adversarial learning</li>
<li>âœ… Explain the roles of Generator and Discriminator</li>
<li>âœ… Understand DCGAN design principles</li>
<li>âœ… Explain StyleGAN mechanisms</li>
<li>âœ… Implement GAN training stabilization techniques</li>
</ul>
<p><strong><a href="./chapter3-gan.html">Read Chapter 3 â†’</a></strong></p>
<hr/>
<h3><a href="./chapter4-diffusion-models.html">Chapter 4: Diffusion Models</a></h3>
<p><strong>Difficulty</strong>: Advanced<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 7</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Diffusion Process Fundamentals</strong> - Forward process, Reverse process</li>
<li><strong>DDPM (Denoising Diffusion Probabilistic Models)</strong> - Noise removal, iterative generation</li>
<li><strong>Score-based Models</strong> - Score function, Langevin Dynamics</li>
<li><strong>Stable Diffusion</strong> - Latent Diffusion, Text-to-Image</li>
<li><strong>Fast Sampling</strong> - DDIM, Classifier-free Guidance</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>âœ… Understand the principles of Diffusion Process</li>
<li>âœ… Explain DDPM training and generation methods</li>
<li>âœ… Understand Score-based Models concepts</li>
<li>âœ… Explain Stable Diffusion mechanisms</li>
<li>âœ… Generate images using the Diffusers library</li>
</ul>
<p><strong><a href="./chapter4-diffusion-models.html">Read Chapter 4 â†’</a></strong></p>
<hr/>
<h3><a href="./chapter5-generative-model-applications.html">Chapter 5: Generative Model Applications</a></h3>
<p><strong>Difficulty</strong>: Advanced<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 5</p>
<h4>Learning Content</h4>
<ol>
<li><strong>High-Quality Image Generation</strong> - DALL-E 2, Midjourney, Imagen</li>
<li><strong>Text-to-Image Generation</strong> - CLIP guidance, prompt engineering</li>
<li><strong>Image Editing</strong> - Inpainting, Style Transfer, Image-to-Image</li>
<li><strong>Speech Synthesis</strong> - WaveGAN, Diffusion-based TTS</li>
<li><strong>Video and 3D Generation</strong> - Gen-2, NeRF, DreamFusion</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>âœ… Understand latest image generation systems</li>
<li>âœ… Explain Text-to-Image mechanisms</li>
<li>âœ… Implement image editing techniques</li>
<li>âœ… Understand applications to speech synthesis</li>
<li>âœ… Grasp latest trends in video and 3D generation</li>
</ul>
<p><strong><a href="./chapter5-generative-model-applications.html">Read Chapter 5 â†’</a></strong></p>
<hr/>
<h2 id="outcomes">Overall Learning Outcomes</h2>
<p>Upon completing this series, you will acquire the following skills and knowledge:</p>
<h3>Knowledge Level (Understanding)</h3>
<ul>
<li>âœ… Explain theoretical foundations of generative models</li>
<li>âœ… Understand mechanisms of VAE, GAN, and Diffusion Models</li>
<li>âœ… Explain strengths, weaknesses, and selection criteria for each model</li>
<li>âœ… Understand the significance of latent space representations</li>
<li>âœ… Explain operating principles of latest generative AI systems</li>
</ul>
<h3>Practical Skills (Doing)</h3>
<ul>
<li>âœ… Implement VAE, GAN, and Diffusion models in PyTorch</li>
<li>âœ… Generate high-quality images with Stable Diffusion</li>
<li>âœ… Implement Text-to-Image generation</li>
<li>âœ… Quantitatively evaluate generation quality</li>
<li>âœ… Design effective prompts</li>
</ul>
<h3>Application Ability (Applying)</h3>
<ul>
<li>âœ… Select appropriate generative models according to tasks</li>
<li>âœ… Apply generative models to practical work</li>
<li>âœ… Build image generation and editing systems</li>
<li>âœ… Understand and utilize latest generative AI technologies</li>
</ul>
<hr/>
<h2 id="prerequisites">Prerequisites</h2>
<p>To effectively learn this series, it is desirable to have the following knowledge:</p>
<h3>Required (Must Have)</h3>
<ul>
<li>âœ… <strong>Python Fundamentals</strong>: Variables, functions, classes, loops, conditional statements</li>
<li>âœ… <strong>NumPy Fundamentals</strong>: Array operations, broadcasting, basic mathematical functions</li>
<li>âœ… <strong>Deep Learning Fundamentals</strong>: Neural networks, backpropagation, gradient descent</li>
<li>âœ… <strong>PyTorch Fundamentals</strong>: Tensor operations, nn.Module, Dataset and DataLoader</li>
<li>âœ… <strong>Probability and Statistics Fundamentals</strong>: Probability distributions, expectation, variance, normal distribution</li>
<li>âœ… <strong>CNN Fundamentals</strong>: Convolutional layers, pooling layers, image processing</li>
</ul>
<h3>Recommended (Nice to Have)</h3>
<ul>
<li>ğŸ’¡ <strong>Autoencoders</strong>: Encoder-Decoder, latent representations</li>
<li>ğŸ’¡ <strong>Variational Inference</strong>: ELBO, KL divergence</li>
<li>ğŸ’¡ <strong>Optimization Algorithms</strong>: Adam, learning rate scheduling</li>
<li>ğŸ’¡ <strong>Transformer Fundamentals</strong>: Attention mechanism (for understanding Text-to-Image)</li>
<li>ğŸ’¡ <strong>GPU Environment</strong>: Basic understanding of CUDA</li>
</ul>
<p><strong>Recommended Prior Learning</strong>:</p>
<ul>
<!-- Content under preparation <li>ğŸ“š <a href="../deep-learning-basics/">Deep Learning Fundamentals Series</a> - Neural network basics</li>
            <li>ğŸ“š <a href="../pytorch-introduction/">PyTorch Introduction Series</a> - Basic PyTorch operations</li>
            <li>ğŸ“š <a href="../cnn-introduction/">CNN Introduction Series</a> - Convolutional Neural Networks</li>
            <li>ğŸ“š <a href="../transformer-introduction/">Transformer Introduction Series</a> - Attention mechanism (recommended)</li> -->
</ul>
<hr/>
<h2 id="tech">Technologies and Tools</h2>
<h3>Main Libraries</h3>
<ul>
<li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
<li><strong>torchvision 0.15+</strong> - Image processing and datasets</li>
<li><strong>diffusers 0.20+</strong> - Hugging Face Diffusers library</li>
<li><strong>transformers 4.30+</strong> - CLIP, text encoders</li>
<li><strong>NumPy 1.24+</strong> - Numerical computation</li>
<li><strong>Matplotlib 3.7+</strong> - Visualization</li>
<li><strong>Pillow 10.0+</strong> - Image processing</li>
<li><strong>scipy 1.11+</strong> - Scientific computing, evaluation metrics</li>
</ul>
<h3>Development Environment</h3>
<ul>
<li><strong>Python 3.8+</strong> - Programming language</li>
<li><strong>Jupyter Notebook / Lab</strong> - Interactive development environment</li>
<li><strong>Google Colab</strong> - GPU environment (available for free)</li>
<li><strong>CUDA 11.8+ / cuDNN</strong> - GPU acceleration (recommended)</li>
</ul>
<h3>Datasets</h3>
<ul>
<li><strong>MNIST</strong> - Handwritten digit dataset</li>
<li><strong>CelebA</strong> - Face image dataset</li>
<li><strong>ImageNet</strong> - Large-scale image dataset</li>
<li><strong>COCO</strong> - Images and captions (Text-to-Image)</li>
</ul>
<hr/>
<h2 id="start">Let's Get Started!</h2>
<p>Are you ready? Start with Chapter 1 and master generative model technologies!</p>
<p><strong><a href="chapter-1.html">Chapter 1: Generative Model Fundamentals â†’</a></strong></p>
<hr/>
<h2 id="next">Next Steps</h2>
<p>After completing this series, we recommend proceeding to the following topics:</p>
<h3>Deep Dive Learning</h3>
<ul>
<li>ğŸ“š <strong>ControlNet</strong>: Conditional image generation, spatial control</li>
<li>ğŸ“š <strong>LoRA and DreamBooth</strong>: Model customization, fine-tuning</li>
<li>ğŸ“š <strong>3D Generation</strong>: NeRF, 3D Gaussian Splatting, DreamFusion</li>
<li>ğŸ“š <strong>Video Generation</strong>: Gen-2, Pika, Sora</li>
</ul>
<h3>Related Series</h3>
<ul>
<li>ğŸ¯ <a href="../computer-vision-advanced/">Advanced Computer Vision</a> - Image recognition, object detection</li>
<li>ğŸ¯ <a href="../multimodal-ai/">Multimodal AI</a> - CLIP, DALL-E, Vision-Language Models</li>
<li>ğŸ¯ <a href="../creative-ai/">Creative AI Practice</a> - Practical generative AI applications</li>
</ul>
<h3>Practical Projects</h3>
<ul>
<li>ğŸš€ Avatar Generation System - Face generation with StyleGAN</li>
<li>ğŸš€ Text-to-Image App - Image generation using Stable Diffusion</li>
<li>ğŸš€ Image Editing Tool - Inpainting, Style Transfer</li>
<li>ğŸš€ AI Art Generator - Prompt-based creative support</li>
</ul>
<hr/>
<p><strong>Update History</strong></p>
<ul>
<li><strong>2025-10-21</strong>: v1.0 Initial release</li>
</ul>
<hr/>
<p><strong>Your generative model learning journey begins here!</strong></p>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranty, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University assume no liability for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are governed by the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<div class="container">
<p>Â© 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
