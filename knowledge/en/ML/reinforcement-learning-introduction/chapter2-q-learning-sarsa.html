<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Q-Learning and SARSA - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Q-Learning and SARSA</h1>
            <p class="subtitle">Value Function Estimation and Policy Optimization using Temporal Difference Learning</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
                <span class="meta-item">üíª Code Examples: 8</span>
                <span class="meta-item">üìù Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understanding the basic principles of Temporal Difference (TD) learning and its differences from Monte Carlo methods</li>
<li>‚úÖ Explaining the mechanism and update equations of the Q-Learning algorithm (off-policy)</li>
<li>‚úÖ Understanding the characteristics and application scenarios of the SARSA algorithm (on-policy)</li>
<li>‚úÖ Balancing exploration and exploitation using Œµ-greedy policies</li>
<li>‚úÖ Explaining the effects of hyperparameters: learning rate and discount factor</li>
<li>‚úÖ Implementing solutions for OpenAI Gym's Taxi-v3 and Cliff Walking environments</li>
</ul>

<hr>

<h2>2.1 Fundamentals of Temporal Difference (TD) Learning</h2>

<h3>Challenges of Monte Carlo Methods</h3>

<p>The Monte Carlo method we learned in Chapter 1 had the constraint of <strong>needing to wait until the end of an episode</strong>:</p>

$$
V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]
$$

<p>where $G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots$ is the actual return.</p>

<h3>Basic Idea of Temporal Difference Learning</h3>

<p><strong>Temporal Difference (TD) learning</strong> <strong>updates the value function at each step</strong> without waiting for the episode to end:</p>

$$
V(s_t) \leftarrow V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
$$

<p>where:</p>
<ul>
<li>$r_{t+1} + \gamma V(s_{t+1})$: <strong>TD target</strong></li>
<li>$\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$: <strong>TD error</strong></li>
<li>$\alpha$: learning rate</li>
</ul>

<div class="mermaid">
graph LR
    S1["State s_t"] --> A["Action a_t"]
    A --> S2["State s_t+1"]
    S2 --> R["Reward r_t+1"]
    R --> Update["Update V(s_t)"]
    S2 --> Update

    style S1 fill:#b3e5fc
    style S2 fill:#c5e1a5
    style R fill:#fff9c4
    style Update fill:#ffab91
</div>

<h3>Implementation of TD(0)</h3>

<pre><code class="language-python">import numpy as np
import gym

def td_0_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.99):
    """
    State value function estimation using TD(0)

    Args:
        env: Environment
        policy: Policy (state -> action probability distribution)
        num_episodes: Number of episodes
        alpha: Learning rate
        gamma: Discount factor

    Returns:
        V: State value function
    """
    # Initialize state value function
    V = np.zeros(env.observation_space.n)

    for episode in range(num_episodes):
        state, _ = env.reset()

        while True:
            # Select action according to policy
            action = np.random.choice(env.action_space.n, p=policy[state])

            # Interact with environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # TD(0) update
            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]
            V[state] = V[state] + alpha * td_error

            if done:
                break

            state = next_state

    return V


# Example usage: FrozenLake environment
print("=== Value Function Estimation using TD(0) ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

# Random policy
policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n

# Execute TD(0)
V = td_0_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.99)

print(f"State value function:\n{V.reshape(4, 4)}")
env.close()
</code></pre>

<h3>Comparison of Monte Carlo Methods and TD Learning</h3>

<table>
<thead>
<tr>
<th>Aspect</th>
<th>Monte Carlo Method</th>
<th>TD Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Update Timing</strong></td>
<td>After episode ends</td>
<td>After each step</td>
</tr>
<tr>
<td><strong>Return Calculation</strong></td>
<td>Actual return $G_t$</td>
<td>Estimated return $r + \gamma V(s')$</td>
</tr>
<tr>
<td><strong>Bias</strong></td>
<td>None (unbiased)</td>
<td>Present (depends on initial values)</td>
</tr>
<tr>
<td><strong>Variance</strong></td>
<td>High</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Continuing Tasks</strong></td>
<td>Not applicable</td>
<td>Applicable</td>
</tr>
<tr>
<td><strong>Convergence Speed</strong></td>
<td>Slow</td>
<td>Fast</td>
</tr>
</tbody>
</table>

<blockquote>
<p>"TD learning achieves efficient learning through bootstrapping (updating using its own estimates)"</p>
</blockquote>

<hr>

<h2>2.2 Q-Learning</h2>

<h3>Action-Value Function Q(s, a)</h3>

<p>Instead of the state value function $V(s)$, we learn the <strong>action-value function</strong> $Q(s, a)$:</p>

$$
Q(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]
$$

<p>This represents "the expected return after taking action $a$ in state $s$".</p>

<h3>Q-Learning Update Equation</h3>

<p><strong>Q-learning</strong> applies TD learning to the action-value function:</p>

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$

<p>Key points:</p>
<ul>
<li>$\max_{a'} Q(s_{t+1}, a')$: Uses the value of the <strong>best action</strong> in the next state</li>
<li><strong>Off-policy type</strong>: The actual action differs from the action used for updating</li>
<li>Can directly learn the optimal policy</li>
</ul>

<div class="mermaid">
graph TB
    Start["State s, Action a"] --> Execute["Execute in environment"]
    Execute --> Observe["Observe s', r"]
    Observe --> MaxQ["max_a' Q(s', a')"]
    MaxQ --> Target["TD target = r + Œ≥ max Q(s', a')"]
    Target --> Update["Update Q(s,a)"]
    Update --> Next["Next step"]

    style Start fill:#b3e5fc
    style MaxQ fill:#fff59d
    style Update fill:#ffab91
</div>

<h3>Implementation of Q-Learning Algorithm</h3>

<pre><code class="language-python">import numpy as np
import gym

class QLearningAgent:
    """Q-Learning Agent"""

    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        """
        Args:
            n_states: Number of states
            n_actions: Number of actions
            alpha: Learning rate
            gamma: Discount factor
            epsilon: Œµ for Œµ-greedy
        """
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_actions = n_actions

    def select_action(self, state):
        """Select action using Œµ-greedy policy"""
        if np.random.rand() < self.epsilon:
            # Random action (exploration)
            return np.random.randint(self.n_actions)
        else:
            # Best action (exploitation)
            return np.argmax(self.Q[state])

    def update(self, state, action, reward, next_state, done):
        """Update Q-value"""
        if done:
            # Terminal state
            td_target = reward
        else:
            # Q-learning update equation
            td_target = reward + self.gamma * np.max(self.Q[next_state])

        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error


def train_q_learning(env, agent, num_episodes=1000):
    """Train Q-learning"""
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0

        while True:
            # Select action
            action = agent.select_action(state)

            # Execute in environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Update Q-value
            agent.update(state, action, reward, next_state, done)

            total_reward += reward

            if done:
                break

            state = next_state

        episode_rewards.append(total_reward)

        # Display progress
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}")

    return episode_rewards


# Example usage: FrozenLake
print("\n=== Training Q-Learning ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

rewards = train_q_learning(env, agent, num_episodes=1000)

print(f"\nLearned Q-table (partial):")
print(agent.Q[:16].reshape(4, 4, -1)[:, :, 0])  # Q-values for action 0
env.close()
</code></pre>

<h3>Visualizing the Q-Table</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

def visualize_q_table(Q, env_shape=(4, 4)):
    """Visualize Q-table"""
    n_states = Q.shape[0]
    n_actions = Q.shape[1]

    fig, axes = plt.subplots(1, n_actions, figsize=(16, 4))

    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']

    for action in range(n_actions):
        Q_action = Q[:, action].reshape(env_shape)

        sns.heatmap(Q_action, annot=True, fmt='.2f', cmap='YlOrRd',
                   ax=axes[action], cbar=True, square=True)
        axes[action].set_title(f'Q-value: {action_names[action]}')
        axes[action].set_xlabel('Column')
        axes[action].set_ylabel('Row')

    plt.tight_layout()
    plt.savefig('q_table_visualization.png', dpi=150, bbox_inches='tight')
    print("Q-table saved: q_table_visualization.png")
    plt.close()


# Visualize Q-table
visualize_q_table(agent.Q)
</code></pre>

<h3>Visualizing Learning Curves</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def plot_learning_curve(rewards, window=100):
    """Plot learning curve"""
    # Calculate moving average
    smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')

    plt.figure(figsize=(12, 5))

    # Episode rewards
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.3, label='Episode Reward')
    plt.plot(range(window-1, len(rewards)), smoothed_rewards,
             linewidth=2, label=f'{window}-Episode Moving Average')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Q-Learning Learning Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Cumulative rewards
    plt.subplot(1, 2, 2)
    cumulative_rewards = np.cumsum(rewards)
    plt.plot(cumulative_rewards, linewidth=2, color='green')
    plt.xlabel('Episode')
    plt.ylabel('Cumulative Reward')
    plt.title('Cumulative Reward')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('q_learning_curve.png', dpi=150, bbox_inches='tight')
    print("Learning curve saved: q_learning_curve.png")
    plt.close()


plot_learning_curve(rewards)
</code></pre>

<hr>

<h2>2.3 SARSA (State-Action-Reward-State-Action)</h2>

<h3>Basic Principle of SARSA</h3>

<p><strong>SARSA</strong> is the <strong>on-policy version</strong> of Q-learning. It updates using the action actually taken:</p>

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$

<p>Key difference:</p>
<ul>
<li>Q-learning: Uses $\max_{a'} Q(s_{t+1}, a')$ (best action)</li>
<li>SARSA: Uses $Q(s_{t+1}, a_{t+1})$ (action actually taken)</li>
</ul>

<div class="mermaid">
graph LR
    S1["S_t"] --> A1["A_t"]
    A1 --> R["R_t+1"]
    R --> S2["S_t+1"]
    S2 --> A2["A_t+1"]
    A2 --> Update["Update Q(S_t, A_t)"]

    style S1 fill:#b3e5fc
    style A1 fill:#c5e1a5
    style R fill:#fff9c4
    style S2 fill:#b3e5fc
    style A2 fill:#c5e1a5
    style Update fill:#ffab91
</div>

<h3>Comparison of Q-Learning and SARSA</h3>

<table>
<thead>
<tr>
<th>Aspect</th>
<th>Q-Learning</th>
<th>SARSA</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Learning Type</strong></td>
<td>Off-policy</td>
<td>On-policy</td>
</tr>
<tr>
<td><strong>Update Equation</strong></td>
<td>$r + \gamma \max_a Q(s', a)$</td>
<td>$r + \gamma Q(s', a')$</td>
</tr>
<tr>
<td><strong>Exploration Impact</strong></td>
<td>Does not affect learning</td>
<td>Affects learning</td>
</tr>
<tr>
<td><strong>Convergence Target</strong></td>
<td>Optimal policy</td>
<td>Value of current policy</td>
</tr>
<tr>
<td><strong>Safety</strong></td>
<td>Does not consider risk</td>
<td>Considers risk</td>
</tr>
<tr>
<td><strong>Application Scenario</strong></td>
<td>Simulation environments</td>
<td>Real environment learning</td>
</tr>
</tbody>
</table>

<h3>Implementation of SARSA</h3>

<pre><code class="language-python">import numpy as np
import gym

class SARSAAgent:
    """SARSA Agent"""

    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        """
        Args:
            n_states: Number of states
            n_actions: Number of actions
            alpha: Learning rate
            gamma: Discount factor
            epsilon: Œµ for Œµ-greedy
        """
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_actions = n_actions

    def select_action(self, state):
        """Select action using Œµ-greedy policy"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])

    def update(self, state, action, reward, next_state, next_action, done):
        """Update Q-value (SARSA)"""
        if done:
            td_target = reward
        else:
            # SARSA update equation (uses action actually taken next)
            td_target = reward + self.gamma * self.Q[next_state, next_action]

        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error


def train_sarsa(env, agent, num_episodes=1000):
    """Train SARSA"""
    episode_rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        action = agent.select_action(state)  # Initial action selection
        total_reward = 0

        while True:
            # Execute in environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            if not done:
                # Select next action (characteristic of SARSA)
                next_action = agent.select_action(next_state)
            else:
                next_action = None

            # Update Q-value
            agent.update(state, action, reward, next_state, next_action, done)

            total_reward += reward

            if done:
                break

            state = next_state
            action = next_action  # Transition to next action

        episode_rewards.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}")

    return episode_rewards


# Example usage
print("\n=== Training SARSA ===")

env = gym.make('FrozenLake-v1', is_slippery=False)

sarsa_agent = SARSAAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

sarsa_rewards = train_sarsa(env, sarsa_agent, num_episodes=1000)

print(f"\nLearned Q-table (SARSA):")
print(sarsa_agent.Q[:16].reshape(4, 4, -1)[:, :, 0])
env.close()
</code></pre>

<hr>

<h2>2.4 Œµ-Greedy Exploration Strategy</h2>

<h3>Exploration vs Exploitation Trade-off</h3>

<p>In reinforcement learning, balancing <strong>exploration</strong> and <strong>exploitation</strong> is crucial:</p>

<ul>
<li><strong>Exploration</strong>: Try new states and actions to understand the environment</li>
<li><strong>Exploitation</strong>: Select the best action based on current knowledge</li>
</ul>

<h3>Œµ-Greedy Policy</h3>

<p>The simplest exploration strategy:</p>

$$
a = \begin{cases}
\text{random action} & \text{with probability } \epsilon \\
\arg\max_a Q(s, a) & \text{with probability } 1 - \epsilon
\end{cases}
$$

<h3>Epsilon Decay</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

class EpsilonGreedy:
    """Œµ-greedy policy (with decay functionality)"""

    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        """
        Args:
            epsilon_start: Initial Œµ
            epsilon_end: Minimum Œµ
            epsilon_decay: Decay rate
        """
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

    def select_action(self, Q, state, n_actions):
        """Select action"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(n_actions)
        else:
            return np.argmax(Q[state])

    def decay(self):
        """Decay Œµ"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)


# Visualize epsilon decay patterns
print("\n=== Visualizing Œµ Decay Patterns ===")

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Different decay rates
decay_rates = [0.99, 0.995, 0.999]

for i, decay_rate in enumerate(decay_rates):
    epsilon_greedy = EpsilonGreedy(epsilon_start=1.0, epsilon_end=0.01,
                                   epsilon_decay=decay_rate)
    epsilons = [epsilon_greedy.epsilon]

    for _ in range(1000):
        epsilon_greedy.decay()
        epsilons.append(epsilon_greedy.epsilon)

    axes[i].plot(epsilons, linewidth=2)
    axes[i].set_xlabel('Episode')
    axes[i].set_ylabel('Œµ')
    axes[i].set_title(f'Decay Rate = {decay_rate}')
    axes[i].grid(True, alpha=0.3)
    axes[i].set_ylim([0, 1.1])

plt.tight_layout()
plt.savefig('epsilon_decay.png', dpi=150, bbox_inches='tight')
print("Œµ decay patterns saved: epsilon_decay.png")
plt.close()
</code></pre>

<h3>Other Exploration Strategies</h3>

<h4>Softmax (Boltzmann) Exploration</h4>

<p>Probabilistic selection based on action values:</p>

$$
P(a | s) = \frac{\exp(Q(s,a) / \tau)}{\sum_{a'} \exp(Q(s,a') / \tau)}
$$

<p>$\tau$ is the temperature parameter (higher means more random)</p>

<h4>Upper Confidence Bound (UCB)</h4>

<p>Exploration considering uncertainty:</p>

$$
a = \arg\max_a \left[ Q(s,a) + c \sqrt{\frac{\ln t}{N(s,a)}} \right]
$$

<p>$N(s,a)$ is the number of times action $a$ was selected, $c$ is the exploration coefficient</p>

<hr>

<h2>2.5 Impact of Hyperparameters</h2>

<h3>Learning Rate Œ±</h3>

<p>The learning rate $\alpha$ controls the strength of updates:</p>

<ul>
<li><strong>Large Œ± (e.g., 0.5)</strong>: Fast learning but unstable</li>
<li><strong>Small Œ± (e.g., 0.01)</strong>: Stable but slow convergence</li>
<li><strong>Recommended value</strong>: 0.1 - 0.3</li>
</ul>

<h3>Discount Factor Œ≥</h3>

<p>The discount factor $\gamma$ determines the importance of future rewards:</p>

<ul>
<li><strong>Œ≥ = 0</strong>: Only considers immediate rewards (myopic)</li>
<li><strong>Œ≥ ‚Üí 1</strong>: Considers distant future (far-sighted)</li>
<li><strong>Recommended value</strong>: 0.95 - 0.99</li>
</ul>

<h3>Implementation of Hyperparameter Search</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import gym

def hyperparameter_search(env_name, param_name, param_values, num_episodes=500):
    """Investigate the impact of hyperparameters"""
    results = {}

    for value in param_values:
        print(f"\nTraining with {param_name} = {value}...")

        env = gym.make(env_name)

        if param_name == 'alpha':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=value, gamma=0.99, epsilon=0.1)
        elif param_name == 'gamma':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=0.1, gamma=value, epsilon=0.1)
        elif param_name == 'epsilon':
            agent = QLearningAgent(env.observation_space.n, env.action_space.n,
                                  alpha=0.1, gamma=0.99, epsilon=value)

        rewards = train_q_learning(env, agent, num_episodes=num_episodes)
        results[value] = rewards
        env.close()

    return results


# Investigate impact of learning rate
print("=== Investigating Impact of Learning Rate Œ± ===")

alpha_values = [0.01, 0.05, 0.1, 0.3, 0.5]
alpha_results = hyperparameter_search('FrozenLake-v1', 'alpha', alpha_values)

# Visualization
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
for alpha, rewards in alpha_results.items():
    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')
    plt.plot(smoothed, label=f'Œ± = {alpha}', linewidth=2)

plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('Impact of Learning Rate Œ±')
plt.legend()
plt.grid(True, alpha=0.3)

# Investigate impact of discount factor
gamma_values = [0.5, 0.9, 0.95, 0.99, 0.999]
gamma_results = hyperparameter_search('FrozenLake-v1', 'gamma', gamma_values)

plt.subplot(1, 2, 2)
for gamma, rewards in gamma_results.items():
    smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')
    plt.plot(smoothed, label=f'Œ≥ = {gamma}', linewidth=2)

plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('Impact of Discount Factor Œ≥')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('hyperparameter_impact.png', dpi=150, bbox_inches='tight')
print("\nHyperparameter impact saved: hyperparameter_impact.png")
plt.close()
</code></pre>

<hr>

<h2>2.6 Practice: Taxi-v3 Environment</h2>

<h3>Overview of Taxi-v3 Environment</h3>

<p><strong>Taxi-v3</strong> is an environment where a taxi picks up a passenger and delivers them to a destination:</p>

<ul>
<li><strong>State space</strong>: 500 states (5√ó5 grid √ó 5 passenger locations √ó 4 destinations)</li>
<li><strong>Action space</strong>: 6 actions (up, down, left, right, pickup, dropoff)</li>
<li><strong>Rewards</strong>: +20 for reaching correct destination, -1 per step, -10 for illegal pickup/dropoff</li>
</ul>

<h3>Q-Learning on Taxi-v3</h3>

<pre><code class="language-python">import numpy as np
import gym
import matplotlib.pyplot as plt

# Taxi-v3 environment
print("=== Q-Learning on Taxi-v3 Environment ===")

env = gym.make('Taxi-v3', render_mode=None)

print(f"State space: {env.observation_space.n}")
print(f"Action space: {env.action_space.n}")
print(f"Actions: {['South', 'North', 'East', 'West', 'Pickup', 'Dropoff']}")

# Q-learning agent
taxi_agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

# Training
taxi_rewards = train_q_learning(env, taxi_agent, num_episodes=5000)

# Learning curve
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
smoothed = np.convolve(taxi_rewards, np.ones(100)/100, mode='valid')
plt.plot(smoothed, linewidth=2, color='blue')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('Taxi-v3 Q-Learning Learning Curve')
plt.grid(True, alpha=0.3)

# Calculate success rate
success_rate = []
window = 100
for i in range(len(taxi_rewards) - window):
    success = np.sum(np.array(taxi_rewards[i:i+window]) > 0) / window
    success_rate.append(success)

plt.subplot(1, 2, 2)
plt.plot(success_rate, linewidth=2, color='green')
plt.xlabel('Episode')
plt.ylabel('Success Rate')
plt.title('Task Success Rate (100-Episode Moving Average)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('taxi_training.png', dpi=150, bbox_inches='tight')
print("Taxi training results saved: taxi_training.png")
plt.close()

env.close()
</code></pre>

<h3>Evaluating the Trained Agent</h3>

<pre><code class="language-python">def evaluate_agent(env, agent, num_episodes=100, render=False):
    """Evaluate trained agent"""
    total_rewards = []
    total_steps = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        steps = 0

        while steps < 200:  # Maximum steps
            # Select best action (no exploration)
            action = np.argmax(agent.Q[state])

            state, reward, terminated, truncated, _ = env.step(action)
            episode_reward += reward
            steps += 1

            if terminated or truncated:
                break

        total_rewards.append(episode_reward)
        total_steps.append(steps)

    return total_rewards, total_steps


# Evaluation
print("\n=== Evaluating Trained Agent ===")

env = gym.make('Taxi-v3', render_mode=None)
eval_rewards, eval_steps = evaluate_agent(env, taxi_agent, num_episodes=100)

print(f"Average reward: {np.mean(eval_rewards):.2f} ¬± {np.std(eval_rewards):.2f}")
print(f"Average steps: {np.mean(eval_steps):.2f} ¬± {np.std(eval_steps):.2f}")
print(f"Success rate: {np.sum(np.array(eval_rewards) > 0) / len(eval_rewards) * 100:.1f}%")

env.close()
</code></pre>

<hr>

<h2>2.7 Practice: Cliff Walking Environment</h2>

<h3>Definition of Cliff Walking Environment</h3>

<p><strong>Cliff Walking</strong> is an environment where you must reach the goal while avoiding cliffs. It clearly demonstrates the difference between Q-learning and SARSA:</p>

<ul>
<li><strong>4√ó12 grid</strong>: Start at bottom-left, goal at bottom-right</li>
<li><strong>Cliff area</strong>: Central portion of bottom edge (stepping on it gives -100 penalty)</li>
<li><strong>Rewards</strong>: -1 per step, -100 for cliff, 0 for goal</li>
</ul>

<h3>Implementation on Cliff Walking Environment</h3>

<pre><code class="language-python">import numpy as np
import gym

# Cliff Walking environment
print("=== Cliff Walking Environment ===")

env = gym.make('CliffWalking-v0')

print(f"State space: {env.observation_space.n}")
print(f"Action space: {env.action_space.n}")
print(f"Grid size: 4√ó12")

# Q-learning agent
cliff_q_agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# SARSA agent
cliff_sarsa_agent = SARSAAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# Training
print("\nTraining with Q-learning...")
q_rewards = train_q_learning(env, cliff_q_agent, num_episodes=500)

env = gym.make('CliffWalking-v0')
print("\nTraining with SARSA...")
sarsa_rewards = train_sarsa(env, cliff_sarsa_agent, num_episodes=500)

# Comparison visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
smoothed_q = np.convolve(q_rewards, np.ones(10)/10, mode='valid')
smoothed_sarsa = np.convolve(sarsa_rewards, np.ones(10)/10, mode='valid')

plt.plot(smoothed_q, label='Q-Learning', linewidth=2)
plt.plot(smoothed_sarsa, label='SARSA', linewidth=2)
plt.xlabel('Episode')
plt.ylabel('Reward')
plt.title('Cliff Walking: Q-Learning vs SARSA')
plt.legend()
plt.grid(True, alpha=0.3)

# Visualize policy (display with arrows)
plt.subplot(1, 2, 2)

def visualize_policy(Q, shape=(4, 12)):
    """Visualize learned policy"""
    policy = np.argmax(Q, axis=1)
    policy_grid = policy.reshape(shape)

    # Arrow directions
    arrows = {0: '‚Üë', 1: '‚Üí', 2: '‚Üì', 3: '‚Üê'}

    fig, ax = plt.subplots(figsize=(12, 4))
    ax.imshow(np.zeros(shape), cmap='Blues', alpha=0.3)

    for i in range(shape[0]):
        for j in range(shape[1]):
            state = i * shape[1] + j
            action = policy[state]

            # Display cliff area in red
            if i == 3 and 1 <= j <= 10:
                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                          fill=True, color='red', alpha=0.3))

            # Goal
            if i == 3 and j == 11:
                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                          fill=True, color='green', alpha=0.3))

            # Arrow
            ax.text(j, i, arrows[action], ha='center', va='center',
                   fontsize=16, fontweight='bold')

    ax.set_xlim(-0.5, shape[1]-0.5)
    ax.set_ylim(shape[0]-0.5, -0.5)
    ax.set_xticks(range(shape[1]))
    ax.set_yticks(range(shape[0]))
    ax.grid(True)
    ax.set_title('Learned Policy (Q-Learning)')


visualize_policy(cliff_q_agent.Q)

plt.tight_layout()
plt.savefig('cliff_walking_comparison.png', dpi=150, bbox_inches='tight')
print("\nCliff Walking comparison saved: cliff_walking_comparison.png")
plt.close()

env.close()
</code></pre>

<h3>Path Differences between Q-Learning and SARSA</h3>

<blockquote>
<p><strong>Important Observation</strong>: In Cliff Walking, Q-learning learns the <strong>shortest path (close to cliff)</strong>, while SARSA learns a <strong>safe path (away from cliff)</strong>. This is because SARSA incorporates accidental cliff falls from Œµ-greedy exploration into its learning.</p>
</blockquote>

<hr>

<h2>Exercises</h2>

<details>
<summary><strong>Exercise 1: Comparing Convergence Speed of Q-Learning and SARSA</strong></summary>

<p>Train Q-learning and SARSA on the FrozenLake environment with the same hyperparameters and compare their convergence speeds.</p>

<pre><code class="language-python">import gym
import numpy as np

# TODO: Train Q-learning and SARSA with same settings
# TODO: Plot episode rewards
# TODO: Compare number of episodes needed for convergence
# Expected: Convergence speed differs depending on environment
</code></pre>

</details>

<details>
<summary><strong>Exercise 2: Optimizing Œµ Decay Schedules</strong></summary>

<p>Implement different Œµ decay patterns (linear decay, exponential decay, step decay) and compare their performance on Taxi-v3.</p>

<pre><code class="language-python">import numpy as np

# TODO: Implement 3 types of Œµ decay schedules
# TODO: Evaluate each schedule on Taxi-v3
# TODO: Compare learning curves and final performance
# Hint: Emphasize exploration early, exploitation later
</code></pre>

</details>

<details>
<summary><strong>Exercise 3: Implementing Double Q-Learning</strong></summary>

<p>Implement Double Q-Learning to prevent overestimation and compare performance with standard Q-learning.</p>

<pre><code class="language-python">import numpy as np

# TODO: Implement Double Q-Learning using two Q-tables
# TODO: Train on FrozenLake environment
# TODO: Compare Q-value estimation error with standard Q-learning
# Theory: Double algorithm reduces overestimation bias
</code></pre>

</details>

<details>
<summary><strong>Exercise 4: Adaptive Learning Rate Adjustment</strong></summary>

<p>Implement an adaptive learning rate that adjusts based on visit count and compare it with fixed learning rate.</p>

<pre><code class="language-python">import numpy as np

# TODO: Implement adaptive learning rate Œ±(s,a) = 1 / (1 + N(s,a))
# TODO: Compare performance with fixed learning rate
# TODO: Visualize visit counts for each state
# Expected: Adaptive learning rate leads to more stable convergence
</code></pre>

</details>

<details>
<summary><strong>Exercise 5: Experiments on Custom Environments</strong></summary>

<p>Discretize the state space for other OpenAI Gym environments (CartPole-v1, MountainCar-v0, etc.) and apply Q-learning.</p>

<pre><code class="language-python">import gym
import numpy as np

# TODO: Implement function to discretize continuous state space
# TODO: Apply Q-learning on discretized CartPole environment
# TODO: Investigate relationship between discretization granularity and performance
# Challenge: Appropriate discretization of continuous space is critical
</code></pre>

</details>

<hr>

<h2>Summary</h2>

<p>In this chapter, we learned Q-learning and SARSA based on temporal difference learning.</p>

<h3>Key Points</h3>

<ul>
<li><strong>TD Learning</strong>: Updates at each step without waiting for episode end</li>
<li><strong>Q-Learning</strong>: Off-policy type, directly learns optimal policy</li>
<li><strong>SARSA</strong>: On-policy type, learning that considers exploration impact</li>
<li><strong>Œµ-greedy</strong>: Simple policy that controls exploration-exploitation balance</li>
<li><strong>Learning rate Œ±</strong>: Controls update strength (0.1-0.3 recommended)</li>
<li><strong>Discount factor Œ≥</strong>: Importance of future rewards (0.95-0.99 recommended)</li>
<li><strong>Q-table</strong>: Stores value for each state-action pair</li>
<li><strong>Application</strong>: Tasks with discrete state and action spaces</li>
</ul>

<h3>When to Use Q-Learning vs SARSA</h3>

<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Algorithm</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Simulation environment</strong></td>
<td>Q-Learning</td>
<td>Efficiently learns optimal policy</td>
</tr>
<tr>
<td><strong>Real environment/Robotics</strong></td>
<td>SARSA</td>
<td>Learns safe policy</td>
</tr>
<tr>
<td><strong>Dangerous states present</strong></td>
<td>SARSA</td>
<td>Risk-averse tendency</td>
</tr>
<tr>
<td><strong>Fast convergence needed</strong></td>
<td>Q-Learning</td>
<td>Flexible with off-policy</td>
</tr>
</tbody>
</table>

<h3>Next Steps</h3>

<p>In the next chapter, we will learn about <strong>Deep Q-Network (DQN)</strong>. We will master techniques for approximating action-value functions using neural networks for large-scale and continuous state spaces that cannot be handled by Q-tables, including Experience Replay, Target Network, and applications to Atari games.</p>

<div class="navigation">
    <a href="chapter1-reinforcement-learning-basics.html" class="nav-button">‚Üê Chapter 1: Reinforcement Learning Basics</a>
    <a href="chapter3-dqn.html" class="nav-button">Chapter 3: Deep Q-Network ‚Üí</a>
</div>

</main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>In the event of direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, the author and Tohoku University disclaim all liability to the maximum extent permitted by applicable law.</li>
            <li>The content of this material is subject to change, update, or discontinuation without notice.</li>
            <li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
