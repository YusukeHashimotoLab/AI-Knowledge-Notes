<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Policy Gradient Methods - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 4: Policy Gradient Methods</h1>
<p class="subtitle">Policy-based Reinforcement Learning: Theory and Implementation of REINFORCE, Actor-Critic, A2C, and PPO</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 28 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Exercises: 6</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the differences between policy-based and value-based approaches</li>
<li>‚úÖ Understand the mathematical formulation of policy gradients</li>
<li>‚úÖ Implement the REINFORCE algorithm</li>
<li>‚úÖ Understand and implement the Actor-Critic architecture</li>
<li>‚úÖ Implement Advantage Actor-Critic (A2C)</li>
<li>‚úÖ Understand Proximal Policy Optimization (PPO)</li>
<li>‚úÖ Solve continuous control tasks such as LunarLander</li>
</ul>
<hr/>
<h2>4.1 Policy-based vs Value-based</h2>
<h3>4.1.1 Two Approaches</h3>
<p>There are two major approaches in reinforcement learning:</p>
<table>
<thead>
<tr>
<th>Characteristic</th>
<th>Value-based</th>
<th>Policy-based</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Learning Target</strong></td>
<td>Value function $Q(s, a)$ or $V(s)$</td>
<td>Learn policy $\pi(a|s)$ directly</td>
</tr>
<tr>
<td><strong>Action Selection</strong></td>
<td>Indirect ($\arg\max_a Q(s,a)$)</td>
<td>Direct (sample from $\pi(a|s)$)</td>
</tr>
<tr>
<td><strong>Action Space</strong></td>
<td>Suitable for discrete actions</td>
<td>Can handle continuous actions</td>
</tr>
<tr>
<td><strong>Stochastic Policy</strong></td>
<td>Difficult to handle (use Œµ-greedy, etc.)</td>
<td>Naturally handled</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>Optimal policy guaranteed (under conditions)</td>
<td>Possible local optima</td>
</tr>
<tr>
<td><strong>Sample Efficiency</strong></td>
<td>High (experience replay possible)</td>
<td>Low (on-policy learning)</td>
</tr>
<tr>
<td><strong>Representative Algorithms</strong></td>
<td>Q-learning, DQN, Double DQN</td>
<td>REINFORCE, A2C, PPO, TRPO</td>
</tr>
</tbody>
</table>
<h3>4.1.2 Motivation for Policy Gradient</h3>
<p><strong>Challenges of value-based approaches</strong>:</p>
<ul>
<li><strong>Continuous action space</strong>: In robot control with infinite action choices, $\arg\max$ computation is difficult</li>
<li><strong>Stochastic policy</strong>: Difficult to handle cases where stochastic actions are optimal, like rock-paper-scissors</li>
<li><strong>High-dimensional action space</strong>: Computing all action values is inefficient when action combinations are vast</li>
</ul>
<p><strong>Solutions provided by policy-based approaches</strong>:</p>
<ul>
<li>Model policy with parameter $\theta$: $\pi_\theta(a|s)$</li>
<li>Directly optimize $\theta$ to maximize expected return</li>
<li>Can represent policy with neural networks</li>
</ul>
<div class="mermaid">
graph LR
    subgraph "Value-based Approach"
        S1["State s"]
        Q["Q-function<br/>Q(s,a)"]
        AM["argmax"]
        A1["Action a"]

        S1 --&gt; Q
        Q --&gt; AM
        AM --&gt; A1

        style Q fill:#e74c3c,color:#fff
    end

    subgraph "Policy-based Approach"
        S2["State s"]
        P["Policy œÄ(a|s)<br/>parameterized by Œ∏"]
        A2["Action a<br/>(sampled)"]

        S2 --&gt; P
        P --&gt; A2

        style P fill:#27ae60,color:#fff
    end
</div>
<h3>4.1.3 Policy Gradient Formulation</h3>
<p>We represent policy $\pi_\theta(a|s)$ with parameter $\theta$ and maximize the expected return (objective function) $J(\theta)$:</p>

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
$$

<p>Where:</p>
<ul>
<li>$\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \ldots)$: trajectory</li>
<li>$R(\tau) = \sum_{t=0}^{T} \gamma^t r_t$: cumulative reward of trajectory</li>
</ul>
<p><strong>Policy Gradient Theorem</strong> states that the gradient of $J(\theta)$ can be expressed as:</p>

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t \right]
$$

<p>Where $R_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$ is the cumulative reward from time $t$.</p>
<blockquote>
<p><strong>Intuitive Understanding</strong>: Increase the probability of actions that led to high returns, and decrease the probability of actions that led to low returns. This updates the policy parameters toward generating better trajectories.</p>
</blockquote>
<hr/>
<h2>4.2 REINFORCE Algorithm</h2>
<h3>4.2.1 Basic Principle of REINFORCE</h3>
<p><strong>REINFORCE</strong> (Williams, 1992) is the simplest policy gradient algorithm. It uses Monte Carlo methods to estimate returns.</p>
<h4>Algorithm</h4>
<ol>
<li>Execute one episode with policy $\pi_\theta(a|s)$ and collect trajectory $\tau$</li>
<li>Calculate return $R_t$ at each time step $t$</li>
<li>Update parameters by gradient ascent:
$$
\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t
$$
</li>
</ol>
<h4>Variance Reduction: Baseline</h4>
<p>Subtracting a constant $b$ from the return does not change the expected gradient (unbiasedness), which allows variance reduction:</p>

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) (R_t - b) \right]
$$

<p>Common choice: <strong>$b = V(s_t)$</strong> (state value function)</p>
<div class="mermaid">
graph TB
    subgraph "REINFORCE Algorithm"
        Init["Initialize Œ∏"]
        Episode["Run Episode<br/>Sample œÑ ~ œÄ_Œ∏"]
        Compute["Compute Returns<br/>R_t for all t"]
        Grad["Compute Gradient<br/>‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) R_t"]
        Update["Update Parameters<br/>Œ∏ ‚Üê Œ∏ + Œ± ‚àá_Œ∏ J(Œ∏)"]
        Check["Converged?"]

        Init --&gt; Episode
        Episode --&gt; Compute
        Compute --&gt; Grad
        Grad --&gt; Update
        Update --&gt; Check
        Check --&gt;|No| Episode
        Check --&gt;|Yes| Done["Done"]

        style Init fill:#7b2cbf,color:#fff
        style Done fill:#27ae60,color:#fff
        style Grad fill:#e74c3c,color:#fff
    end
</div>
<h3>4.2.2 REINFORCE Implementation (CartPole)</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
import matplotlib.pyplot as plt
from collections import deque

class PolicyNetwork(nn.Module):
    """
    Policy Network for REINFORCE

    Input: state s
    Output: action probability œÄ(a|s)
    """

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: state [batch_size, state_dim]

        Returns:
            action_probs: action probabilities [batch_size, action_dim]
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        logits = self.fc3(x)
        action_probs = F.softmax(logits, dim=-1)
        return action_probs


class REINFORCE:
    """REINFORCE Algorithm"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        """
        Args:
            state_dim: dimension of state space
            action_dim: dimension of action space
            lr: learning rate
            gamma: discount factor
        """
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # Save episode data
        self.saved_log_probs = []
        self.rewards = []

    def select_action(self, state):
        """
        Select action according to policy

        Args:
            state: state

        Returns:
            action: selected action
        """
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy(state)

        # Sample from probability distribution
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()

        # Save log œÄ(a|s) for gradient computation
        self.saved_log_probs.append(m.log_prob(action))

        return action.item()

    def update(self):
        """
        Update parameters after episode completion
        """
        R = 0
        returns = []

        # Compute returns (calculate in reverse order)
        for r in self.rewards[::-1]:
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.tensor(returns)

        # Normalize (variance reduction)
        if len(returns) &gt; 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Compute policy gradient
        policy_loss = []
        for log_prob, R in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * R)

        # Gradient ascent (minimize loss = minimize -objective function)
        self.optimizer.zero_grad()
        policy_loss = torch.cat(policy_loss).sum()
        policy_loss.backward()
        self.optimizer.step()

        # Reset
        self.saved_log_probs = []
        self.rewards = []

        return policy_loss.item()


# Demonstration
print("=== REINFORCE on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

print(f"Environment: CartPole-v1")
print(f"  State dimension: {state_dim}")
print(f"  Action dimension: {action_dim}")

agent = REINFORCE(state_dim, action_dim, lr=0.01, gamma=0.99)

print(f"\nAgent: REINFORCE")
total_params = sum(p.numel() for p in agent.policy.parameters())
print(f"  Total parameters: {total_params:,}")

# Training
num_episodes = 500
episode_rewards = []
moving_avg = deque(maxlen=100)

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0

    for t in range(1000):
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        agent.rewards.append(reward)
        episode_reward += reward

        state = next_state

        if done:
            break

    # Update after episode completion
    loss = agent.update()

    episode_rewards.append(episode_reward)
    moving_avg.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(moving_avg)
        print(f"Episode {episode+1:3d}, Avg Reward (last 100): {avg_reward:.2f}, Loss: {loss:.4f}")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(moving_avg):.2f}")

# Visualization
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')

# Moving average
window = 50
moving_avg_plot = [np.mean(episode_rewards[max(0, i-window):i+1])
                   for i in range(len(episode_rewards))]
ax.plot(moving_avg_plot, linewidth=2, color='darkorange', label=f'Moving Average ({window} episodes)')

ax.axhline(y=195, color='red', linestyle='--', label='Solved Threshold (195)')
ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
ax.set_title('REINFORCE on CartPole-v1', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\n‚úì REINFORCE characteristics:")
print("  ‚Ä¢ Simple and easy to implement")
print("  ‚Ä¢ Monte Carlo method (update after episode completion)")
print("  ‚Ä¢ High variance (large fluctuation in returns)")
print("  ‚Ä¢ On-policy (sampling with current policy)")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== REINFORCE on CartPole ===

Environment: CartPole-v1
  State dimension: 4
  Action dimension: 2

Agent: REINFORCE
  Total parameters: 16,642

Training...
Episode  50, Avg Reward (last 100): 23.45, Loss: 15.2341
Episode 100, Avg Reward (last 100): 45.67, Loss: 12.5678
Episode 150, Avg Reward (last 100): 89.23, Loss: 8.3456
Episode 200, Avg Reward (last 100): 142.56, Loss: 5.6789
Episode 250, Avg Reward (last 100): 178.34, Loss: 3.4567
Episode 300, Avg Reward (last 100): 195.78, Loss: 2.1234
Episode 350, Avg Reward (last 100): 210.45, Loss: 1.5678
Episode 400, Avg Reward (last 100): 230.67, Loss: 1.2345
Episode 450, Avg Reward (last 100): 245.89, Loss: 0.9876
Episode 500, Avg Reward (last 100): 260.34, Loss: 0.7654

Training completed!
Final average reward (last 100 episodes): 260.34

‚úì REINFORCE characteristics:
  ‚Ä¢ Simple and easy to implement
  ‚Ä¢ Monte Carlo method (update after episode completion)
  ‚Ä¢ High variance (large fluctuation in returns)
  ‚Ä¢ On-policy (sampling with current policy)
</code></pre>
<h3>4.2.3 Challenges of REINFORCE</h3>
<p>REINFORCE has the following challenges:</p>
<ul>
<li><strong>High variance</strong>: Large variance in return $R_t$, resulting in unstable learning</li>
<li><strong>Sample inefficient</strong>: Cannot update until episode completion</li>
<li><strong>Monte Carlo method</strong>: Learning is slow for long episodes</li>
</ul>
<p><strong>Solution</strong>: <strong>Actor-Critic</strong> architecture estimates returns using value functions</p>
<hr/>
<h2>4.3 Actor-Critic Method</h2>
<h3>4.3.1 Principle of Actor-Critic</h3>
<p><strong>Actor-Critic</strong> combines policy gradient (Actor) and value-based (Critic) methods.</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Role</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong></td>
<td>Learns policy $\pi_\theta(a|s)$</td>
<td>Action probability distribution</td>
</tr>
<tr>
<td><strong>Critic</strong></td>
<td>Learns value function $V_\phi(s)$</td>
<td>State value estimation</td>
</tr>
</tbody>
</table>
<h4>Advantages</h4>
<ul>
<li><strong>Low variance</strong>: Variance reduction using Critic's baseline $V(s)$</li>
<li><strong>TD learning</strong>: Can update during episodes (using TD error)</li>
<li><strong>Efficient</strong>: Faster learning than Monte Carlo methods</li>
</ul>
<h4>Update Equations</h4>
<p><strong>TD Error (Advantage)</strong>:</p>

$$
A_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$

<p><strong>Actor Update</strong>:</p>

$$
\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a_t|s_t) A_t
$$

<p><strong>Critic Update</strong>:</p>

$$
\phi \leftarrow \phi - \alpha_\phi \nabla_\phi (r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t))^2
$$

<div class="mermaid">
graph TB
    subgraph "Actor-Critic Architecture"
        State["State s_t"]

        Actor["Actor<br/>œÄ_Œ∏(a|s)"]
        Critic["Critic<br/>V_œÜ(s)"]

        Action["Action a_t"]
        Value["Value V(s_t)"]

        Env["Environment"]
        Reward["Reward r_t"]
        NextState["Next State s_{t+1}"]

        TDError["TD Error<br/>A_t = r_t + Œ≥V(s_{t+1}) - V(s_t)"]

        ActorUpdate["Actor Update<br/>Œ∏ ‚Üê Œ∏ + Œ± ‚àá_Œ∏ log œÄ A_t"]
        CriticUpdate["Critic Update<br/>œÜ ‚Üê œÜ - Œ± ‚àá_œÜ (A_t)¬≤"]

        State --&gt; Actor
        State --&gt; Critic

        Actor --&gt; Action
        Critic --&gt; Value

        Action --&gt; Env
        State --&gt; Env

        Env --&gt; Reward
        Env --&gt; NextState

        Reward --&gt; TDError
        Value --&gt; TDError
        NextState --&gt; Critic

        TDError --&gt; ActorUpdate
        TDError --&gt; CriticUpdate

        ActorUpdate -.-&gt; Actor
        CriticUpdate -.-&gt; Critic

        style Actor fill:#27ae60,color:#fff
        style Critic fill:#e74c3c,color:#fff
        style TDError fill:#f39c12,color:#fff
    end
</div>
<h3>4.3.2 Actor-Critic Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym

class ActorCriticNetwork(nn.Module):
    """
    Actor-Critic Network

    Has a shared feature extractor and two heads for Actor and Critic
    """

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCriticNetwork, self).__init__()

        # Shared layers
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actor head (policy)
        self.actor_head = nn.Linear(hidden_dim, action_dim)

        # Critic head (value function)
        self.critic_head = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        Args:
            state: state [batch_size, state_dim]

        Returns:
            action_probs: action probabilities [batch_size, action_dim]
            state_value: state value [batch_size, 1]
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # Actor output
        logits = self.actor_head(x)
        action_probs = F.softmax(logits, dim=-1)

        # Critic output
        state_value = self.critic_head(x)

        return action_probs, state_value


class ActorCritic:
    """Actor-Critic Algorithm"""

    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        """
        Args:
            state_dim: dimension of state space
            action_dim: dimension of action space
            lr: learning rate
            gamma: discount factor
        """
        self.gamma = gamma
        self.network = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """
        Select action according to policy

        Args:
            state: state

        Returns:
            action: selected action
            log_prob: log œÄ(a|s)
            state_value: V(s)
        """
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs, state_value = self.network(state)

        # Sample from probability distribution
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()
        log_prob = m.log_prob(action)

        return action.item(), log_prob, state_value

    def update(self, log_prob, state_value, reward, next_state, done):
        """
        Update parameters at each step (TD learning)

        Args:
            log_prob: log œÄ(a|s)
            state_value: V(s)
            reward: reward r
            next_state: next state s'
            done: episode termination flag

        Returns:
            loss: loss value
        """
        # Value estimation of next state
        if done:
            next_value = torch.tensor([0.0])
        else:
            next_state = torch.FloatTensor(next_state).unsqueeze(0)
            with torch.no_grad():
                _, next_value = self.network(next_state)

        # TD error (Advantage)
        td_target = reward + self.gamma * next_value
        td_error = td_target - state_value

        # Actor loss: -log œÄ(a|s) * A
        actor_loss = -log_prob * td_error.detach()

        # Critic loss: (TD error)^2
        critic_loss = td_error.pow(2)

        # Total loss
        loss = actor_loss + critic_loss

        # Update
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()


# Demonstration
print("=== Actor-Critic on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = ActorCritic(state_dim, action_dim, lr=0.001, gamma=0.99)

print(f"Environment: CartPole-v1")
print(f"Agent: Actor-Critic")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# Training
num_episodes = 300
episode_rewards = []

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    episode_loss = 0
    steps = 0

    for t in range(1000):
        action, log_prob, state_value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        # Update at each step
        loss = agent.update(log_prob, state_value, reward, next_state, done)

        episode_reward += reward
        episode_loss += loss
        steps += 1

        state = next_state

        if done:
            break

    episode_rewards.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        avg_loss = episode_loss / steps
        print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, Avg Loss: {avg_loss:.4f}")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\n‚úì Actor-Critic characteristics:")
print("  ‚Ä¢ Two networks: Actor and Critic")
print("  ‚Ä¢ TD learning (update at each step)")
print("  ‚Ä¢ Lower variance than REINFORCE")
print("  ‚Ä¢ More stable learning")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Actor-Critic on CartPole ===

Environment: CartPole-v1
Agent: Actor-Critic
  Total parameters: 17,027

Training...
Episode  50, Avg Reward: 45.67, Avg Loss: 2.3456
Episode 100, Avg Reward: 98.23, Avg Loss: 1.5678
Episode 150, Avg Reward: 165.45, Avg Loss: 0.9876
Episode 200, Avg Reward: 210.34, Avg Loss: 0.6543
Episode 250, Avg Reward: 245.67, Avg Loss: 0.4321
Episode 300, Avg Reward: 280.89, Avg Loss: 0.2987

Training completed!
Final average reward (last 100 episodes): 280.89

‚úì Actor-Critic characteristics:
  ‚Ä¢ Two networks: Actor and Critic
  ‚Ä¢ TD learning (update at each step)
  ‚Ä¢ Lower variance than REINFORCE
  ‚Ä¢ More stable learning
</code></pre>
<hr/>
<h2>4.4 Advantage Actor-Critic (A2C)</h2>
<h3>4.3.1 Improvements in A2C</h3>
<p><strong>A2C (Advantage Actor-Critic)</strong> is an improved version of Actor-Critic with the following features:</p>
<ul>
<li><strong>n-step returns</strong>: Uses returns looking ahead multiple steps</li>
<li><strong>Parallel environments</strong>: Simultaneous sampling from multiple environments (data diversity)</li>
<li><strong>Entropy regularization</strong>: Promotes exploration</li>
<li><strong>Generalized Advantage Estimation (GAE)</strong>: Adjusts bias-variance tradeoff</li>
</ul>
<h4>n-step Returns</h4>
<p>Uses rewards up to $n$ steps ahead instead of 1-step TD:</p>

$$
R_t^{(n)} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})
$$

<h4>Entropy Regularization</h4>
<p>Adds policy entropy to the objective function to promote exploration:</p>

$$
J(\theta) = \mathbb{E} \left[ \sum_t \log \pi_\theta(a_t|s_t) A_t + \beta H(\pi_\theta(\cdot|s_t)) \right]
$$

<p>Where $H(\pi) = -\sum_a \pi(a|s) \log \pi(a|s)$ is the entropy.</p>
<h3>4.4.2 A2C Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical

class A2CNetwork(nn.Module):
    """A2C Network with shared feature extractor"""

    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(A2CNetwork, self).__init__()

        # Shared feature extraction layers
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actor
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Critic
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        action_logits = self.actor(x)
        state_value = self.critic(x)

        return action_logits, state_value


class A2C:
    """
    Advantage Actor-Critic (A2C)

    Features:
      - n-step returns
      - Entropy regularization
      - Parallel environment support
    """

    def __init__(self, state_dim, action_dim, lr=0.0007, gamma=0.99,
                 n_steps=5, entropy_coef=0.01, value_coef=0.5):
        """
        Args:
            state_dim: dimension of state space
            action_dim: dimension of action space
            lr: learning rate
            gamma: discount factor
            n_steps: n-step returns
            entropy_coef: entropy regularization coefficient
            value_coef: value loss coefficient
        """
        self.gamma = gamma
        self.n_steps = n_steps
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef

        self.network = A2CNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """Action selection"""
        state = torch.FloatTensor(state).unsqueeze(0)
        action_logits, state_value = self.network(state)

        # Action distribution
        dist = Categorical(logits=action_logits)
        action = dist.sample()

        return action.item(), dist.log_prob(action), dist.entropy(), state_value

    def compute_returns(self, rewards, values, dones, next_value):
        """
        Compute n-step returns

        Args:
            rewards: reward sequence [n_steps]
            values: state value sequence [n_steps]
            dones: termination flag sequence [n_steps]
            next_value: value of next state after last state

        Returns:
            returns: n-step returns [n_steps]
            advantages: Advantage [n_steps]
        """
        returns = []
        R = next_value

        # Calculate in reverse order
        for step in reversed(range(len(rewards))):
            R = rewards[step] + self.gamma * R * (1 - dones[step])
            returns.insert(0, R)

        returns = torch.tensor(returns, dtype=torch.float32)
        values = torch.cat(values)

        # Advantage = Returns - V(s)
        advantages = returns - values.detach()

        return returns, advantages

    def update(self, log_probs, entropies, values, returns, advantages):
        """
        Parameter update

        Args:
            log_probs: list of log œÄ(a|s)
            entropies: list of entropies
            values: list of V(s)
            returns: n-step returns
            advantages: Advantage
        """
        log_probs = torch.cat(log_probs)
        entropies = torch.cat(entropies)
        values = torch.cat(values)

        # Actor loss: -log œÄ(a|s) * A
        actor_loss = -(log_probs * advantages.detach()).mean()

        # Critic loss: MSE(returns, V(s))
        critic_loss = F.mse_loss(values, returns)

        # Entropy regularization
        entropy_loss = -entropies.mean()

        # Total loss
        loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss

        # Update
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        self.optimizer.step()

        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()


# Demonstration
print("=== A2C on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = A2C(state_dim, action_dim, lr=0.0007, gamma=0.99, n_steps=5)

print(f"Environment: CartPole-v1")
print(f"Agent: A2C")
print(f"  n_steps: {agent.n_steps}")
print(f"  entropy_coef: {agent.entropy_coef}")
print(f"  value_coef: {agent.value_coef}")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# Training
num_episodes = 500
episode_rewards = []

print("\nTraining...")
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0

    # Collect n-step data
    log_probs = []
    entropies = []
    values = []
    rewards = []
    dones = []

    done = False
    while not done:
        action, log_prob, entropy, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        log_probs.append(log_prob)
        entropies.append(entropy)
        values.append(value)
        rewards.append(reward)
        dones.append(done)

        episode_reward += reward
        state = next_state

        # Update every n-steps or at episode end
        if len(rewards) &gt;= agent.n_steps or done:
            # Next state value
            if done:
                next_value = 0
            else:
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                with torch.no_grad():
                    _, next_value = agent.network(next_state_tensor)
                    next_value = next_value.item()

            # Compute returns and advantages
            returns, advantages = agent.compute_returns(rewards, values, dones, next_value)

            # Update
            loss, actor_loss, critic_loss, entropy_loss = agent.update(
                log_probs, entropies, values, returns, advantages
            )

            # Reset
            log_probs = []
            entropies = []
            values = []
            rewards = []
            dones = []

    episode_rewards.append(episode_reward)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-100:])
        print(f"Episode {episode+1:3d}, Avg Reward: {avg_reward:.2f}, "
              f"Loss: {loss:.4f} (Actor: {actor_loss:.4f}, Critic: {critic_loss:.4f}, Entropy: {entropy_loss:.4f})")

print(f"\nTraining completed!")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\n‚úì A2C characteristics:")
print("  ‚Ä¢ n-step returns (more accurate return estimation)")
print("  ‚Ä¢ Entropy regularization (exploration promotion)")
print("  ‚Ä¢ Gradient clipping (stability improvement)")
print("  ‚Ä¢ Parallel environment support (single environment in this example)")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== A2C on CartPole ===

Environment: CartPole-v1
Agent: A2C
  n_steps: 5
  entropy_coef: 0.01
  value_coef: 0.5
  Total parameters: 68,097

Training...
Episode  50, Avg Reward: 56.78, Loss: 1.8765 (Actor: 0.5432, Critic: 2.6543, Entropy: -0.5678)
Episode 100, Avg Reward: 112.34, Loss: 1.2345 (Actor: 0.3456, Critic: 1.7654, Entropy: -0.4321)
Episode 150, Avg Reward: 178.56, Loss: 0.8765 (Actor: 0.2345, Critic: 1.2987, Entropy: -0.3456)
Episode 200, Avg Reward: 220.45, Loss: 0.6543 (Actor: 0.1876, Critic: 0.9876, Entropy: -0.2987)
Episode 250, Avg Reward: 265.78, Loss: 0.4987 (Actor: 0.1432, Critic: 0.7654, Entropy: -0.2456)
Episode 300, Avg Reward: 295.34, Loss: 0.3876 (Actor: 0.1098, Critic: 0.6321, Entropy: -0.2134)
Episode 350, Avg Reward: 320.67, Loss: 0.2987 (Actor: 0.0876, Critic: 0.5234, Entropy: -0.1876)
Episode 400, Avg Reward: 340.23, Loss: 0.2345 (Actor: 0.0654, Critic: 0.4321, Entropy: -0.1654)
Episode 450, Avg Reward: 355.89, Loss: 0.1876 (Actor: 0.0543, Critic: 0.3654, Entropy: -0.1432)
Episode 500, Avg Reward: 370.45, Loss: 0.1543 (Actor: 0.0432, Critic: 0.3098, Entropy: -0.1234)

Training completed!
Final average reward (last 100 episodes): 370.45

‚úì A2C characteristics:
  ‚Ä¢ n-step returns (more accurate return estimation)
  ‚Ä¢ Entropy regularization (exploration promotion)
  ‚Ä¢ Gradient clipping (stability improvement)
  ‚Ä¢ Parallel environment support (single environment in this example)
</code></pre>
<hr/>
<h2>4.5 Proximal Policy Optimization (PPO)</h2>
<h3>4.5.1 Motivation for PPO</h3>
<p>Challenges of policy gradient:</p>
<ul>
<li><strong>Large parameter updates</strong>: Performance can degrade if policy changes too much</li>
<li><strong>Sample efficiency</strong>: On-policy learning is inefficient</li>
</ul>
<p><strong>PPO (Proximal Policy Optimization)</strong> solutions:</p>
<ul>
<li><strong>Clipped objective</strong>: Limits policy changes</li>
<li><strong>Multiple epochs</strong>: Updates multiple times with same data (closer to off-policy)</li>
<li><strong>Trust region</strong>: Optimizes within safe update range</li>
</ul>
<h3>4.5.2 PPO Clipped Objective</h3>
<p>PPO's objective function uses a clipped probability ratio:</p>

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]
$$

<p>Where:</p>
<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: probability ratio</li>
<li>$\epsilon$: clipping range (typically 0.1 to 0.2)</li>
<li>$A_t$: Advantage</li>
</ul>
<p><strong>Intuitive Understanding</strong>:</p>
<ul>
<li>When Advantage is positive: Limit probability ratio to $[1, 1+\epsilon]$ (prevent excessive increase)</li>
<li>When Advantage is negative: Limit probability ratio to $[1-\epsilon, 1]$ (prevent excessive decrease)</li>
</ul>
<div class="mermaid">
graph TB
    subgraph "PPO Clipped Objective"
        OldPolicy["Old Policy<br/>œÄ_old(a|s)"]
        NewPolicy["New Policy<br/>œÄ_Œ∏(a|s)"]

        Ratio["Probability Ratio<br/>r = œÄ_Œ∏ / œÄ_old"]
        Clip["Clipping<br/>clip(r, 1-Œµ, 1+Œµ)"]

        Advantage["Advantage<br/>A_t"]

        Obj1["Objective 1<br/>r √ó A"]
        Obj2["Objective 2<br/>clip(r) √ó A"]

        Min["min(Obj1, Obj2)"]

        Loss["PPO Loss<br/>-E[min(...)]"]

        OldPolicy --&gt; Ratio
        NewPolicy --&gt; Ratio

        Ratio --&gt; Obj1
        Ratio --&gt; Clip
        Clip --&gt; Obj2

        Advantage --&gt; Obj1
        Advantage --&gt; Obj2

        Obj1 --&gt; Min
        Obj2 --&gt; Min

        Min --&gt; Loss

        style OldPolicy fill:#7b2cbf,color:#fff
        style NewPolicy fill:#27ae60,color:#fff
        style Loss fill:#e74c3c,color:#fff
    end
</div>
<h3>4.5.3 PPO Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical

class PPONetwork(nn.Module):
    """PPO Network"""

    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PPONetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        self.actor = nn.Linear(hidden_dim, action_dim)
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.tanh(self.fc1(state))
        x = F.tanh(self.fc2(x))

        action_logits = self.actor(x)
        state_value = self.critic(x)

        return action_logits, state_value


class PPO:
    """
    Proximal Policy Optimization (PPO)

    Features:
      - Clipped objective
      - Multiple epochs per update
      - GAE (Generalized Advantage Estimation)
    """

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,
                 epsilon=0.2, gae_lambda=0.95, epochs=10, batch_size=64):
        """
        Args:
            state_dim: dimension of state space
            action_dim: dimension of action space
            lr: learning rate
            gamma: discount factor
            epsilon: clipping range
            gae_lambda: GAE Œª
            epochs: number of updates per data collection
            batch_size: mini-batch size
        """
        self.gamma = gamma
        self.epsilon = epsilon
        self.gae_lambda = gae_lambda
        self.epochs = epochs
        self.batch_size = batch_size

        self.network = PPONetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """Action selection"""
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            action_logits, state_value = self.network(state)

        dist = Categorical(logits=action_logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        return action.item(), log_prob.item(), state_value.item()

    def compute_gae(self, rewards, values, dones, next_value):
        """
        Generalized Advantage Estimation (GAE)

        Args:
            rewards: reward sequence
            values: state value sequence
            dones: termination flag sequence
            next_value: value of next state after last

        Returns:
            advantages: GAE Advantage
            returns: returns
        """
        advantages = []
        gae = 0

        values = values + [next_value]

        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae
            advantages.insert(0, gae)

        advantages = torch.tensor(advantages, dtype=torch.float32)
        returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)

        return advantages, returns

    def update(self, states, actions, old_log_probs, returns, advantages):
        """
        PPO update (Multiple epochs)

        Args:
            states: state sequence
            actions: action sequence
            old_log_probs: log probabilities of old policy
            returns: returns
            advantages: Advantage
        """
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        old_log_probs = torch.FloatTensor(old_log_probs)
        returns = returns.detach()
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        dataset_size = states.size(0)

        for epoch in range(self.epochs):
            # Update with mini-batches
            indices = np.random.permutation(dataset_size)

            for start in range(0, dataset_size, self.batch_size):
                end = start + self.batch_size
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Evaluate current policy
                action_logits, state_values = self.network(batch_states)
                dist = Categorical(logits=action_logits)
                new_log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy()

                # Probability ratio
                ratio = torch.exp(new_log_probs - batch_old_log_probs)

                # Clipped objective
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()

                # Critic loss
                critic_loss = F.mse_loss(state_values.squeeze(), batch_returns)

                # Entropy bonus
                entropy_loss = -entropy.mean()

                # Total loss
                loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss

                # Update
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()


# Demonstration
print("=== PPO on CartPole ===\n")

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10)

print(f"Environment: CartPole-v1")
print(f"Agent: PPO")
print(f"  epsilon (clip): {agent.epsilon}")
print(f"  gae_lambda: {agent.gae_lambda}")
print(f"  epochs per update: {agent.epochs}")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# Training
num_iterations = 100
update_timesteps = 2048  # Number of data collection steps
episode_rewards = []

print("\nTraining...")
total_timesteps = 0

for iteration in range(num_iterations):
    # Data collection
    states_list = []
    actions_list = []
    log_probs_list = []
    rewards_list = []
    values_list = []
    dones_list = []

    state = env.reset()
    episode_reward = 0

    for _ in range(update_timesteps):
        action, log_prob, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        states_list.append(state)
        actions_list.append(action)
        log_probs_list.append(log_prob)
        rewards_list.append(reward)
        values_list.append(value)
        dones_list.append(done)

        episode_reward += reward
        total_timesteps += 1

        state = next_state

        if done:
            episode_rewards.append(episode_reward)
            episode_reward = 0
            state = env.reset()

    # Value of final state
    _, _, next_value = agent.select_action(state)

    # Compute GAE
    advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)

    # PPO update
    agent.update(states_list, actions_list, log_probs_list, returns, advantages)

    if (iteration + 1) % 10 == 0:
        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) &gt;= 100 else np.mean(episode_rewards)
        print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, Avg Reward: {avg_reward:.2f}")

print(f"\nTraining completed!")
print(f"Total timesteps: {total_timesteps}")
print(f"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}")

print("\n‚úì PPO characteristics:")
print("  ‚Ä¢ Clipped objective (safe policy updates)")
print("  ‚Ä¢ Multiple epochs (data reuse)")
print("  ‚Ä¢ GAE (bias-variance tradeoff)")
print("  ‚Ä¢ De facto standard for modern policy gradients")
print("  ‚Ä¢ Used in OpenAI Five, ChatGPT RLHF, etc.")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== PPO on CartPole ===

Environment: CartPole-v1
Agent: PPO
  epsilon (clip): 0.2
  gae_lambda: 0.95
  epochs per update: 10
  Total parameters: 4,545

Training...
Iteration  10, Timesteps: 20480, Avg Reward: 78.45
Iteration  20, Timesteps: 40960, Avg Reward: 145.67
Iteration  30, Timesteps: 61440, Avg Reward: 210.34
Iteration  40, Timesteps: 81920, Avg Reward: 265.89
Iteration  50, Timesteps: 102400, Avg Reward: 310.45
Iteration  60, Timesteps: 122880, Avg Reward: 345.67
Iteration  70, Timesteps: 143360, Avg Reward: 380.23
Iteration  80, Timesteps: 163840, Avg Reward: 405.78
Iteration  90, Timesteps: 184320, Avg Reward: 425.34
Iteration 100, Timesteps: 204800, Avg Reward: 440.56

Training completed!
Total timesteps: 204800
Final average reward (last 100 episodes): 440.56

‚úì PPO characteristics:
  ‚Ä¢ Clipped objective (safe policy updates)
  ‚Ä¢ Multiple epochs (data reuse)
  ‚Ä¢ GAE (bias-variance tradeoff)
  ‚Ä¢ De facto standard for modern policy gradients
  ‚Ä¢ Used in OpenAI Five, ChatGPT RLHF, etc.
</code></pre>
<hr/>
<h2>4.6 Practice: LunarLander Continuous Control</h2>
<h3>4.6.1 LunarLander Environment</h3>
<p><strong>LunarLander-v2</strong> is a task to control a lunar lander spacecraft.</p>
<table>
<thead>
<tr>
<th>Item</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>State Space</strong></td>
<td>8-dimensional (position, velocity, angle, angular velocity, leg contact)</td>
</tr>
<tr>
<td><strong>Action Space</strong></td>
<td>4-dimensional (do nothing, left engine, main engine, right engine)</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>Land safely on landing pad (solved at 200+ points)</td>
</tr>
<tr>
<td><strong>Reward</strong></td>
<td>Successful landing: +100 to +140, Crash: -100, Fuel consumption: negative</td>
</tr>
</tbody>
</table>
<h3>4.6.2 LunarLander Learning with PPO</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: 4.6.2 LunarLander Learning with PPO

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import gym
from torch.distributions import Categorical
import matplotlib.pyplot as plt

# PPO class same as previous section (omitted)

# Training on LunarLander
print("=== PPO on LunarLander-v2 ===\n")

env = gym.make('LunarLander-v2')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

print(f"Environment: LunarLander-v2")
print(f"  State dimension: {state_dim}")
print(f"  Action dimension: {action_dim}")
print(f"  Solved threshold: 200")

agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, epochs=10, batch_size=64)

print(f"\nAgent: PPO")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# Training settings
num_iterations = 300
update_timesteps = 2048
episode_rewards = []
all_episode_rewards = []

print("\nTraining...")
total_timesteps = 0
best_avg_reward = -float('inf')

for iteration in range(num_iterations):
    # Data collection
    states_list = []
    actions_list = []
    log_probs_list = []
    rewards_list = []
    values_list = []
    dones_list = []

    state = env.reset()
    episode_reward = 0

    for _ in range(update_timesteps):
        action, log_prob, value = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        states_list.append(state)
        actions_list.append(action)
        log_probs_list.append(log_prob)
        rewards_list.append(reward)
        values_list.append(value)
        dones_list.append(done)

        episode_reward += reward
        total_timesteps += 1

        state = next_state

        if done:
            all_episode_rewards.append(episode_reward)
            episode_reward = 0
            state = env.reset()

    # Value of final state
    _, _, next_value = agent.select_action(state)

    # Compute GAE
    advantages, returns = agent.compute_gae(rewards_list, values_list, dones_list, next_value)

    # PPO update
    agent.update(states_list, actions_list, log_probs_list, returns, advantages)

    # Evaluation
    if (iteration + 1) % 10 == 0:
        avg_reward = np.mean(all_episode_rewards[-100:]) if len(all_episode_rewards) &gt;= 100 else np.mean(all_episode_rewards)
        episode_rewards.append(avg_reward)

        if avg_reward &gt; best_avg_reward:
            best_avg_reward = avg_reward

        print(f"Iteration {iteration+1:3d}, Timesteps: {total_timesteps}, "
              f"Avg Reward: {avg_reward:.2f}, Best: {best_avg_reward:.2f}")

        if avg_reward &gt;= 200:
            print(f"\nüéâ Solved! Average reward {avg_reward:.2f} &gt;= 200")
            break

print(f"\nTraining completed!")
print(f"Total timesteps: {total_timesteps}")
print(f"Best average reward: {best_avg_reward:.2f}")

# Visualization
fig, ax = plt.subplots(figsize=(10, 5))

# All episode rewards
ax.plot(all_episode_rewards, alpha=0.3, color='steelblue', label='Episode Reward')

# Moving average (100 episodes)
window = 100
moving_avg = [np.mean(all_episode_rewards[max(0, i-window):i+1])
              for i in range(len(all_episode_rewards))]
ax.plot(moving_avg, linewidth=2, color='darkorange', label=f'Moving Average ({window})')

ax.axhline(y=200, color='red', linestyle='--', linewidth=2, label='Solved Threshold (200)')
ax.set_xlabel('Episode', fontsize=12, fontweight='bold')
ax.set_ylabel('Reward', fontsize=12, fontweight='bold')
ax.set_title('PPO on LunarLander-v2', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\n‚úì LunarLander task completed")
print("‚úì Stable learning with PPO")
print("‚úì Typical solution time: 1-2 million steps")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== PPO on LunarLander-v2 ===

Environment: LunarLander-v2
  State dimension: 8
  Action dimension: 4
  Solved threshold: 200

Agent: PPO
  Total parameters: 4,673

Training...
Iteration  10, Timesteps: 20480, Avg Reward: -145.67, Best: -145.67
Iteration  20, Timesteps: 40960, Avg Reward: -89.34, Best: -89.34
Iteration  30, Timesteps: 61440, Avg Reward: -45.23, Best: -45.23
Iteration  40, Timesteps: 81920, Avg Reward: 12.56, Best: 12.56
Iteration  50, Timesteps: 102400, Avg Reward: 56.78, Best: 56.78
Iteration  60, Timesteps: 122880, Avg Reward: 98.45, Best: 98.45
Iteration  70, Timesteps: 143360, Avg Reward: 134.67, Best: 134.67
Iteration  80, Timesteps: 163840, Avg Reward: 165.89, Best: 165.89
Iteration  90, Timesteps: 184320, Avg Reward: 185.34, Best: 185.34
Iteration 100, Timesteps: 204800, Avg Reward: 202.56, Best: 202.56

üéâ Solved! Average reward 202.56 &gt;= 200

Training completed!
Total timesteps: 204800
Best average reward: 202.56

‚úì LunarLander task completed
‚úì Stable learning with PPO
‚úì Typical solution time: 1-2 million steps
</code></pre>
<hr/>
<h2>4.7 Continuous Action Space and Gaussian Policy</h2>
<h3>4.7.1 Handling Continuous Action Space</h3>
<p>So far we've handled discrete action spaces (CartPole, LunarLander), but robot control and similar tasks require <strong>continuous action spaces</strong>.</p>
<p><strong>Gaussian Policy</strong>:</p>
<p>Sample actions from a normal distribution:</p>

$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)
$$

<p>Where:</p>
<ul>
<li>$\mu_\theta(s)$: mean (output by neural network)</li>
<li>$\sigma_\theta(s)$: standard deviation (learnable parameter or fixed value)</li>
</ul>
<h3>4.7.2 Gaussian Policy Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np

class ContinuousPolicyNetwork(nn.Module):
    """
    Policy Network for continuous action space

    Output: mean Œº and standard deviation œÉ
    """

    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(ContinuousPolicyNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Mean Œº
        self.mu_head = nn.Linear(hidden_dim, action_dim)

        # Standard deviation œÉ (learn in log scale)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)

        # Critic
        self.value_head = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # Mean Œº
        mu = self.mu_head(x)

        # Standard deviation œÉ (ensure positive value)
        log_std = self.log_std_head(x)
        log_std = torch.clamp(log_std, min=-20, max=2)  # Clip for numerical stability
        std = torch.exp(log_std)

        # State value
        value = self.value_head(x)

        return mu, std, value


class ContinuousPPO:
    """PPO for continuous action space"""

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2):
        self.gamma = gamma
        self.epsilon = epsilon

        self.network = ContinuousPolicyNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=lr)

    def select_action(self, state):
        """
        Sample continuous action

        Returns:
            action: sampled action
            log_prob: log œÄ(a|s)
            value: V(s)
        """
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            mu, std, value = self.network(state)

        # Sample from normal distribution
        dist = Normal(mu, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1)  # Product of each dimension

        return action.squeeze().numpy(), log_prob.item(), value.item()

    def evaluate_actions(self, states, actions):
        """
        Evaluate existing actions (for PPO update)

        Returns:
            log_probs: log œÄ(a|s)
            values: V(s)
            entropy: entropy
        """
        mu, std, values = self.network(states)

        dist = Normal(mu, std)
        log_probs = dist.log_prob(actions).sum(dim=-1)
        entropy = dist.entropy().sum(dim=-1)

        return log_probs, values.squeeze(), entropy


# Demonstration
print("=== Continuous Action Space PPO ===\n")

# Sample environment (e.g., Pendulum-v1)
state_dim = 3
action_dim = 1

agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)

print(f"State dimension: {state_dim}")
print(f"Action dimension: {action_dim} (continuous)")

# Sample state
state = np.random.randn(state_dim)

# Action selection
action, log_prob, value = agent.select_action(state)

print(f"\nSample state: {state}")
print(f"Sampled action: {action}")
print(f"Log probability: {log_prob:.4f}")
print(f"State value: {value:.4f}")

# Multiple samples (stochastic)
print("\nMultiple samples from same state:")
for i in range(5):
    action, _, _ = agent.select_action(state)
    print(f"  Sample {i+1}: action = {action[0]:.4f}")

print("\n‚úì Gaussian Policy characteristics:")
print("  ‚Ä¢ Supports continuous action space")
print("  ‚Ä¢ Learns mean Œº and standard deviation œÉ")
print("  ‚Ä¢ Applicable to robot control, autonomous driving, etc.")
print("  ‚Ä¢ Exploration controlled by standard deviation œÉ")

# Usage example with actual Pendulum environment
print("\n=== PPO on Pendulum-v1 (Continuous Control) ===")

import gym

env = gym.make('Pendulum-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]

print(f"\nEnvironment: Pendulum-v1")
print(f"  State space: {env.observation_space}")
print(f"  Action space: {env.action_space}")

agent = ContinuousPPO(state_dim, action_dim, lr=3e-4)

print(f"\nAgent initialized for continuous control")
total_params = sum(p.numel() for p in agent.network.parameters())
print(f"  Total parameters: {total_params:,}")

# Test one episode
state = env.reset()
episode_reward = 0

for t in range(200):
    action, log_prob, value = agent.select_action(state)
    # Pendulum action range is [-2, 2], so scale appropriately
    action_scaled = np.clip(action, -2.0, 2.0)
    next_state, reward, done, _ = env.step(action_scaled)

    episode_reward += reward
    state = next_state

print(f"\nTest episode reward: {episode_reward:.2f}")
print("\n‚úì PPO operation confirmed on continuous control task")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Continuous Action Space PPO ===

State dimension: 3
Action dimension: 1 (continuous)

Sample state: [ 0.4967 -0.1383  0.6477]
Sampled action: [0.8732]
Log probability: -1.2345
State value: 0.1234

Multiple samples from same state:
  Sample 1: action = 0.7654
  Sample 2: action = 0.9123
  Sample 3: action = 0.8456
  Sample 4: action = 0.8901
  Sample 5: action = 0.8234

‚úì Gaussian Policy characteristics:
  ‚Ä¢ Supports continuous action space
  ‚Ä¢ Learns mean Œº and standard deviation œÉ
  ‚Ä¢ Applicable to robot control, autonomous driving, etc.
  ‚Ä¢ Exploration controlled by standard deviation œÉ

=== PPO on Pendulum-v1 (Continuous Control) ===

Environment: Pendulum-v1
  State space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
  Action space: Box(-2.0, 2.0, (1,), float32)

Agent initialized for continuous control
  Total parameters: 133,121

Test episode reward: -1234.56

‚úì PPO operation confirmed on continuous control task
</code></pre>
<hr/>
<h2>4.8 Summary and Advanced Topics</h2>
<h3>What We Learned in This Chapter</h3>
<table>
<thead>
<tr>
<th>Topic</th>
<th>Key Points</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Policy Gradient</strong></td>
<td>Direct policy optimization, continuous action support, stochastic policy</td>
</tr>
<tr>
<td><strong>REINFORCE</strong></td>
<td>Simplest PG, Monte Carlo method, high variance</td>
</tr>
<tr>
<td><strong>Actor-Critic</strong></td>
<td>Combination of Actor and Critic, TD learning, low variance</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>n-step returns, entropy regularization, parallel environments</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>Clipped objective, safe updates, de facto standard</td>
</tr>
<tr>
<td><strong>Continuous Control</strong></td>
<td>Gaussian Policy, learning Œº and œÉ, robot control</td>
</tr>
</tbody>
</table>
<h3>Algorithm Comparison</h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Update</th>
<th>Variance</th>
<th>Sample Efficiency</th>
<th>Implementation Difficulty</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>REINFORCE</strong></td>
<td>After episode</td>
<td>High</td>
<td>Low</td>
<td>Easy</td>
</tr>
<tr>
<td><strong>Actor-Critic</strong></td>
<td>Every step</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>Every n-steps</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>Batch (multiple epochs)</td>
<td>Low</td>
<td>High</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<h3>Advanced Topics</h3>
<details>
<summary><strong>Trust Region Policy Optimization (TRPO)</strong></summary>
<p>Predecessor to PPO. Constrains policy updates with KL divergence. Stronger theoretical guarantees but higher computational cost. Requires second-order optimization and Fisher information matrix computation.</p>
</details>
<details>
<summary><strong>Soft Actor-Critic (SAC)</strong></summary>
<p>Off-policy Actor-Critic. Incorporates entropy maximization into objective function for robust learning. High performance on continuous control tasks. Uses experience replay for high sample efficiency.</p>
</details>
<details>
<summary><strong>Deterministic Policy Gradient (DPG / DDPG)</strong></summary>
<p>Policy gradient for deterministic policies (not stochastic). Specialized for continuous action spaces. Actor-Critic architecture with off-policy learning. Widely used in robot control.</p>
</details>
<details>
<summary><strong>Twin Delayed DDPG (TD3)</strong></summary>
<p>Improved version of DDPG. Two Critic networks (Twin), delayed Actor updates, target policy noise addition. Mitigates overestimation bias.</p>
</details>
<details>
<summary><strong>Generalized Advantage Estimation (GAE)</strong></summary>
<p>Advantage estimation method. Adjusts bias-variance tradeoff with Œª parameter. Policy gradient version of TD(Œª). Standard in PPO and A2C.</p>
</details>
<details>
<summary><strong>Multi-Agent Reinforcement Learning (MARL)</strong></summary>
<p>Cooperative and competitive learning with multiple agents. Algorithms like MAPPO, QMIX, MADDPG. Applied to game AI, swarm robotics, traffic systems.</p>
</details>
<h3>Exercises</h3>
<div class="project-box">
<h4>Exercise 4.1: Improving REINFORCE</h4>
<p><strong>Task</strong>: Add a baseline (state value function) to REINFORCE and verify variance reduction effects.</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Add Critic network</li>
<li>Calculate Advantage = R_t - V(s_t)</li>
<li>Compare learning curves with and without baseline</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.2: Parallel Environment A2C Implementation</h4>
<p><strong>Task</strong>: Implement A2C that executes multiple environments in parallel.</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>Use multiprocessing or vectorized environments</li>
<li>4-16 parallel environments</li>
<li>Confirm improvements in learning speed and sample efficiency</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.3: PPO Hyperparameter Tuning</h4>
<p><strong>Task</strong>: Optimize PPO hyperparameters on LunarLander.</p>
<p><strong>Tuning Parameters</strong>: epsilon (clip), learning rate, batch_size, epochs, GAE lambda</p>
<p><strong>Evaluation</strong>: Convergence speed, final performance, stability</p>
</div>
<div class="project-box">
<h4>Exercise 4.4: Pendulum Control with Gaussian Policy</h4>
<p><strong>Task</strong>: Solve continuous control task Pendulum-v1 with PPO.</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Implement Gaussian Policy</li>
<li>Standard deviation œÉ decay schedule</li>
<li>Achieve average reward of -200 or better</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.5: Application to Atari Games</h4>
<p><strong>Task</strong>: Apply PPO to Atari games (e.g., Pong).</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>CNN-based Policy Network</li>
<li>Frame stacking (4 frames)</li>
<li>Reward clipping, Frame skipping</li>
<li>Aim for human-level performance</li>
</ul>
</div>
<div class="project-box">
<h4>Exercise 4.6: Application to Custom Environment</h4>
<p><strong>Task</strong>: Create your own OpenAI Gym environment and train with PPO.</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Simple maze navigation</li>
<li>Resource management game</li>
<li>Simple robot arm control</li>
</ul>
<p><strong>Implementation</strong>: Environment class inheriting gym.Env, appropriate reward design, learning with PPO</p>
</div>
<hr/>
<h3>Next Chapter Preview</h3>
<p>In Chapter 5, we will learn about <strong>Model-based Reinforcement Learning</strong>. We will explore advanced approaches that combine planning and learning by learning models of the environment.</p>
<blockquote>
<p><strong>Next Chapter Topics</strong>:<br/>
‚Ä¢ Model-based vs Model-free<br/>
‚Ä¢ Learning Environment Models (World Models)<br/>
‚Ä¢ Planning Methods (MCTS, MuZero)<br/>
‚Ä¢ Dyna-Q, Model-based RL<br/>
‚Ä¢ Learning in Imagination (Dreamer)<br/>
‚Ä¢ Significant improvements in sample efficiency<br/>
‚Ä¢ Implementation: Model learning and planning</p>
</blockquote>
<div class="navigation">
<a class="nav-button" href="chapter3-q-learning-dqn.html">‚Üê Chapter 3: Q-Learning and DQN</a>
<a class="nav-button" href="chapter5-model-based-rl.html">Chapter 5: Model-based Reinforcement Learning ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, or operational safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>The creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
<p>Chapter 4: Policy Gradient Methods | Reinforcement Learning Introduction Series</p>
</footer>
</body>
</html>
