<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Introduction to Reinforcement Learning Series - Complete Implementation Guide from Q-Learning to DQN and PPO">
    <title>Introduction to Reinforcement Learning Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Reinforcement Learning</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>üéÆ Introduction to Reinforcement Learning Series v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">Implementation Guide from Q-Learning to DQN and PPO</p>
            <div class="meta">
                <span>üìñ Total Learning Time: 120-150 minutes</span>
                <span>üìä Level: Advanced</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Systematically master reinforcement learning algorithms that learn optimal actions through trial and error, from fundamentals to advanced techniques</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is practical educational content structured in 5 chapters, allowing you to progressively learn reinforcement learning (RL) theory and implementation from the ground up.</p>

        <p><strong>Reinforcement Learning (RL)</strong> is a branch of machine learning where agents learn optimal action policies through trial and error via interaction with their environment. Through problem formalization using Markov Decision Process (MDP), value function calculation using Bellman equations, classical methods like Q-learning and SARSA, conquering Atari games with Deep Q-Network (DQN), addressing continuous action spaces with Policy Gradient methods, and state-of-the-art algorithms like Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), these technologies are bringing innovation to diverse fields including robot control, game AI, autonomous driving, financial trading, and resource optimization. You will understand and be able to implement the foundational technology for decision-making that companies like DeepMind, OpenAI, and Google are putting into practical use. We provide systematic knowledge from tabular methods to Deep RL.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>‚úÖ <strong>From Theory to Implementation</strong>: Systematic learning from MDP fundamentals to the latest PPO and SAC</li>
            <li>‚úÖ <strong>Implementation-Focused</strong>: Over 35 executable PyTorch/Gymnasium/Stable-Baselines3 code examples</li>
            <li>‚úÖ <strong>Intuitive Understanding</strong>: Understand principles through visualization in Cliff Walking, CartPole, and Atari</li>
            <li>‚úÖ <strong>Latest Technology Compliant</strong>: Implementation using Gymnasium (OpenAI Gym successor) and Stable-Baselines3</li>
            <li>‚úÖ <strong>Practical Applications</strong>: Application to practical tasks including game AI, robot control, and resource optimization</li>
        </ul>

        <p><strong>Total Learning Time</strong>: 120-150 minutes (including code execution and exercises)</p>

        <h2 id="learning">How to Study</h2>

        <h3>Recommended Learning Order</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: Fundamentals of RL] --> B[Chapter 2: Q-Learning and SARSA]
    B --> C[Chapter 3: Deep Q-Network]
    C --> D[Chapter 4: Policy Gradient Methods]
    D --> E[Chapter 5: Advanced RL Methods]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>For Beginners (No prior RL knowledge):</strong><br>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (all chapters recommended)<br>
        - Time Required: 120-150 minutes</p>

        <p><strong>For Intermediate Learners (Experience with MDP):</strong><br>
        - Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br>
        - Time Required: 90-110 minutes</p>

        <p><strong>Focused Study on Specific Topics:</strong><br>
        - MDP and Bellman Equations: Chapter 1 (focused study)<br>
        - Tabular methods: Chapter 2 (focused study)<br>
        - Deep Q-Network: Chapter 3 (focused study)<br>
        - Policy Gradient: Chapter 4 (focused study)<br>
        - Time Required: 25-30 minutes per chapter</p>

        <h2 id="chapters">Chapter Details</h2>

        <h3><a href="./chapter1-rl-fundamentals.html">Chapter 1: Fundamentals of Reinforcement Learning</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 7</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Basic RL Concepts</strong> - Agent, environment, state, action, reward</li>
            <li><strong>Markov Decision Process (MDP)</strong> - State transition probability, reward function, discount factor</li>
            <li><strong>Bellman Equations</strong> - State value function, action value function, optimality</li>
            <li><strong>Policy</strong> - Deterministic policy, stochastic policy, optimal policy</li>
            <li><strong>Gymnasium Introduction</strong> - Environment creation, state-action spaces, step execution</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand basic RL terminology</li>
            <li>‚úÖ Be able to formalize problems as MDP</li>
            <li>‚úÖ Be able to explain Bellman equations</li>
            <li>‚úÖ Understand the relationship between value functions and policies</li>
            <li>‚úÖ Be able to manipulate environments in Gymnasium</li>
        </ul>

        <p><strong><a href="./chapter1-rl-fundamentals.html">Read Chapter 1 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter2-q-learning-sarsa.html">Chapter 2: Q-Learning and SARSA</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Tabular methods</strong> - Q-table, tabular representation of state-action values</li>
            <li><strong>Q-Learning</strong> - Off-policy TD control, Q-value update rule</li>
            <li><strong>SARSA</strong> - On-policy TD control, differences from Q-learning</li>
            <li><strong>Exploration-Exploitation Tradeoff</strong> - Œµ-greedy, Œµ-decay, Boltzmann exploration</li>
            <li><strong>Cliff Walking Problem</strong> - Q-learning/SARSA implementation in grid world</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the Q-learning algorithm</li>
            <li>‚úÖ Be able to explain differences between SARSA and Q-learning</li>
            <li>‚úÖ Be able to implement Œµ-greedy exploration strategy</li>
            <li>‚úÖ Be able to implement learning using Q-table</li>
            <li>‚úÖ Be able to compare both methods in Cliff Walking</li>
        </ul>

        <p><strong><a href="./chapter2-q-learning-sarsa.html">Read Chapter 2 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter3-dqn.html">Chapter 3: Deep Q-Network (DQN)</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 30-35 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Function Approximation</strong> - Q-table limitations, neural network approximation</li>
            <li><strong>DQN Mechanism</strong> - Q-network learning, loss function, gradient descent</li>
            <li><strong>Experience Replay</strong> - Experience reuse, correlation reduction, stabilization</li>
            <li><strong>Target Network</strong> - Fixed targets, learning stability improvement</li>
            <li><strong>Application to Atari Games</strong> - Image input, CNN, Pong/Breakout</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand DQN components</li>
            <li>‚úÖ Be able to explain the role of Experience Replay</li>
            <li>‚úÖ Understand the necessity of Target Network</li>
            <li>‚úÖ Be able to implement DQN in PyTorch</li>
            <li>‚úÖ Be able to train agents in CartPole/Atari</li>
        </ul>

        <p><strong><a href="./chapter3-dqn.html">Read Chapter 3 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter4-policy-gradient.html">Chapter 4: Policy Gradient Methods</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 30-35 minutes<br>
        <strong>Code Examples</strong>: 7</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>REINFORCE</strong> - Policy gradient theorem, Monte Carlo policy gradient</li>
            <li><strong>Actor-Critic</strong> - Actor and critic, bias-variance tradeoff</li>
            <li><strong>Advantage Actor-Critic (A2C)</strong> - Advantage function, variance reduction</li>
            <li><strong>Proximal Policy Optimization (PPO)</strong> - Clipped objective function, stable learning</li>
            <li><strong>Continuous Action Spaces</strong> - Gaussian policy, application to robot control</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the policy gradient theorem</li>
            <li>‚úÖ Be able to implement the REINFORCE algorithm</li>
            <li>‚úÖ Be able to explain the Actor-Critic mechanism</li>
            <li>‚úÖ Understand the PPO objective function</li>
            <li>‚úÖ Be able to create agents for continuous action spaces</li>
        </ul>

        <p><strong><a href="./chapter4-policy-gradient.html">Read Chapter 4 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter5-advanced-applications.html">Chapter 5: Advanced RL Methods</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 5</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Asynchronous Advantage Actor-Critic (A3C)</strong> - Parallel learning, inter-thread synchronization</li>
            <li><strong>Soft Actor-Critic (SAC)</strong> - Entropy regularization, maximum entropy RL</li>
            <li><strong>Multi-agent RL</strong> - Multiple agents, cooperation and competition</li>
            <li><strong>Real-World Applications</strong> - Robot control, resource optimization, autonomous driving</li>
            <li><strong>Stable-Baselines3</strong> - Utilizing pre-implemented algorithms, hyperparameter tuning</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand A3C parallel learning</li>
            <li>‚úÖ Be able to explain SAC entropy regularization</li>
            <li>‚úÖ Understand challenges in multi-agent RL</li>
            <li>‚úÖ Be able to utilize algorithms with Stable-Baselines3</li>
            <li>‚úÖ Be able to apply RL to real-world problems</li>
        </ul>

        <p><strong><a href="./chapter5-advanced-applications.html">Read Chapter 5 ‚Üí</a></strong></p>

        <hr>

        <h2 id="outcomes">Overall Learning Outcomes</h2>

        <p>Upon completing this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>‚úÖ Be able to explain the theoretical foundations of MDP and Bellman equations</li>
            <li>‚úÖ Understand the mechanisms of Q-learning, SARSA, DQN, PPO, and SAC</li>
            <li>‚úÖ Be able to explain differences between value-based and policy-based methods</li>
            <li>‚úÖ Understand the roles of Experience Replay and Target Network</li>
            <li>‚úÖ Be able to explain when to use each algorithm</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>‚úÖ Be able to implement RL agents in PyTorch/Gymnasium</li>
            <li>‚úÖ Be able to implement Q-learning, DQN, and PPO from scratch</li>
            <li>‚úÖ Be able to utilize advanced algorithms with Stable-Baselines3</li>
            <li>‚úÖ Be able to implement exploration strategies (Œµ-greedy, Œµ-decay)</li>
            <li>‚úÖ Be able to train agents in CartPole and Atari games</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>‚úÖ Be able to select appropriate RL algorithms based on tasks</li>
            <li>‚úÖ Be able to design agents for continuous and discrete action spaces</li>
            <li>‚úÖ Be able to appropriately tune hyperparameters</li>
            <li>‚úÖ Be able to apply reinforcement learning to robot control and game AI</li>
        </ul>

        <hr>

        <h2 id="prerequisites">Prerequisites</h2>

        <p>To effectively learn this series, it is desirable to have the following knowledge:</p>

        <h3>Required (Must Have)</h3>
        <ul>
            <li>‚úÖ <strong>Python Fundamentals</strong>: Variables, functions, classes, loops, conditionals</li>
            <li>‚úÖ <strong>NumPy Fundamentals</strong>: Array operations, matrix operations, random number generation</li>
            <li>‚úÖ <strong>Deep Learning Fundamentals</strong>: Neural networks, backpropagation, gradient descent</li>
            <li>‚úÖ <strong>PyTorch Fundamentals</strong>: Tensor operations, nn.Module, optimizers</li>
            <li>‚úÖ <strong>Probability & Statistics Fundamentals</strong>: Expected value, variance, probability distributions</li>
            <li>‚úÖ <strong>Calculus Fundamentals</strong>: Gradients, partial derivatives, chain rule</li>
        </ul>

        <h3>Recommended (Nice to Have)</h3>
        <ul>
            <li>üí° <strong>Dynamic Programming</strong>: Value Iteration, Policy Iteration (for theoretical understanding)</li>
            <li>üí° <strong>CNN Fundamentals</strong>: Convolutional layers, pooling (for Atari learning)</li>
            <li>üí° <strong>Optimization Algorithms</strong>: Adam, RMSprop, learning rate scheduling</li>
            <li>üí° <strong>Linear Algebra</strong>: Vectors, matrix operations</li>
            <li>üí° <strong>GPU Environment</strong>: Basic understanding of CUDA</li>
        </ul>

        <p><strong>Recommended Prior Learning</strong>:</p>
        <ul>
            <!-- Content in preparation <li>üìö <a href="../deep-learning-basics/">Deep Learning Fundamentals Series</a> - Basics of neural networks</li>
            <li>üìö <a href="../pytorch-introduction/">PyTorch Introduction Series</a> - Basic PyTorch operations</li>
            <li>üìö <a href="../probability-statistics-ml/">Probability & Statistics for Machine Learning</a> - Probability distributions, expected value</li>
            <li>üìö <a href="../optimization-algorithms/">Introduction to Optimization Algorithms</a> - Gradient descent, Adam (recommended)</li> -->
        </ul>

        <hr>

        <h2 id="tech">Technologies and Tools Used</h2>

        <h3>Main Libraries</h3>
        <ul>
            <li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
            <li><strong>Gymnasium 0.29+</strong> - Reinforcement learning environment (OpenAI Gym successor)</li>
            <li><strong>Stable-Baselines3 2.1+</strong> - Pre-implemented RL algorithm library</li>
            <li><strong>NumPy 1.24+</strong> - Numerical computation</li>
            <li><strong>Matplotlib 3.7+</strong> - Visualization</li>
            <li><strong>TensorBoard 2.14+</strong> - Learning process visualization</li>
            <li><strong>imageio 2.31+</strong> - Video saving, GIF creation</li>
        </ul>

        <h3>Development Environment</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - Programming language</li>
            <li><strong>Jupyter Notebook / Lab</strong> - Interactive development environment</li>
            <li><strong>Google Colab</strong> - GPU environment (freely available)</li>
            <li><strong>CUDA 11.8+ / cuDNN</strong> - GPU acceleration (recommended)</li>
        </ul>

        <h3>Environments</h3>
        <ul>
            <li><strong>FrozenLake</strong> - Grid world (tabular methods)</li>
            <li><strong>Cliff Walking</strong> - Grid world (Q-learning vs SARSA)</li>
            <li><strong>CartPole-v1</strong> - Inverted pendulum (classic control problem)</li>
            <li><strong>LunarLander-v2</strong> - Lunar landing (continuous control)</li>
            <li><strong>Atari: Pong, Breakout</strong> - Game AI (image input, DQN)</li>
            <li><strong>MuJoCo: Humanoid, Ant</strong> - Robot control (continuous action space)</li>
        </ul>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Are you ready? Start with Chapter 1 and master reinforcement learning techniques!</p>

        <p><strong><a href="./chapter1-rl-fundamentals.html">Chapter 1: Fundamentals of Reinforcement Learning ‚Üí</a></strong></p>

        <hr>

        <h2 id="next">Next Steps</h2>

        <p>After completing this series, we recommend proceeding to the following topics:</p>

        <h3>Advanced Learning</h3>
        <ul>
            <li>üìö <strong>Model-Based RL</strong>: Learning environment models, planning-based methods</li>
            <li>üìö <strong>Meta-RL</strong>: Learning to learn, few-shot RL</li>
            <li>üìö <strong>Offline RL</strong>: Learning from batch data, behavioral cloning</li>
            <li>üìö <strong>Hierarchical RL</strong>: Options, hierarchical policies</li>
        </ul>

        <h3>Related Series</h3>
        <ul>
            <li>üéØ <a href="../imitation-learning/">Introduction to Imitation Learning</a> - Behavioral Cloning, Inverse RL</li>
            <li>üéØ <a href="../robot-control-rl/">Robot Control and RL</a> - MuJoCo, real robot control</li>
            <li>üéØ <a href="../game-ai/">Game AI Development</a> - AlphaGo, Monte Carlo Tree Search</li>
        </ul>

        <h3>Practical Projects</h3>
        <ul>
            <li>üöÄ Atari Game Master AI - Conquering Pong and Breakout with DQN/PPO</li>
            <li>üöÄ Inverted Pendulum Control - CartPole stabilization and robot applications</li>
            <li>üöÄ Autonomous Drone Control - Flight control in continuous action spaces</li>
            <li>üöÄ Trading Bot - Decision-making optimization in financial markets</li>
        </ul>

        <hr>

        <p><strong>Update History</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 initial release</li>
        </ul>

        <hr>

        <p><strong>Your journey into reinforcement learning begins here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
