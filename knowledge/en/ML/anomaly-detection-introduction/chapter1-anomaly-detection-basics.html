<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Anomaly Detection Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Anomaly Detection Fundamentals</h1>
<p class="subtitle">Basic Concepts and Task Design of Anomaly Detection</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the definition and types of anomaly detection</li>
<li>‚úÖ Distinguish between Point, Contextual, and Collective anomalies</li>
<li>‚úÖ Understand task classification and selection criteria for anomaly detection</li>
<li>‚úÖ Select appropriate evaluation metrics</li>
<li>‚úÖ Use representative datasets and visualization techniques</li>
<li>‚úÖ Understand challenges and countermeasures in anomaly detection</li>
</ul>
<hr/>
<h2>1.1 What is Anomaly Detection?</h2>
<h3>Definition of Anomalies</h3>
<p><strong>Anomaly Detection</strong> is a task that identifies data points that significantly deviate from normal patterns.</p>
<blockquote>
<p>"Anomaly" refers to observations with rare and unexpected patterns that differ from the majority of normal data.</p>
</blockquote>
<h3>Three Types of Anomalies</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Point Anomaly</strong></td>
<td>Individual data points are anomalous</td>
<td>Sudden high-value credit card transaction</td>
</tr>
<tr>
<td><strong>Contextual Anomaly</strong></td>
<td>Anomalous only in specific context</td>
<td>35¬∞C temperature is normal in summer, anomalous in winter</td>
</tr>
<tr>
<td><strong>Collective Anomaly</strong></td>
<td>Collection of data forms anomalous pattern</td>
<td>Continuous abnormal waveform in ECG</td>
</tr>
</tbody>
</table>
<h3>Applications of Anomaly Detection</h3>
<h4>1. Fraud Detection</h4>
<ul>
<li>Credit card fraud detection</li>
<li>Insurance claim fraud detection</li>
<li>Money laundering detection</li>
</ul>
<h4>2. Manufacturing</h4>
<ul>
<li>Defective product detection</li>
<li>Equipment failure prediction</li>
<li>Quality control anomaly detection</li>
</ul>
<h4>3. Healthcare</h4>
<ul>
<li>Early disease detection</li>
<li>Tumor detection in medical images</li>
<li>Vital sign anomaly detection</li>
</ul>
<h4>4. IT Systems (Cybersecurity &amp; Operations)</h4>
<ul>
<li>Network intrusion detection</li>
<li>Server failure prediction</li>
<li>Abnormal traffic detection</li>
</ul>
<h3>Business Value of Anomaly Detection</h3>
<div class="mermaid">
graph LR
    A[Anomaly Detection] --&gt; B[Cost Reduction]
    A --&gt; C[Risk Mitigation]
    A --&gt; D[Revenue Growth]

    B --&gt; B1[Preventive Maintenance Before Failures]
    B --&gt; B2[Early Defect Detection]

    C --&gt; C1[Security Breach Prevention]
    C --&gt; C2[Fraudulent Transaction Prevention]

    D --&gt; D1[Downtime Reduction]
    D --&gt; D2[Customer Satisfaction Improvement]

    style A fill:#7b2cbf,color:#fff
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style D fill:#e3f2fd
</div>
<h3>Example: Basic Anomaly Detection</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Example: Basic Anomaly Detection

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Generate normal data
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=1,
                         cluster_std=1.0, center_box=(0, 0))

# Add anomalous data (3 types)
# 1. Point Anomaly: distant points
point_anomalies = np.array([[8, 8], [-8, -8], [8, -8]])

# 2. Contextual Anomaly: within normal range but contextually anomalous
# (e.g., out-of-season values in time series data)
contextual_anomalies = np.array([[2, 2], [-2, -2]])

# 3. Collective Anomaly: anomalous as a group
collective_anomalies = np.random.normal(loc=[5, 5], scale=0.3, size=(10, 2))

# Combine all data
X_all = np.vstack([X_normal, point_anomalies,
                   contextual_anomalies, collective_anomalies])

# Visualization
plt.figure(figsize=(12, 8))

plt.scatter(X_normal[:, 0], X_normal[:, 1],
            c='blue', alpha=0.5, s=50, label='Normal Data', edgecolors='black')
plt.scatter(point_anomalies[:, 0], point_anomalies[:, 1],
            c='red', s=200, marker='X', label='Point Anomaly',
            edgecolors='black', linewidths=2)
plt.scatter(contextual_anomalies[:, 0], contextual_anomalies[:, 1],
            c='orange', s=200, marker='s', label='Contextual Anomaly',
            edgecolors='black', linewidths=2)
plt.scatter(collective_anomalies[:, 0], collective_anomalies[:, 1],
            c='purple', s=100, marker='^', label='Collective Anomaly',
            edgecolors='black', linewidths=2)

plt.xlabel('Feature 1', fontsize=12)
plt.ylabel('Feature 2', fontsize=12)
plt.title('Three Types of Anomalies', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Data Statistics ===")
print(f"Normal Data: {len(X_normal)} samples")
print(f"Point Anomaly: {len(point_anomalies)} samples")
print(f"Contextual Anomaly: {len(contextual_anomalies)} samples")
print(f"Collective Anomaly: {len(collective_anomalies)} samples")
print(f"Anomaly Rate: {(len(point_anomalies) + len(contextual_anomalies) + len(collective_anomalies)) / len(X_all) * 100:.1f}%")
</code></pre>
<blockquote>
<p><strong>Important</strong>: In anomaly detection, normal data forms the vast majority, while anomalous data is rare (typically 1-5%).</p>
</blockquote>
<hr/>
<h2>1.2 Task Classification of Anomaly Detection</h2>
<h3>1. Classification by Learning Method</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Label Information</th>
<th>Use Case</th>
<th>Algorithm Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Supervised Learning</strong></td>
<td>Both normal and anomaly labels</td>
<td>Abundant labeled data</td>
<td>Random Forest, SVM</td>
</tr>
<tr>
<td><strong>Semi-supervised Learning</strong></td>
<td>Normal labels only</td>
<td>Only normal data labeled</td>
<td>One-Class SVM, Autoencoder</td>
</tr>
<tr>
<td><strong>Unsupervised Learning</strong></td>
<td>No labels</td>
<td>Label acquisition difficult</td>
<td>Isolation Forest, LOF, DBSCAN</td>
</tr>
</tbody>
</table>
<h3>2. Novelty Detection vs Outlier Detection</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Training Data</th>
<th>Purpose</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Novelty Detection</strong></td>
<td>Normal data only</td>
<td>Detection of new patterns</td>
<td>New malware detection</td>
</tr>
<tr>
<td><strong>Outlier Detection</strong></td>
<td>Mixed normal and anomalies</td>
<td>Anomaly detection in existing data</td>
<td>Noise removal in sensor data</td>
</tr>
</tbody>
</table>
<h3>3. Online vs Offline Detection</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Processing Timing</th>
<th>Characteristics</th>
<th>Application Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Online Detection</strong><br/>(Real-time)</td>
<td>Upon data arrival</td>
<td>Low latency, incremental updates</td>
<td>Network intrusion detection</td>
</tr>
<tr>
<td><strong>Offline Detection</strong><br/>(Batch processing)</td>
<td>Batch processing</td>
<td>High accuracy, global optimization</td>
<td>Monthly report anomaly analysis</td>
</tr>
</tbody>
</table>
<h3>Task Selection Decision Flow</h3>
<div class="mermaid">
graph TD
    A[Anomaly Detection Task Design] --&gt; B{Label Data Available?}
    B --&gt;|Both available| C[Supervised Learning]
    B --&gt;|Normal only| D[Semi-supervised Learning / Novelty Detection]
    B --&gt;|None| E[Unsupervised Learning / Outlier Detection]

    C --&gt; F{Real-time?}
    D --&gt; F
    E --&gt; F

    F --&gt;|Yes| G[Online Detection]
    F --&gt;|No| H[Offline Detection]

    G --&gt; I[Method Selection]
    H --&gt; I

    style A fill:#7b2cbf,color:#fff
    style C fill:#e8f5e9
    style D fill:#fff3e0
    style E fill:#ffebee
    style G fill:#e3f2fd
    style H fill:#f3e5f5
</div>
<h3>Example: Comparison of Three Approaches</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Example: Comparison of Three Approaches

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, accuracy_score

# Generate data (imbalanced data)
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=10, n_informative=8,
                           n_redundant=2, n_classes=2, weights=[0.95, 0.05],
                           flip_y=0, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print("=== Data Distribution ===")
print(f"Training Data: {len(y_train)} samples (Normal: {(y_train==0).sum()}, Anomaly: {(y_train==1).sum()})")
print(f"Test Data: {len(y_test)} samples (Normal: {(y_test==0).sum()}, Anomaly: {(y_test==1).sum()})")

# 1. Supervised learning (with labels)
print("\n=== 1. Supervised Learning ===")
clf_supervised = RandomForestClassifier(n_estimators=100, random_state=42)
clf_supervised.fit(X_train, y_train)
y_pred_supervised = clf_supervised.predict(X_test)
acc_supervised = accuracy_score(y_test, y_pred_supervised)
print(f"Accuracy: {acc_supervised:.3f}")
print(classification_report(y_test, y_pred_supervised, target_names=['Normal', 'Anomaly']))

# 2. Semi-supervised learning (train with normal data only)
print("\n=== 2. Semi-supervised Learning (Novelty Detection) ===")
X_train_normal = X_train[y_train == 0]  # Normal data only
clf_novelty = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')
clf_novelty.fit(X_train_normal)
y_pred_novelty = clf_novelty.predict(X_test)
# One-Class SVM output: 1=normal, -1=anomaly ‚Üí convert to 0=normal, 1=anomaly
y_pred_novelty = (y_pred_novelty == -1).astype(int)
acc_novelty = accuracy_score(y_test, y_pred_novelty)
print(f"Accuracy: {acc_novelty:.3f}")
print(classification_report(y_test, y_pred_novelty, target_names=['Normal', 'Anomaly']))

# 3. Unsupervised learning (no labels)
print("\n=== 3. Unsupervised Learning (Outlier Detection) ===")
clf_unsupervised = IsolationForest(contamination=0.05, random_state=42)
clf_unsupervised.fit(X_train)
y_pred_unsupervised = clf_unsupervised.predict(X_test)
# Isolation Forest output: 1=normal, -1=anomaly ‚Üí convert to 0=normal, 1=anomaly
y_pred_unsupervised = (y_pred_unsupervised == -1).astype(int)
acc_unsupervised = accuracy_score(y_test, y_pred_unsupervised)
print(f"Accuracy: {acc_unsupervised:.3f}")
print(classification_report(y_test, y_pred_unsupervised, target_names=['Normal', 'Anomaly']))

# Comparison summary
print("\n=== Accuracy Comparison ===")
print(f"Supervised Learning:   {acc_supervised:.3f}")
print(f"Semi-supervised Learning: {acc_novelty:.3f}")
print(f"Unsupervised Learning:   {acc_unsupervised:.3f}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: Supervised learning provides the highest accuracy but requires labeled data. In real business scenarios, select methods considering the cost of label acquisition.</p>
</blockquote>
<hr/>
<h2>1.3 Evaluation Metrics</h2>
<h3>Class Imbalance Problem</h3>
<p>In anomaly detection, normal data overwhelmingly outnumber anomalies, making accuracy alone insufficient.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: In anomaly detection, normal data overwhelmingly outnumber a

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Example: 95% normal, 5% anomaly data
y_true = np.array([0]*95 + [1]*5)

# Bad predictor: predicts everything as normal
y_pred_bad = np.array([0]*100)

# Good predictor: correctly detects anomalies
y_pred_good = np.concatenate([np.array([0]*95), np.array([1]*5)])

print("=== Bad Predictor (Predicts Everything as Normal) ===")
print(f"Accuracy: {accuracy_score(y_true, y_pred_bad):.3f}")
print(f"Precision: {precision_score(y_true, y_pred_bad, zero_division=0):.3f}")
print(f"Recall: {recall_score(y_true, y_pred_bad, zero_division=0):.3f}")
print(f"F1: {f1_score(y_true, y_pred_bad, zero_division=0):.3f}")

print("\n=== Good Predictor (Correctly Detects Anomalies) ===")
print(f"Accuracy: {accuracy_score(y_true, y_pred_good):.3f}")
print(f"Precision: {precision_score(y_true, y_pred_good):.3f}")
print(f"Recall: {recall_score(y_true, y_pred_good):.3f}")
print(f"F1: {f1_score(y_true, y_pred_good):.3f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Bad Predictor (Predicts Everything as Normal) ===
Accuracy: 0.950
Precision: 0.000
Recall: 0.000
F1: 0.000

=== Good Predictor (Correctly Detects Anomalies) ===
Accuracy: 1.000
Precision: 1.000
Recall: 1.000
F1: 1.000
</code></pre>
<blockquote>
<p><strong>Lesson</strong>: Even with 95% accuracy, the predictor may not detect a single anomaly.</p>
</blockquote>
<h3>Confusion Matrix and Key Metrics</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Precision</strong></td>
<td>$\frac{TP}{TP + FP}$</td>
<td>Proportion of true anomalies among predicted anomalies</td>
</tr>
<tr>
<td><strong>Recall</strong></td>
<td>$\frac{TP}{TP + FN}$</td>
<td>Proportion of actual anomalies detected</td>
</tr>
<tr>
<td><strong>F1 Score</strong></td>
<td>$2 \cdot \frac{P \cdot R}{P + R}$</td>
<td>Harmonic mean of Precision and Recall</td>
</tr>
<tr>
<td><strong>ROC-AUC</strong></td>
<td>Area under ROC curve</td>
<td>Threshold-independent overall performance</td>
</tr>
<tr>
<td><strong>PR-AUC</strong></td>
<td>Area under PR curve</td>
<td>More appropriate than ROC-AUC for imbalanced data</td>
</tr>
</tbody>
</table>
<h3>ROC-AUC vs PR-AUC</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: ROC-AUC vs PR-AUC

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

# Generate imbalanced data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                           n_redundant=5, n_classes=2, weights=[0.95, 0.05],
                           random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Train model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Get prediction probabilities
y_scores = clf.predict_proba(X_test)[:, 1]

# ROC curve
fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# PR curve
precision, recall, _ = precision_recall_curve(y_test, y_scores)
pr_auc = average_precision_score(y_test, y_scores)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ROC curve
axes[0].plot(fpr, tpr, color='blue', lw=2,
             label=f'ROC Curve (AUC = {roc_auc:.3f})')
axes[0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--',
             label='Random Prediction')
axes[0].set_xlabel('False Positive Rate', fontsize=12)
axes[0].set_ylabel('True Positive Rate', fontsize=12)
axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# PR curve
axes[1].plot(recall, precision, color='green', lw=2,
             label=f'PR Curve (AUC = {pr_auc:.3f})')
baseline = (y_test == 1).sum() / len(y_test)
axes[1].axhline(y=baseline, color='gray', lw=1, linestyle='--',
                label=f'Baseline ({baseline:.3f})')
axes[1].set_xlabel('Recall', fontsize=12)
axes[1].set_ylabel('Precision', fontsize=12)
axes[1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Evaluation Metrics ===")
print(f"ROC-AUC: {roc_auc:.3f}")
print(f"PR-AUC: {pr_auc:.3f}")
print(f"Anomaly Data Ratio: {baseline:.3f}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: For imbalanced data, PR-AUC is a more appropriate metric than ROC-AUC. ROC-AUC tends to be overly optimistic due to the large proportion of normal data.</p>
</blockquote>
<h3>Domain-Specific Evaluation Metrics</h3>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Emphasized Metric</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Medical Diagnosis</strong></td>
<td>Recall (High)</td>
<td>Minimize false negatives (reduce FN)</td>
</tr>
<tr>
<td><strong>Spam Filter</strong></td>
<td>Precision (High)</td>
<td>Minimize false positives (reduce FP)</td>
</tr>
<tr>
<td><strong>Fraud Detection</strong></td>
<td>F1, PR-AUC</td>
<td>Balance emphasis</td>
</tr>
<tr>
<td><strong>Predictive Maintenance</strong></td>
<td>Recall (High)</td>
<td>Prevent failure oversight</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.4 Datasets and Visualization</h2>
<h3>Synthetic Datasets</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Synthetic Datasets

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_moons
from scipy.stats import multivariate_normal

np.random.seed(42)

# Dataset 1: Gaussian distribution
X_gaussian, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0,
                           center_box=(0, 0), random_state=42)
outliers_gaussian = np.random.uniform(low=-8, high=8, size=(15, 2))
X1 = np.vstack([X_gaussian, outliers_gaussian])
y1 = np.array([0]*300 + [1]*15)

# Dataset 2: Crescent shape
X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)
outliers_moons = np.random.uniform(low=-2, high=3, size=(15, 2))
X2 = np.vstack([X_moons, outliers_moons])
y2 = np.array([0]*300 + [1]*15)

# Dataset 3: Donut shape
theta = np.linspace(0, 2*np.pi, 300)
r = 3 + np.random.normal(0, 0.3, 300)
X_donut = np.column_stack([r * np.cos(theta), r * np.sin(theta)])
outliers_donut = np.random.normal(0, 1, size=(15, 2))
X3 = np.vstack([X_donut, outliers_donut])
y3 = np.array([0]*300 + [1]*15)

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

datasets = [
    (X1, y1, 'Gaussian Distribution'),
    (X2, y2, 'Crescent Shape'),
    (X3, y3, 'Donut Shape')
]

for ax, (X, y, title) in zip(axes, datasets):
    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', alpha=0.6,
               s=50, label='Normal', edgecolors='black')
    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', alpha=0.9,
               s=150, marker='X', label='Anomaly', edgecolors='black', linewidths=2)
    ax.set_xlabel('Feature 1', fontsize=11)
    ax.set_ylabel('Feature 2', fontsize=11)
    ax.set_title(title, fontsize=13, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Synthetic Dataset Characteristics ===")
print("1. Gaussian Distribution: Linearly separable, suitable for statistical methods")
print("2. Crescent Shape: Nonlinear pattern, complex boundaries")
print("3. Donut Shape: Density-based methods are effective")
</code></pre>
<h3>Real-world Datasets</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Domain</th>
<th>Samples</th>
<th>Anomaly Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Credit Card Fraud</strong></td>
<td>Finance</td>
<td>284,807</td>
<td>0.17%</td>
</tr>
<tr>
<td><strong>KDD Cup 99</strong></td>
<td>Network</td>
<td>4,898,431</td>
<td>19.7%</td>
</tr>
<tr>
<td><strong>MNIST (Anomaly Detection Version)</strong></td>
<td>Image</td>
<td>70,000</td>
<td>Variable</td>
</tr>
<tr>
<td><strong>Thyroid Disease</strong></td>
<td>Medical</td>
<td>3,772</td>
<td>2.5%</td>
</tr>
<tr>
<td><strong>NASA Bearing</strong></td>
<td>Manufacturing</td>
<td>Time Series</td>
<td>Variable</td>
</tr>
</tbody>
</table>
<h3>Visualization Techniques</h3>
<h4>1. Visualization by Dimensionality Reduction</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: 1. Visualization by Dimensionality Reduction

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Generate high-dimensional data (20 dimensions)
X, y = make_classification(n_samples=500, n_features=20, n_informative=15,
                           n_redundant=5, n_classes=2, weights=[0.95, 0.05],
                           random_state=42)

# Dimensionality reduction by PCA
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)

# Dimensionality reduction by t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# PCA
axes[0].scatter(X_pca[y==0, 0], X_pca[y==0, 1], c='blue', alpha=0.6,
                s=50, label='Normal', edgecolors='black')
axes[0].scatter(X_pca[y==1, 0], X_pca[y==1, 1], c='red', alpha=0.9,
                s=150, marker='X', label='Anomaly', edgecolors='black', linewidths=2)
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})', fontsize=12)
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})', fontsize=12)
axes[0].set_title('PCA Visualization', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# t-SNE
axes[1].scatter(X_tsne[y==0, 0], X_tsne[y==0, 1], c='blue', alpha=0.6,
                s=50, label='Normal', edgecolors='black')
axes[1].scatter(X_tsne[y==1, 0], X_tsne[y==1, 1], c='red', alpha=0.9,
                s=150, marker='X', label='Anomaly', edgecolors='black', linewidths=2)
axes[1].set_xlabel('t-SNE 1', fontsize=12)
axes[1].set_ylabel('t-SNE 2', fontsize=12)
axes[1].set_title('t-SNE Visualization', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Dimensionality Reduction Comparison ===")
print(f"PCA Cumulative Variance Ratio (2 components): {pca.explained_variance_ratio_.sum():.2%}")
print("t-SNE: Excellent at preserving nonlinear structure (emphasizes local structure)")
</code></pre>
<h4>2. Anomaly Score Visualization</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: 2. Anomaly Score Visualization

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# Generate data
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0,
                         center_box=(0, 0), random_state=42)
X_outliers = np.random.uniform(low=-8, high=8, size=(15, 2))
X = np.vstack([X_normal, X_outliers])

# Calculate anomaly scores with Isolation Forest
clf = IsolationForest(contamination=0.05, random_state=42)
clf.fit(X)
anomaly_scores = -clf.score_samples(X)  # Convert negative values to positive

# Calculate scores on grid (for heatmap)
xx, yy = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))
Z = -clf.score_samples(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Heatmap
contour = axes[0].contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.7)
axes[0].scatter(X[:, 0], X[:, 1], c=anomaly_scores, cmap='RdYlBu_r',
                s=50, edgecolors='black', linewidths=1)
plt.colorbar(contour, ax=axes[0], label='Anomaly Score')
axes[0].set_xlabel('Feature 1', fontsize=12)
axes[0].set_ylabel('Feature 2', fontsize=12)
axes[0].set_title('Anomaly Score Heatmap', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Histogram
axes[1].hist(anomaly_scores, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
axes[1].axvline(x=np.percentile(anomaly_scores, 95), color='red',
                linestyle='--', linewidth=2, label='95th Percentile (Threshold)')
axes[1].set_xlabel('Anomaly Score', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Anomaly Score Statistics ===")
print(f"Minimum: {anomaly_scores.min():.3f}")
print(f"Maximum: {anomaly_scores.max():.3f}")
print(f"Mean: {anomaly_scores.mean():.3f}")
print(f"95th Percentile (Threshold Candidate): {np.percentile(anomaly_scores, 95):.3f}")
</code></pre>
<hr/>
<h2>1.5 Challenges in Anomaly Detection</h2>
<h3>1. Label Scarcity</h3>
<p><strong>Problem</strong>: Labeling anomalous data is costly and difficult</p>
<p><strong>Countermeasures</strong>:</p>
<ul>
<li>Unsupervised learning (Isolation Forest, LOF)</li>
<li>Semi-supervised learning (One-Class SVM, Autoencoder)</li>
<li>Active Learning (label only important samples)</li>
<li>Weak Supervision (utilize noisy labels)</li>
</ul>
<h3>2. High Dimensionality</h3>
<p><strong>Problem</strong>: Curse of dimensionality (distances lose meaning)</p>
<p><strong>Countermeasures</strong>:</p>
<ul>
<li>Dimensionality reduction (PCA, Autoencoder)</li>
<li>Feature selection (use important features only)</li>
<li>Subspace methods</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Countermeasures:

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist

# Experiment on curse of dimensionality
dimensions = [2, 5, 10, 20, 50, 100, 200]
avg_distances = []

np.random.seed(42)
for d in dimensions:
    # Generate random points
    X = np.random.uniform(0, 1, size=(100, d))
    # Calculate pairwise distances
    distances = pdist(X, metric='euclidean')
    avg_distances.append(distances.mean())

# Visualization
plt.figure(figsize=(10, 6))
plt.plot(dimensions, avg_distances, marker='o', linewidth=2, markersize=8)
plt.xlabel('Number of Dimensions', fontsize=12)
plt.ylabel('Average Euclidean Distance', fontsize=12)
plt.title('Curse of Dimensionality: Relationship Between Dimensions and Distance', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Curse of Dimensionality ===")
for d, dist in zip(dimensions, avg_distances):
    print(f"Dimensions {d:3d}: Average Distance = {dist:.3f}")
print("\n‚Üí In high dimensions, all points appear equidistant (distances lose meaning)")
</code></pre>
<h3>3. Concept Drift</h3>
<p><strong>Problem</strong>: Normal patterns change over time</p>
<p><strong>Countermeasures</strong>:</p>
<ul>
<li>Online Learning (incremental updates)</li>
<li>Sliding Window (retrain on recent data)</li>
<li>Ensemble Methods (models from multiple time periods)</li>
<li>Adaptive Thresholds (dynamic threshold adjustment)</li>
</ul>
<h3>4. Interpretability</h3>
<p><strong>Problem</strong>: Difficult to explain why something was classified as anomalous</p>
<p><strong>Countermeasures</strong>:</p>
<ul>
<li>Rule-based methods</li>
<li>Feature importance</li>
<li>SHAP values (Shapley value-based explanations)</li>
<li>Attention mechanisms</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Countermeasures:

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.tree import DecisionTreeClassifier

# Sample data
np.random.seed(42)
X_normal = np.random.normal(0, 1, size=(100, 5))
X_anomaly = np.random.normal(5, 1, size=(5, 5))
X = np.vstack([X_normal, X_anomaly])
y = np.array([0]*100 + [1]*5)

# Anomaly detection with Isolation Forest
clf_if = IsolationForest(contamination=0.05, random_state=42)
clf_if.fit(X)
predictions = clf_if.predict(X)

# Analyze feature importance of anomalous samples
# Simply calculate deviation for each feature
X_mean = X_normal.mean(axis=0)
X_std = X_normal.std(axis=0)

anomaly_idx = np.where(predictions == -1)[0][:3]  # First 3 anomalous samples

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, idx in enumerate(anomaly_idx):
    deviations = np.abs((X[idx] - X_mean) / X_std)
    axes[i].bar(range(5), deviations, color='steelblue', edgecolor='black')
    axes[i].axhline(y=2, color='red', linestyle='--', linewidth=2, label='2œÉ')
    axes[i].set_xlabel('Feature', fontsize=11)
    axes[i].set_ylabel('Standard Deviation', fontsize=11)
    axes[i].set_title(f'Anomaly Sample {idx} Deviations', fontsize=12, fontweight='bold')
    axes[i].set_xticks(range(5))
    axes[i].set_xticklabels([f'F{j}' for j in range(5)])
    axes[i].legend(fontsize=9)
    axes[i].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("=== Interpretability Example ===")
for i, idx in enumerate(anomaly_idx):
    deviations = np.abs((X[idx] - X_mean) / X_std)
    max_dev_feature = deviations.argmax()
    print(f"Anomaly Sample {idx}: Feature {max_dev_feature} is most anomalous ({deviations[max_dev_feature]:.2f}œÉ)")
</code></pre>
<h3>Challenge Prioritization</h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Impact</th>
<th>Difficulty</th>
<th>Priority</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Label Scarcity</strong></td>
<td>High</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>Concept Drift</strong></td>
<td>High</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td><strong>High Dimensionality</strong></td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>Interpretability</strong></td>
<td>Medium</td>
<td>High</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Basics of Anomaly Detection</strong></p>
<ul>
<li>Three types: Point, Contextual, and Collective anomalies</li>
<li>Applications to fraud detection, manufacturing, healthcare, and IT systems</li>
<li>Understanding business value</li>
</ul></li>
<li><p><strong>Task Classification</strong></p>
<ul>
<li>Supervised, semi-supervised, and unsupervised learning</li>
<li>Novelty Detection vs Outlier Detection</li>
<li>Online vs Offline Detection</li>
</ul></li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li>Using Precision, Recall, and F1 appropriately</li>
<li>ROC-AUC vs PR-AUC</li>
<li>Handling class imbalance problems</li>
</ul></li>
<li><p><strong>Datasets and Visualization</strong></p>
<ul>
<li>Validation with synthetic data</li>
<li>Characteristics of real-world data</li>
<li>Visualization with PCA, t-SNE, and anomaly scores</li>
</ul></li>
<li><p><strong>Challenges and Countermeasures</strong></p>
<ul>
<li>Label Scarcity: Unsupervised and semi-supervised learning</li>
<li>High Dimensionality: Dimensionality reduction</li>
<li>Concept Drift: Online Learning</li>
<li>Interpretability: SHAP, Feature Importance</li>
</ul></li>
</ol>
<h3>Principles of Anomaly Detection</h3>
<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Leverage Domain Knowledge</strong></td>
<td>Reflect business knowledge in method selection and threshold setting</td>
</tr>
<tr>
<td><strong>Appropriate Evaluation Metrics</strong></td>
<td>Prioritize PR-AUC and F1 for imbalanced data</td>
</tr>
<tr>
<td><strong>Continuous Monitoring</strong></td>
<td>Retraining to handle Concept Drift</td>
</tr>
<tr>
<td><strong>Emphasize Interpretability</strong></td>
<td>Explainable models necessary for production deployment</td>
</tr>
<tr>
<td><strong>Cost Awareness</strong></td>
<td>Consider business costs of FP and FN</td>
</tr>
</tbody>
</table>
<h3>To the Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>Statistical Anomaly Detection</strong>:</p>
<ul>
<li>Z-score, Grubbs Test</li>
<li>Gaussian Mixture Models</li>
<li>Statistical Process Control</li>
<li>Bayesian Anomaly Detection</li>
<li>Applications to time series data</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the differences between Point Anomaly, Contextual Anomaly, and Collective Anomaly, and provide specific examples for each.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Point Anomaly</strong></p>
<ul>
<li>Description: Individual data points that differ significantly from all other data</li>
<li>Example: Sudden credit card transaction of $100,000</li>
</ul></li>
<li><p><strong>Contextual Anomaly</strong></p>
<ul>
<li>Description: Considered anomalous only in specific contexts (time, location, etc.)</li>
<li>Example: 35¬∞C temperature is normal in summer but anomalous in winter. Access to an office building at 3 AM is anomalous</li>
</ul></li>
<li><p><strong>Collective Anomaly</strong></p>
<ul>
<li>Description: Individual data points are normal but form an anomalous pattern as a collection</li>
<li>Example: Continuous abnormal waveforms in ECG, distributed DoS attack on web server</li>
</ul></li>
</ol>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>For the following scenario, select the appropriate anomaly detection task setting (supervised/semi-supervised/unsupervised) and explain your reasoning.</p>
<p><strong>Scenario</strong>: You want to detect defective products from product images on a manufacturing line. There are abundant images of normal products but only a few images of defective products.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Recommended Task</strong>: <strong>Semi-supervised Learning (Novelty Detection)</strong></p>
<p><strong>Reasoning</strong>:</p>
<ol>
<li><p><strong>Label Situation</strong></p>
<ul>
<li>Abundant images of normal products (labeled)</li>
<li>Only a few images of defective products (insufficient for supervised learning)</li>
</ul></li>
<li><p><strong>Task Nature</strong></p>
<ul>
<li>Learn patterns of normal products and consider deviations as anomalies</li>
<li>Typical use case for Novelty Detection</li>
</ul></li>
<li><p><strong>Specific Methods</strong></p>
<ul>
<li>One-Class SVM: Learn boundaries with normal data only</li>
<li>Autoencoder: Anomaly detection based on reconstruction error of normal images</li>
<li>Deep SVDD: Hypersphere representation of normal data using deep learning</li>
</ul></li>
</ol>
<p><strong>Why Supervised Learning is Inappropriate</strong>:</p>
<ul>
<li>Too few defective product samples (difficult to generalize with only a few)</li>
<li>Unknown types of defects (cannot detect defect patterns not included in training data)</li>
</ul>
<p><strong>Why Unsupervised Learning is Inappropriate</strong>:</p>
<ul>
<li>Inefficient not to utilize available normal product labels</li>
<li>Semi-supervised learning provides higher accuracy</li>
</ul>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Explain why Accuracy alone is insufficient for evaluation in anomaly detection, and propose alternative metrics to use.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Why Accuracy is Insufficient</strong>:</p>
<p>In anomaly detection, there is a class imbalance problem where normal data overwhelmingly outnumber anomalies (95-99%).</p>
<p><strong>Specific Example</strong>:</p>
<ul>
<li>Data: 95% normal, 5% anomaly</li>
<li>Predictor A: Predicts everything as normal ‚Üí Accuracy = 95% (not detecting a single anomaly)</li>
<li>Predictor B: Correctly detects all anomalies ‚Üí Accuracy = 100%</li>
</ul>
<p>Predictor A is useless yet receives a high evaluation of 95% accuracy.</p>
<p><strong>Recommended Evaluation Metrics</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Recommendation Reason</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PR-AUC</strong></td>
<td>Appropriate for imbalanced data, threshold-independent</td>
<td>Overall evaluation</td>
</tr>
<tr>
<td><strong>F1 Score</strong></td>
<td>Balance of Precision/Recall</td>
<td>Single threshold evaluation</td>
</tr>
<tr>
<td><strong>Recall</strong></td>
<td>Minimize missed anomalies</td>
<td>Medical, predictive maintenance</td>
</tr>
<tr>
<td><strong>Precision</strong></td>
<td>Minimize false detections</td>
<td>Spam filter</td>
</tr>
</tbody>
</table>
<p><strong>Formulas</strong>:</p>
<ul>
<li>Precision = TP / (TP + FP): Proportion of true anomalies among predicted anomalies</li>
<li>Recall = TP / (TP + FN): Proportion of actual anomalies detected</li>
<li>F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)</li>
</ul>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>Explain the impact of the curse of dimensionality on anomaly detection and provide three countermeasures. Show the relationship between number of dimensions and distance with Python code.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Impact of Curse of Dimensionality</strong>:</p>
<p>In high-dimensional spaces, distances between all data points become similar, causing distance-based anomaly detection methods (KNN, LOF, etc.) to become ineffective.</p>
<p><strong>Specific Problems</strong>:</p>
<ol>
<li>Distance between nearest neighbor and farthest point converges</li>
<li>Difference in distances between anomalous and normal data becomes smaller</li>
<li>Euclidean distance loses meaning</li>
</ol>
<p><strong>Countermeasures</strong>:</p>
<ol>
<li><p><strong>Dimensionality Reduction</strong></p>
<ul>
<li>PCA: Keep only important axes through principal component analysis</li>
<li>Autoencoder: Nonlinear dimensionality reduction</li>
<li>t-SNE/UMAP: Visualization and structure preservation</li>
</ul></li>
<li><p><strong>Feature Selection</strong></p>
<ul>
<li>Mutual Information: Select features that contribute to anomaly detection</li>
<li>L1 regularization: Set weights of unnecessary features to zero</li>
<li>Domain knowledge: Feature selection by experts</li>
</ul></li>
<li><p><strong>Subspace Methods</strong></p>
<ul>
<li>Subspace methods: Anomaly detection in multiple low-dimensional subspaces</li>
<li>Random Projection: Use multiple random low-dimensional projections</li>
</ul></li>
</ol>
<p><strong>Python Code</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Python Code:

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist, squareform

# Experiment on curse of dimensionality
np.random.seed(42)
dimensions = [2, 5, 10, 20, 50, 100, 200, 500]
results = []

for d in dimensions:
    # Generate random points from uniform distribution
    X = np.random.uniform(0, 1, size=(100, d))

    # Calculate pairwise distances
    distances = pdist(X, metric='euclidean')

    # Record statistics
    results.append({
        'dim': d,
        'mean': distances.mean(),
        'std': distances.std(),
        'min': distances.min(),
        'max': distances.max()
    })

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Mean distance and standard deviation
dims = [r['dim'] for r in results]
means = [r['mean'] for r in results]
stds = [r['std'] for r in results]

axes[0].plot(dims, means, marker='o', linewidth=2, markersize=8, label='Mean Distance')
axes[0].fill_between(dims,
                      [m - s for m, s in zip(means, stds)],
                      [m + s for m, s in zip(means, stds)],
                      alpha=0.3, label='¬±1œÉ')
axes[0].set_xlabel('Number of Dimensions', fontsize=12)
axes[0].set_ylabel('Euclidean Distance', fontsize=12)
axes[0].set_title('Relationship Between Dimensions and Distance', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Ratio of minimum to maximum distance
ratios = [r['min'] / r['max'] for r in results]
axes[1].plot(dims, ratios, marker='s', linewidth=2, markersize=8, color='red')
axes[1].axhline(y=1.0, color='gray', linestyle='--', label='Perfect Match')
axes[1].set_xlabel('Number of Dimensions', fontsize=12)
axes[1].set_ylabel('Min Distance / Max Distance', fontsize=12)
axes[1].set_title('Disappearance of Relative Distance Differences', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Curse of Dimensionality: Distance Statistics ===")
for r in results:
    print(f"Dimensions {r['dim']:3d}: Mean={r['mean']:.3f}, "
          f"Std Dev={r['std']:.3f}, Min/Max Ratio={r['min']/r['max']:.3f}")

print("\n‚Üí As dimensions increase:")
print("  1. Mean distance increases (scale effect)")
print("  2. Relative distance differences shrink (min/max ratio approaches 1)")
print("  3. All points appear equidistant (anomaly detection becomes difficult)")
</code></pre>
<p><strong>Conclusion</strong>:</p>
<ul>
<li>In high dimensions, distance differences become smaller, reducing anomaly detection accuracy</li>
<li>Preserve meaningful distances through dimensionality reduction or feature selection</li>
<li>Feature engineering utilizing domain knowledge is important</li>
</ul>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Explain the impact of Concept Drift on anomaly detection and demonstrate countermeasures using Online Learning. Include a simple implementation example with time series data.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Impact of Concept Drift</strong>:</p>
<p>Concept Drift refers to the phenomenon where the distribution of normal data changes over time. This causes models trained on past data to become unsuitable for current data.</p>
<p><strong>Specific Examples</strong>:</p>
<ul>
<li>E-commerce: Seasonal variations (purchasing patterns change between summer and winter)</li>
<li>Manufacturing: Normal vibration patterns change due to equipment aging</li>
<li>Network: Evolution of traffic patterns</li>
</ul>
<p><strong>Problems</strong>:</p>
<ol>
<li>Past models become outdated, increasing false positives (FP)</li>
<li>New normal patterns are misclassified as anomalies</li>
<li>Detection performance degrades over time</li>
</ol>
<p><strong>Countermeasures with Online Learning</strong>:</p>
<ol>
<li><p><strong>Sliding Window Approach</strong></p>
<ul>
<li>Retrain model with only recent N samples</li>
<li>Discard old data and adapt to new patterns</li>
</ul></li>
<li><p><strong>Incremental Learning</strong></p>
<ul>
<li>Update model incrementally with new data</li>
<li>Efficient without retraining on all data</li>
</ul></li>
<li><p><strong>Adaptive Thresholds</strong></p>
<ul>
<li>Dynamically adjust anomaly detection thresholds</li>
<li>Update thresholds based on recent data distribution</li>
</ul></li>
</ol>
<p><strong>Implementation Example (Sliding Window)</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Implementation Example (Sliding Window):

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# Generate time series data (with concept drift)
np.random.seed(42)
n_samples = 1000
time = np.arange(n_samples)

# Normal data: mean changes over time (Concept Drift)
mean_shift = time / 200  # Mean gradually increases
X = np.random.normal(loc=mean_shift, scale=1.0, size=(n_samples, 5))

# Add some anomalous data
anomaly_indices = np.random.choice(n_samples, size=50, replace=False)
X[anomaly_indices] += np.random.uniform(5, 10, size=(50, 5))

# True labels
y_true = np.zeros(n_samples)
y_true[anomaly_indices] = 1

# 1. Static model (train on initial data only)
print("=== 1. Static Model (No Concept Drift Handling) ===")
static_model = IsolationForest(contamination=0.05, random_state=42)
static_model.fit(X[:200])  # Only initial 200 samples

static_predictions = static_model.predict(X)
static_predictions = (static_predictions == -1).astype(int)

from sklearn.metrics import precision_score, recall_score, f1_score
static_precision = precision_score(y_true, static_predictions)
static_recall = recall_score(y_true, static_predictions)
static_f1 = f1_score(y_true, static_predictions)

print(f"Precision: {static_precision:.3f}")
print(f"Recall: {static_recall:.3f}")
print(f"F1 Score: {static_f1:.3f}")

# 2. Online Learning (Sliding Window)
print("\n=== 2. Online Learning (Sliding Window, window=200) ===")
window_size = 200
online_predictions = np.zeros(n_samples)

for i in range(window_size, n_samples):
    # Train model on recent window_size samples
    window_data = X[i-window_size:i]
    online_model = IsolationForest(contamination=0.05, random_state=42)
    online_model.fit(window_data)

    # Predict current sample
    pred = online_model.predict(X[i:i+1])
    online_predictions[i] = (pred == -1).astype(int)

online_precision = precision_score(y_true[window_size:], online_predictions[window_size:])
online_recall = recall_score(y_true[window_size:], online_predictions[window_size:])
online_f1 = f1_score(y_true[window_size:], online_predictions[window_size:])

print(f"Precision: {online_precision:.3f}")
print(f"Recall: {online_recall:.3f}")
print(f"F1 Score: {online_f1:.3f}")

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Change in data mean (Concept Drift)
axes[0].plot(time, X.mean(axis=1), alpha=0.7, label='Data Mean')
axes[0].scatter(anomaly_indices, X[anomaly_indices].mean(axis=1),
                c='red', s=50, marker='X', label='Anomalous Data', zorder=5)
axes[0].set_xlabel('Time', fontsize=12)
axes[0].set_ylabel('Mean Value', fontsize=12)
axes[0].set_title('Concept Drift: Normal Data Distribution Changes Over Time',
                  fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Prediction comparison
axes[1].scatter(time, static_predictions, alpha=0.5, label='Static Model', s=10)
axes[1].scatter(time, online_predictions, alpha=0.5, label='Online Learning', s=10)
axes[1].scatter(anomaly_indices, y_true[anomaly_indices],
                c='red', marker='X', s=100, label='True Anomalies', zorder=5, edgecolors='black')
axes[1].set_xlabel('Time', fontsize=12)
axes[1].set_ylabel('Anomaly Flag', fontsize=12)
axes[1].set_title('Static Model vs Online Learning', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n=== Performance Comparison ===")
print(f"Static Model:      F1={static_f1:.3f}")
print(f"Online Learning: F1={online_f1:.3f}")
print(f"Improvement: {(online_f1 - static_f1):.3f}")
</code></pre>
<p><strong>Conclusion</strong>:</p>
<ul>
<li>In environments with Concept Drift, static models suffer performance degradation</li>
<li>Online learning with Sliding Window adapts to new patterns</li>
<li>Window size is a tradeoff between stability (large) and adaptation speed (small)</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Chandola, V., Banerjee, A., &amp; Kumar, V. (2009). <em>Anomaly detection: A survey</em>. ACM computing surveys (CSUR), 41(3), 1-58.</li>
<li>Aggarwal, C. C. (2017). <em>Outlier analysis</em> (2nd ed.). Springer.</li>
<li>Goldstein, M., &amp; Uchida, S. (2016). <em>A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data</em>. PloS one, 11(4), e0152173.</li>
<li>Pang, G., Shen, C., Cao, L., &amp; Hengel, A. V. D. (2021). <em>Deep learning for anomaly detection: A review</em>. ACM Computing Surveys (CSUR), 54(2), 1-38.</li>
<li>Rousseeuw, P. J., &amp; Hubert, M. (2011). <em>Robust statistics for outlier detection</em>. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 1(1), 73-79.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter2-statistical-methods.html">Next Chapter: Statistical Anomaly Detection ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material is subject to change, updates, or discontinuation without notice.</li>
<li>The copyright and license of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
