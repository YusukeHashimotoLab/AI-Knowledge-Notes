<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Statistical Anomaly Detection - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-link disabled">Êó•Êú¨Ë™û (Ê∫ñÂÇô‰∏≠)</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 2: Statistical Anomaly Detection</h1>
<p class="subtitle">Fundamentals and Applications of Statistical Methods for Anomaly Detection</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Statistical Anomaly Detection. You will learn outlier detection using Z-score, Mahalanobis distance, and statistical hypothesis testing (Grubbs.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Implement outlier detection using Z-score and IQR</li>
<li>‚úÖ Understand Mahalanobis distance and multivariate Gaussian distribution</li>
<li>‚úÖ Apply statistical hypothesis testing (Grubbs, ESD)</li>
<li>‚úÖ Use time series anomaly detection techniques</li>
<li>‚úÖ Build a complete pipeline for statistical methods</li>
</ul>
<hr/>
<h2>2.1 Statistical Outlier Detection</h2>
<h3>Z-score (Standardized Score)</h3>
<p>The <strong>Z-score</strong> is a metric that indicates how many standard deviations a data point is away from the mean.</p>
<blockquote>
<p>Z-score = $\frac{x - \mu}{\sigma}$</p>
<p>Common threshold: Points with $|Z| &gt; 3$ are considered anomalous</p>
</blockquote>
<h4>Characteristics of Z-score</h4>
<ul>
<li><strong>Advantages</strong>: Simple and interpretable, fast computation</li>
<li><strong>Disadvantages</strong>: Assumes normal distribution, sensitive to outliers</li>
<li><strong>Application</strong>: Univariate data, data with near-normal distribution</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

"""
Example: Characteristics of Z-score

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate data
np.random.seed(42)
normal_data = np.random.normal(loc=0, scale=1, size=300)
outliers = np.array([5, -5, 6, -6, 7])
data = np.concatenate([normal_data, outliers])

# Calculate Z-scores
z_scores = np.abs(stats.zscore(data))
threshold = 3
anomalies = z_scores &gt; threshold

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Data distribution
axes[0].hist(data, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
axes[0].axvline(x=data.mean() + 3*data.std(), color='red',
                linestyle='--', linewidth=2, label='¬±3œÉ')
axes[0].axvline(x=data.mean() - 3*data.std(), color='red',
                linestyle='--', linewidth=2)
axes[0].set_xlabel('Value', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].set_title('Data Distribution and Z-score Threshold', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Z-score plot
axes[1].scatter(range(len(data)), z_scores, alpha=0.6, s=30, c='blue')
axes[1].scatter(np.where(anomalies)[0], z_scores[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black')
axes[1].axhline(y=threshold, color='red', linestyle='--', linewidth=2, label='Threshold=3')
axes[1].set_xlabel('Sample Index', fontsize=12)
axes[1].set_ylabel('|Z-score|', fontsize=12)
axes[1].set_title('Anomaly Detection Using Z-score', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Z-score Anomaly Detection Results ===")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Anomaly indices: {np.where(anomalies)[0]}")
print(f"Anomaly values: {data[anomalies]}")
</code></pre>
<h3>IQR (Interquartile Range)</h3>
<p>The <strong>IQR method</strong> is a robust outlier detection technique resistant to outliers.</p>
<blockquote>
<p>IQR = Q3 - Q1<br/>
Anomaly criteria: $x &lt; Q1 - 1.5 \times IQR$ or $x &gt; Q3 + 1.5 \times IQR$</p>
</blockquote>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: IQR = Q3 - Q1Anomaly criteria: $x  Q3 + 1.5 \times IQR$

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
normal_data = np.random.normal(loc=50, scale=10, size=300)
outliers = np.array([100, 5, 110, 0])
data = np.concatenate([normal_data, outliers])

# Calculate IQR
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Detect anomalies
anomalies = (data &lt; lower_bound) | (data &gt; upper_bound)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Box plot
bp = axes[0].boxplot(data, vert=True, patch_artist=True)
bp['boxes'][0].set_facecolor('lightblue')
bp['boxes'][0].set_edgecolor('black')
axes[0].scatter([1]*anomalies.sum(), data[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black')
axes[0].set_ylabel('Value', fontsize=12)
axes[0].set_title('Box Plot and IQR Anomaly Detection', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3, axis='y')

# Scatter plot
axes[1].scatter(range(len(data)), data, alpha=0.6, s=30, c='blue', label='Normal')
axes[1].scatter(np.where(anomalies)[0], data[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black')
axes[1].axhline(y=upper_bound, color='red', linestyle='--', linewidth=2, label='IQR Boundary')
axes[1].axhline(y=lower_bound, color='red', linestyle='--', linewidth=2)
axes[1].axhline(y=Q1, color='green', linestyle=':', linewidth=1.5, label='Q1/Q3')
axes[1].axhline(y=Q3, color='green', linestyle=':', linewidth=1.5)
axes[1].set_xlabel('Sample Index', fontsize=12)
axes[1].set_ylabel('Value', fontsize=12)
axes[1].set_title('Anomaly Detection Using IQR Method', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== IQR Anomaly Detection Results ===")
print(f"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
print(f"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Anomaly values: {data[anomalies]}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: Unlike Z-score, IQR does not assume a normal distribution and is less sensitive to the influence of outliers, making it a robust method.</p>
</blockquote>
<hr/>
<h2>2.2 Probability Distribution-Based Anomaly Detection</h2>
<h3>Mahalanobis Distance</h3>
<p><strong>Mahalanobis distance</strong> is a distance metric that accounts for covariance in multivariate data.</p>
<blockquote>
<p>$D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}$</p>
<p>Where $\mu$ is the mean vector and $\Sigma$ is the covariance matrix</p>
</blockquote>
<h4>Characteristics</h4>
<ul>
<li><strong>Advantages</strong>: Accounts for correlation between variables, scale-invariant</li>
<li><strong>Disadvantages</strong>: Requires inverse of covariance matrix, computationally expensive</li>
<li><strong>Application</strong>: Multivariate data, correlated features</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Characteristics

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import mahalanobis
from scipy.stats import chi2

# Generate correlated bivariate data
np.random.seed(42)
mean = [0, 0]
cov = [[1, 0.8], [0.8, 1]]  # Correlation coefficient 0.8
normal_data = np.random.multivariate_normal(mean, cov, size=300)

# Add anomalous data
outliers = np.array([[4, 4], [-4, -4], [4, -4]])
data = np.vstack([normal_data, outliers])

# Calculate Mahalanobis distance
mean_vec = normal_data.mean(axis=0)
cov_matrix = np.cov(normal_data.T)
cov_inv = np.linalg.inv(cov_matrix)

mahal_distances = np.array([mahalanobis(x, mean_vec, cov_inv) for x in data])

# Threshold (99th percentile of chi-square distribution with df=2)
threshold = np.sqrt(chi2.ppf(0.99, df=2))
anomalies = mahal_distances &gt; threshold

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Data distribution
axes[0].scatter(normal_data[:, 0], normal_data[:, 1],
                alpha=0.6, s=50, c='blue', label='Normal', edgecolors='black')
axes[0].scatter(outliers[:, 0], outliers[:, 1],
                c='red', s=150, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)

# Confidence ellipse (99%)
from matplotlib.patches import Ellipse
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
width, height = 2 * threshold * np.sqrt(eigenvalues)
ellipse = Ellipse(mean_vec, width, height, angle=angle,
                  edgecolor='red', facecolor='none', linewidth=2, linestyle='--', label='99% Confidence Ellipse')
axes[0].add_patch(ellipse)

axes[0].set_xlabel('Feature 1', fontsize=12)
axes[0].set_ylabel('Feature 2', fontsize=12)
axes[0].set_title('Anomaly Detection Using Mahalanobis Distance', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)
axes[0].axis('equal')

# Mahalanobis distance distribution
axes[1].hist(mahal_distances, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
axes[1].axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold={threshold:.2f}')
axes[1].set_xlabel('Mahalanobis Distance', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Mahalanobis Distance Distribution', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Mahalanobis Distance Anomaly Detection Results ===")
print(f"Threshold: {threshold:.3f}")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Mahalanobis distances of anomalies: {mahal_distances[anomalies]}")
</code></pre>
<h3>Multivariate Gaussian Distribution</h3>
<p>Anomaly detection using <strong>multivariate Gaussian distribution</strong> determines anomalies based on probability density.</p>
<blockquote>
<p>$p(x) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)$</p>
<p>Anomaly criteria: $p(x) &lt; \epsilon$ (threshold)</p>
</blockquote>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Anomaly criteria: $p(x) &lt; \epsilon$ (threshold)

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

# Generate data
np.random.seed(42)
mean = [0, 0]
cov = [[1, 0.5], [0.5, 1]]
normal_data = np.random.multivariate_normal(mean, cov, size=300)
outliers = np.array([[5, 5], [-5, -5], [5, -5]])
data = np.vstack([normal_data, outliers])

# Estimate multivariate Gaussian distribution parameters
mean_vec = normal_data.mean(axis=0)
cov_matrix = np.cov(normal_data.T)
mvn = multivariate_normal(mean=mean_vec, cov=cov_matrix)

# Calculate probability density
densities = mvn.pdf(data)

# Threshold (1st percentile)
threshold = np.percentile(densities, 1)
anomalies = densities &lt; threshold

# Calculate probability density on grid (for heatmap)
x_range = np.linspace(-6, 6, 200)
y_range = np.linspace(-6, 6, 200)
xx, yy = np.meshgrid(x_range, y_range)
positions = np.dstack((xx, yy))
Z = mvn.pdf(positions)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Probability density heatmap
contour = axes[0].contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.7)
axes[0].scatter(normal_data[:, 0], normal_data[:, 1],
                alpha=0.6, s=30, c='blue', label='Normal', edgecolors='black')
axes[0].scatter(data[anomalies, 0], data[anomalies, 1],
                c='red', s=150, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
plt.colorbar(contour, ax=axes[0], label='Probability Density')
axes[0].set_xlabel('Feature 1', fontsize=12)
axes[0].set_ylabel('Feature 2', fontsize=12)
axes[0].set_title('Anomaly Detection Using Multivariate Gaussian', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Probability density histogram
axes[1].hist(np.log(densities), bins=30, alpha=0.7, edgecolor='black', color='steelblue')
axes[1].axvline(x=np.log(threshold), color='red', linestyle='--', linewidth=2, label='Threshold')
axes[1].set_xlabel('log(Probability Density)', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Probability Density Distribution', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Multivariate Gaussian Anomaly Detection Results ===")
print(f"Threshold: {threshold:.6f}")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Probability densities of anomalies: {densities[anomalies]}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: Mahalanobis distance and multivariate Gaussian distribution are mathematically equivalent. The squared Mahalanobis distance is proportional to the log probability density.</p>
</blockquote>
<hr/>
<h2>2.3 Statistical Hypothesis Testing</h2>
<h3>Grubbs' Test</h3>
<p><strong>Grubbs' Test</strong> is a hypothesis test for detecting a single outlier.</p>
<blockquote>
<p>Null hypothesis $H_0$: No outliers exist<br/>
Test statistic: $G = \frac{\max|x_i - \bar{x}|}{s}$<br/>
Where $s$ is the standard deviation</p>
</blockquote>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def grubbs_test(data, alpha=0.05):
    """Outlier detection using Grubbs' Test"""
    n = len(data)
    mean = np.mean(data)
    std = np.std(data, ddof=1)

    # Calculate test statistic
    deviations = np.abs(data - mean)
    max_idx = np.argmax(deviations)
    G = deviations[max_idx] / std

    # Calculate critical value
    t_dist = stats.t.ppf(1 - alpha / (2 * n), n - 2)
    G_critical = ((n - 1) / np.sqrt(n)) * np.sqrt(t_dist**2 / (n - 2 + t_dist**2))

    is_outlier = G &gt; G_critical

    return {
        'G': G,
        'G_critical': G_critical,
        'is_outlier': is_outlier,
        'outlier_idx': max_idx if is_outlier else None,
        'outlier_value': data[max_idx] if is_outlier else None,
        'p_value': 1 - stats.t.cdf(G * np.sqrt(n) / np.sqrt(n - 1), n - 2)
    }

# Generate data
np.random.seed(42)
data = np.concatenate([np.random.normal(50, 5, size=30), [80]])  # 80 is outlier

# Run Grubbs' Test
result = grubbs_test(data, alpha=0.05)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Data plot
axes[0].scatter(range(len(data)), data, alpha=0.6, s=50, c='blue', label='Data')
if result['is_outlier']:
    axes[0].scatter(result['outlier_idx'], result['outlier_value'],
                    c='red', s=200, marker='X', label='Outlier', zorder=5, edgecolors='black', linewidths=2)
axes[0].axhline(y=data.mean(), color='green', linestyle='--', linewidth=2, label='Mean')
axes[0].axhline(y=data.mean() + 3*data.std(), color='orange', linestyle=':', linewidth=1.5, label='¬±3œÉ')
axes[0].axhline(y=data.mean() - 3*data.std(), color='orange', linestyle=':', linewidth=1.5)
axes[0].set_xlabel('Sample Index', fontsize=12)
axes[0].set_ylabel('Value', fontsize=12)
axes[0].set_title("Grubbs' Test Results", fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Test statistic comparison
axes[1].bar(['G Statistic', 'Critical Value'], [result['G'], result['G_critical']],
            color=['steelblue', 'red'], edgecolor='black', alpha=0.7)
axes[1].set_ylabel('Value', fontsize=12)
axes[1].set_title('Test Statistic vs Critical Value', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("=== Grubbs' Test Results ===")
print(f"G statistic: {result['G']:.3f}")
print(f"Critical value: {result['G_critical']:.3f}")
print(f"p-value: {result['p_value']:.4f}")
print(f"Outlier detected: {'Yes' if result['is_outlier'] else 'No'}")
if result['is_outlier']:
    print(f"Outlier: index={result['outlier_idx']}, value={result['outlier_value']:.2f}")
</code></pre>
<h3>ESD Test (Extreme Studentized Deviate Test)</h3>
<p><strong>Generalized ESD Test</strong> is an extended version that can detect multiple outliers.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def generalized_esd_test(data, max_outliers, alpha=0.05):
    """Multiple outlier detection using Generalized ESD Test"""
    n = len(data)
    outliers = []
    data_copy = data.copy()

    for i in range(max_outliers):
        mean = np.mean(data_copy)
        std = np.std(data_copy, ddof=1)

        # Calculate test statistic
        deviations = np.abs(data_copy - mean)
        max_idx = np.argmax(deviations)
        R = deviations[max_idx] / std

        # Calculate critical value
        n_current = len(data_copy)
        p = 1 - alpha / (2 * (n_current - i))
        t_dist = stats.t.ppf(p, n_current - i - 2)
        lambda_critical = ((n_current - i - 1) * t_dist) / np.sqrt((n_current - i - 2 + t_dist**2) * (n_current - i))

        if R &gt; lambda_critical:
            outlier_idx = np.where(data == data_copy[max_idx])[0][0]
            outliers.append({'index': outlier_idx, 'value': data_copy[max_idx], 'R': R})
            data_copy = np.delete(data_copy, max_idx)
        else:
            break

    return outliers

# Generate data
np.random.seed(42)
normal_data = np.random.normal(50, 5, size=30)
outlier_values = [80, 85, 15]
data = np.concatenate([normal_data, outlier_values])

# Run ESD Test
outliers = generalized_esd_test(data, max_outliers=5, alpha=0.05)

# Visualization
plt.figure(figsize=(12, 6))
plt.scatter(range(len(data)), data, alpha=0.6, s=50, c='blue', label='Normal Data')

if outliers:
    outlier_indices = [o['index'] for o in outliers]
    outlier_values_detected = [o['value'] for o in outliers]
    plt.scatter(outlier_indices, outlier_values_detected,
                c='red', s=200, marker='X', label='Outlier', zorder=5, edgecolors='black', linewidths=2)

plt.axhline(y=data.mean(), color='green', linestyle='--', linewidth=2, label='Mean')
plt.axhline(y=data.mean() + 3*data.std(), color='orange', linestyle=':', linewidth=1.5, label='¬±3œÉ')
plt.axhline(y=data.mean() - 3*data.std(), color='orange', linestyle=':', linewidth=1.5)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.title('Generalized ESD Test Results', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=== Generalized ESD Test Results ===")
print(f"Number of outliers detected: {len(outliers)}")
for i, o in enumerate(outliers, 1):
    print(f"Outlier {i}: index={o['index']}, value={o['value']:.2f}, R={o['R']:.3f}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: While Grubbs' Test can detect only one outlier, ESD Test can sequentially detect multiple outliers.</p>
</blockquote>
<hr/>
<h2>2.4 Time Series Anomaly Detection</h2>
<h3>Anomaly Detection Using Moving Average</h3>
<p><strong>Moving average</strong> captures trends in time series data and detects deviations.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Moving averagecaptures trends in time series data and detect

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# Generate time series data
np.random.seed(42)
n_samples = 300
time = np.arange(n_samples)
trend = 0.05 * time
seasonal = 10 * np.sin(2 * np.pi * time / 50)
noise = np.random.normal(0, 2, n_samples)
data = trend + seasonal + noise

# Add anomalies
anomaly_indices = [50, 150, 250]
data[anomaly_indices] += [20, -25, 30]

# Calculate moving average
window_size = 20
moving_avg = np.convolve(data, np.ones(window_size)/window_size, mode='same')
moving_std = np.array([data[max(0, i-window_size//2):min(len(data), i+window_size//2)].std()
                       for i in range(len(data))])

# Detect anomalies (3-sigma rule)
residuals = np.abs(data - moving_avg)
threshold = 3 * moving_std
anomalies = residuals &gt; threshold

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Time series data
axes[0].plot(time, data, alpha=0.7, linewidth=1, label='Original Data', color='blue')
axes[0].plot(time, moving_avg, linewidth=2, label=f'Moving Average (window={window_size})', color='green')
axes[0].fill_between(time, moving_avg - 3*moving_std, moving_avg + 3*moving_std,
                     alpha=0.2, color='green', label='¬±3œÉ')
axes[0].scatter(time[anomalies], data[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
axes[0].set_xlabel('Time', fontsize=12)
axes[0].set_ylabel('Value', fontsize=12)
axes[0].set_title('Time Series Anomaly Detection Using Moving Average', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Residual plot
axes[1].plot(time, residuals, alpha=0.7, linewidth=1, color='blue')
axes[1].plot(time, threshold, linestyle='--', linewidth=2, color='red', label='Threshold (3œÉ)')
axes[1].scatter(time[anomalies], residuals[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
axes[1].set_xlabel('Time', fontsize=12)
axes[1].set_ylabel('Residual', fontsize=12)
axes[1].set_title('Residuals and Anomaly Threshold', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Moving Average Anomaly Detection Results ===")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Anomaly indices: {np.where(anomalies)[0]}")
</code></pre>
<h3>Anomaly Detection Using Seasonal Decomposition</h3>
<p><strong>STL decomposition</strong> (Seasonal and Trend decomposition using Loess) decomposes data into seasonal, trend, and residual components.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: STL decomposition(Seasonal and Trend decomposition using Loe

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Generate time series data (with clear seasonality)
np.random.seed(42)
n_samples = 200
time = np.arange(n_samples)
trend = 0.1 * time
seasonal = 15 * np.sin(2 * np.pi * time / 30)  # Period 30
noise = np.random.normal(0, 2, n_samples)
data = trend + seasonal + noise

# Add anomalies
anomaly_indices = [50, 120, 180]
data[anomaly_indices] += [30, -30, 25]

# Seasonal decomposition
result = seasonal_decompose(data, model='additive', period=30, extrapolate_trend='freq')

# Detect anomalies from residuals
residual = result.resid
threshold = 3 * np.nanstd(residual)
anomalies = np.abs(residual) &gt; threshold

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(14, 12))

# Original data
axes[0].plot(time, data, linewidth=1, color='blue')
axes[0].scatter(time[anomalies], data[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
axes[0].set_ylabel('Original Data', fontsize=11)
axes[0].set_title('Time Series Anomaly Detection Using Seasonal Decomposition', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Trend
axes[1].plot(time, result.trend, linewidth=2, color='green')
axes[1].set_ylabel('Trend', fontsize=11)
axes[1].grid(True, alpha=0.3)

# Seasonality
axes[2].plot(time, result.seasonal, linewidth=2, color='orange')
axes[2].set_ylabel('Seasonal', fontsize=11)
axes[2].grid(True, alpha=0.3)

# Residual
axes[3].plot(time, residual, linewidth=1, color='blue')
axes[3].axhline(y=threshold, color='red', linestyle='--', linewidth=2, label='Threshold (¬±3œÉ)')
axes[3].axhline(y=-threshold, color='red', linestyle='--', linewidth=2)
axes[3].scatter(time[anomalies], residual[anomalies],
                c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
axes[3].set_xlabel('Time', fontsize=12)
axes[3].set_ylabel('Residual', fontsize=11)
axes[3].legend(fontsize=10)
axes[3].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Seasonal Decomposition Anomaly Detection Results ===")
print(f"Period: 30")
print(f"Number of anomalies detected: {anomalies.sum()}")
print(f"Anomaly indices: {np.where(anomalies)[0]}")
</code></pre>
<blockquote>
<p><strong>Important</strong>: Seasonal decomposition can clearly detect true anomalies as residuals by removing trend and seasonality.</p>
</blockquote>
<hr/>
<h2>2.5 Implementation and Applications</h2>
<h3>Complete Pipeline for Statistical Anomaly Detection</h3>
<p>Build a practical statistical anomaly detection system for real-world use.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scipy&gt;=1.11.0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from statsmodels.tsa.seasonal import seasonal_decompose

class StatisticalAnomalyDetector:
    """Integrated class for statistical anomaly detection"""

    def __init__(self, method='zscore', threshold=3.0, window_size=20):
        """
        Parameters:
        -----------
        method : str
            Detection method ('zscore', 'iqr', 'mahalanobis', 'moving_avg', 'seasonal')
        threshold : float
            Anomaly detection threshold
        window_size : int
            Window size for moving average
        """
        self.method = method
        self.threshold = threshold
        self.window_size = window_size
        self.fitted = False

    def fit(self, X):
        """Learn statistics from training data"""
        if self.method == 'zscore':
            self.mean_ = np.mean(X, axis=0)
            self.std_ = np.std(X, axis=0)
        elif self.method == 'iqr':
            self.q1_ = np.percentile(X, 25, axis=0)
            self.q3_ = np.percentile(X, 75, axis=0)
            self.iqr_ = self.q3_ - self.q1_
        elif self.method == 'mahalanobis':
            self.mean_ = np.mean(X, axis=0)
            self.cov_ = np.cov(X.T)
            self.cov_inv_ = np.linalg.inv(self.cov_)

        self.fitted = True
        return self

    def predict(self, X):
        """Calculate anomaly scores and detect anomalies"""
        if not self.fitted and self.method not in ['moving_avg', 'seasonal']:
            raise ValueError("Model not fitted. Run fit() first.")

        if self.method == 'zscore':
            scores = np.abs((X - self.mean_) / self.std_)
            anomalies = np.any(scores &gt; self.threshold, axis=1)

        elif self.method == 'iqr':
            lower = self.q1_ - 1.5 * self.iqr_
            upper = self.q3_ + 1.5 * self.iqr_
            anomalies = np.any((X &lt; lower) | (X &gt; upper), axis=1)
            scores = np.max(np.abs(X - self.mean_) / self.std_, axis=1)

        elif self.method == 'mahalanobis':
            from scipy.spatial.distance import mahalanobis
            scores = np.array([mahalanobis(x, self.mean_, self.cov_inv_) for x in X])
            anomalies = scores &gt; self.threshold

        elif self.method == 'moving_avg':
            # Only supports 1D time series
            moving_avg = np.convolve(X.flatten(), np.ones(self.window_size)/self.window_size, mode='same')
            moving_std = np.array([X.flatten()[max(0, i-self.window_size//2):min(len(X), i+self.window_size//2)].std()
                                   for i in range(len(X))])
            scores = np.abs(X.flatten() - moving_avg)
            anomalies = scores &gt; self.threshold * moving_std

        elif self.method == 'seasonal':
            # Only supports 1D time series
            result = seasonal_decompose(X.flatten(), model='additive', period=self.window_size, extrapolate_trend='freq')
            scores = np.abs(result.resid)
            threshold_val = self.threshold * np.nanstd(result.resid)
            anomalies = scores &gt; threshold_val

        return anomalies.astype(int), scores

    def fit_predict(self, X):
        """Perform fitting and prediction in one step"""
        self.fit(X)
        return self.predict(X)

# Demonstration
np.random.seed(42)

# Generate dataset
n_samples = 300
n_features = 2
X_normal = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], size=n_samples)
X_outliers = np.array([[5, 5], [-5, -5], [5, -5], [-5, 5]])
X = np.vstack([X_normal, X_outliers])
y_true = np.array([0]*n_samples + [1]*len(X_outliers))

# Detect anomalies with each method
methods = ['zscore', 'iqr', 'mahalanobis']
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

for i, method in enumerate(methods):
    detector = StatisticalAnomalyDetector(method=method, threshold=3.0)
    detector.fit(X_normal)  # Train on normal data only
    y_pred, scores = detector.predict(X)

    # Visualization
    axes[i].scatter(X[y_pred==0, 0], X[y_pred==0, 1],
                    alpha=0.6, s=50, c='blue', label='Normal', edgecolors='black')
    axes[i].scatter(X[y_pred==1, 0], X[y_pred==1, 1],
                    c='red', s=150, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)
    axes[i].set_xlabel('Feature 1', fontsize=12)
    axes[i].set_ylabel('Feature 2', fontsize=12)
    axes[i].set_title(f'{method.upper()} Method', fontsize=14, fontweight='bold')
    axes[i].legend(fontsize=10)
    axes[i].grid(True, alpha=0.3)

    # Evaluation
    from sklearn.metrics import precision_score, recall_score, f1_score
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print(f"=== {method.upper()} Method ===")
    print(f"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\n")

plt.tight_layout()
plt.show()
</code></pre>
<blockquote>
<p><strong>Important</strong>: In practice, combining multiple statistical methods into an ensemble is effective. By leveraging the strengths of each method, robust anomaly detection becomes possible.</p>
</blockquote>
<hr/>
<h2>Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Statistical Outlier Detection</strong></p>
<ul>
<li>Z-score: Simple and fast, assumes normal distribution</li>
<li>IQR: Robust, distribution-independent</li>
</ul></li>
<li><p><strong>Probability Distribution-Based</strong></p>
<ul>
<li>Mahalanobis distance: Multivariate, accounts for covariance</li>
<li>Multivariate Gaussian distribution: Probability density-based detection</li>
</ul></li>
<li><p><strong>Statistical Hypothesis Testing</strong></p>
<ul>
<li>Grubbs' Test: Rigorous testing for single outlier</li>
<li>ESD Test: Sequential detection of multiple outliers</li>
</ul></li>
<li><p><strong>Time Series Anomaly Detection</strong></p>
<ul>
<li>Moving average: Trend following and residual detection</li>
<li>Seasonal decomposition: Remove seasonality and trend</li>
</ul></li>
<li><p><strong>Implementation and Applications</strong></p>
<ul>
<li>Integrated pipeline: Unified interface for multiple methods</li>
<li>Practical application: Method selection based on domain</li>
</ul></li>
</ol>
<h3>Selection Criteria for Statistical Methods</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Data Type</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Z-score</strong></td>
<td>Univariate, normal distribution</td>
<td>Simple, fast</td>
<td>Sensitive to outliers</td>
</tr>
<tr>
<td><strong>IQR</strong></td>
<td>Univariate, any distribution</td>
<td>Robust</td>
<td>Not suitable for multivariate</td>
</tr>
<tr>
<td><strong>Mahalanobis</strong></td>
<td>Multivariate, with correlation</td>
<td>Accounts for covariance</td>
<td>High computational cost</td>
</tr>
<tr>
<td><strong>Grubbs/ESD</strong></td>
<td>Univariate, normal distribution</td>
<td>Clear statistical basis</td>
<td>Sequential processing</td>
</tr>
<tr>
<td><strong>Moving average</strong></td>
<td>Time series</td>
<td>Trend following</td>
<td>Has lag</td>
</tr>
<tr>
<td><strong>Seasonal decomposition</strong></td>
<td>Seasonal time series</td>
<td>Removes seasonality</td>
<td>Requires prior knowledge of period</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 3, we will learn <strong>anomaly detection using machine learning</strong>:</p>
<ul>
<li>Isolation Forest, LOF</li>
<li>One-Class SVM</li>
<li>Clustering-based methods</li>
<li>Ensemble methods</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain the differences between Z-score and IQR methods, and describe in which situations each is appropriate.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Z-score</strong>:</p>
<ul>
<li>Formula: $(x - \mu) / \sigma$</li>
<li>Assumption: Data follows normal distribution</li>
<li>Threshold: Typically $|Z| &gt; 3$</li>
<li>Characteristic: Uses mean and standard deviation, sensitive to outlier influence</li>
</ul>
<p><strong>IQR Method</strong>:</p>
<ul>
<li>Formula: $IQR = Q3 - Q1$, anomalies are $x &lt; Q1 - 1.5 \times IQR$ or $x &gt; Q3 + 1.5 \times IQR$</li>
<li>Assumption: Distribution-independent (non-parametric)</li>
<li>Characteristic: Uses quartiles, robust to outliers</li>
</ul>
<p><strong>Appropriate Applications</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data close to normal distribution</td>
<td>Z-score</td>
<td>Clear statistical basis</td>
</tr>
<tr>
<td>Unknown/non-normal distribution</td>
<td>IQR</td>
<td>No distributional assumptions</td>
</tr>
<tr>
<td>Outliers already present</td>
<td>IQR</td>
<td>High robustness</td>
</tr>
<tr>
<td>Fast processing required</td>
<td>Z-score</td>
<td>Simple computation</td>
</tr>
</tbody>
</table>
</details>
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Explain the advantages of Mahalanobis distance over Euclidean distance using an example with correlated data. Include simple Python code.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Advantages of Mahalanobis Distance</strong>:</p>
<ol>
<li><strong>Accounts for covariance</strong>: Reflects correlation between variables</li>
<li><strong>Scale-invariant</strong>: Independent of feature scales</li>
<li><strong>Elliptical boundaries</strong>: Adapts to data distribution shape</li>
</ol>
<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Implementation Example:

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import euclidean, mahalanobis

# Generate strongly correlated data
np.random.seed(42)
mean = [0, 0]
cov = [[1, 0.9], [0.9, 1]]  # Correlation coefficient 0.9
data = np.random.multivariate_normal(mean, cov, size=300)

# Test points (outside data distribution)
test_point1 = np.array([2, 2])  # Point along correlation direction
test_point2 = np.array([2, -2])  # Point perpendicular to correlation

# Calculate distances
mean_vec = data.mean(axis=0)
cov_matrix = np.cov(data.T)
cov_inv = np.linalg.inv(cov_matrix)

euclidean_dist1 = euclidean(test_point1, mean_vec)
euclidean_dist2 = euclidean(test_point2, mean_vec)
mahal_dist1 = mahalanobis(test_point1, mean_vec, cov_inv)
mahal_dist2 = mahalanobis(test_point2, mean_vec, cov_inv)

# Visualization
plt.figure(figsize=(10, 8))
plt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=30, c='blue', label='Data')
plt.scatter(*test_point1, c='red', s=200, marker='X', label='Point 1 (along correlation)',
            edgecolors='black', linewidths=2, zorder=5)
plt.scatter(*test_point2, c='orange', s=200, marker='X', label='Point 2 (perpendicular)',
            edgecolors='black', linewidths=2, zorder=5)
plt.scatter(*mean_vec, c='green', s=200, marker='o', label='Center',
            edgecolors='black', linewidths=2, zorder=5)

# Confidence ellipse
from matplotlib.patches import Ellipse
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
width, height = 2 * 3 * np.sqrt(eigenvalues)
ellipse = Ellipse(mean_vec, width, height, angle=angle,
                  edgecolor='green', facecolor='none', linewidth=2, linestyle='--')
plt.gca().add_patch(ellipse)

plt.xlabel('Feature 1', fontsize=12)
plt.ylabel('Feature 2', fontsize=12)
plt.title('Euclidean Distance vs Mahalanobis Distance', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.tight_layout()
plt.show()

print("=== Distance Comparison ===")
print(f"Point 1 (along correlation):")
print(f"  Euclidean distance: {euclidean_dist1:.3f}")
print(f"  Mahalanobis distance: {mahal_dist1:.3f}")
print(f"\nPoint 2 (perpendicular):")
print(f"  Euclidean distance: {euclidean_dist2:.3f}")
print(f"  Mahalanobis distance: {mahal_dist2:.3f}")
print(f"\n‚Üí Euclidean distances are equal, but Mahalanobis distance shows Point 2 is farther")
print("  (correctly reflects data distribution shape)")
</code></pre>
<p><strong>Conclusion</strong>:</p>
<ul>
<li>Euclidean distance judges both points as equidistant ($\sqrt{8} \approx 2.83$)</li>
<li>Mahalanobis distance correctly identifies Point 2 as anomalous</li>
<li>By accounting for correlation, captures true data distribution</li>
</ul>
</details>
<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Explain the differences between Grubbs' Test and Generalized ESD Test, and describe which is appropriate when multiple outliers are present.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Grubbs' Test</strong>:</p>
<ul>
<li><strong>Purpose</strong>: Detect a single outlier</li>
<li><strong>Procedure</strong>: Test only the most extreme value</li>
<li><strong>Issue</strong>: Fails to detect when multiple outliers exist due to masking effect</li>
<li><strong>Masking effect</strong>: Multiple outliers distort mean and standard deviation, hindering detection</li>
</ul>
<p><strong>Generalized ESD Test</strong>:</p>
<ul>
<li><strong>Purpose</strong>: Detect multiple outliers (up to k maximum)</li>
<li><strong>Procedure</strong>: Sequentially remove outliers while repeating tests</li>
<li><strong>Advantage</strong>: Avoids masking effect</li>
<li><strong>Note</strong>: Requires specifying maximum number of outliers k in advance</li>
</ul>
<p><strong>Recommendation</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Certain only one outlier</td>
<td>Grubbs' Test</td>
<td>Simple and clear</td>
</tr>
<tr>
<td>Possibility of multiple outliers</td>
<td>Generalized ESD</td>
<td>Avoids masking</td>
</tr>
<tr>
<td>Number of outliers unknown</td>
<td>Generalized ESD</td>
<td>Set k conservatively</td>
</tr>
</tbody>
</table>
<p><strong>Specific Example</strong>:</p>
<p>Data: [50, 51, 49, 52, 48, 100, 105] (two outliers: 100, 105)</p>
<ul>
<li>Grubbs' Test: Detects only 105 (100 fails detection due to distorted mean)</li>
<li>ESD Test: Detects 105, removes it ‚Üí detects 100 (succeeds through sequential processing)</li>
</ul>
</details>
<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Explain why setting the period parameter is crucial in time series anomaly detection using seasonal decomposition (STL), and show the problems that arise from incorrect period settings. Include implementation examples.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Importance of Period Parameter</strong>:</p>
<ol>
<li><strong>Accurate seasonal removal</strong>: Decomposing with incorrect period leaves seasonal components in residuals</li>
<li><strong>Anomaly detection accuracy</strong>: Remaining seasonality in residuals increases false positives (FP)</li>
<li><strong>Trend estimation</strong>: Inappropriate period also distorts trend component</li>
</ol>
<p><strong>Problems with Incorrect Period Settings</strong>:</p>
<ul>
<li><strong>Underestimation</strong> (shorter than true period): Over-removes seasonality, misidentifies true trend as seasonal</li>
<li><strong>Overestimation</strong> (longer than true period): Seasonality remains in residuals, normal seasonal variations flagged as anomalies</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Implementation Example:

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Generate time series data with true period of 30
np.random.seed(42)
n_samples = 300
time = np.arange(n_samples)
trend = 0.05 * time
seasonal = 10 * np.sin(2 * np.pi * time / 30)  # Period 30
noise = np.random.normal(0, 1, n_samples)
data = trend + seasonal + noise

# Add anomalies
data[100] += 25
data[200] -= 25

# Decompose with 3 different period settings
periods = [15, 30, 60]  # Underestimated, accurate, overestimated
fig, axes = plt.subplots(3, 3, figsize=(16, 12))

for i, period in enumerate(periods):
    result = seasonal_decompose(data, model='additive', period=period, extrapolate_trend='freq')

    # Original data
    axes[i, 0].plot(time, data, linewidth=1, alpha=0.7)
    axes[i, 0].set_ylabel(f'Period={period}\nOriginal', fontsize=10)
    axes[i, 0].grid(True, alpha=0.3)

    # Seasonality
    axes[i, 1].plot(time, result.seasonal, linewidth=1, color='orange')
    axes[i, 1].set_ylabel('Seasonal', fontsize=10)
    axes[i, 1].grid(True, alpha=0.3)

    # Residual and anomaly detection
    residual = result.resid
    threshold = 3 * np.nanstd(residual)
    anomalies = np.abs(residual) &gt; threshold

    axes[i, 2].plot(time, residual, linewidth=1, alpha=0.7)
    axes[i, 2].axhline(y=threshold, color='red', linestyle='--', linewidth=2)
    axes[i, 2].axhline(y=-threshold, color='red', linestyle='--', linewidth=2)
    axes[i, 2].scatter(time[anomalies], residual[anomalies],
                       c='red', s=50, marker='X', zorder=5)
    axes[i, 2].set_ylabel('Residual', fontsize=10)
    axes[i, 2].grid(True, alpha=0.3)

    # Evaluation
    print(f"=== Period={period} ===")
    print(f"Number of anomalies detected: {anomalies.sum()}")
    print(f"Anomaly indices: {np.where(anomalies)[0]}\n")

axes[0, 0].set_title('Original Data', fontsize=12, fontweight='bold')
axes[0, 1].set_title('Seasonal Component', fontsize=12, fontweight='bold')
axes[0, 2].set_title('Residual (Anomaly Detection)', fontsize=12, fontweight='bold')
axes[2, 0].set_xlabel('Time', fontsize=11)
axes[2, 1].set_xlabel('Time', fontsize=11)
axes[2, 2].set_xlabel('Time', fontsize=11)

plt.tight_layout()
plt.show()

print("=== Conclusion ===")
print("Period=15 (underestimated): Complex seasonality, increased false detections")
print("Period=30 (accurate): Proper decomposition, accurate anomaly detection")
print("Period=60 (overestimated): Seasonality remains in residuals, normal variations misdetected")
</code></pre>
<p><strong>Conclusion</strong>:</p>
<ul>
<li>Accurate period setting determines anomaly detection accuracy</li>
<li>Period determined from prior knowledge (business cycles, seasonal patterns) or ACF/PACF analysis</li>
<li>When uncertain, try multiple periods and select one that minimizes residual variance</li>
</ul>
</details>
<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Design an ensemble anomaly detection system combining three statistical methods (Z-score, IQR, Mahalanobis distance). Implement majority voting or weighted voting, and show that performance improves over single methods.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Ensemble Anomaly Detection Design</strong>:</p>
<ol>
<li><strong>Combine base methods</strong>: Integrate methods with different principles (Z-score, IQR, Mahalanobis)</li>
<li><strong>Voting strategy</strong>: Majority voting or soft voting (average of scores)</li>
<li><strong>Weighting</strong>: Weights according to each method's reliability</li>
</ol>
<p><strong>Implementation</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.spatial.distance import mahalanobis
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

class EnsembleAnomalyDetector:
    """Ensemble anomaly detection"""

    def __init__(self, voting='hard', weights=None):
        """
        Parameters:
        -----------
        voting : str
            'hard' (majority voting) or 'soft' (score averaging)
        weights : list or None
            Weights for each method [zscore, iqr, mahalanobis]
        """
        self.voting = voting
        self.weights = weights if weights is not None else [1/3, 1/3, 1/3]

    def fit(self, X):
        """Learn statistics"""
        # For Z-score
        self.mean_ = np.mean(X, axis=0)
        self.std_ = np.std(X, axis=0)

        # For IQR
        self.q1_ = np.percentile(X, 25, axis=0)
        self.q3_ = np.percentile(X, 75, axis=0)
        self.iqr_ = self.q3_ - self.q1_

        # For Mahalanobis
        self.cov_ = np.cov(X.T)
        self.cov_inv_ = np.linalg.inv(self.cov_)

        return self

    def predict(self, X, threshold_zscore=3, threshold_mahal=3):
        """Ensemble prediction"""
        n_samples = len(X)

        # 1. Z-score
        z_scores = np.abs((X - self.mean_) / self.std_)
        z_anomalies = np.any(z_scores &gt; threshold_zscore, axis=1).astype(int)
        z_scores_norm = np.max(z_scores, axis=1) / 5  # Normalize

        # 2. IQR
        lower = self.q1_ - 1.5 * self.iqr_
        upper = self.q3_ + 1.5 * self.iqr_
        iqr_anomalies = np.any((X &lt; lower) | (X &gt; upper), axis=1).astype(int)
        iqr_scores = np.max(np.abs(X - self.mean_) / (self.iqr_ + 1e-10), axis=1)
        iqr_scores_norm = np.clip(iqr_scores / 5, 0, 1)  # Normalize

        # 3. Mahalanobis distance
        mahal_scores = np.array([mahalanobis(x, self.mean_, self.cov_inv_) for x in X])
        mahal_anomalies = (mahal_scores &gt; threshold_mahal).astype(int)
        mahal_scores_norm = mahal_scores / 10  # Normalize

        # Ensemble voting
        if self.voting == 'hard':
            # Majority voting (2/3 or more classify as anomaly)
            votes = z_anomalies + iqr_anomalies + mahal_anomalies
            predictions = (votes &gt;= 2).astype(int)
            scores = votes / 3

        elif self.voting == 'soft':
            # Weighted average of scores
            scores = (self.weights[0] * z_scores_norm +
                     self.weights[1] * iqr_scores_norm +
                     self.weights[2] * mahal_scores_norm)
            predictions = (scores &gt; 0.5).astype(int)

        return predictions, scores, {
            'zscore': z_anomalies,
            'iqr': iqr_anomalies,
            'mahalanobis': mahal_anomalies
        }

# Evaluation experiment
np.random.seed(42)

# Generate data
n_normal = 300
n_anomaly = 30
X_normal = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], size=n_normal)
X_anomaly = np.random.uniform(-5, 5, size=(n_anomaly, 2))
X_anomaly += np.array([[3, 3], [-3, -3], [3, -3], [-3, 3]]).mean(axis=0)  # Bias

X = np.vstack([X_normal, X_anomaly])
y_true = np.array([0]*n_normal + [1]*n_anomaly)

# Train model (on normal data only)
ensemble = EnsembleAnomalyDetector(voting='soft', weights=[0.3, 0.3, 0.4])
ensemble.fit(X_normal)

# Predict
y_pred_ensemble, scores_ensemble, individual_preds = ensemble.predict(X)

# Evaluate each method individually
results = {}
for method_name, preds in individual_preds.items():
    results[method_name] = {
        'precision': precision_score(y_true, preds),
        'recall': recall_score(y_true, preds),
        'f1': f1_score(y_true, preds)
    }

# Evaluate ensemble
results['ensemble'] = {
    'precision': precision_score(y_true, y_pred_ensemble),
    'recall': recall_score(y_true, y_pred_ensemble),
    'f1': f1_score(y_true, y_pred_ensemble)
}

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

methods = ['zscore', 'iqr', 'mahalanobis', 'ensemble']
titles = ['Z-score', 'IQR', 'Mahalanobis Distance', 'Ensemble']
predictions_list = [individual_preds['zscore'], individual_preds['iqr'],
                    individual_preds['mahalanobis'], y_pred_ensemble]

for i, (method, title, preds) in enumerate(zip(methods, titles, predictions_list)):
    ax = axes[i // 2, i % 2]
    ax.scatter(X[preds==0, 0], X[preds==0, 1],
               alpha=0.6, s=50, c='blue', label='Normal', edgecolors='black')
    ax.scatter(X[preds==1, 0], X[preds==1, 1],
               c='red', s=100, marker='X', label='Anomaly', zorder=5, edgecolors='black', linewidths=2)

    # Display performance
    r = results[method]
    ax.set_xlabel('Feature 1', fontsize=12)
    ax.set_ylabel('Feature 2', fontsize=12)
    ax.set_title(f'{title}\nF1={r["f1"]:.3f}, Precision={r["precision"]:.3f}, Recall={r["recall"]:.3f}',
                 fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Results summary
print("=== Performance Comparison ===")
for method, metrics in results.items():
    print(f"{method.upper():15s} - Precision: {metrics['precision']:.3f}, "
          f"Recall: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}")

print("\n=== Conclusion ===")
print("Ensemble combines strengths of each method for more stable performance than single methods")
</code></pre>
<p><strong>Conclusion</strong>:</p>
<ul>
<li>Ensemble reduces bias of individual methods</li>
<li>Soft voting provides flexibility and can be optimized through weight adjustment</li>
<li>In practice, weights should be set based on domain knowledge</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Rousseeuw, P. J., &amp; Hubert, M. (2011). <em>Robust statistics for outlier detection</em>. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 1(1), 73-79.</li>
<li>Barnett, V., &amp; Lewis, T. (1994). <em>Outliers in statistical data</em> (3rd ed.). John Wiley &amp; Sons.</li>
<li>Grubbs, F. E. (1969). <em>Procedures for detecting outlying observations in samples</em>. Technometrics, 11(1), 1-21.</li>
<li>Rosner, B. (1983). <em>Percentage points for a generalized ESD many-outlier procedure</em>. Technometrics, 25(2), 165-172.</li>
<li>Cleveland, R. B., Cleveland, W. S., McRae, J. E., &amp; Terpenning, I. (1990). <em>STL: A seasonal-trend decomposition procedure based on loess</em>. Journal of Official Statistics, 6(1), 3-73.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter1-anomaly-detection-basics.html">‚Üê Previous Chapter: Anomaly Detection Basics</a>
<a class="nav-button" href="chapter3-ml-anomaly-detection.html">Next Chapter: Anomaly Detection Using Machine Learning ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Created by</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
