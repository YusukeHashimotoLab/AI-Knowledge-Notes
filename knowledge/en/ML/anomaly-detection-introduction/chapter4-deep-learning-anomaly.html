<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 - Deep Learning for Anomaly Detection - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 4: Deep Learning for Anomaly Detection</h1>
            <p class="subtitle">Autoencoder, VAE, GAN, Time Series Anomaly Detection</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 80-90 minutes</span>
                <span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
                <span class="meta-item">üíª Code Examples: 9</span>
                <span class="meta-item">üìù Exercises: 6</span>
            </div>
        </div>
    </header>

    <main class="container">

<div class="learning-objectives">
<h2>Learning Objectives</h2>
<ul>
<li>Understand the principles and implementation of Autoencoder-based anomaly detection</li>
<li>Learn the probabilistic approach of Variational Autoencoder (VAE)</li>
<li>Understand the mechanisms of GAN-based anomaly detection (AnoGAN)</li>
<li>Implement time series anomaly detection with LSTM Autoencoder</li>
<li>Build end-to-end anomaly detection pipelines</li>
</ul>
</div>

<h2>4.1 Autoencoder-based Anomaly Detection</h2>

<h3>4.1.1 Autoencoder Fundamentals</h3>

<p>An <strong>Autoencoder</strong> is an unsupervised learning model that compresses input data and reconstructs it. By training on normal data, it can detect anomalies based on the principle that reconstruction errors are larger for anomalous data.</p>

<p><strong>Architecture:</strong></p>

<pre><code>Input (x)
    ‚Üì
Encoder: x ‚Üí z (latent representation)
    ‚Üì
Latent Space (z)
    ‚Üì
Decoder: z ‚Üí xÃÇ (reconstruction)
    ‚Üì
Reconstruction Error: ||x - xÃÇ||¬≤
</code></pre>

<p><strong>Principles of Anomaly Detection:</strong></p>

<ul>
<li>Train on normal data: Learn normal patterns</li>
<li>Small reconstruction error: Normal data</li>
<li>Large reconstruction error: Anomalous data (patterns not learned)</li>
</ul>

<p><strong>Mathematical Expression:</strong></p>

<p>$$
\text{Anomaly Score} = \|x - \text{Decoder}(\text{Encoder}(x))\|^2
$$</p>

<h3>4.1.2 Reconstruction Error and Threshold Selection</h3>

<p>For anomaly determination, a threshold is set for the reconstruction error.</p>

<p><strong>Threshold Setting Methods:</strong></p>

<table>
<tr>
<th>Method</th>
<th>Description</th>
<th>Application Scenario</th>
</tr>
<tr>
<td>Percentile Method</td>
<td>95th percentile of training data reconstruction error</td>
<td>Learning with normal data only</td>
</tr>
<tr>
<td>Statistical Method</td>
<td>Mean + 3œÉ</td>
<td>Assumes normal distribution</td>
</tr>
<tr>
<td>ROC Curve</td>
<td>Maximize AUC on validation data</td>
<td>Small amount of anomaly labels available</td>
</tr>
<tr>
<td>Business Requirements</td>
<td>Specify False Positive rate</td>
<td>Emphasis on production operations</td>
</tr>
</table>

<h3>4.1.3 PyTorch Implementation (Complete Version)</h3>

<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve

# Autoencoder model definition
class Autoencoder(nn.Module):
    """Simple Autoencoder"""
    def __init__(self, input_dim=784, hidden_dims=[128, 64, 32]):
        super(Autoencoder, self).__init__()

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # Decoder (reverse order of Encoder)
        decoder_layers = []
        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())  # Normalize output to [0,1]

        self.decoder = nn.Sequential(*decoder_layers)

    def forward(self, x):
        """Forward pass"""
        z = self.encoder(x)  # Encode
        x_reconstructed = self.decoder(z)  # Decode
        return x_reconstructed

    def encode(self, x):
        """Get latent representation"""
        return self.encoder(x)


def train_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """Train Autoencoder"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()  # Reconstruction error (Mean Squared Error)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model, train_losses


def compute_reconstruction_errors(model, data_loader):
    """Compute reconstruction errors"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # Reconstruction error per sample (MSE)
            batch_errors = torch.mean((batch_x - reconstructed) ** 2, dim=1)
            errors.extend(batch_errors.cpu().numpy())

    return np.array(errors)


def detect_anomalies(model, test_loader, threshold):
    """Perform anomaly detection"""
    errors = compute_reconstruction_errors(model, test_loader)
    predictions = (errors > threshold).astype(int)
    return predictions, errors


# Usage example
if __name__ == "__main__":
    # Generate sample data (normal data from normal distribution)
    np.random.seed(42)
    torch.manual_seed(42)

    # Normal data (28x28 = 784 dimensions)
    n_normal = 1000
    normal_data = np.random.randn(n_normal, 784) * 0.5 + 0.5
    normal_data = np.clip(normal_data, 0, 1)

    # Anomalous data (different distribution from normal)
    n_anomaly = 50
    anomaly_data = np.random.uniform(0, 1, (n_anomaly, 784))

    # PyTorch Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    test_data = np.vstack([normal_data[:100], anomaly_data])
    test_labels = np.array([0] * 100 + [1] * n_anomaly)  # 0: Normal, 1: Anomaly

    test_dataset = TensorDataset(torch.FloatTensor(test_data))
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Model training
    print("=== Autoencoder Training Started ===")
    model = Autoencoder(input_dim=784, hidden_dims=[256, 128, 64])
    trained_model, losses = train_autoencoder(model, train_loader, n_epochs=50, lr=0.001)

    # Threshold setting (95th percentile of training data)
    train_errors = compute_reconstruction_errors(trained_model, train_loader)
    threshold = np.percentile(train_errors, 95)
    print(f"\nThreshold (95th percentile): {threshold:.6f}")

    # Anomaly detection on test data
    predictions, test_errors = detect_anomalies(trained_model, test_loader, threshold)

    # Evaluation
    from sklearn.metrics import classification_report, roc_auc_score

    print("\n=== Anomaly Detection Results ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))

    auc_score = roc_auc_score(test_labels, test_errors)
    print(f"ROC-AUC: {auc_score:.3f}")

    # Visualization
    plt.figure(figsize=(12, 4))

    # Training curve
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Reconstruction Loss')
    plt.title('Training Loss Curve')
    plt.grid(True)

    # Reconstruction error distribution
    plt.subplot(1, 2, 2)
    plt.hist(test_errors[test_labels == 0], bins=30, alpha=0.6, label='Normal')
    plt.hist(test_errors[test_labels == 1], bins=30, alpha=0.6, label='Anomaly')
    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.3f}')
    plt.xlabel('Reconstruction Error')
    plt.ylabel('Frequency')
    plt.title('Reconstruction Error Distribution')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('autoencoder_anomaly_detection.png', dpi=150)
    print("\nGraph saved: autoencoder_anomaly_detection.png")
</code></pre>

<h3>4.1.4 Network Architecture Selection</h3>

<p><strong>Architecture Design Considerations:</strong></p>

<table>
<tr>
<th>Component</th>
<th>Recommended Value</th>
<th>Reason</th>
</tr>
<tr>
<td>Latent Dimension</td>
<td>10-30% of input dimension</td>
<td>Excessive compression causes information loss, too large leads to identity mapping</td>
</tr>
<tr>
<td>Number of Hidden Layers</td>
<td>2-4 layers</td>
<td>Too deep is difficult to train, too shallow lacks expressiveness</td>
</tr>
<tr>
<td>Activation Function</td>
<td>ReLU (hidden layers), Sigmoid (output)</td>
<td>Prevents vanishing gradients, constrains output range</td>
</tr>
<tr>
<td>Dropout</td>
<td>0.2-0.3</td>
<td>Prevents overfitting (use cautiously for anomaly detection)</td>
</tr>
</table>

<hr>

<h2>4.2 Variational Autoencoder (VAE)</h2>

<h3>4.2.1 Motivation for VAE</h3>

<p><strong>Limitations of Standard Autoencoders:</strong></p>
<ul>
<li>Latent space is discontinuous and lacks meaningful structure</li>
<li>Prone to overfitting to training data</li>
<li>Limited generative capability</li>
</ul>

<p><strong>Features of VAE:</strong></p>
<ul>
<li>Models latent variables as probability distributions</li>
<li>Learns smooth latent space through regularization</li>
<li>Functions as a generative model</li>
</ul>

<h3>4.2.2 Mathematical Foundation of VAE</h3>

<p><strong>Probabilistic Encoder:</strong></p>

<p>$$
q_\phi(z|x) = \mathcal{N}(z; \mu(x), \sigma^2(x))
$$</p>

<p>The encoder outputs mean $\mu(x)$ and variance $\sigma^2(x)$.</p>

<p><strong>Decoder:</strong></p>

<p>$$
p_\theta(x|z) = \mathcal{N}(x; \mu_{\text{dec}}(z), \sigma^2_{\text{dec}})
$$</p>

<p><strong>Loss Function (ELBO):</strong></p>

<p>$$
\mathcal{L} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL Divergence}}
$$</p>

<ul>
<li>First term: Reconstruction loss (same as Autoencoder)</li>
<li>Second term: KL divergence (regularization term), assuming $p(z) = \mathcal{N}(0, I)$</li>
</ul>

<p><strong>Closed-form KL Divergence:</strong></p>

<p>$$
D_{KL} = -\frac{1}{2} \sum_{j=1}^{J} (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
$$</p>

<h3>4.2.3 Anomaly Detection with VAE</h3>

<p>In VAE, the anomaly score is calculated by combining reconstruction error and KL divergence.</p>

<p>$$
\text{Anomaly Score} = \text{Reconstruction Error} + \beta \cdot D_{KL}
$$</p>

<h3>4.2.4 PyTorch Implementation</h3>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    """Variational Autoencoder"""
    def __init__(self, input_dim=784, latent_dim=32, hidden_dims=[256, 128]):
        super(VAE, self).__init__()

        self.latent_dim = latent_dim

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # Latent distribution parameters (mean and variance)
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)

        # Decoder
        decoder_layers = []
        decoder_layers.append(nn.Linear(latent_dim, hidden_dims[-1]))
        decoder_layers.append(nn.ReLU())

        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))
            decoder_layers.append(nn.ReLU())

        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
        decoder_layers.append(nn.Sigmoid())

        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        """Encode: output mean and log variance"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """Reparameterization trick"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # Sample from N(0, 1)
        z = mu + eps * std
        return z

    def decode(self, z):
        """Decode"""
        return self.decoder(z)

    def forward(self, x):
        """Forward pass"""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar


def vae_loss(x, x_reconstructed, mu, logvar, beta=1.0):
    """VAE loss function

    Args:
        beta: Weight for KL divergence (Œ≤-VAE)
    """
    # Reconstruction loss (binary cross entropy)
    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')

    # KL Divergence
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Total loss
    total_loss = recon_loss + beta * kl_div

    return total_loss, recon_loss, kl_div


def train_vae(model, train_loader, n_epochs=50, lr=0.001, beta=1.0):
    """Train VAE"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        train_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            x_recon, mu, logvar = model(batch_x)
            loss, recon, kl = vae_loss(batch_x, x_recon, mu, logvar, beta)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        avg_loss = train_loss / len(train_loader.dataset)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}")

    return model


def vae_anomaly_score(model, x, beta=1.0):
    """Compute anomaly score with VAE"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        x = x.to(device)
        x_recon, mu, logvar = model(x)

        # Reconstruction error (per sample)
        recon_error = F.binary_cross_entropy(x_recon, x, reduction='none').sum(dim=1)

        # KL divergence (per sample)
        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)

        # Anomaly score
        anomaly_scores = recon_error + beta * kl_div

    return anomaly_scores.cpu().numpy()


# Usage example
if __name__ == "__main__":
    # Data preparation (same as before)
    train_dataset = TensorDataset(torch.FloatTensor(normal_data))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    # VAE model
    print("=== VAE Training Started ===")
    vae_model = VAE(input_dim=784, latent_dim=32, hidden_dims=[256, 128])
    trained_vae = train_vae(vae_model, train_loader, n_epochs=50, lr=0.001, beta=1.0)

    # Compute anomaly scores
    test_tensor = torch.FloatTensor(test_data)
    anomaly_scores = vae_anomaly_score(trained_vae, test_tensor, beta=1.0)

    # Threshold setting and evaluation
    threshold = np.percentile(anomaly_scores[:100], 95)  # 95th percentile of normal data
    predictions = (anomaly_scores > threshold).astype(int)

    print("\n=== VAE Anomaly Detection Results ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, anomaly_scores):.3f}")
</code></pre>

<h3>4.2.5 Latent Space Analysis</h3>

<p>The latent space of VAE has a smooth distribution of normal data. Anomalous data is expected to be located in outlying regions of the latent space.</p>

<pre><code>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def visualize_latent_space(model, data, labels):
    """Visualize latent space (2D projection)"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        data_tensor = torch.FloatTensor(data).to(device)
        mu, _ = model.encode(data_tensor)
        latent_codes = mu.cpu().numpy()

    # Compress to 2D (if latent dimension is greater than 2)
    if latent_codes.shape[1] > 2:
        pca = PCA(n_components=2)
        latent_2d = pca.fit_transform(latent_codes)
    else:
        latent_2d = latent_codes

    # Plot
    plt.figure(figsize=(8, 6))
    plt.scatter(latent_2d[labels == 0, 0], latent_2d[labels == 0, 1],
                c='blue', alpha=0.5, label='Normal')
    plt.scatter(latent_2d[labels == 1, 0], latent_2d[labels == 1, 1],
                c='red', alpha=0.5, label='Anomaly')
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.title('VAE Latent Space Visualization')
    plt.legend()
    plt.grid(True)
    plt.savefig('vae_latent_space.png', dpi=150)
    print("Latent space visualization saved: vae_latent_space.png")

# Usage example
visualize_latent_space(trained_vae, test_data, test_labels)
</code></pre>

<hr>

<h2>4.3 GAN-based Anomaly Detection</h2>

<h3>4.3.1 AnoGAN (Anomaly Detection with GAN)</h3>

<p><strong>AnoGAN</strong> learns a generative model of normal data using GANs and detects anomalies based on how much the test data deviates from this generative distribution.</p>

<p><strong>Training Phase:</strong></p>
<ul>
<li>Train GAN on normal data</li>
<li>Generator G learns the distribution of normal data</li>
</ul>

<p><strong>Testing Phase:</strong></p>
<ol>
<li>For test sample $x$, optimize latent variable $z$: $G(z) \approx x$</li>
<li>Calculate anomaly score: Residual Loss + Discrimination Loss</li>
</ol>

<h3>4.3.2 Definition of Anomaly Score</h3>

<p>$$
A(x) = (1 - \lambda) \cdot L_R(x) + \lambda \cdot L_D(x)
$$</p>

<ul>
<li>$L_R(x) = \|x - G(z^*)\|_1$: Residual Loss (reconstruction error)</li>
<li>$L_D(x) = \|f(x) - f(G(z^*))\|_1$: Discrimination Loss (distance in feature space)</li>
<li>$f(\cdot)$: Features from intermediate layer of Discriminator</li>
</ul>

<h3>4.3.3 Latent Variable Optimization</h3>

<p>For test sample $x$, search for $z$ such that $G(z) \approx x$ using gradient descent:</p>

<p>$$
z^* = \arg\min_z \|x - G(z)\|_1 + \lambda \|f(x) - f(G(z))\|_1
$$</p>

<h3>4.3.4 Implementation Overview</h3>

<pre><code>import torch
import torch.nn as nn

class Generator(nn.Module):
    """GAN Generator"""
    def __init__(self, latent_dim=100, output_dim=784):
        super(Generator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, output_dim),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.model(z)


class Discriminator(nn.Module):
    """GAN Discriminator (also extracts intermediate layer features)"""
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()

        self.features = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2)
        )

        self.classifier = nn.Sequential(
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x, return_features=False):
        feat = self.features(x)
        output = self.classifier(feat)

        if return_features:
            return output, feat
        return output


def find_latent_code(generator, discriminator, x, n_iterations=500, lr=0.01, lambda_weight=0.1):
    """Search for optimal latent variable z for test sample x"""
    device = next(generator.parameters()).device

    # Initialize
    z = torch.randn(x.size(0), generator.model[0].in_features, device=device, requires_grad=True)
    optimizer = torch.optim.Adam([z], lr=lr)

    for i in range(n_iterations):
        optimizer.zero_grad()

        # Generate
        G_z = generator(z)

        # Residual Loss
        residual_loss = torch.mean(torch.abs(x - G_z))

        # Discrimination Loss (distance in feature space)
        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z, return_features=True)
        discrimination_loss = torch.mean(torch.abs(feat_real - feat_fake))

        # Total loss
        loss = (1 - lambda_weight) * residual_loss + lambda_weight * discrimination_loss

        loss.backward()
        optimizer.step()

    # Anomaly score
    with torch.no_grad():
        G_z_final = generator(z)
        residual = torch.mean(torch.abs(x - G_z_final), dim=1)

        _, feat_real = discriminator(x, return_features=True)
        _, feat_fake = discriminator(G_z_final, return_features=True)
        discrimination = torch.mean(torch.abs(feat_real - feat_fake), dim=1)

        anomaly_scores = (1 - lambda_weight) * residual + lambda_weight * discrimination

    return anomaly_scores.cpu().numpy()


# Note: GAN training code is omitted (perform standard GAN training)
# In actual use, first train the GAN on normal data, then use the above function for anomaly detection
</code></pre>

<blockquote>
<p><strong>Note</strong>: AnoGAN is time-consuming due to latent variable optimization, making it unsuitable for real-time anomaly detection. To address this issue, improved methods such as Fast-AnoGAN and EGBAd have been proposed.</p>
</blockquote>

<hr>

<h2>4.4 Time Series Anomaly Detection</h2>

<h3>4.4.1 Characteristics of Time Series Data</h3>

<p>Time series anomaly detection requires consideration of the following characteristics:</p>

<ul>
<li><strong>Temporal Dependencies</strong>: Past values influence the future</li>
<li><strong>Seasonality and Periodicity</strong>: Daily, weekly, and yearly patterns</li>
<li><strong>Trends</strong>: Long-term upward or downward tendencies</li>
<li><strong>Multivariate Nature</strong>: Multiple sensor values are interrelated</li>
</ul>

<h3>4.4.2 LSTM Autoencoder</h3>

<p><strong>LSTM Autoencoder</strong> learns temporal patterns in time series using LSTM and detects anomalies through reconstruction error.</p>

<p><strong>Architecture:</strong></p>

<pre><code>Input: (batch, seq_len, features)
    ‚Üì
LSTM Encoder: Compress time series to fixed-length vector
    ‚Üì
Latent Vector: (batch, latent_dim)
    ‚Üì
LSTM Decoder: Reconstruct time series from latent vector
    ‚Üì
Output: (batch, seq_len, features)
</code></pre>

<h3>4.4.3 PyTorch Implementation</h3>

<pre><code>import torch
import torch.nn as nn

class LSTMAutoencoder(nn.Module):
    """LSTM-based Autoencoder for time series"""
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, latent_dim=32):
        super(LSTMAutoencoder, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.latent_dim = latent_dim

        # Encoder LSTM
        self.encoder_lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # Compress to latent representation
        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)

        # FC for Decoder (latent representation to LSTM initial state)
        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)

        # Decoder LSTM
        self.decoder_lstm = nn.LSTM(
            input_size=latent_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # Output layer
        self.output_fc = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """Encode: time series ‚Üí latent vector"""
        # x: (batch, seq_len, input_dim)
        lstm_out, (hidden, cell) = self.encoder_lstm(x)

        # Use last hidden state
        last_hidden = hidden[-1]  # (batch, hidden_dim)

        # Compress to latent vector
        z = self.encoder_fc(last_hidden)  # (batch, latent_dim)

        return z

    def decode(self, z, seq_len):
        """Decode: latent vector ‚Üí time series"""
        batch_size = z.size(0)

        # Decoder LSTM initial state
        hidden = self.decoder_fc(z).unsqueeze(0)  # (1, batch, hidden_dim)
        hidden = hidden.repeat(self.num_layers, 1, 1)  # (num_layers, batch, hidden_dim)
        cell = torch.zeros_like(hidden)

        # Decoder input (repeat latent vector seq_len times)
        decoder_input = z.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, latent_dim)

        # LSTM Decoder
        lstm_out, _ = self.decoder_lstm(decoder_input, (hidden, cell))
        # lstm_out: (batch, seq_len, hidden_dim)

        # Output layer
        output = self.output_fc(lstm_out)  # (batch, seq_len, input_dim)

        return output

    def forward(self, x):
        """Forward pass"""
        seq_len = x.size(1)

        z = self.encode(x)
        x_reconstructed = self.decode(z, seq_len)

        return x_reconstructed


def train_lstm_autoencoder(model, train_loader, n_epochs=50, lr=0.001):
    """Train LSTM Autoencoder"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_x, in train_loader:
            batch_x = batch_x.to(device)

            # Forward pass
            reconstructed = model(batch_x)
            loss = criterion(reconstructed, batch_x)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.6f}")

    return model


def detect_ts_anomalies(model, data_loader, threshold):
    """Time series anomaly detection"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    all_errors = []

    with torch.no_grad():
        for batch_x, in data_loader:
            batch_x = batch_x.to(device)
            reconstructed = model(batch_x)

            # Reconstruction error for entire sequence (average)
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            all_errors.extend(errors.cpu().numpy())

    all_errors = np.array(all_errors)
    predictions = (all_errors > threshold).astype(int)

    return predictions, all_errors


# Usage example
if __name__ == "__main__":
    # Generate sample time series data (normal: sine wave, anomaly: noise)
    np.random.seed(42)
    torch.manual_seed(42)

    seq_len = 50
    input_dim = 5  # 5 sensors

    # Normal data (sine wave based)
    n_normal_sequences = 500
    t = np.linspace(0, 4*np.pi, seq_len)
    normal_sequences = []
    for _ in range(n_normal_sequences):
        seq = np.array([np.sin(t + np.random.randn() * 0.1) for _ in range(input_dim)]).T
        seq += np.random.randn(seq_len, input_dim) * 0.1
        normal_sequences.append(seq)

    normal_sequences = np.array(normal_sequences)  # (n_normal, seq_len, input_dim)

    # Anomalous data (random noise)
    n_anomaly_sequences = 50
    anomaly_sequences = np.random.randn(n_anomaly_sequences, seq_len, input_dim)

    # Dataset
    train_dataset = TensorDataset(torch.FloatTensor(normal_sequences))
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    test_sequences = np.vstack([normal_sequences[:50], anomaly_sequences])
    test_labels = np.array([0] * 50 + [1] * n_anomaly_sequences)

    test_dataset = TensorDataset(torch.FloatTensor(test_sequences))
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Model training
    print("=== LSTM Autoencoder Training Started ===")
    lstm_ae = LSTMAutoencoder(input_dim=input_dim, hidden_dim=64, num_layers=2, latent_dim=32)
    trained_lstm_ae = train_lstm_autoencoder(lstm_ae, train_loader, n_epochs=50, lr=0.001)

    # Threshold setting
    train_errors = []
    trained_lstm_ae.eval()
    with torch.no_grad():
        for batch_x, in train_loader:
            reconstructed = trained_lstm_ae(batch_x)
            errors = torch.mean((batch_x - reconstructed) ** 2, dim=(1, 2))
            train_errors.extend(errors.cpu().numpy())

    threshold = np.percentile(train_errors, 95)
    print(f"\nThreshold (95th percentile): {threshold:.6f}")

    # Anomaly detection
    predictions, test_errors = detect_ts_anomalies(trained_lstm_ae, test_loader, threshold)

    print("\n=== LSTM Autoencoder Anomaly Detection Results ===")
    print(classification_report(test_labels, predictions,
                                target_names=['Normal', 'Anomaly']))
    print(f"ROC-AUC: {roc_auc_score(test_labels, test_errors):.3f}")
</code></pre>

<h3>4.4.4 Multivariate Time Series Anomaly Detection</h3>

<p>When handling data from multiple sensors simultaneously, correlations between variables must also be considered.</p>

<p><strong>Methods:</strong></p>
<ul>
<li><strong>LSTM Autoencoder</strong>: Already supported in the above implementation</li>
<li><strong>Attention Mechanism</strong>: Interpret which variables contribute to anomalies</li>
<li><strong>Transformer</strong>: Learn long-term dependencies</li>
</ul>

<hr>

<h2>4.5 End-to-End Practice</h2>

<h3>4.5.1 Data Preparation</h3>

<p>In real-world anomaly detection, data is prepared through the following steps.</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

class AnomalyDetectionPipeline:
    """Anomaly detection pipeline"""
    def __init__(self, model_type='autoencoder'):
        self.model_type = model_type
        self.scaler = StandardScaler()
        self.model = None
        self.threshold = None

    def preprocess(self, data, fit_scaler=False):
        """Preprocessing: normalization, missing value handling, etc."""
        # Missing value imputation (mean)
        data = data.fillna(data.mean())

        # Standardization
        if fit_scaler:
            data_scaled = self.scaler.fit_transform(data)
        else:
            data_scaled = self.scaler.transform(data)

        return data_scaled

    def create_sequences(self, data, seq_len=50):
        """Split time series data into sequences"""
        sequences = []
        for i in range(len(data) - seq_len + 1):
            sequences.append(data[i:i+seq_len])

        return np.array(sequences)

    def train(self, normal_data, seq_len=50, n_epochs=50):
        """Model training"""
        # Preprocessing
        normal_scaled = self.preprocess(normal_data, fit_scaler=True)

        # Sequence creation
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(normal_scaled, seq_len)
            train_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            # For Autoencoder
            train_dataset = TensorDataset(torch.FloatTensor(normal_scaled))

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

        # Model selection and training
        if self.model_type == 'autoencoder':
            self.model = Autoencoder(input_dim=normal_scaled.shape[1])
            self.model, _ = train_autoencoder(self.model, train_loader, n_epochs)
        elif self.model_type == 'vae':
            self.model = VAE(input_dim=normal_scaled.shape[1])
            self.model = train_vae(self.model, train_loader, n_epochs)
        elif self.model_type == 'lstm_ae':
            self.model = LSTMAutoencoder(input_dim=normal_scaled.shape[1])
            self.model = train_lstm_autoencoder(self.model, train_loader, n_epochs)

        # Threshold setting (95th percentile of training data)
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(normal_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, train_loader)

        self.threshold = np.percentile(scores, 95)
        print(f"Threshold set: {self.threshold:.6f}")

    def predict(self, test_data, seq_len=50):
        """Anomaly prediction"""
        # Preprocessing
        test_scaled = self.preprocess(test_data, fit_scaler=False)

        # Sequence creation
        if self.model_type in ['lstm_ae', 'transformer']:
            sequences = self.create_sequences(test_scaled, seq_len)
            test_dataset = TensorDataset(torch.FloatTensor(sequences))
        else:
            test_dataset = TensorDataset(torch.FloatTensor(test_scaled))

        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        # Anomaly score calculation
        if self.model_type == 'vae':
            scores = vae_anomaly_score(self.model, torch.FloatTensor(test_scaled))
        else:
            scores = compute_reconstruction_errors(self.model, test_loader)

        # Anomaly determination
        predictions = (scores > self.threshold).astype(int)

        return predictions, scores


# Usage example
if __name__ == "__main__":
    # Dummy dataframe
    normal_df = pd.DataFrame(np.random.randn(1000, 10))
    test_df = pd.DataFrame(np.random.randn(100, 10))

    # Pipeline
    pipeline = AnomalyDetectionPipeline(model_type='autoencoder')
    pipeline.train(normal_df, n_epochs=30)

    predictions, scores = pipeline.predict(test_df)
    print(f"\nNumber of anomalies detected: {predictions.sum()} / {len(predictions)}")
</code></pre>

<h3>4.5.2 Model Selection</h3>

<p>Select an appropriate model based on data characteristics.</p>

<table>
<tr>
<th>Data Type</th>
<th>Recommended Model</th>
<th>Reason</th>
</tr>
<tr>
<td>Image Data</td>
<td>Convolutional AE, VAE</td>
<td>Preserves spatial structure</td>
</tr>
<tr>
<td>Time Series Data</td>
<td>LSTM AE, Transformer</td>
<td>Captures temporal dependencies</td>
</tr>
<tr>
<td>Tabular Data</td>
<td>Autoencoder, VAE</td>
<td>Simple and effective</td>
</tr>
<tr>
<td>High-dimensional Sparse</td>
<td>Sparse AE, VAE</td>
<td>Dimensionality reduction and regularization</td>
</tr>
</table>

<h3>4.5.3 Threshold Adjustment</h3>

<p>In production operations, thresholds are adjusted according to business requirements.</p>

<ul>
<li><strong>Emphasize False Positive Rate</strong>: Set threshold high (reduce false alarms)</li>
<li><strong>Emphasize Recall</strong>: Set threshold low (don't miss anomalies)</li>
<li><strong>Maximize F1</strong>: Select point that maximizes F1 score on validation data</li>
</ul>

<h3>4.5.4 Production Deployment</h3>

<p><strong>Real-time Anomaly Detection System Architecture:</strong></p>

<pre><code>Data Collection (Sensors, Logs)
    ‚Üì
Preprocessing Pipeline (Normalization, Sequencing)
    ‚Üì
Anomaly Detection Model (PyTorch ‚Üí ONNX ‚Üí TorchScript)
    ‚Üì
Threshold Determination
    ‚Üì
Alerts and Visualization (Grafana, Slack Notifications)
</code></pre>

<p><strong>Deployment Considerations:</strong></p>
<ul>
<li><strong>Model Optimization</strong>: Speed up inference with TorchScript, ONNX conversion</li>
<li><strong>Batch Processing</strong>: Use batching for efficiency when real-time is not required</li>
<li><strong>Regular Retraining</strong>: Adapt to changes in data distribution (Concept Drift)</li>
<li><strong>Automatic Threshold Adjustment</strong>: Adaptively adjust from operational data</li>
</ul>

<h3>4.5.5 Monitoring and Alerts</h3>

<pre><code>import logging
from datetime import datetime

class AnomalyMonitor:
    """Anomaly detection monitoring"""
    def __init__(self, alert_threshold=0.9):
        self.alert_threshold = alert_threshold
        self.logger = self._setup_logger()

    def _setup_logger(self):
        logger = logging.getLogger('AnomalyDetection')
        logger.setLevel(logging.INFO)

        # File handler
        fh = logging.FileHandler('anomaly_detection.log')
        fh.setLevel(logging.INFO)

        # Format
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)

        logger.addHandler(fh)
        return logger

    def log_anomaly(self, timestamp, anomaly_score, features):
        """Log anomaly"""
        self.logger.info(f"Anomaly detected - Time: {timestamp}, Score: {anomaly_score:.4f}")
        self.logger.info(f"Features: {features}")

    def send_alert(self, anomaly_score, message):
        """Send alert (example implementation)"""
        if anomaly_score > self.alert_threshold:
            # Send to Slack, Email, PagerDuty, etc.
            print(f"[ALERT] High anomaly detected: {message}")
            self.logger.warning(f"High severity alert: {message}")

    def monitor(self, pipeline, data_stream):
        """Real-time monitoring"""
        for timestamp, data in data_stream:
            predictions, scores = pipeline.predict(data)

            if predictions.any():
                self.log_anomaly(timestamp, scores.max(), data)
                self.send_alert(scores.max(), f"Anomaly at {timestamp}")


# Usage example (virtual data stream)
monitor = AnomalyMonitor(alert_threshold=0.9)

# Virtual data stream
def data_stream_generator():
    for i in range(10):
        timestamp = datetime.now()
        data = pd.DataFrame(np.random.randn(1, 10))
        yield timestamp, data

# Run monitoring
# monitor.monitor(pipeline, data_stream_generator())
</code></pre>

<hr>

<h2>Summary</h2>

<p>What we learned in this chapter:</p>

<ol>
<li><strong>Autoencoder-based Anomaly Detection:</strong>
<ul>
<li>Detecting anomalies with reconstruction error</li>
<li>Network architecture design</li>
<li>Threshold selection methods</li>
<li>Complete PyTorch implementation</li>
</ul>
</li>

<li><strong>Variational Autoencoder (VAE):</strong>
<ul>
<li>Anomaly detection with probabilistic latent representations</li>
<li>Reconstruction error + KL divergence</li>
<li>Latent space visualization and analysis</li>
<li>Adjustment with Œ≤-VAE</li>
</ul>
</li>

<li><strong>GAN-based Anomaly Detection:</strong>
<ul>
<li>Principles and implementation of AnoGAN</li>
<li>Latent variable optimization</li>
<li>Utilizing Discriminator features</li>
<li>Improved methods such as Fast-AnoGAN</li>
</ul>
</li>

<li><strong>Time Series Anomaly Detection:</strong>
<ul>
<li>LSTM Autoencoder implementation</li>
<li>Learning temporal patterns</li>
<li>Handling multivariate time series</li>
<li>Processing sequence data</li>
</ul>
</li>

<li><strong>End-to-End Practice:</strong>
<ul>
<li>Data preprocessing pipelines</li>
<li>Guidelines for model selection</li>
<li>Threshold adjustment methods</li>
<li>Production deployment design</li>
<li>Monitoring and alerts</li>
</ul>
</li>
</ol>

<hr>

<h2>Exercises</h2>

<p><strong>Question 1:</strong> Explain why the latent dimension in Autoencoder-based anomaly detection is set to 10% of the input dimension.</p>

<p><strong>Question 2:</strong> Explain the role of the KL divergence term in the VAE loss function from the perspective of the latent space.</p>

<p><strong>Question 3:</strong> List three main differences between AnoGAN and standard Autoencoder in anomaly detection.</p>

<p><strong>Question 4:</strong> Discuss how to determine the sequence length when performing time series anomaly detection with LSTM Autoencoder from three perspectives.</p>

<p><strong>Question 5:</strong> If there is a business requirement that the False Positive rate be 5% or less when setting the threshold for an anomaly detection model, explain specifically how the threshold should be determined.</p>

<p><strong>Question 6:</strong> List five technical challenges to consider when building a real-time anomaly detection system and propose solutions for each.</p>

<hr>

<h2>References</h2>

<ol>
<li>Goodfellow, I. et al. "Deep Learning." MIT Press (2016).</li>
<li>Kingma, D. P., & Welling, M. "Auto-Encoding Variational Bayes." <em>ICLR</em> (2014).</li>
<li>Schlegl, T. et al. "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery." <em>IPMI</em> (2017). [AnoGAN]</li>
<li>Malhotra, P. et al. "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection." <em>ICML Anomaly Detection Workshop</em> (2016).</li>
<li>Park, D. et al. "A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder." <em>IEEE Robotics and Automation Letters</em> (2018).</li>
<li>Su, Y. et al. "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network." <em>KDD</em> (2019).</li>
<li>Vaswani, A. et al. "Attention is All You Need." <em>NeurIPS</em> (2017). [Transformer]</li>
<li>Audibert, J. et al. "USAD: UnSupervised Anomaly Detection on Multivariate Time Series." <em>KDD</em> (2020).</li>
</ol>

<hr>

<div class="navigation">
    <a href="chapter3-ml-anomaly-detection.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Return to Series Index</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
