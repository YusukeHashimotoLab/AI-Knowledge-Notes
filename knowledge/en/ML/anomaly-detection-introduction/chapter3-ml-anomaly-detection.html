<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Machine Learning-Based Anomaly Detection - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/anomaly-detection-introduction/index.html">Anomaly Detection</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 3: Machine Learning-Based Anomaly Detection</h1>
<p class="subtitle">Anomaly Detection with Isolation Forest, LOF, and One-Class SVM</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 70-80 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 3: Machine Learning-Based Anomaly Detection</h1>
<div class="learning-objectives">
<h2>Learning Objectives</h2>
<ul>
<li>Understand the algorithmic principles of Isolation Forest</li>
<li>Detect local anomalies using LOF (Local Outlier Factor)</li>
<li>Master boundary learning of normal data with One-Class SVM</li>
<li>Apply DBSCAN and other methods to anomaly detection</li>
<li>Learn implementation methods for ensemble anomaly detection</li>
</ul>
</div>
<p><strong>Reading Time</strong>: 70-80 minutes</p>
<hr/>
<h2>3.1 Isolation Forest</h2>
<p>Isolation Forest is an anomaly detection algorithm that exploits the property that anomalous data is easier to "isolate" than normal data. Proposed by Liu et al. in 2008, it can be effectively applied to high-dimensional data.</p>
<h3>3.1.1 Algorithm Principles</h3>
<p><strong>Basic Idea:</strong></p>
<ul>
<li>Anomalous data is rare and has different feature values from normal data</li>
<li>When repeatedly splitting with randomly selected features, anomalous data becomes isolated earlier</li>
<li>The shorter the number of splits until isolation (path length), the higher the anomaly score</li>
</ul>
<p><strong>Algorithm Steps:</strong></p>
<pre><code>1. Randomly select a feature
2. Randomly choose a split point between the minimum and maximum values of that feature
3. Divide data into two groups
4. Recursively repeat steps 1-3 for each group
5. Record the path length until each data point is isolated
6. Build multiple trees (forest) and calculate anomaly score from average path length
</code></pre>
<h3>3.1.2 Path Length and Anomaly Score</h3>
<p><strong>Path Length:</strong></p>
<p>Let $h(x)$ be the number of splits until data point $x$ is isolated. Normal data is isolated at deeper positions (larger $h(x)$), while anomalous data is isolated at shallower positions (smaller $h(x)$).</p>
<p><strong>Anomaly Score Calculation:</strong></p>

$$
s(x, n) = 2^{-\frac{E[h(x)]}{c(n)}}
$$

<p>Where:</p>
<ul>
<li>$E[h(x)]$: Average path length across multiple trees</li>
<li>$c(n)$: Normalization constant for average path length at sample size $n$</li>
<li>$c(n) = 2H(n-1) - \frac{2(n-1)}{n}$ (where $H(i)$ is the harmonic number)</li>
</ul>
<p><strong>Score Interpretation:</strong></p>
<ul>
<li>$s \approx 1$: Anomalous (clear anomaly)</li>
<li>$s \approx 0.5$: Normal (average path length)</li>
<li>$s &lt; 0.5$: Normal (deeper than average position)</li>
</ul>
<h3>3.1.3 Hyperparameter Tuning</h3>
<p><strong>Main Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Recommended Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>n_estimators</code></td>
<td>Number of trees</td>
<td>100-200 (default: 100)</td>
</tr>
<tr>
<td><code>max_samples</code></td>
<td>Number of samples to draw for each tree</td>
<td>256 (default: auto)</td>
</tr>
<tr>
<td><code>contamination</code></td>
<td>Proportion of anomalies</td>
<td>0.1 (depends on data)</td>
</tr>
<tr>
<td><code>max_features</code></td>
<td>Number of features to consider for each split</td>
<td>1.0 (all features)</td>
</tr>
</tbody>
</table>
<p><strong>Parameter Selection Guidelines:</strong></p>
<ul>
<li><code>n_estimators</code>: More provides stability but increases computational cost (100-200 is sufficient)</li>
<li><code>max_samples</code>: 256 is recommended (paper default), reduce for large-scale data to speed up</li>
<li><code>contamination</code>: Set to known anomaly rate if available, otherwise use 0.1</li>
</ul>
<h3>3.1.4 scikit-learn Implementation</h3>
<p><strong>Basic Implementation:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# Generate sample data (normal data + anomalous data)
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5, random_state=42)
X_anomaly = np.random.uniform(low=-4, high=4, size=(20, 2))  # Anomalous data
X = np.vstack([X_normal, X_anomaly])

# Isolation Forest model
iso_forest = IsolationForest(
    n_estimators=100,
    max_samples=256,
    contamination=0.1,  # Assume 10% is anomalous
    random_state=42
)

# Train and predict
y_pred = iso_forest.fit_predict(X)  # -1: Anomaly, 1: Normal
scores = iso_forest.score_samples(X)  # Anomaly score (lower is more anomalous)

# Visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Isolation Forest: Anomaly Detection Results')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('Isolation Forest: Anomaly Scores')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Anomaly Score')

plt.tight_layout()
plt.show()

print(f"Number of detected anomalies: {np.sum(y_pred == -1)}")
print(f"Anomaly score range: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>
<p><strong>Sample Output:</strong></p>
<pre><code>Number of detected anomalies: 32
Anomaly score range: [-0.234, 0.178]
</code></pre>
<p><strong>Application to Real Data (Credit Card Fraud Detection):</strong></p>
<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Load data (hypothetical example)
# Actual data can be obtained from Kaggle Credit Card Fraud Detection
# URL: https://www.kaggle.com/mlg-ulb/creditcardfraud

# Generate sample data (substitute for real data)
np.random.seed(42)
n_normal = 1000
n_fraud = 50

# Normal transactions (small amount, many transactions, geographically concentrated)
normal_features = np.random.randn(n_normal, 5) * [10, 5, 2, 1, 0.5]
normal_labels = np.zeros(n_normal)

# Fraudulent transactions (large amount, few transactions, geographically dispersed)
fraud_features = np.random.randn(n_fraud, 5) * [50, 1, 10, 5, 3] + [100, 0, 50, 20, 10]
fraud_labels = np.ones(n_fraud)

X = np.vstack([normal_features, fraud_features])
y = np.hstack([normal_labels, fraud_labels])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Isolation Forest (train on normal data only)
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.05,  # Assume 5% is fraudulent
    random_state=42
)

# Train on training data
iso_forest.fit(X_train)

# Predict on test data
y_pred = iso_forest.predict(X_test)
y_pred = np.where(y_pred == -1, 1, 0)  # Convert -1 to 1 (fraud)

# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))
</code></pre>
<p><strong>Sample Output:</strong></p>
<pre><code>Confusion Matrix:
[[285  15]
 [  3  12]]

Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.95      0.97       300
       Fraud       0.44      0.80      0.57        15

    accuracy                           0.94       315
   macro avg       0.72      0.88      0.77       315
weighted avg       0.96      0.94      0.95       315
</code></pre>
<hr/>
<h2>3.2 LOF (Local Outlier Factor)</h2>
<p>LOF is a method for detecting anomalies based on the local density of each data point. Proposed by Breunig et al. in 2000.</p>
<h3>3.2.1 Density-Based Anomaly Detection</h3>
<p><strong>Basic Principle:</strong></p>
<ul>
<li>Normal data exists in high-density regions</li>
<li>Anomalous data exists in low-density regions</li>
<li>Calculate anomaly score by comparing density of each point with density of neighboring points</li>
</ul>
<p><strong>Why "Local":</strong></p>
<ul>
<li>Can detect anomalies that cannot be detected with global density</li>
<li>Effective when multiple clusters with different densities exist</li>
<li>Calculates relative anomaly score considering each point's neighborhood</li>
</ul>
<h3>3.2.2 Local Reachability Density</h3>
<p><strong>k-distance:</strong></p>
<p>Let $d_k(p)$ be the distance from point $p$ to the k-th nearest point.</p>
<p><strong>Reachability Distance:</strong></p>

$$
\text{reach-dist}_k(p, o) = \max\{d_k(o), d(p, o)\}
$$

<ul>
<li>$d(p, o)$: Actual distance between points $p$ and $o$</li>
<li>When neighbor $o$ is in a dense region, the reachability distance has a lower bound of $d_k(o)$</li>
</ul>
<p><strong>Local Reachability Density (LRD):</strong></p>

$$
\text{LRD}_k(p) = \frac{1}{\frac{\sum_{o \in N_k(p)} \text{reach-dist}_k(p, o)}{|N_k(p)|}}
$$

<ul>
<li>$N_k(p)$: Set of k-nearest neighbors of point $p$</li>
<li>Inverse of average reachability distance = density</li>
</ul>
<h3>3.2.3 LOF Score Calculation</h3>
<p><strong>LOF (Local Outlier Factor):</strong></p>

$$
\text{LOF}_k(p) = \frac{\sum_{o \in N_k(p)} \frac{\text{LRD}_k(o)}{\text{LRD}_k(p)}}{|N_k(p)|}
$$

<p><strong>Score Interpretation:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$: Normal (similar density to neighbors)</li>
<li>$\text{LOF} \gg 1$: Anomalous (lower density than neighbors)</li>
<li>$\text{LOF} &lt; 1$: Normal (higher density than neighbors)</li>
</ul>
<p>Generally, $\text{LOF} &gt; 1.5$ is considered anomalous.</p>
<h3>3.2.4 Complete Implementation Example</h3>
<p><strong>Basic Implementation:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_moons

# Generate sample data (moon-shaped data + outliers)
np.random.seed(42)
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)
X_outliers = np.random.uniform(low=-1, high=2, size=(20, 2))
X = np.vstack([X, X_outliers])

# LOF model
lof = LocalOutlierFactor(
    n_neighbors=20,  # Number of neighbors
    contamination=0.1,  # Anomaly rate
    novelty=False  # Use True for new data prediction
)

# Prediction
y_pred = lof.fit_predict(X)  # -1: Anomaly, 1: Normal
scores = lof.negative_outlier_factor_  # Negative anomaly score (lower is more anomalous)

# Visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('LOF: Anomaly Detection Results')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='viridis', edgecolors='k')
plt.title('LOF: Anomaly Scores')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Negative Outlier Factor')

plt.tight_layout()
plt.show()

print(f"Number of detected anomalies: {np.sum(y_pred == -1)}")
print(f"Anomaly score range: [{scores.min():.3f}, {scores.max():.3f}]")
</code></pre>
<p><strong>Impact of n_neighbors Parameter:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor

# Data generation
np.random.seed(42)
X_normal = np.random.randn(200, 2) * 0.5
X_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# Compare with different n_neighbors
n_neighbors_list = [5, 20, 50]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, n_neighbors in enumerate(n_neighbors_list):
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1)
    y_pred = lof.fit_predict(X)

    axes[idx].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
    axes[idx].set_title(f'LOF (n_neighbors={n_neighbors})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

    anomaly_count = np.sum(y_pred == -1)
    axes[idx].text(0.05, 0.95, f'Anomalies: {anomaly_count}',
                   transform=axes[idx].transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Anomaly Detection for New Data (novelty=True):</strong></p>
<pre><code class="language-python">from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import train_test_split

# Data preparation
np.random.seed(42)
X_train = np.random.randn(500, 2) * 0.5  # Normal data only
X_test_normal = np.random.randn(100, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# LOF (novelty=True: new data prediction mode)
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)
lof.fit(X_train)  # Train on normal data only

# Prediction on new data
y_pred = lof.predict(X_test)
scores = lof.score_samples(X_test)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.3, label='Training Data', color='blue')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm',
            edgecolors='k', s=100, label='Test Data')
plt.title('LOF: Anomaly Detection for New Data (novelty=True)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"Number of detected anomalies: {np.sum(y_pred == -1)}/{len(y_pred)}")
</code></pre>
<hr/>
<h2>3.3 One-Class SVM</h2>
<p>One-Class SVM is a method that learns the boundary of normal data and detects data outside that boundary as anomalies.</p>
<h3>3.3.1 Maximum Margin Hyperplane</h3>
<p><strong>Basic Principle:</strong></p>
<ul>
<li>Find the hyperplane that best separates normal data from the origin</li>
<li>Maximize the margin between the hyperplane and data points</li>
<li>Learn non-linear boundaries with the kernel trick</li>
</ul>
<p><strong>Mathematical Definition:</strong></p>
<p>Decision function:</p>

$$
f(x) = \text{sign}(w \cdot \phi(x) - \rho)
$$

<ul>
<li>$w$: Normal vector</li>
<li>$\phi(x)$: Feature vector after kernel transformation</li>
<li>$\rho$: Bias term</li>
</ul>
<p>Optimization problem:</p>

$$
\min_{w, \rho, \xi} \frac{1}{2} \|w\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho
$$

<p>Constraints:</p>

$$
w \cdot \phi(x_i) \geq \rho - \xi_i, \quad \xi_i \geq 0
$$

<h3>3.3.2 Kernel Trick</h3>
<p><strong>Linear Kernel:</strong></p>

$$
K(x, x') = x \cdot x'
$$

<ul>
<li>Fast, easy to interpret</li>
<li>Applied to linearly separable data</li>
</ul>
<p><strong>RBF (Gaussian) Kernel:</strong></p>

$$
K(x, x') = \exp\left(-\gamma \|x - x'\|^2\right)
$$

<ul>
<li>Can learn non-linear boundaries</li>
<li>Most commonly used kernel</li>
<li>$\gamma$: Kernel width (larger values create more complex boundaries)</li>
</ul>
<p><strong>Polynomial Kernel:</strong></p>

$$
K(x, x') = (\gamma x \cdot x' + r)^d
$$

<ul>
<li>Polynomial boundary of degree $d$</li>
<li>More constrained than RBF</li>
</ul>
<h3>3.3.3 nu Parameter</h3>
<p><strong>Meaning of nu:</strong></p>
<p>$\nu \in (0, 1]$ controls the upper and lower bounds of the following two quantities:</p>
<ul>
<li><strong>Upper bound</strong> on the fraction of anomalies in training data</li>
<li><strong>Lower bound</strong> on the fraction of support vectors</li>
</ul>
<p><strong>Recommended Values:</strong></p>
<ul>
<li>$\nu = 0.1$: Assume 10% is anomalous</li>
<li>$\nu = 0.05$: Assume 5% is anomalous</li>
<li>$\nu = 0.01$: Assume 1% is anomalous</li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li>Setting $\nu$ too small will result in almost no anomalies being detected</li>
<li>Setting $\nu$ too large will result in normal data also being classified as anomalous</li>
<li>Should be set based on domain knowledge or prior anomaly rate</li>
</ul>
<h3>3.3.4 scikit-learn Implementation</h3>
<p><strong>Basic Implementation:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# Data generation
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5  # Normal data
X_test_normal = np.random.randn(50, 2) * 0.5
X_test_outliers = np.random.uniform(low=-3, high=3, size=(10, 2))
X_test = np.vstack([X_test_normal, X_test_outliers])

# One-Class SVM
oc_svm = OneClassSVM(
    kernel='rbf',  # RBF kernel
    gamma='auto',  # gamma = 1 / n_features
    nu=0.1  # Assume 10% is anomalous
)

# Training
oc_svm.fit(X_train)

# Prediction
y_pred_train = oc_svm.predict(X_train)
y_pred_test = oc_svm.predict(X_test)
scores_test = oc_svm.decision_function(X_test)

# Visualize decision boundary
xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k', label='Training')
plt.title('One-Class SVM: Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='coolwarm',
            edgecolors='k', s=100)
plt.title('One-Class SVM: Test Data Prediction')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')

plt.tight_layout()
plt.show()

print(f"Number of anomalies in training data: {np.sum(y_pred_train == -1)}/{len(y_pred_train)}")
print(f"Number of anomalies in test data: {np.sum(y_pred_test == -1)}/{len(y_pred_test)}")
</code></pre>
<p><strong>Impact of gamma Parameter:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM

# Data generation
np.random.seed(42)
X_train = np.random.randn(200, 2) * 0.5

# Compare with different gamma values
gamma_list = [0.01, 0.1, 1.0]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, gamma in enumerate(gamma_list):
    oc_svm = OneClassSVM(kernel='rbf', gamma=gamma, nu=0.1)
    oc_svm.fit(X_train)

    # Decision boundary
    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-3, 3, 300))
    Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[idx].contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
    axes[idx].contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolors='k')
    axes[idx].set_title(f'One-Class SVM (gamma={gamma})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>3.4 Other Machine Learning Methods</h2>
<h3>3.4.1 DBSCAN (Density-Based Clustering)</h3>
<p><strong>Principle:</strong></p>
<ul>
<li>Detect high-density regions as clusters</li>
<li>Consider points that don't belong to any cluster as noise (anomalies)</li>
<li>No need to specify the number of clusters in advance</li>
</ul>
<p><strong>Main Parameters:</strong></p>
<ul>
<li><code>eps</code>: Neighborhood radius (distance threshold)</li>
<li><code>min_samples</code>: Minimum number of neighbors to become a core point</li>
</ul>
<p><strong>Implementation Example:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN

# Data generation
np.random.seed(42)
X_cluster1 = np.random.randn(100, 2) * 0.3 + [0, 0]
X_cluster2 = np.random.randn(100, 2) * 0.3 + [3, 3]
X_outliers = np.random.uniform(low=-2, high=5, size=(20, 2))
X = np.vstack([X_cluster1, X_cluster2, X_outliers])

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# Label -1 represents noise (anomalies)
outliers = labels == -1

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X[~outliers, 0], X[~outliers, 1], c=labels[~outliers],
            cmap='viridis', edgecolors='k', label='Clusters')
plt.scatter(X[outliers, 0], X[outliers, 1], c='red', marker='x',
            s=100, label='Outliers (Anomalies)')
plt.title('DBSCAN: Density-Based Anomaly Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

print(f"Number of clusters detected: {len(set(labels)) - (1 if -1 in labels else 0)}")
print(f"Number of anomalies: {np.sum(outliers)}")
</code></pre>
<h3>3.4.2 Elliptic Envelope</h3>
<p><strong>Principle:</strong></p>
<ul>
<li>Assume normal distribution and estimate data center and covariance</li>
<li>Detect anomalies with Mahalanobis distance</li>
<li>Suppress influence of outliers with robust estimation (Minimum Covariance Determinant)</li>
</ul>
<p><strong>Implementation Example:</strong></p>
<pre><code class="language-python">from sklearn.covariance import EllipticEnvelope
import numpy as np
import matplotlib.pyplot as plt

# Data generation
np.random.seed(42)
X_normal = np.random.randn(200, 2)
X_outliers = np.random.uniform(low=-5, high=5, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# Elliptic Envelope
elliptic = EllipticEnvelope(contamination=0.1, random_state=42)
y_pred = elliptic.fit_predict(X)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolors='k')
plt.title('Elliptic Envelope: Anomaly Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Prediction (-1: Anomaly, 1: Normal)')
plt.show()

print(f"Number of detected anomalies: {np.sum(y_pred == -1)}")
</code></pre>
<h3>3.4.3 Robust Covariance</h3>
<p><strong>Minimum Covariance Determinant (MCD):</strong></p>
<ul>
<li>Search for subset that minimizes the determinant of covariance matrix</li>
<li>Robust estimation against outliers</li>
<li>Used for Mahalanobis distance calculation</li>
</ul>
<pre><code class="language-python">from sklearn.covariance import MinCovDet
import numpy as np

# Data generation
np.random.seed(42)
X = np.random.randn(100, 2)
X[:5] = X[:5] + 5  # Add outliers

# MCD estimation
mcd = MinCovDet(random_state=42)
mcd.fit(X)

# Calculate Mahalanobis distance
distances = mcd.mahalanobis(X)

# Anomaly detection (95th percentile of chi-square distribution)
from scipy import stats
threshold = stats.chi2.ppf(0.95, df=2)
outliers = distances &gt; threshold

print(f"Number of anomalies: {np.sum(outliers)}")
print(f"Distance threshold: {threshold:.2f}")
</code></pre>
<h3>3.4.4 PyOD Library</h3>
<p><strong>PyOD (Python Outlier Detection)</strong> is a specialized library for anomaly detection providing over 40 algorithms.</p>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">pip install pyod
</code></pre>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from pyod.utils.utility import standardizer
import numpy as np

# Data generation
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=2,
    contamination=0.1, random_state=42
)

# Data standardization
X_train = standardizer(X_train)
X_test = standardizer(X_test)

# Compare multiple models
models = {
    'KNN': KNN(contamination=0.1),
    'IForest': IForest(contamination=0.1, random_state=42),
    'LOF': LOF(contamination=0.1)
}

for name, model in models.items():
    model.fit(X_train)
    y_pred = model.predict(X_test)
    scores = model.decision_function(X_test)

    # Evaluation (with hypothetical ground truth labels)
    from sklearn.metrics import roc_auc_score
    auc = roc_auc_score(y_test, scores)

    print(f"{name}:")
    print(f"  AUC-ROC: {auc:.3f}")
    print(f"  Number of detected anomalies: {np.sum(y_pred == 1)}")
    print()
</code></pre>
<p><strong>Sample Output:</strong></p>
<pre><code>KNN:
  AUC-ROC: 0.892
  Number of detected anomalies: 10

IForest:
  AUC-ROC: 0.915
  Number of detected anomalies: 10

LOF:
  AUC-ROC: 0.903
  Number of detected anomalies: 10
</code></pre>
<hr/>
<h2>3.5 Ensemble Anomaly Detection</h2>
<p>By combining multiple anomaly detection algorithms, more accurate and stable anomaly detection becomes possible.</p>
<h3>3.5.1 Feature Bagging</h3>
<p><strong>Principle:</strong></p>
<ul>
<li>Randomly select feature subsets</li>
<li>Train anomaly detection models on each subset</li>
<li>Aggregate predictions from multiple models</li>
</ul>
<p><strong>Implementation Example:</strong></p>
<pre><code class="language-python">from pyod.models.feature_bagging import FeatureBagging
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
from sklearn.metrics import roc_auc_score

# Data generation
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=10,
    contamination=0.1, random_state=42
)

# Feature Bagging (base model: LOF)
fb = FeatureBagging(
    base_estimator=LOF(),
    n_estimators=10,  # Number of models
    contamination=0.1,
    random_state=42
)

# Training and prediction
fb.fit(X_train)
y_pred = fb.predict(X_test)
scores = fb.decision_function(X_test)

# Evaluation
auc = roc_auc_score(y_test, scores)
print(f"Feature Bagging AUC-ROC: {auc:.3f}")
print(f"Number of detected anomalies: {np.sum(y_pred == 1)}")
</code></pre>
<h3>3.5.2 Model Averaging</h3>
<p><strong>Principle:</strong></p>
<ul>
<li>Train multiple different algorithms</li>
<li>Average anomaly scores from each model</li>
<li>More robust prediction than single models</li>
</ul>
<p><strong>Implementation Example:</strong></p>
<pre><code class="language-python">from pyod.models.combination import average, maximization
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data
import numpy as np

# Data generation
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# Train multiple models
models = [
    KNN(contamination=0.1),
    IForest(contamination=0.1, random_state=42),
    LOF(contamination=0.1)
]

# Calculate scores for each model
scores_list = []
for model in models:
    model.fit(X_train)
    scores = model.decision_function(X_test)
    scores_list.append(scores)

scores_array = np.array(scores_list)

# Score aggregation (average)
scores_avg = average(scores_array)

# Score aggregation (maximum)
scores_max = maximization(scores_array)

# Evaluation
from sklearn.metrics import roc_auc_score
auc_avg = roc_auc_score(y_test, scores_avg)
auc_max = roc_auc_score(y_test, scores_max)

print(f"Average Combination AUC-ROC: {auc_avg:.3f}")
print(f"Maximum Combination AUC-ROC: {auc_max:.3f}")
</code></pre>
<h3>3.5.3 Isolation-Based Ensemble</h3>
<p><strong>LSCP (Locally Selective Combination in Parallel):</strong></p>
<ul>
<li>Select locally optimal model for each test sample</li>
<li>Weight models based on performance in neighborhood</li>
<li>Higher accuracy than global averaging</li>
</ul>
<pre><code class="language-python">from pyod.models.lscp import LSCP
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.utils.data import generate_data

# Data generation
X_train, X_test, y_train, y_test = generate_data(
    n_train=200, n_test=100, n_features=5,
    contamination=0.1, random_state=42
)

# List of base models
detector_list = [
    KNN(),
    IForest(random_state=42),
    LOF()
]

# LSCP
lscp = LSCP(detector_list, contamination=0.1, random_state=42)
lscp.fit(X_train)

# Prediction
y_pred = lscp.predict(X_test)
scores = lscp.decision_function(X_test)

# Evaluation
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, scores)
print(f"LSCP AUC-ROC: {auc:.3f}")
print(f"Number of detected anomalies: {np.sum(y_pred == 1)}")
</code></pre>
<h3>3.5.4 Complete Pipeline Example</h3>
<p><strong>Data Preprocessing ‚Üí Multiple Model Training ‚Üí Ensemble ‚Üí Evaluation:</strong></p>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.combination import average
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Data generation (substitute for real data)
np.random.seed(42)
n_samples = 1000
n_features = 10
contamination = 0.05

# Normal data
X_normal = np.random.randn(int(n_samples * (1 - contamination)), n_features)
# Anomalous data
X_anomaly = np.random.uniform(low=-5, high=5, size=(int(n_samples * contamination), n_features))
X = np.vstack([X_normal, X_anomaly])
y = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Data standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple models
models = {
    'KNN': KNN(contamination=contamination),
    'IForest': IForest(contamination=contamination, random_state=42),
    'LOF': LOF(contamination=contamination),
    'OCSVM': OCSVM(contamination=contamination)
}

scores_dict = {}
predictions_dict = {}

for name, model in models.items():
    model.fit(X_train_scaled)
    scores = model.decision_function(X_test_scaled)
    y_pred = model.predict(X_test_scaled)

    scores_dict[name] = scores
    predictions_dict[name] = y_pred

    auc = roc_auc_score(y_test, scores)
    print(f"{name} AUC-ROC: {auc:.3f}")

# Ensemble (average)
scores_list = [scores_dict[name] for name in models.keys()]
scores_ensemble = average(np.array(scores_list))
auc_ensemble = roc_auc_score(y_test, scores_ensemble)
print(f"\nEnsemble AUC-ROC: {auc_ensemble:.3f}")

# ROC curve visualization
plt.figure(figsize=(10, 6))

for name, scores in scores_dict.items():
    fpr, tpr, _ = roc_curve(y_test, scores)
    auc_val = roc_auc_score(y_test, scores)
    plt.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})')

# Ensemble ROC
fpr_ens, tpr_ens, _ = roc_curve(y_test, scores_ensemble)
plt.plot(fpr_ens, tpr_ens, 'k--', linewidth=2,
         label=f'Ensemble (AUC={auc_ensemble:.3f})')

plt.plot([0, 1], [0, 1], 'r--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves: Multiple Models and Ensemble')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
</code></pre>
<hr/>
<h2>3.6 Summary</h2>
<h3>What We Learned in This Chapter</h3>
<ol>
<li>
<p><strong>Isolation Forest:</strong></p>
<ul>
<li>Anomaly detection by random isolation</li>
<li>Calculate anomaly score from path length</li>
<li>Hyperparameters (n_estimators, max_samples, contamination)</li>
<li>Effective for high-dimensional data</li>
</ul>
</li>
<li>
<p><strong>LOF (Local Outlier Factor):</strong></p>
<ul>
<li>Anomaly detection based on local density</li>
<li>Reachability distance and local reachability density</li>
<li>LOF score calculation and interpretation</li>
<li>Handles clusters with different densities</li>
</ul>
</li>
<li>
<p><strong>One-Class SVM:</strong></p>
<ul>
<li>Boundary learning with maximum margin hyperplane</li>
<li>Kernel trick (RBF, linear, polynomial)</li>
<li>Anomaly rate control with nu parameter</li>
<li>Learning non-linear boundaries</li>
</ul>
</li>
<li>
<p><strong>Other Methods:</strong></p>
<ul>
<li>DBSCAN (density-based clustering)</li>
<li>Elliptic Envelope</li>
<li>Robust Covariance</li>
<li>PyOD library (over 40 algorithms)</li>
</ul>
</li>
<li>
<p><strong>Ensemble Anomaly Detection:</strong></p>
<ul>
<li>Feature Bagging (feature subsets)</li>
<li>Model Averaging (score averaging)</li>
<li>Isolation-Based Ensemble (LSCP)</li>
<li>Improved accuracy by combining multiple models</li>
</ul>
</li>
</ol>
<h3>Method Selection Guide</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Application Scenarios</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Isolation Forest</td>
<td>High-dimensional data, large-scale data</td>
<td>Fast, scalable</td>
<td>Parameter tuning required</td>
</tr>
<tr>
<td>LOF</td>
<td>Clusters with different densities</td>
<td>Detects local anomalies</td>
<td>High computational cost</td>
</tr>
<tr>
<td>One-Class SVM</td>
<td>Non-linear boundaries, theoretical guarantees</td>
<td>Robust, theoretical foundation</td>
<td>Slow on large-scale data</td>
</tr>
<tr>
<td>DBSCAN</td>
<td>Clustering + anomaly detection</td>
<td>No need to specify number of clusters</td>
<td>Sensitive to parameters</td>
</tr>
<tr>
<td>Ensemble</td>
<td>When high accuracy is needed</td>
<td>Robust, high accuracy</td>
<td>Increased computational cost</td>
</tr>
</tbody>
</table>
<h3>Next Steps</h3>
<p>In Chapter 4, we will learn deep learning-based anomaly detection:</p>
<ul>
<li>Autoencoder (reconstruction error-based)</li>
<li>VAE (Variational Autoencoder)</li>
<li>GAN (Generative Adversarial Network)</li>
<li>LSTM Autoencoder (time-series anomaly detection)</li>
<li>Transformer (Attention mechanism)</li>
</ul>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Question 1:</strong> In Isolation Forest, should a data point with anomaly score $s(x, n) = 0.8$ be classified as anomalous? Answer with reasoning.</summary>
<p><strong>Answer:</strong></p>
<p>Yes, it should be classified as anomalous.</p>
<p><strong>Reasoning:</strong></p>
<ul>
<li>Anomaly score $s \approx 1$ indicates a clear anomaly</li>
<li>$s \approx 0.5$ is normal (average path length)</li>
<li>$s = 0.8$ is close to 1, meaning it is isolated earlier than usual</li>
<li>Generally, $s &gt; 0.6$ is used as the threshold for anomalies</li>
</ul>
</details>
<details>
<summary><strong>Question 2:</strong> If the LOF score is $\text{LOF}_k(p) = 2.5$, is this point anomalous? Also, explain what this score means.</summary>
<p><strong>Answer:</strong></p>
<p>Yes, it is anomalous.</p>
<p><strong>Meaning:</strong></p>
<ul>
<li>$\text{LOF} \approx 1$ indicates similar density to neighbors (normal)</li>
<li>$\text{LOF} &gt; 1$ indicates lower density than neighbors (possibly anomalous)</li>
<li>$\text{LOF} = 2.5$ indicates that this point's density is approximately 1/2.5 of the average neighbor density</li>
<li>Since $\text{LOF} &gt; 1.5$ is generally considered anomalous, 2.5 is a clear anomaly</li>
</ul>
</details>
<details>
<summary><strong>Question 3:</strong> If the One-Class SVM nu parameter is set to 0.05, what percentage of training data will be classified as anomalous? Also, explain the impact of increasing nu.</summary>
<p><strong>Answer:</strong></p>
<p>At most 5% of training data will be classified as anomalous.</p>
<p><strong>Impact of Increasing nu:</strong></p>
<ul>
<li>$\nu = 0.1$: At most 10% classified as anomalous</li>
<li>$\nu = 0.2$: At most 20% classified as anomalous</li>
<li>Increasing nu results in more data being classified as anomalous</li>
<li>Risk of misclassifying normal data as anomalous increases (increased false positives)</li>
<li>Sensitivity of anomaly detection increases, but accuracy may decrease</li>
</ul>
</details>
<details>
<summary><strong>Question 4:</strong> When performing anomaly detection with DBSCAN, how should the eps and min_samples parameters be selected? Describe three specific selection methods.</summary>
<p><strong>Answer:</strong></p>
<ol>
<li>
<p><strong>K-Distance Graph Method:</strong></p>
<ul>
<li>Calculate the k-th nearest neighbor distance for each point (k is the min_samples candidate)</li>
<li>Sort distances in descending order and plot</li>
<li>Select the point where the distance increases sharply (elbow point) as eps</li>
</ul>
</li>
<li>
<p><strong>Domain Knowledge-Based Selection:</strong></p>
<ul>
<li>Estimate appropriate neighborhood size from data characteristics</li>
<li>Example: For 2D data, min_samples=4; for high dimensions, min_samples=2√ódimensions</li>
<li>Adjust eps according to data scale</li>
</ul>
</li>
<li>
<p><strong>Grid Search:</strong></p>
<ul>
<li>Try multiple combinations of (eps, min_samples)</li>
<li>Evaluate with silhouette score or number of clusters</li>
<li>Select the optimal combination</li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>Question 5:</strong> In ensemble anomaly detection, explain the differences between Feature Bagging and Model Averaging, and discuss which situations each is effective in (within 300 characters).</summary>
<p><strong>Sample Answer:</strong></p>
<p>Feature Bagging trains multiple models on feature subsets, addressing correlation and redundancy among features in high-dimensional data. It is effective when there are many correlated features. Model Averaging aggregates predictions from different algorithms (KNN, Isolation Forest, LOF, etc.), leveraging the strengths of each method. It is effective when the data characteristics are unclear and the optimal method is unknown in advance. Feature Bagging enhances diversity of the same algorithm, while Model Averaging utilizes algorithmic diversity. In practice, combining both can build a more robust anomaly detection system.</p>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008). "Isolation Forest." <em>IEEE International Conference on Data Mining (ICDM)</em>.</li>
<li>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000). "LOF: Identifying Density-Based Local Outliers." <em>ACM SIGMOD International Conference on Management of Data</em>.</li>
<li>Sch√∂lkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., &amp; Williamson, R. C. (2001). "Estimating the Support of a High-Dimensional Distribution." <em>Neural Computation</em>, 13(7), 1443-1471.</li>
<li>Ester, M., Kriegel, H. P., Sander, J., &amp; Xu, X. (1996). "A Density-Based Algorithm for Discovering Clusters." <em>KDD</em>, 96(34), 226-231.</li>
<li>Zhao, Y., Nasrullah, Z., &amp; Li, Z. (2019). "PyOD: A Python Toolbox for Scalable Outlier Detection." <em>Journal of Machine Learning Research</em>, 20(96), 1-7.</li>
</ol>
<hr/>
<p><strong>Next Chapter</strong>: <a href="chapter4-deep-learning-anomaly.html">Chapter 4: Deep Learning-Based Anomaly Detection</a></p>
<p><strong>License</strong>: This content is provided under the CC BY 4.0 license.</p>
<div class="navigation">
<a class="nav-button" href="chapter2-statistical-methods.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Return to Series Index</a>
<a class="nav-button" href="chapter4-deep-learning-anomaly.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Created by</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
