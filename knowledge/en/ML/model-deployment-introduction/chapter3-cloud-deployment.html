<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Cloud Deployment - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/model-deployment-introduction/index.html">Model Deployment</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/model-deployment-introduction/chapter3-cloud-deployment.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 3: Cloud Deployment</h1>
<p class="subtitle">Building Scalable ML Systems with AWS, GCP, and Azure</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code examples: 8</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Cloud Deployment. You will learn characteristics of major cloud platforms (AWS, Deploy models with AWS SageMaker, and basics of GCP Vertex AI.</p>
<h2>Learning Objectives</h2>
<p>After reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the characteristics of major cloud platforms (AWS, GCP, Azure)</li>
<li>‚úÖ Deploy models with AWS SageMaker</li>
<li>‚úÖ Build serverless inference environments with AWS Lambda</li>
<li>‚úÖ Understand the basics of GCP Vertex AI and Azure ML</li>
<li>‚úÖ Implement multi-cloud strategies with Terraform and CI/CD</li>
</ul>
<hr/>
<h2>3.1 Cloud Deployment Options</h2>
<h3>Comparison of Major Cloud Platforms</h3>
<p>Three major cloud platforms are primarily used for deploying machine learning models.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>ML Services</th>
<th>Strengths</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AWS</strong></td>
<td>SageMaker, Lambda, ECS</td>
<td>Largest market share, extensive services</td>
<td>Enterprise, large-scale systems</td>
</tr>
<tr>
<td><strong>GCP</strong></td>
<td>Vertex AI, Cloud Run</td>
<td>TensorFlow integration, BigQuery connectivity</td>
<td>Data analytics focus, startups</td>
</tr>
<tr>
<td><strong>Azure</strong></td>
<td>Azure ML, Functions</td>
<td>Microsoft product integration</td>
<td>Enterprise (Microsoft environments)</td>
</tr>
</tbody>
</table>
<h3>Types of Deployment Services</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>AWS</th>
<th>GCP</th>
<th>Azure</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Managed</strong></td>
<td>Fully managed ML platform</td>
<td>SageMaker</td>
<td>Vertex AI</td>
<td>Azure ML</td>
</tr>
<tr>
<td><strong>Serverless</strong></td>
<td>Event-driven, auto-scaling</td>
<td>Lambda</td>
<td>Cloud Functions</td>
<td>Azure Functions</td>
</tr>
<tr>
<td><strong>Container</strong></td>
<td>Docker/Kubernetes-based</td>
<td>ECS/EKS</td>
<td>Cloud Run/GKE</td>
<td>AKS</td>
</tr>
</tbody>
</table>
<h3>Cost Considerations</h3>
<p>Cloud deployment cost factors:</p>
<ul>
<li><strong>Compute</strong>: Instance type, uptime</li>
<li><strong>Storage</strong>: Model files, log storage</li>
<li><strong>Network</strong>: Data transfer volume</li>
<li><strong>Inference requests</strong>: API call count</li>
</ul>
<blockquote>
<p><strong>Cost Optimization Tips</strong>: Auto-scaling, spot instances, and appropriate instance sizing are key.</p>
</blockquote>
<div class="mermaid">
graph TD
    A[Deployment Strategy] --&gt; B[Traffic Pattern]
    B --&gt; C{Request Frequency}
    C --&gt;|High frequency, stable| D[Managed<br/>SageMaker/Vertex AI]
    C --&gt;|Low frequency, irregular| E[Serverless<br/>Lambda/Cloud Functions]
    C --&gt;|Burst handling| F[Auto-scaling<br/>ECS/Cloud Run]

    A --&gt; G[Cost Constraints]
    G --&gt; H{Budget}
    H --&gt;|Low budget| I[Serverless]
    H --&gt;|Medium budget| J[Container]
    H --&gt;|High budget| K[Dedicated Managed]

    style D fill:#e3f2fd
    style E fill:#fff3e0
    style F fill:#f3e5f5
</div>
<hr/>
<h2>3.2 AWS SageMaker Deployment</h2>
<h3>What is a SageMaker Endpoint?</h3>
<p><strong>Amazon SageMaker</strong> is a managed service that integrates building, training, and deploying machine learning models.</p>
<h3>Model Packaging</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Model Packaging

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: 30-60 seconds
Dependencies: None
"""

# model_package.py
import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
X_train = np.random.randn(1000, 10)
y_train = np.random.randint(0, 2, 1000)
model.fit(X_train, y_train)

# Save the model
joblib.dump(model, 'model.joblib')
print("‚úì Model saved: model.joblib")
</code></pre>
<h3>Custom Inference Script</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0
# - numpy&gt;=1.24.0, &lt;2.0.0

# inference.py
import joblib
import json
import numpy as np

def model_fn(model_dir):
    """Load the model"""
    model = joblib.load(f"{model_dir}/model.joblib")
    return model

def input_fn(request_body, content_type):
    """Parse input data"""
    if content_type == 'application/json':
        data = json.loads(request_body)
        return np.array(data['instances'])
    raise ValueError(f"Unsupported content type: {content_type}")

def predict_fn(input_data, model):
    """Execute inference"""
    predictions = model.predict(input_data)
    probabilities = model.predict_proba(input_data)
    return {
        'predictions': predictions.tolist(),
        'probabilities': probabilities.tolist()
    }

def output_fn(prediction, accept):
    """Format response"""
    if accept == 'application/json':
        return json.dumps(prediction), accept
    raise ValueError(f"Unsupported accept type: {accept}")
</code></pre>
<h3>Deploying to SageMaker</h3>
<pre><code class="language-python">import boto3
import sagemaker
from sagemaker.sklearn.model import SKLearnModel
from datetime import datetime

# Session configuration
session = sagemaker.Session()
role = 'arn:aws:iam::123456789012:role/SageMakerRole'
bucket = session.default_bucket()

# Upload model to S3
model_data = session.upload_data(
    path='model.joblib',
    bucket=bucket,
    key_prefix='models/sklearn-model'
)

# Create SageMaker model
sklearn_model = SKLearnModel(
    model_data=model_data,
    role=role,
    entry_point='inference.py',
    framework_version='1.0-1',
    py_version='py3'
)

# Deploy endpoint
endpoint_name = f'sklearn-endpoint-{datetime.now().strftime("%Y%m%d-%H%M%S")}'
predictor = sklearn_model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large',
    endpoint_name=endpoint_name
)

print(f"‚úì Endpoint deployed: {endpoint_name}")
print(f"‚úì Instance type: ml.m5.large")
print(f"‚úì Instance count: 1")
</code></pre>
<h3>Executing Inference Requests</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Executing Inference Requests

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import boto3
import json
import numpy as np

# SageMaker Runtime client
runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')

# Test data
test_data = {
    'instances': np.random.randn(5, 10).tolist()
}

# Inference request
response = runtime.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='application/json',
    Accept='application/json',
    Body=json.dumps(test_data)
)

# Parse response
result = json.loads(response['Body'].read().decode())
print("\n=== Inference Results ===")
print(f"Predictions: {result['predictions']}")
print(f"Probabilities: {result['probabilities']}")

# Performance information
print(f"\nInference time: {response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invocation-timestamp', 'N/A')}")
</code></pre>
<h3>Configuring Auto-Scaling</h3>
<pre><code class="language-python">import boto3

# Auto Scaling client
autoscaling = boto3.client('application-autoscaling', region_name='us-east-1')

# Register scalable target
resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic'
autoscaling.register_scalable_target(
    ServiceNamespace='sagemaker',
    ResourceId=resource_id,
    ScalableDimension='sagemaker:variant:DesiredInstanceCount',
    MinCapacity=1,
    MaxCapacity=5
)

# Configure scaling policy
autoscaling.put_scaling_policy(
    PolicyName=f'{endpoint_name}-scaling-policy',
    ServiceNamespace='sagemaker',
    ResourceId=resource_id,
    ScalableDimension='sagemaker:variant:DesiredInstanceCount',
    PolicyType='TargetTrackingScaling',
    TargetTrackingScalingPolicyConfiguration={
        'TargetValue': 70.0,  # Target CPU utilization 70%
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'
        },
        'ScaleInCooldown': 300,   # Scale-in cooldown (seconds)
        'ScaleOutCooldown': 60    # Scale-out cooldown (seconds)
    }
)

print("‚úì Auto-scaling configured")
print(f"  Minimum instances: 1")
print(f"  Maximum instances: 5")
print(f"  Target metric: Requests/instance")
</code></pre>
<blockquote>
<p><strong>Best Practice</strong>: In production, ensure availability with a minimum of 2 instances and adjust scale-out thresholds according to traffic patterns.</p>
</blockquote>
<hr/>
<h2>3.3 AWS Lambda Serverless Deployment</h2>
<h3>Advantages of Serverless Architecture</h3>
<ul>
<li><strong>Cost efficiency</strong>: Charged only for execution time</li>
<li><strong>Auto-scaling</strong>: Automatically adjusts to concurrent executions</li>
<li><strong>Reduced operational burden</strong>: No infrastructure management required</li>
<li><strong>High availability</strong>: Automatic multi-AZ deployment</li>
</ul>
<h3>Creating a Lambda Function</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0
# - numpy&gt;=1.24.0, &lt;2.0.0

# lambda_function.py
import json
import joblib
import numpy as np
import base64
import io

# Load model in global scope (cold start optimization)
model = None

def load_model():
    """Load model (executed only once)"""
    global model
    if model is None:
        # Load model from S3 or include in layer
        model = joblib.load('/opt/model.joblib')
    return model

def lambda_handler(event, context):
    """Lambda function main handler"""
    try:
        # Load model
        ml_model = load_model()

        # Parse request body
        body = json.loads(event.get('body', '{}'))
        instances = body.get('instances', [])

        if not instances:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'No instances provided'})
            }

        # Execute inference
        input_data = np.array(instances)
        predictions = ml_model.predict(input_data)
        probabilities = ml_model.predict_proba(input_data)

        # Response
        response = {
            'predictions': predictions.tolist(),
            'probabilities': probabilities.tolist(),
            'model_version': '1.0'
        }

        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps(response)
        }

    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
</code></pre>
<h3>Deploying with Container Image</h3>
<pre><code class="language-dockerfile"># Dockerfile
FROM public.ecr.aws/lambda/python:3.9

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt --target "${LAMBDA_TASK_ROOT}"

# Copy model file
COPY model.joblib ${LAMBDA_TASK_ROOT}/opt/

# Copy Lambda function code
COPY lambda_function.py ${LAMBDA_TASK_ROOT}

# Specify handler
CMD ["lambda_function.lambda_handler"]
</code></pre>
<pre><code class="language-bash">#!/bin/bash
# deploy.sh - Build and deploy Lambda container image

# Variable configuration
AWS_REGION="us-east-1"
AWS_ACCOUNT_ID="123456789012"
ECR_REPO="ml-inference-lambda"
IMAGE_TAG="latest"

# Create ECR repository (first time only)
aws ecr create-repository \
    --repository-name ${ECR_REPO} \
    --region ${AWS_REGION} 2&gt;/dev/null || true

# Login to ECR
aws ecr get-login-password --region ${AWS_REGION} | \
    docker login --username AWS --password-stdin \
    ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com

# Build Docker image
docker build -t ${ECR_REPO}:${IMAGE_TAG} .

# Tag image
docker tag ${ECR_REPO}:${IMAGE_TAG} \
    ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO}:${IMAGE_TAG}

# Push to ECR
docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO}:${IMAGE_TAG}

echo "‚úì Image pushed to ECR"
</code></pre>
<h3>Integration with API Gateway</h3>
<pre><code class="language-python">import boto3
import json

# Create API Gateway
apigateway = boto3.client('apigateway', region_name='us-east-1')
lambda_client = boto3.client('lambda', region_name='us-east-1')

# Create REST API
api = apigateway.create_rest_api(
    name='ML-Inference-API',
    description='Machine Learning Inference API',
    endpointConfiguration={'types': ['REGIONAL']}
)
api_id = api['id']

# Get resources
resources = apigateway.get_resources(restApiId=api_id)
root_id = resources['items'][0]['id']

# Create /predict resource
predict_resource = apigateway.create_resource(
    restApiId=api_id,
    parentId=root_id,
    pathPart='predict'
)

# Create POST method
apigateway.put_method(
    restApiId=api_id,
    resourceId=predict_resource['id'],
    httpMethod='POST',
    authorizationType='NONE'
)

# Configure Lambda integration
lambda_arn = f"arn:aws:lambda:us-east-1:123456789012:function:ml-inference"
apigateway.put_integration(
    restApiId=api_id,
    resourceId=predict_resource['id'],
    httpMethod='POST',
    type='AWS_PROXY',
    integrationHttpMethod='POST',
    uri=f'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/{lambda_arn}/invocations'
)

# Deploy
deployment = apigateway.create_deployment(
    restApiId=api_id,
    stageName='prod'
)

endpoint_url = f"https://{api_id}.execute-api.us-east-1.amazonaws.com/prod/predict"
print(f"‚úì API Gateway deployed")
print(f"‚úì Endpoint: {endpoint_url}")
</code></pre>
<h3>Cold Start Mitigation</h3>
<p>Methods to mitigate Lambda <strong>cold start</strong> (initial startup delay):</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Provisioned Concurrency</strong></td>
<td>Reserve always-on instances</td>
<td>Complete cold start avoidance</td>
</tr>
<tr>
<td><strong>Model Optimization</strong></td>
<td>Lightweight models, quantization</td>
<td>Reduced load time</td>
</tr>
<tr>
<td><strong>Layer Utilization</strong></td>
<td>Separate dependencies into layers</td>
<td>Reduced deployment package</td>
</tr>
<tr>
<td><strong>Periodic Warmup</strong></td>
<td>Scheduled execution with EventBridge</td>
<td>Avoid idle state</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.4 GCP Vertex AI and Azure ML</h2>
<h3>GCP Vertex AI Endpoints</h3>
<p><strong>Vertex AI</strong> is Google's managed ML platform, featuring deep integration with TensorFlow.</p>
<pre><code class="language-python"># vertex_ai_deploy.py
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project='my-gcp-project',
    location='us-central1',
    staging_bucket='gs://my-ml-models'
)

# Upload model
model = aiplatform.Model.upload(
    display_name='sklearn-classifier',
    artifact_uri='gs://my-ml-models/sklearn-model',
    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'
)

# Create endpoint
endpoint = aiplatform.Endpoint.create(display_name='sklearn-endpoint')

# Deploy model
endpoint.deploy(
    model=model,
    deployed_model_display_name='sklearn-v1',
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=5,
    traffic_percentage=100
)

print(f"‚úì Endpoint deployed: {endpoint.resource_name}")

# Inference request
instances = [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]]
prediction = endpoint.predict(instances=instances)
print(f"Prediction result: {prediction.predictions}")
</code></pre>
<h3>Azure ML Managed Endpoints</h3>
<p><strong>Azure Machine Learning</strong> is Microsoft Azure's managed machine learning service.</p>
<pre><code class="language-python"># azure_ml_deploy.py
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    ManagedOnlineEndpoint,
    ManagedOnlineDeployment,
    Model,
    Environment,
    CodeConfiguration
)
from azure.identity import DefaultAzureCredential

# Initialize Azure ML Client
credential = DefaultAzureCredential()
ml_client = MLClient(
    credential=credential,
    subscription_id='subscription-id',
    resource_group_name='ml-resources',
    workspace_name='ml-workspace'
)

# Register model
model = Model(
    path='./model',
    name='sklearn-classifier',
    description='Scikit-learn classification model'
)
registered_model = ml_client.models.create_or_update(model)

# Create endpoint
endpoint = ManagedOnlineEndpoint(
    name='sklearn-endpoint',
    description='Sklearn classification endpoint',
    auth_mode='key'
)
ml_client.online_endpoints.begin_create_or_update(endpoint).result()

# Create deployment
deployment = ManagedOnlineDeployment(
    name='blue',
    endpoint_name='sklearn-endpoint',
    model=registered_model,
    environment='AzureML-sklearn-1.0-ubuntu20.04-py38-cpu',
    code_configuration=CodeConfiguration(
        code='./src',
        scoring_script='score.py'
    ),
    instance_type='Standard_DS3_v2',
    instance_count=1
)
ml_client.online_deployments.begin_create_or_update(deployment).result()

# Allocate traffic
endpoint.traffic = {'blue': 100}
ml_client.online_endpoints.begin_create_or_update(endpoint).result()

print(f"‚úì Azure ML endpoint deployed: {endpoint.name}")
</code></pre>
<h3>Cloud Platform Comparison</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>AWS SageMaker</th>
<th>GCP Vertex AI</th>
<th>Azure ML</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deployment Method</strong></td>
<td>Endpoint, Lambda</td>
<td>Endpoint, Cloud Run</td>
<td>Managed Endpoint</td>
</tr>
<tr>
<td><strong>Auto-scaling</strong></td>
<td>‚óé (Flexible)</td>
<td>‚óé (Automatic)</td>
<td>‚óã (Configuration required)</td>
</tr>
<tr>
<td><strong>Model Management</strong></td>
<td>Model Registry</td>
<td>Model Registry</td>
<td>Model Registry</td>
</tr>
<tr>
<td><strong>Monitoring</strong></td>
<td>CloudWatch</td>
<td>Cloud Monitoring</td>
<td>Application Insights</td>
</tr>
<tr>
<td><strong>Pricing Model</strong></td>
<td>Instance hours</td>
<td>Instance hours</td>
<td>Instance hours</td>
</tr>
<tr>
<td><strong>Learning Curve</strong></td>
<td>Medium</td>
<td>Low (GCP experienced users)</td>
<td>Low (Azure experienced users)</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.5 Practice: Multi-Cloud Strategy and CI/CD</h2>
<h3>Infrastructure Management with Terraform</h3>
<p><strong>Infrastructure as Code (IaC)</strong> enables reproducible deployments.</p>
<pre><code class="language-hcl"># terraform/main.tf - AWS SageMaker endpoint
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~&gt; 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# SageMaker execution role
resource "aws_iam_role" "sagemaker_role" {
  name = "sagemaker-execution-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "sagemaker.amazonaws.com"
      }
    }]
  })
}

# SageMaker model
resource "aws_sagemaker_model" "ml_model" {
  name               = "sklearn-model-${var.environment}"
  execution_role_arn = aws_iam_role.sagemaker_role.arn

  primary_container {
    image          = var.container_image
    model_data_url = var.model_data_s3_uri
  }

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# SageMaker endpoint configuration
resource "aws_sagemaker_endpoint_configuration" "endpoint_config" {
  name = "sklearn-endpoint-config-${var.environment}"

  production_variants {
    variant_name           = "AllTraffic"
    model_name             = aws_sagemaker_model.ml_model.name
    initial_instance_count = var.initial_instance_count
    instance_type          = var.instance_type
  }

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# SageMaker endpoint
resource "aws_sagemaker_endpoint" "endpoint" {
  name                 = "sklearn-endpoint-${var.environment}"
  endpoint_config_name = aws_sagemaker_endpoint_configuration.endpoint_config.name

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Variable definitions
variable "aws_region" {
  default = "us-east-1"
}

variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
}

variable "container_image" {
  description = "SageMaker container image URI"
  type        = string
}

variable "model_data_s3_uri" {
  description = "S3 URI of model artifacts"
  type        = string
}

variable "instance_type" {
  default = "ml.m5.large"
}

variable "initial_instance_count" {
  default = 1
}

# Outputs
output "endpoint_name" {
  value = aws_sagemaker_endpoint.endpoint.name
}

output "endpoint_arn" {
  value = aws_sagemaker_endpoint.endpoint.arn
}
</code></pre>
<h3>CI/CD with GitHub Actions</h3>
<pre><code class="language-yaml"># .github/workflows/deploy-ml-model.yml
name: Deploy ML Model

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main

env:
  AWS_REGION: us-east-1
  MODEL_BUCKET: ml-models-artifacts

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run tests
        run: |
          pytest tests/ --cov=src --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'

    steps:
      - uses: actions/checkout@v3

      - name: Set environment
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "ENVIRONMENT=prod" &gt;&gt; $GITHUB_ENV
          else
            echo "ENVIRONMENT=dev" &gt;&gt; $GITHUB_ENV
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Train and package model
        run: |
          python src/train.py
          tar -czf model.tar.gz model.joblib

      - name: Upload model to S3
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          aws s3 cp model.tar.gz \
            s3://${{ env.MODEL_BUCKET }}/${{ env.ENVIRONMENT }}/model-${TIMESTAMP}.tar.gz
          echo "MODEL_S3_URI=s3://${{ env.MODEL_BUCKET }}/${{ env.ENVIRONMENT }}/model-${TIMESTAMP}.tar.gz" &gt;&gt; $GITHUB_ENV

      - name: Build Docker image
        run: |
          docker build -t ml-inference:${{ github.sha }} .

      - name: Push to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

          docker tag ml-inference:${{ github.sha }} \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}

          docker push \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          terraform plan \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="model_data_s3_uri=${{ env.MODEL_S3_URI }}" \
            -var="container_image=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ml-inference:${{ github.sha }}" \
            -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      - name: Smoke test
        run: |
          ENDPOINT_NAME=$(terraform -chdir=./terraform output -raw endpoint_name)
          python scripts/smoke_test.py --endpoint-name $ENDPOINT_NAME

  notify:
    needs: build-and-deploy
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'ML Model deployment ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
</code></pre>
<h3>Environment-Specific Deployment Strategy</h3>
<div class="mermaid">
graph LR
    A[Code Changes] --&gt; B[GitHub Push]
    B --&gt; C{Branch}
    C --&gt;|develop| D[Dev Environment]
    C --&gt;|staging| E[Staging Environment]
    C --&gt;|main| F[Production Environment]

    D --&gt; G[Automated Tests]
    E --&gt; H[Integration Tests]
    F --&gt; I[Smoke Tests]

    G --&gt; J[Auto Deploy]
    H --&gt; K[Manual Approval]
    K --&gt; L[Deploy]
    I --&gt; M[Health Check]

    style D fill:#e8f5e9
    style E fill:#fff3e0
    style F fill:#ffebee
</div>
<table>
<thead>
<tr>
<th>Environment</th>
<th>Purpose</th>
<th>Instance Count</th>
<th>Deployment Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dev</strong></td>
<td>Development, testing</td>
<td>1</td>
<td>Automatic (develop branch)</td>
</tr>
<tr>
<td><strong>Staging</strong></td>
<td>Integration testing, QA</td>
<td>2</td>
<td>Automatic (staging branch)</td>
</tr>
<tr>
<td><strong>Production</strong></td>
<td>Production operation</td>
<td>3+ (auto-scaling)</td>
<td>After manual approval</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Choosing Cloud Platforms</strong></p>
<ul>
<li>Characteristics and use cases of AWS, GCP, and Azure</li>
<li>Comparison of managed, serverless, and container options</li>
<li>Cost optimization considerations</li>
</ul></li>
<li><p><strong>AWS SageMaker</strong></p>
<ul>
<li>Creating and deploying endpoints</li>
<li>Implementing custom inference scripts</li>
<li>Configuring auto-scaling</li>
</ul></li>
<li><p><strong>AWS Lambda Serverless</strong></p>
<ul>
<li>Creating Lambda functions and container deployment</li>
<li>Integration with API Gateway</li>
<li>Cold start mitigation strategies</li>
</ul></li>
<li><p><strong>GCP Vertex AI and Azure ML</strong></p>
<ul>
<li>Deploying Vertex AI Endpoints</li>
<li>Utilizing Azure ML Managed Endpoints</li>
<li>Cross-platform comparison</li>
</ul></li>
<li><p><strong>Multi-Cloud Strategy</strong></p>
<ul>
<li>Infrastructure as Code with Terraform</li>
<li>CI/CD pipelines with GitHub Actions</li>
<li>Environment-specific deployment management</li>
</ul></li>
</ol>
<h3>Selection Guidelines</h3>
<table>
<thead>
<tr>
<th>Requirements</th>
<th>Recommended Solution</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>High-frequency requests</td>
<td>SageMaker/Vertex AI</td>
<td>Low latency with dedicated instances</td>
</tr>
<tr>
<td>Low-frequency, irregular</td>
<td>Lambda/Cloud Functions</td>
<td>Highly cost-efficient</td>
</tr>
<tr>
<td>Burst handling</td>
<td>ECS/Cloud Run</td>
<td>Flexible scaling</td>
</tr>
<tr>
<td>Multi-model</td>
<td>Kubernetes (EKS/GKE)</td>
<td>Unified management and resource efficiency</td>
</tr>
<tr>
<td>TensorFlow-centric</td>
<td>GCP Vertex AI</td>
<td>Native integration</td>
</tr>
<tr>
<td>Microsoft environment</td>
<td>Azure ML</td>
<td>Compatibility with existing systems</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 4, we'll learn about <strong>Monitoring and Operations Management</strong>:</p>
<ul>
<li>Performance monitoring</li>
<li>Log management and tracing</li>
<li>Model drift detection</li>
<li>A/B testing and canary deployment</li>
<li>Incident response</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: medium)</h3>
<p>For the following scenarios, determine whether to choose AWS SageMaker or AWS Lambda, and explain your reasoning.</p>
<p><strong>Scenario A</strong>: E-commerce product recommendation system (100,000 requests/day, response time within 100ms)<br/>
<strong>Scenario B</strong>: Batch processing for monthly report generation (once per month, 1-hour processing time)</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Scenario A: AWS SageMaker recommended</strong></p>
<ul>
<li><strong>Reasoning</strong>:
<ul>
<li>High-frequency requests (100,000/day = approximately 1.2 requests/second) with stable traffic</li>
<li>Response time within 100ms is required, cold start is not acceptable</li>
<li>Dedicated instances with constant availability guarantee low latency</li>
<li>Auto-scaling can handle traffic peaks</li>
</ul></li>
<li><strong>Configuration</strong>: ml.m5.large √ó 2 instances (minimum), auto-scale up to 5 instances</li>
<li><strong>Cost</strong>: Approximately $300-500/month (based on instance uptime)</li>
</ul>
<p><strong>Scenario B: AWS Lambda recommended</strong></p>
<ul>
<li><strong>Reasoning</strong>:
<ul>
<li>Low frequency (once per month), constant availability not required</li>
<li>Even with 1-hour processing time, can be addressed by dividing into Lambda (maximum 15 minutes) √ó 4 executions</li>
<li>Significant cost reduction with charges only for execution time</li>
<li>No strict response time requirements</li>
</ul></li>
<li><strong>Configuration</strong>: Lambda (3008MB memory), orchestration with Step Functions</li>
<li><strong>Cost</strong>: Approximately $5-10/month (execution time only)</li>
</ul>
<p><strong>Decision Criteria Summary</strong>:</p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>SageMaker</th>
<th>Lambda</th>
</tr>
</thead>
<tbody>
<tr>
<td>Request frequency</td>
<td>High frequency, stable</td>
<td>Low frequency, irregular</td>
</tr>
<tr>
<td>Latency requirements</td>
<td>Strict (&lt; 100ms)</td>
<td>Relaxed (&gt; 1s OK)</td>
</tr>
<tr>
<td>Cost characteristics</td>
<td>High fixed cost</td>
<td>Pay-per-use</td>
</tr>
<tr>
<td>Operational burden</td>
<td>Medium (scale management)</td>
<td>Low (fully managed)</td>
</tr>
</tbody>
</table>
</details>
<h3>Exercise 2 (Difficulty: hard)</h3>
<p>Using Terraform and GitHub Actions, design a configuration to deploy SageMaker endpoints with different settings (instance count, type) for development and production environments.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>1. Terraform variable files (by environment)</strong></p>
<pre><code class="language-hcl"># terraform/environments/dev.tfvars
environment            = "dev"
instance_type          = "ml.t3.medium"
initial_instance_count = 1
min_capacity           = 1
max_capacity           = 2
enable_autoscaling     = false

# terraform/environments/prod.tfvars
environment            = "prod"
instance_type          = "ml.m5.xlarge"
initial_instance_count = 2
min_capacity           = 2
max_capacity           = 10
enable_autoscaling     = true
</code></pre>
<p><strong>2. Terraform main file</strong></p>
<pre><code class="language-hcl"># terraform/main.tf
resource "aws_sagemaker_endpoint_configuration" "endpoint_config" {
  name = "sklearn-endpoint-config-${var.environment}"

  production_variants {
    variant_name           = "AllTraffic"
    model_name             = aws_sagemaker_model.ml_model.name
    initial_instance_count = var.initial_instance_count
    instance_type          = var.instance_type
  }
}

resource "aws_appautoscaling_target" "sagemaker_target" {
  count              = var.enable_autoscaling ? 1 : 0
  service_namespace  = "sagemaker"
  resource_id        = "endpoint/${aws_sagemaker_endpoint.endpoint.name}/variant/AllTraffic"
  scalable_dimension = "sagemaker:variant:DesiredInstanceCount"
  min_capacity       = var.min_capacity
  max_capacity       = var.max_capacity
}

resource "aws_appautoscaling_policy" "sagemaker_policy" {
  count              = var.enable_autoscaling ? 1 : 0
  name               = "sagemaker-scaling-policy-${var.environment}"
  service_namespace  = "sagemaker"
  resource_id        = aws_appautoscaling_target.sagemaker_target[0].resource_id
  scalable_dimension = aws_appautoscaling_target.sagemaker_target[0].scalable_dimension
  policy_type        = "TargetTrackingScaling"

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "SageMakerVariantInvocationsPerInstance"
    }
    target_value = 1000.0
  }
}
</code></pre>
<p><strong>3. GitHub Actions Workflow</strong></p>
<pre><code class="language-yaml"># .github/workflows/deploy.yml
jobs:
  deploy:
    steps:
      - name: Set environment variables
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "TFVARS_FILE=prod.tfvars" &gt;&gt; $GITHUB_ENV
            echo "REQUIRE_APPROVAL=true" &gt;&gt; $GITHUB_ENV
          else
            echo "TFVARS_FILE=dev.tfvars" &gt;&gt; $GITHUB_ENV
            echo "REQUIRE_APPROVAL=false" &gt;&gt; $GITHUB_ENV
          fi

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          terraform plan \
            -var-file="environments/${{ env.TFVARS_FILE }}" \
            -out=tfplan

      - name: Wait for approval (prod only)
        if: env.REQUIRE_APPROVAL == 'true'
        uses: trstringer/manual-approval@v1
        with:
          approvers: platform-team
          minimum-approvals: 2

      - name: Terraform Apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan
</code></pre>
<p><strong>4. Deployment Flow</strong></p>
<div class="mermaid">
graph TD
    A[Git Push] --&gt; B{Branch Detection}
    B --&gt;|develop| C[Dev Environment<br/>ml.t3.medium√ó1]
    B --&gt;|main| D[Prod Environment<br/>ml.m5.xlarge√ó2]
    C --&gt; E[Auto Deploy]
    D --&gt; F[Awaiting Approval]
    F --&gt; G[Manual Approval]
    G --&gt; H[Execute Deploy]
    E --&gt; I[Smoke Test]
    H --&gt; I
</div>
<p><strong>Configuration Key Points</strong>:</p>
<ul>
<li>Define different instance types and counts for each environment</li>
<li>Enable auto-scaling only for production environment</li>
<li>Add manual approval gate for production deployments</li>
<li>Environment separation with Terraform Workspaces</li>
</ul>
</details>
<h3>Exercise 3 (Difficulty: hard)</h3>
<p>Explain what methods should be combined to mitigate AWS Lambda's cold start problem, including specific implementations.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>1. Configuring Provisioned Concurrency</strong></p>
<pre><code class="language-python">import boto3

lambda_client = boto3.client('lambda')

# Configure provisioned concurrency
lambda_client.put_provisioned_concurrency_config(
    FunctionName='ml-inference',
    Qualifier='$LATEST',  # Or version/alias
    ProvisionedConcurrentExecutions=5  # Always keep 5 instances running
)

print("‚úì Provisioned concurrency configured (5 instances)")
</code></pre>
<p><strong>Effect</strong>: Complete cold start avoidance with always-on instances<br/>
<strong>Cost</strong>: Approximately 2x normal execution (constant charges)</p>
<p><strong>2. Lightweight Model and Layer Separation</strong></p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0

"""
Example: 2. Lightweight Model and Layer Separation

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

# Model lightweighting
import joblib
from sklearn.ensemble import RandomForestClassifier

# Original model
model = RandomForestClassifier(n_estimators=100, max_depth=10)
# Size: approximately 50MB

# Lightweight (reduce number of trees)
model_light = RandomForestClassifier(n_estimators=20, max_depth=8)
# Size: approximately 10MB (80% reduction)

# Quantization (optional)
import onnx
import onnxruntime
# Reduce size with ONNX format quantization
</code></pre>
<p><strong>Utilizing Lambda Layers</strong>:</p>
<pre><code class="language-bash"># Separate dependencies into layers
mkdir -p layer/python/lib/python3.9/site-packages
pip install scikit-learn numpy -t layer/python/lib/python3.9/site-packages
cd layer
zip -r layer.zip .

# Publish layer
aws lambda publish-layer-version \
    --layer-name ml-dependencies \
    --zip-file fileb://layer.zip \
    --compatible-runtimes python3.9
</code></pre>
<p><strong>Effect</strong>: Reduced deployment package shortens cold start time (10MB ‚Üí 1-2 seconds, 50MB ‚Üí 5-10 seconds)</p>
<p><strong>3. Periodic Warmup with EventBridge</strong></p>
<pre><code class="language-python">import boto3

events = boto3.client('events')
lambda_arn = 'arn:aws:lambda:us-east-1:123456789012:function:ml-inference'

# Create CloudWatch Events rule
rule_response = events.put_rule(
    Name='lambda-warmup-rule',
    ScheduleExpression='rate(5 minutes)',  # Execute every 5 minutes
    State='ENABLED',
    Description='Keep Lambda warm to avoid cold starts'
)

# Set Lambda function as target
events.put_targets(
    Rule='lambda-warmup-rule',
    Targets=[
        {
            'Id': '1',
            'Arn': lambda_arn,
            'Input': json.dumps({'warmup': True})  # Warmup flag
        }
    ]
)

print("‚úì Warmup every 5 minutes configured")
</code></pre>
<p><strong>Lambda function modification</strong>:</p>
<pre><code class="language-python">def lambda_handler(event, context):
    # Detect warmup request
    if event.get('warmup'):
        print("Warmup request - keeping instance alive")
        return {'statusCode': 200, 'body': 'warmed up'}

    # Normal inference processing
    # ...
</code></pre>
<p><strong>Effect</strong>: Avoid idle state (cost increase: approximately $1-5/month)</p>
<p><strong>4. Optimal Combination Strategy</strong></p>
<table>
<thead>
<tr>
<th>Traffic Pattern</th>
<th>Recommended Method</th>
<th>Expected Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Constant high frequency (&gt;10 req/s)</td>
<td>Provisioned concurrency</td>
<td>0% cold starts</td>
</tr>
<tr>
<td>Medium frequency (1-10 req/s)</td>
<td>Lightweighting + periodic warmup</td>
<td>&lt;5% cold starts</td>
</tr>
<tr>
<td>Low frequency (&lt;1 req/s)</td>
<td>Lightweighting only</td>
<td>1-2 second startup time</td>
</tr>
<tr>
<td>Burst handling</td>
<td>All methods combined</td>
<td>Maximum performance</td>
</tr>
</tbody>
</table>
<p><strong>Implementation example (all methods integrated)</strong>:</p>
<pre><code class="language-python"># Integrated strategy
# 1. Lightweight model (under 10MB)
# 2. Lambda Layer utilization
# 3. Provisioned concurrency (peak hours only)
# 4. EventBridge warmup (5-minute intervals)

# Cost estimate (assuming 100,000 requests/month)
# - Regular Lambda: $5
# - Provisioned: $30 (8 hours/day peak)
# - Warmup: $3
# - Total: approximately $40/month (70% reduction vs SageMaker)
</code></pre>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Amazon Web Services. (2024). <em>Amazon SageMaker Developer Guide</em>. AWS Documentation.</li>
<li>Google Cloud. (2024). <em>Vertex AI Documentation</em>. Google Cloud Documentation.</li>
<li>Microsoft Azure. (2024). <em>Azure Machine Learning Documentation</em>. Microsoft Learn.</li>
<li>HashiCorp. (2024). <em>Terraform AWS Provider Documentation</em>. Terraform Registry.</li>
<li>G√©ron, A. (2022). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (3rd ed.). O'Reilly Media.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter2-containerization.html">‚Üê Previous: Containerization and Docker</a>
<a class="nav-button" href="chapter4-monitoring.html">Next: Monitoring and Operations Management ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material is subject to change, update, or discontinuation without notice.</li>
<li>The copyright and license of this content are subject to the stated terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-23</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
