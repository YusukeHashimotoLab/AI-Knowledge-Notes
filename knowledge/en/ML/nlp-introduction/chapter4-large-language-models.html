<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Large Language Models (LLMs) - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/nlp-introduction/index.html">NLP</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 4: Large Language Models (LLMs)</h1>
<p class="subtitle">GPT, LLaMA, Prompt Engineering - The Frontiers of Language Understanding</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 35-40 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand GPT family architecture and autoregressive generation</li>
<li>‚úÖ Understand LLM training methods (pre-training, Instruction Tuning, RLHF)</li>
<li>‚úÖ Practice Prompt Engineering techniques</li>
<li>‚úÖ Understand characteristics of open-source LLMs and quantization techniques</li>
<li>‚úÖ Implement practical applications such as RAG and Function Calling</li>
<li>‚úÖ Build a complete chatbot system</li>
</ul>
<hr/>
<h2>4.1 The GPT Family</h2>
<h3>Overview of GPT Architecture</h3>
<p><strong>GPT (Generative Pre-trained Transformer)</strong> is an autoregressive language model that adopts a Decoder-only Transformer architecture.</p>
<blockquote>
<p><strong>Decoder-only</strong>: Unlike BERT which has both Encoder and Decoder, GPT consists only of Decoder and specializes in next token prediction.</p>
</blockquote>
<h3>Features of GPT Architecture</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
<th>Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Decoder-only</strong></td>
<td>Uses only self-attention mechanism</td>
<td>Simple and scalable</td>
</tr>
<tr>
<td><strong>Causal Masking</strong></td>
<td>Hides future tokens</td>
<td>Enables autoregressive generation</td>
</tr>
<tr>
<td><strong>Autoregressive</strong></td>
<td>Generates sequentially from left to right</td>
<td>Natural text generation</td>
</tr>
<tr>
<td><strong>Pre-training</strong></td>
<td>Trained on large-scale text</td>
<td>General language understanding</td>
</tr>
</tbody>
</table>
<h3>Evolution of GPT</h3>
<div class="mermaid">
graph LR
    A[GPT-1<br/>117M params<br/>2018] --&gt; B[GPT-2<br/>1.5B params<br/>2019]
    B --&gt; C[GPT-3<br/>175B params<br/>2020]
    C --&gt; D[GPT-3.5<br/>ChatGPT<br/>2022]
    D --&gt; E[GPT-4<br/>Multimodal<br/>2023]

    style A fill:#e3f2fd
    style B fill:#bbdefb
    style C fill:#90caf9
    style D fill:#64b5f6
    style E fill:#42a5f5
</div>
<h3>Comparison of GPT-2/GPT-3/GPT-4</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameters</th>
<th>Context Length</th>
<th>Key Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-2</strong></td>
<td>117M - 1.5B</td>
<td>1,024</td>
<td>High-quality text generation</td>
</tr>
<tr>
<td><strong>GPT-3</strong></td>
<td>175B</td>
<td>2,048</td>
<td>Few-shot learning, In-context Learning</td>
</tr>
<tr>
<td><strong>GPT-3.5</strong></td>
<td>~175B</td>
<td>4,096</td>
<td>Instruction Tuning, improved dialogue</td>
</tr>
<tr>
<td><strong>GPT-4</strong></td>
<td>Undisclosed</td>
<td>8,192 - 32,768</td>
<td>Multimodal, advanced reasoning</td>
</tr>
</tbody>
</table>
<h3>Autoregressive Generation</h3>
<p>GPT generates text <strong>autoregressively</strong> by predicting the next token from previous tokens.</p>
<p>$$
P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} P(x_i | x_1, x_2, \ldots, x_{i-1})
$$</p>
<p>The probability of each token is conditioned on all preceding tokens.</p>
<h3>Example: Text Generation with GPT-2</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

"""
Example: Example: Text Generation with GPT-2

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Text generation
prompt = "Artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")

# Generation parameters
generation_config = {
    'max_length': 100,
    'num_return_sequences': 3,
    'temperature': 0.8,
    'top_k': 50,
    'top_p': 0.95,
    'do_sample': True,
    'no_repeat_ngram_size': 2
}

# Generate text
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        **generation_config
    )

print("=== Text Generation with GPT-2 ===")
print(f"Prompt: '{prompt}'\n")
for i, output in enumerate(outputs):
    text = tokenizer.decode(output, skip_special_tokens=True)
    print(f"Generation {i+1}:")
    print(f"{text}\n")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Text Generation with GPT-2 ===
Prompt: 'Artificial intelligence is'

Generation 1:
Artificial intelligence is becoming more and more important in our daily lives. From smartphones to self-driving cars, AI systems are transforming the way we work and live. The technology has advanced rapidly...

Generation 2:
Artificial intelligence is a field of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence, such as visual perception...

Generation 3:
Artificial intelligence is revolutionizing industries across the globe. Companies are investing billions in AI research to develop systems that can learn from data and make decisions autonomously...
</code></pre>
<h3>Controlling Generation Parameters</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Controlling Generation Parameters

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import matplotlib.pyplot as plt
import numpy as np

# Generation with different temperatures
prompt = "The future of AI is"
temperatures = [0.3, 0.7, 1.0, 1.5]

print("=== Impact of Temperature ===\n")

for temp in temperatures:
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=50,
            temperature=temp,
            do_sample=True,
            top_k=50
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Temperature = {temp}:")
    print(f"{text}\n")

# Visualize temperature and probability distribution
def softmax_with_temperature(logits, temperature):
    """Apply softmax with temperature parameter"""
    return torch.softmax(logits / temperature, dim=-1)

# Sample logits
logits = torch.tensor([2.0, 1.0, 0.5, 0.2, 0.1])
temps = [0.5, 1.0, 2.0]

plt.figure(figsize=(12, 4))
for i, temp in enumerate(temps):
    probs = softmax_with_temperature(logits, temp)

    plt.subplot(1, 3, i+1)
    plt.bar(range(len(probs)), probs.numpy())
    plt.title(f'Temperature = {temp}')
    plt.xlabel('Token ID')
    plt.ylabel('Probability')
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nüìä Low temperature ‚Üí Deterministic (select high-probability tokens)")
print("üìä High temperature ‚Üí Diverse (also select low-probability tokens)")
</code></pre>
<h3>Beam Search and Sampling</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - transformers&gt;=4.30.0

"""
Example: Beam Search and Sampling

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

from transformers import GenerationConfig

prompt = "Machine learning can be used for"
inputs = tokenizer(prompt, return_tensors="pt")

print("=== Comparison of Generation Strategies ===\n")

# 1. Greedy Decoding
print("1. Greedy Decoding:")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=False
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 2. Beam Search
print("2. Beam Search (num_beams=5):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        num_beams=5,
        do_sample=False
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 3. Top-k Sampling
print("3. Top-k Sampling (k=50):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=True,
        top_k=50
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print()

# 4. Top-p (Nucleus) Sampling
print("4. Top-p Sampling (p=0.95):")
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        do_sample=True,
        top_p=0.95,
        top_k=0
    )
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<blockquote>
<p><strong>Important</strong>: Greedy/Beam Search are deterministic and consistent, while Sampling provides diversity but lower reproducibility.</p>
</blockquote>
<hr/>
<h2>4.2 LLM Training Methods</h2>
<h3>Overall Picture of LLM Training</h3>
<div class="mermaid">
graph TD
    A[Large-scale Text Corpus] --&gt; B[Pre-training<br/>Pre-training]
    B --&gt; C[Base Model<br/>Base LLM]
    C --&gt; D[Instruction Tuning<br/>Instruction Tuning]
    D --&gt; E[Instruction-following Model<br/>Instruction-following LLM]
    E --&gt; F[RLHF<br/>Reinforcement Learning from Human Feedback]
    F --&gt; G[Aligned Model<br/>Aligned LLM]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>
<h3>1. Pre-training</h3>
<p><strong>Pre-training</strong> involves learning next token prediction tasks on large-scale text corpora.</p>
<table>
<thead>
<tr>
<th>Element</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Hundreds of billions to trillions of tokens from web, books, papers, etc.</td>
</tr>
<tr>
<td><strong>Objective Function</strong></td>
<td>Next token prediction (Language Modeling)</td>
</tr>
<tr>
<td><strong>Optimization</strong></td>
<td>AdamW, learning rate scheduling</td>
</tr>
<tr>
<td><strong>Compute Resources</strong></td>
<td>Thousands to tens of thousands of GPUs, weeks to months</td>
</tr>
</tbody>
</table>
<p><strong>Loss Function</strong>:</p>
<p>$$
\mathcal{L}_{\text{LM}} = -\sum_{i=1}^{n} \log P(x_i | x_{<i}; $$<="" \theta)="" p="">
<h3>2. Instruction Tuning</h3>
<p><strong>Instruction Tuning</strong> fine-tunes the model to follow user instructions.</p>
<h4>Instruction Tuning Data Format</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Instruction Tuning Data Format

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

instruction_data = [
    {
        "instruction": "Summarize the following text.",
        "input": "Artificial Intelligence (AI) is technology that enables computers to think, learn, and solve problems like humans. It includes fields such as machine learning, deep learning, and natural language processing.",
        "output": "AI is technology that gives computers human-like intelligence, including machine learning and natural language processing."
    },
    {
        "instruction": "Answer the following question.",
        "input": "How far is the distance from Earth to the Moon?",
        "output": "The average distance from Earth to the Moon is approximately 384,400 kilometers (238,855 miles)."
    },
    {
        "instruction": "Fix the bug in the following code.",
        "input": "def add(a, b):\n    return a - b",
        "output": "def add(a, b):\n    return a + b"
    }
]

# Visualize data format
import pandas as pd

df = pd.DataFrame(instruction_data)
print("=== Instruction Tuning Data Format ===")
print(df.to_string(index=False))
print(f"\nNumber of data points: {len(instruction_data)} items")
</code></pre>
<h4>Instruction Tuning Implementation Example (Conceptual)</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - transformers&gt;=4.30.0

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset

# Prepare data
def format_instruction(example):
    """Convert instruction data to prompt format"""
    prompt = f"### Instruction:\n{example['instruction']}\n\n"
    if example['input']:
        prompt += f"### Input:\n{example['input']}\n\n"
    prompt += f"### Response:\n{example['output']}"
    return {"text": prompt}

# Create dataset
dataset = Dataset.from_list(instruction_data)
dataset = dataset.map(format_instruction)

print("=== Formatted Prompt Example ===")
print(dataset[0]['text'])
print("\n" + "="*50 + "\n")

# Prepare model and tokenizer (in practice, use large-scale models)
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

# Tokenize dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Training configuration (for demo)
training_args = TrainingArguments(
    output_dir="./instruction-tuned-model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no"
)

print("=== Instruction Tuning Configuration ===")
print(f"Number of epochs: {training_args.num_train_epochs}")
print(f"Batch size: {training_args.per_device_train_batch_size}")
print(f"Learning rate: {training_args.learning_rate}")
print("\n‚úì Instruction Tuning makes the model follow instructions")
</code></pre>
<h3>3. RLHF (Reinforcement Learning from Human Feedback)</h3>
<p><strong>RLHF</strong> is a technique that uses human feedback to align the model with human values.</p>
<h4>Three Steps of RLHF</h4>
<div class="mermaid">
graph LR
    A[Step 1:<br/>Supervised Fine-tuning<br/>SFT] --&gt; B[Step 2:<br/>Reward Model Training<br/>Reward Model]
    B --&gt; C[Step 3:<br/>PPO Reinforcement Learning<br/>Policy Optimization]

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e9
</div>
<h4>Step 1: Supervised Fine-Tuning (SFT)</h4>
<p>Fine-tune with high-quality human-generated dialogue data.</p>
<h4>Step 2: Reward Model Training</h4>
<p>Train a reward model from human evaluations (rankings).</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Train a reward model from human evaluations (rankings).

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

# Reward model data format (conceptual)
reward_data = [
    {
        "prompt": "What is artificial intelligence?",
        "response_1": "Artificial Intelligence (AI) is technology that enables computers to think like humans.",
        "response_2": "I don't know.",
        "preference": 1  # response_1 is better
    },
    {
        "prompt": "How to reverse a list in Python?",
        "response_1": "Use the list.reverse() method or list[::-1] slicing.",
        "response_2": "Cannot do it.",
        "preference": 1
    }
]

import pandas as pd
df_reward = pd.DataFrame(reward_data)
print("=== Reward Model Training Data ===")
print(df_reward.to_string(index=False))
print("\n‚úì Learn reward function from human preferences")
</code></pre>
<h4>Step 3: PPO (Proximal Policy Optimization)</h4>
<p>Optimize the model using reinforcement learning with the reward model.</p>
<p><strong>Objective Function</strong>:</p>
<p>$$
\mathcal{L}^{\text{PPO}} = \mathbb{E}_{x,y \sim \pi_\theta} \left[ r(x, y) - \beta \cdot \text{KL}(\pi_\theta || \pi_{\text{ref}}) \right]
$$</p>
<ul>
<li>$r(x, y)$: Reward model score</li>
<li>$\beta$: KL regularization coefficient</li>
<li>$\pi_{\text{ref}}$: Original model (to prevent large deviation)</li>
</ul>
<blockquote>
<p><strong>Effect of RLHF</strong>: ChatGPT's human-like dialogue capabilities are realized through RLHF.</p>
</blockquote>
<h3>4. Parameter-Efficient Fine-Tuning (PEFT)</h3>
<p>Since fine-tuning entire large-scale models is computationally expensive, <strong>parameter-efficient fine-tuning</strong> is important.</p>
<h4>LoRA (Low-Rank Adaptation)</h4>
<p>LoRA applies low-rank decomposition to model weight matrices.</p>
<p>$$
W' = W + \Delta W = W + BA
$$</p>
<ul>
<li>$W$: Original weight matrix (frozen)</li>
<li>$B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$: Trainable low-rank matrices</li>
<li>$r \ll \min(d, k)$: Rank (e.g., 8, 16)</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - transformers&gt;=4.30.0

"""
Example: $$
W' = W + \Delta W = W + BA
$$

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

# Load base model
model_name = "gpt2"
base_model = AutoModelForCausalLM.from_pretrained(model_name)

# LoRA configuration
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # Rank
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,
    target_modules=["c_attn"]  # Modules to apply
)

# Create LoRA model
lora_model = get_peft_model(base_model, lora_config)

# Compare parameter counts
total_params = sum(p.numel() for p in lora_model.parameters())
trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)

print("=== LoRA Parameter Efficiency ===")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Trainable ratio: {100 * trainable_params / total_params:.2f}%")
print(f"\n‚úì Fine-tune with only {100 * trainable_params / total_params:.2f}% of parameters")
print(lora_model.print_trainable_parameters())
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== LoRA Parameter Efficiency ===
Total parameters: 124,439,808
Trainable parameters: 294,912
Trainable ratio: 0.24%

‚úì Fine-tune with only 0.24% of parameters
trainable params: 294,912 || all params: 124,439,808 || trainable%: 0.24
</code></pre>
<h4>Adapter Layers</h4>
<p>Add small bottleneck networks to each layer of the Transformer.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Trainable Parameters</th>
<th>Memory</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full Fine-tuning</strong></td>
<td>100%</td>
<td>High</td>
<td>Slow</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>0.1% - 1%</td>
<td>Low</td>
<td>Fast</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>2% - 5%</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>Prompt Tuning</strong></td>
<td>&lt; 0.1%</td>
<td>Very low</td>
<td>Very fast</td>
</tr>
</tbody>
</table>
<hr/>
<h2>4.3 Prompt Engineering</h2>
<h3>What is Prompt Engineering</h3>
<p><strong>Prompt Engineering</strong> is the technique of designing inputs (prompts) to elicit desired outputs from LLMs.</p>
<blockquote>
<p>"A good prompt can increase model performance tenfold" - OpenAI</p>
</blockquote>
<h3>1. Zero-shot Learning</h3>
<p>Directly ask questions to a pre-trained model without providing any task examples.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - transformers&gt;=4.30.0

from transformers import pipeline

# GPT-2 pipeline
generator = pipeline('text-generation', model='gpt2')

# Zero-shot prompt
zero_shot_prompt = """
Question: What is the capital of France?
Answer:
"""

result = generator(zero_shot_prompt, max_length=50, num_return_sequences=1)
print("=== Zero-shot Learning ===")
print(result[0]['generated_text'])
</code></pre>
<h3>2. Few-shot Learning</h3>
<p>Provide a few task examples before giving a new input.</p>
<pre><code class="language-python"># Few-shot prompt (In-Context Learning)
few_shot_prompt = """
Translate English to French:

English: Hello
French: Bonjour

English: Thank you
French: Merci

English: Good morning
French: Bon matin

English: How are you?
French:
"""

result = generator(few_shot_prompt, max_length=100, num_return_sequences=1)
print("\n=== Few-shot Learning ===")
print(result[0]['generated_text'])
print("\n‚úì By showing examples, the model learns patterns and performs translation")
</code></pre>
<h3>3. Chain-of-Thought (CoT) Prompting</h3>
<p><strong>Chain-of-Thought</strong> is a technique that prompts the model to generate intermediate reasoning steps.</p>
<pre><code class="language-python"># Standard prompt (direct answer)
standard_prompt = """
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?
Answer:
"""

# Chain-of-Thought prompt
cot_prompt = """
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?

Let's think step by step:
1. Roger starts with 5 tennis balls.
2. He buys 2 cans, each containing 3 tennis balls.
3. So he gets 2 √ó 3 = 6 new tennis balls.
4. Total tennis balls = 5 + 6 = 11.

Answer: 11 tennis balls.

Question: A restaurant had 23 customers. Then 11 more customers arrived.
Each customer ordered 2 drinks. How many drinks were ordered in total?

Let's think step by step:
"""

result = generator(cot_prompt, max_length=200, num_return_sequences=1)
print("\n=== Chain-of-Thought Prompting ===")
print(result[0]['generated_text'])
print("\n‚úì Step-by-step reasoning solves complex problems")
</code></pre>
<h4>Effectiveness of CoT (Research Results)</h4>
<table>
<thead>
<tr>
<th>Task</th>
<th>Standard Prompt</th>
<th>CoT Prompt</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Math problems</td>
<td>34%</td>
<td>78%</td>
<td>+44%</td>
</tr>
<tr>
<td>Common sense reasoning</td>
<td>61%</td>
<td>89%</td>
<td>+28%</td>
</tr>
<tr>
<td>Logic puzzles</td>
<td>42%</td>
<td>81%</td>
<td>+39%</td>
</tr>
</tbody>
</table>
<h3>4. Best Practices for Prompt Design</h3>
<pre><code class="language-python"># ‚ùå Bad prompt
bad_prompt = "Summarize this."

# ‚úÖ Good prompt
good_prompt = """
Task: Summarize the following article in 3 bullet points.
Focus on key findings and implications.

Article: [Long article text...]

Summary:
-
"""

# Principles of prompt design
prompt_principles = {
    "Clear instructions": "Describe the task specifically",
    "Format specification": "Show the desired output format",
    "Provide context": "Include necessary background information",
    "State constraints": "Specify character limits, style, etc.",
    "Show examples": "Demonstrate expected output with few-shot",
    "Break down steps": "Divide complex tasks into stages"
}

print("=== Principles of Prompt Design ===")
for principle, description in prompt_principles.items():
    print(f"‚úì {principle}: {description}")

# Practical example: Structured prompt
structured_prompt = """
Role: You are an expert Python programmer.

Task: Review the following code and provide feedback.

Code:
```python
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
```

Output Format:
1. Code Quality (1-10):
2. Issues Found:
3. Suggestions:
4. Improved Code:

Analysis:
"""

print("\n=== Structured Prompt Example ===")
print(structured_prompt)
</code></pre>
<h3>5. Prompt Management with LangChain</h3>
<pre><code class="language-python">from langchain import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import HuggingFacePipeline

# Wrap HuggingFace pipeline with LangChain
llm = HuggingFacePipeline(pipeline=generator)

# Create prompt template
template = """
Question: {question}

Context: {context}

Please provide a detailed answer based on the context above.

Answer:
"""

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=template
)

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Execute
question = "What is machine learning?"
context = "Machine learning is a subset of artificial intelligence that enables systems to learn from data."

result = chain.run(question=question, context=context)

print("=== LangChain Prompt Template ===")
print(f"Question: {question}")
print(f"Context: {context}")
print(f"\nGenerated answer:")
print(result)

print("\n‚úì Manage reusable prompt templates with LangChain")
</code></pre>
<hr/>
<h2>4.4 Open-Source LLMs</h2>
<h3>Major Open-Source LLMs</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Developer</th>
<th>Parameters</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LLaMA</strong></td>
<td>Meta AI</td>
<td>7B - 65B</td>
<td>High performance, research use</td>
</tr>
<tr>
<td><strong>LLaMA-2</strong></td>
<td>Meta AI</td>
<td>7B - 70B</td>
<td>Commercial use, Chat version available</td>
</tr>
<tr>
<td><strong>Falcon</strong></td>
<td>TII</td>
<td>7B - 180B</td>
<td>High-quality dataset</td>
</tr>
<tr>
<td><strong>ELYZA</strong></td>
<td>ELYZA Inc.</td>
<td>7B - 13B</td>
<td>Japanese-specialized</td>
</tr>
<tr>
<td><strong>rinna</strong></td>
<td>rinna</td>
<td>3.6B - 36B</td>
<td>Japanese, commercial use allowed</td>
</tr>
</tbody>
</table>
<h3>1. LLaMA / LLaMA-2</h3>
<p><strong>LLaMA (Large Language Model Meta AI)</strong> is an open-source LLM developed by Meta AI.</p>
<h4>Features of LLaMA</h4>
<ul>
<li><strong>Efficient architecture</strong>: Equivalent performance to GPT-3 with fewer parameters</li>
<li><strong>Pre-Normalization</strong>: Uses RMSNorm</li>
<li><strong>SwiGLU activation function</strong>: Improved performance</li>
<li><strong>Rotary Positional Embeddings (RoPE)</strong>: Position encoding</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

"""
Example: Features of LLaMA

Purpose: Demonstrate data manipulation and preprocessing
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load LLaMA-2 model (7B model)
model_name = "meta-llama/Llama-2-7b-hf"  # From Hugging Face Hub

# Note: Access request required on Hugging Face to use LLaMA-2
# Using GPT-2 here for demo purposes

model_name = "gpt2"  # Replace with LLaMA-2 in actual environment
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

print("=== LLaMA Family Information ===")
print(f"Model: {model_name}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Architecture: Decoder-only Transformer")
print(f"Context length: 2048 (LLaMA) / 4096 (LLaMA-2)")

# LLaMA-2 performance (benchmark results)
benchmarks = {
    "MMLU": {"LLaMA-7B": 35.1, "LLaMA-2-7B": 45.3, "GPT-3.5": 70.0},
    "HellaSwag": {"LLaMA-7B": 76.1, "LLaMA-2-7B": 77.2, "GPT-3.5": 85.5},
    "HumanEval": {"LLaMA-7B": 10.5, "LLaMA-2-7B": 12.8, "GPT-3.5": 48.1}
}

import pandas as pd
df_bench = pd.DataFrame(benchmarks)
print("\n=== Benchmark Performance Comparison ===")
print(df_bench.to_string())
</code></pre>
<h3>2. Japanese LLMs (ELYZA, rinna)</h3>
<h4>ELYZA-japanese-Llama-2</h4>
<p>A model that continues pre-training LLaMA-2 with Japanese text.</p>
<pre><code class="language-python"># ELYZA-japanese-Llama-2 usage example (conceptual)
# model_name = "elyza/ELYZA-japanese-Llama-2-7b"

japanese_prompt = """
Please answer the following question in Japanese.

Question: What is the difference between machine learning and deep learning?

Answer:
"""

print("=== Japanese LLM (ELYZA) ===")
print("‚úì LLaMA-2 base + Japanese additional training")
print("‚úì Natural dialogue in Japanese possible")
print("‚úì Commercial use allowed (LLaMA-2 license)")
print(f"\nPrompt example:\n{japanese_prompt}")

# rinna GPT-NeoX
print("\n=== Japanese LLM (rinna) ===")
print("‚úì GPT-NeoX architecture")
print("‚úì Trained on Japanese Wikipedia, etc.")
print("‚úì 3.6B, 36B models available")
print("‚úì Commercial use allowed (MIT License)")
</code></pre>
<h3>3. Model Quantization</h3>
<p><strong>Quantization</strong> is a technique to reduce memory and computation by lowering model precision.</p>
<h4>Types of Quantization</h4>
<table>
<thead>
<tr>
<th>Precision</th>
<th>Memory Reduction</th>
<th>Performance Loss</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FP32 (Original)</strong></td>
<td>-</td>
<td>-</td>
<td>Training</td>
</tr>
<tr>
<td><strong>FP16</strong></td>
<td>50%</td>
<td>Almost none</td>
<td>Inference</td>
</tr>
<tr>
<td><strong>8-bit</strong></td>
<td>75%</td>
<td>Small</td>
<td>Inference, fine-tuning</td>
</tr>
<tr>
<td><strong>4-bit</strong></td>
<td>87.5%</td>
<td>Medium</td>
<td>Memory-constrained environments</td>
</tr>
</tbody>
</table>
<h4>8-bit Quantization Implementation</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

"""
Example: 8-bit Quantization Implementation

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 8-bit quantization configuration
quantization_config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# 4-bit quantization configuration (QLoRA)
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_use_double_quant=True
)

# Load model (8-bit)
# model_8bit = AutoModelForCausalLM.from_pretrained(
#     "meta-llama/Llama-2-7b-hf",
#     quantization_config=quantization_config_8bit,
#     device_map="auto"
# )

print("=== Model Quantization ===")
print("\n8-bit quantization:")
print("  ‚úì Memory usage: ~7GB (7B model, 75% reduction from FP32)")
print("  ‚úì Performance loss: Minimal (1-2%)")
print("  ‚úì Inference speed: Almost the same as FP32")

print("\n4-bit quantization (QLoRA):")
print("  ‚úì Memory usage: ~3.5GB (7B model, 87.5% reduction from FP32)")
print("  ‚úì Performance loss: Small (5-10%)")
print("  ‚úì Fine-tuning possible")

# Calculate memory usage
def calculate_model_memory(num_params, precision):
    """Calculate model memory usage"""
    bytes_per_param = {
        'fp32': 4,
        'fp16': 2,
        '8bit': 1,
        '4bit': 0.5
    }
    memory_gb = num_params * bytes_per_param[precision] / (1024**3)
    return memory_gb

params_7b = 7_000_000_000

print("\n=== Memory Usage for 7B Model ===")
for precision in ['fp32', 'fp16', '8bit', '4bit']:
    memory = calculate_model_memory(params_7b, precision)
    print(f"{precision.upper():6s}: {memory:.2f} GB")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Memory Usage for 7B Model ===
FP32  : 26.08 GB
FP16  : 13.04 GB
8BIT  : 6.52 GB
4BIT  : 3.26 GB
</code></pre>
<blockquote>
<p><strong>QLoRA (Quantized LoRA)</strong>: Combines 4-bit quantization with LoRA, enabling fine-tuning of large models on consumer GPUs.</p>
</blockquote>
<hr/>
<h2>4.5 Practical Applications of LLMs</h2>
<h3>1. Retrieval-Augmented Generation (RAG)</h3>
<p><strong>RAG</strong> is a technique that retrieves relevant information from an external knowledge base and generates answers based on it.</p>
<div class="mermaid">
graph LR
    A[User Query] --&gt; B[Retrieval<br/>Retriever]
    B --&gt; C[Knowledge Base<br/>Vector DB]
    C --&gt; D[Relevant Documents]
    D --&gt; E[LLM<br/>Generator]
    A --&gt; E
    E --&gt; F[Answer Generation]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#c8e6c9
</div>
<h4>RAG Implementation Example</h4>
<pre><code class="language-python">from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# Knowledge base text
documents = [
    "Artificial Intelligence (AI) is technology that enables computers to think and learn like humans.",
    "Machine learning is a branch of AI that learns patterns from data.",
    "Deep learning is a machine learning method using multi-layer neural networks.",
    "Natural Language Processing (NLP) is technology that enables computers to understand human language.",
    "Transformer is a revolutionary neural network architecture introduced in 2017."
]

# Text splitting
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)
texts = text_splitter.create_documents(documents)

# Embedding model
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store
vectorstore = FAISS.from_documents(texts, embeddings)

print("=== RAG System Construction ===")
print(f"Number of documents: {len(documents)}")
print(f"Number of chunks: {len(texts)}")
print(f"Embedding model: sentence-transformers/all-MiniLM-L6-v2")

# Search test
query = "What is Transformer?"
relevant_docs = vectorstore.similarity_search(query, k=2)

print(f"\nQuery: {query}")
print("\nRelevant documents:")
for i, doc in enumerate(relevant_docs, 1):
    print(f"{i}. {doc.page_content}")

# Integrate with LLM (conceptual)
# llm = HuggingFacePipeline(...)
# qa_chain = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=vectorstore.as_retriever(),
#     return_source_documents=True
# )
# result = qa_chain({"query": query})

print("\n‚úì RAG enables LLM to answer with latest/domain-specific knowledge")
</code></pre>
<h3>2. Function Calling</h3>
<p><strong>Function Calling</strong> is a technique that enables LLMs to call external tools and APIs.</p>
<pre><code class="language-python">import json

# Define available functions
available_functions = {
    "get_weather": {
        "description": "Get current weather for specified city",
        "parameters": {
            "city": {"type": "string", "description": "City name"}
        }
    },
    "calculate": {
        "description": "Perform mathematical calculation",
        "parameters": {
            "expression": {"type": "string", "description": "Mathematical expression"}
        }
    },
    "search_web": {
        "description": "Search the web",
        "parameters": {
            "query": {"type": "string", "description": "Search query"}
        }
    }
}

# Function implementations (dummy)
def get_weather(city):
    """Get weather information (dummy)"""
    return f"The weather in {city} is sunny, temperature is 25 degrees."

def calculate(expression):
    """Execute calculation"""
    try:
        result = eval(expression)
        return f"{expression} = {result}"
    except:
        return "Calculation error"

def search_web(query):
    """Web search (dummy)"""
    return f"Search results for '{query}': [Related information...]"

# Function Calling prompt
def create_function_calling_prompt(user_query, functions):
    """Generate prompt for Function Calling"""
    functions_desc = json.dumps(functions, ensure_ascii=False, indent=2)

    prompt = f"""
You are a helpful assistant with access to the following functions:

{functions_desc}

User query: {user_query}

Based on the query, determine which function to call and with what parameters.
Respond in JSON format:
{{
    "function": "function_name",
    "parameters": {{...}}
}}

Response:
"""
    return prompt

# Test
user_query = "What's the weather in Tokyo?"
prompt = create_function_calling_prompt(user_query, available_functions)

print("=== Function Calling ===")
print(f"User query: {user_query}")
print(f"\nPrompt:\n{prompt}")

# Expected response (actually generated by LLM)
function_call = {
    "function": "get_weather",
    "parameters": {"city": "Tokyo"}
}

print(f"\nLLM's function selection:")
print(json.dumps(function_call, ensure_ascii=False, indent=2))

# Execute function
if function_call["function"] == "get_weather":
    result = get_weather(**function_call["parameters"])
    print(f"\nExecution result: {result}")

print("\n‚úì LLM can use tools to retrieve and process information")
</code></pre>
<h3>3. Multi-turn Conversation</h3>
<p>Maintain conversation history to achieve context-aware dialogue.</p>
<pre><code class="language-python">from collections import deque

class ConversationManager:
    """Class to manage conversation history"""

    def __init__(self, max_history=10):
        self.history = deque(maxlen=max_history)

    def add_message(self, role, content):
        """Add message"""
        self.history.append({"role": role, "content": content})

    def get_prompt(self, system_message=""):
        """Generate prompt from conversation history"""
        prompt_parts = []

        if system_message:
            prompt_parts.append(f"System: {system_message}\n")

        for msg in self.history:
            prompt_parts.append(f"{msg['role'].capitalize()}: {msg['content']}")

        prompt_parts.append("Assistant:")
        return "\n".join(prompt_parts)

    def clear(self):
        """Clear history"""
        self.history.clear()

# Usage example
conv_manager = ConversationManager(max_history=6)

system_msg = "You are a helpful AI assistant."

# Conversation simulation
conversation = [
    ("User", "Hello!"),
    ("Assistant", "Hello! How can I help you?"),
    ("User", "Please teach me about Python learning methods."),
    ("Assistant", "For Python learning, I recommend first learning basic syntax, then working on actual projects."),
    ("User", "What projects are good for beginners?"),
]

print("=== Multi-turn Conversation ===\n")

for i, (role, content) in enumerate(conversation):
    conv_manager.add_message(role, content)

    if role == "User":
        print(f"{role}: {content}")
        prompt = conv_manager.get_prompt(system_msg)
        print(f"\n[Generated Prompt]")
        print(prompt)
        print("\n" + "="*50 + "\n")

print("‚úì Conversation history enables context-aware dialogue")
print(f"‚úì History length: {len(conv_manager.history)} messages")
</code></pre>
<h3>4. Complete Chatbot Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class SimpleChatbot:
    """Simple chatbot"""

    def __init__(self, model_name="gpt2", max_history=5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.conversation = ConversationManager(max_history=max_history)
        self.system_message = "You are a helpful AI assistant."

    def generate_response(self, user_input, max_length=100):
        """Generate response to user input"""
        # Add to conversation history
        self.conversation.add_message("User", user_input)

        # Generate prompt
        prompt = self.conversation.get_prompt(self.system_message)

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=len(inputs['input_ids'][0]) + max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # Decode
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Remove prompt part
        response = response[len(prompt):].strip()

        # Add to conversation history
        self.conversation.add_message("Assistant", response)

        return response

    def chat(self):
        """Dialogue loop"""
        print("=== Chatbot Started ===")
        print("Type 'quit' to exit\n")

        while True:
            user_input = input("You: ")

            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("Assistant: Goodbye!")
                break

            response = self.generate_response(user_input)
            print(f"Assistant: {response}\n")

# Instantiate chatbot
chatbot = SimpleChatbot(model_name="gpt2", max_history=5)

# Demo dialogue (in practice use chatbot.chat() for dialogue loop)
demo_inputs = [
    "Hello!",
    "What is AI?",
    "Can you explain more?"
]

print("=== Chatbot Demo ===\n")
for user_input in demo_inputs:
    print(f"You: {user_input}")
    response = chatbot.generate_response(user_input, max_length=50)
    print(f"Assistant: {response}\n")

print("‚úì Complete chatbot maintaining conversation history")
print("‚úì Context-aware response generation")
print("‚úì Extensible design (can add RAG, Function Calling, etc.)")
</code></pre>
<hr/>
<h2>4.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>GPT Family</strong></p>
<ul>
<li>Decoder-only architecture</li>
<li>Autoregressive text generation</li>
<li>Evolution from GPT-2 to GPT-4</li>
<li>Control of generation parameters (temperature, top-k, top-p)</li>
</ul></li>
<li><p><strong>LLM Training Methods</strong></p>
<ul>
<li>Pre-training: Next token prediction on large corpus</li>
<li>Instruction Tuning: Towards instruction-following models</li>
<li>RLHF: Alignment through human feedback</li>
<li>PEFT: Efficient fine-tuning with LoRA, Adapter</li>
</ul></li>
<li><p><strong>Prompt Engineering</strong></p>
<ul>
<li>Zero-shot / Few-shot Learning</li>
<li>Chain-of-Thought Prompting</li>
<li>Best practices for prompt design</li>
<li>Prompt management with LangChain</li>
</ul></li>
<li><p><strong>Open-Source LLMs</strong></p>
<ul>
<li>LLaMA, Falcon, Japanese LLMs</li>
<li>Model quantization (8-bit, 4-bit)</li>
<li>Efficient fine-tuning with QLoRA</li>
</ul></li>
<li><p><strong>Practical Applications</strong></p>
<ul>
<li>RAG: Generation leveraging external knowledge</li>
<li>Function Calling: Tool/API integration</li>
<li>Multi-turn Conversation: Context-aware dialogue</li>
<li>Complete chatbot implementation</li>
</ul></li>
</ol>
<h3>Best Practices for LLM Utilization</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model Selection</strong></td>
<td>Balance size and performance according to task</td>
</tr>
<tr>
<td><strong>Prompt Design</strong></td>
<td>Clear instructions, examples, structure</td>
</tr>
<tr>
<td><strong>Memory Efficiency</strong></td>
<td>Utilize quantization, PEFT</td>
</tr>
<tr>
<td><strong>Knowledge Update</strong></td>
<td>Integrate latest information with RAG</td>
</tr>
<tr>
<td><strong>Safety</strong></td>
<td>Output validation, harmful content filtering</td>
</tr>
</tbody>
</table>
<h3>Next Steps</h3>
<p>To deepen your understanding of large language models:</p>
<ul>
<li>Try larger models (LLaMA-2 13B/70B)</li>
<li>Fine-tune with custom data (LoRA)</li>
<li>Deploy RAG systems to production</li>
<li>Explore multimodal LLMs (GPT-4V, etc.)</li>
<li>Learn LLMOps (operations and monitoring)</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p>Explain the difference between GPT's Decoder-only architecture and BERT's Encoder-only architecture. What tasks is each suited for?</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>GPT (Decoder-only)</strong>:</p>
<ul>
<li>Structure: Self-attention mechanism + Causal Masking (doesn't see future)</li>
<li>Training: Next token prediction (autoregressive)</li>
<li>Strengths: Text generation, dialogue, creative writing</li>
<li>Use cases: ChatGPT, code generation, text composition</li>
</ul>
<p><strong>BERT (Encoder-only)</strong>:</p>
<ul>
<li>Structure: Bidirectional self-attention mechanism (references all tokens)</li>
<li>Training: Masked Language Modeling (fill-in-the-blank)</li>
<li>Strengths: Text understanding, classification, extraction</li>
<li>Use cases: Sentiment analysis, named entity recognition, question answering</li>
</ul>
<p><strong>Usage Guidelines</strong>:</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Recommended Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text generation</td>
<td>GPT</td>
</tr>
<tr>
<td>Document classification</td>
<td>BERT</td>
</tr>
<tr>
<td>Question answering (extractive)</td>
<td>BERT</td>
</tr>
<tr>
<td>Question answering (generative)</td>
<td>GPT</td>
</tr>
<tr>
<td>Summarization</td>
<td>GPT (or Encoder-Decoder)</td>
</tr>
</tbody>
</table>
</details>
<h3>Exercise 2 (Difficulty: medium)</h3>
<p>Explain why Chain-of-Thought (CoT) Prompting is effective for complex reasoning tasks. Provide concrete examples.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Why CoT is Effective</strong>:</p>
<ol>
<li><strong>Explicit intermediate reasoning</strong>: Verbalizing steps helps LLM organize logical thinking</li>
<li><strong>Early error detection</strong>: Easier to notice mistakes at each step</li>
<li><strong>Complexity decomposition</strong>: Divide difficult problems into smaller sub-problems</li>
<li><strong>Enhanced In-context Learning</strong>: Learn reasoning patterns</li>
</ol>
<p><strong>Concrete Example: Math Problem</strong></p>
<pre><code class="language-python"># ‚ùå Standard prompt
prompt_standard = """
Question: A store had 25 apples. They sold 8 apples in the morning
and 12 apples in the afternoon. How many apples are left?
Answer:
"""

# ‚úÖ CoT prompt
prompt_cot = """
Question: A store had 25 apples. They sold 8 apples in the morning
and 12 apples in the afternoon. How many apples are left?

Let's solve this step by step:
1. The store started with 25 apples.
2. They sold 8 apples in the morning: 25 - 8 = 17 apples remaining.
3. They sold 12 apples in the afternoon: 17 - 12 = 5 apples remaining.

Answer: 5 apples are left.
"""
</code></pre>
<p><strong>Demonstrated Effectiveness</strong> (from research):</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Standard</th>
<th>CoT</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K (Math)</td>
<td>17.9%</td>
<td>58.1%</td>
<td>+40.2%</td>
</tr>
<tr>
<td>SVAMP (Math)</td>
<td>69.4%</td>
<td>78.7%</td>
<td>+9.3%</td>
</tr>
</tbody>
</table>
</details>
<h3>Exercise 3 (Difficulty: medium)</h3>
<p>Explain why LoRA (Low-Rank Adaptation) is parameter-efficient using mathematical formulas.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Principle of LoRA</strong>:</p>
<p>Instead of updating the original weight matrix $W \in \mathbb{R}^{d \times k}$, use low-rank decomposition:</p>
<p>$$
W' = W + \Delta W = W + BA
$$</p>
<ul>
<li>$W$: Original weights (frozen, not trained)</li>
<li>$B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$: Trainable matrices</li>
<li>$r \ll \min(d, k)$: Rank (e.g., $r=8$)</li>
</ul>
<p><strong>Parameter Reduction Calculation</strong>:</p>
<p>Example: $d = 4096$, $k = 4096$, $r = 8$:</p>
<ul>
<li>Original weights: $4096 \times 4096 = 16,777,216$ parameters</li>
<li>LoRA: $4096 \times 8 + 8 \times 4096 = 65,536$ parameters</li>
<li>Reduction rate: $\frac{65,536}{16,777,216} \approx 0.39\%$ (99.6% reduction)</li>
</ul>
<p><strong>Implementation Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Implementation Example:

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8):
        super().__init__()
        # Original weights (frozen)
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False

        # LoRA matrices (trainable)
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.randn(out_features, rank))

        self.rank = rank

    def forward(self, x):
        # W*x + B*A*x
        return x @ self.weight.T + x @ self.lora_A.T @ self.lora_B.T

# Example
layer = LoRALayer(4096, 4096, rank=8)
total = sum(p.numel() for p in layer.parameters())
trainable = sum(p.numel() for p in layer.parameters() if p.requires_grad)

print(f"Total parameters: {total:,}")
print(f"Trainable: {trainable:,} ({100*trainable/total:.2f}%)")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Total parameters: 16,842,752
Trainable: 65,536 (0.39%)
</code></pre>
</details>
<h3>Exercise 4 (Difficulty: hard)</h3>
<p>Implement a RAG (Retrieval-Augmented Generation) system. Meet the following requirements:</p>
<ul>
<li>Retrieval from custom knowledge base</li>
<li>Scoring of relevant documents</li>
<li>Answer generation using retrieval results</li>
</ul>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SimpleRAG:
    """Simple RAG system"""

    def __init__(self, knowledge_base, embedding_model="sentence-transformers/all-MiniLM-L6-v2"):
        self.knowledge_base = knowledge_base
        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)
        self.model = AutoModel.from_pretrained(embedding_model)

        # Pre-compute embeddings for knowledge base
        self.kb_embeddings = self._embed_documents(knowledge_base)

    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _embed_documents(self, documents):
        """Convert documents to embedding vectors"""
        encoded = self.tokenizer(documents, padding=True, truncation=True, return_tensors='pt')

        with torch.no_grad():
            model_output = self.model(**encoded)

        embeddings = self._mean_pooling(model_output, encoded['attention_mask'])
        return embeddings.numpy()

    def retrieve(self, query, top_k=3):
        """Retrieve documents relevant to query"""
        # Query embedding
        query_embedding = self._embed_documents([query])

        # Calculate cosine similarity
        similarities = cosine_similarity(query_embedding, self.kb_embeddings)[0]

        # Get top-k documents
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'document': self.knowledge_base[idx],
                'score': similarities[idx]
            })

        return results

    def generate_answer(self, query, retrieved_docs):
        """Generate answer using retrieval results (create prompt)"""
        context = "\n".join([f"- {doc['document']}" for doc in retrieved_docs])

        prompt = f"""
Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:
"""
        return prompt

# Knowledge base
knowledge_base = [
    "Machine learning is a field of AI where computers learn from data.",
    "Deep learning is a machine learning method using multi-layer neural networks.",
    "Transformer is a revolutionary architecture introduced in 2017.",
    "BERT is a text understanding model using bidirectional Transformers.",
    "GPT is a Decoder-only Transformer excellent at text generation.",
    "Fine-tuning is a method to adapt pre-trained models to specific tasks.",
    "LoRA is a parameter-efficient fine-tuning method.",
    "RAG is a technique combining retrieval and generation."
]

# Create RAG system
rag = SimpleRAG(knowledge_base)

# Test
query = "Tell me about Transformer"

print("=== RAG System ===")
print(f"Knowledge base: {len(knowledge_base)} items\n")
print(f"Query: {query}\n")

# Retrieval
retrieved = rag.retrieve(query, top_k=3)

print("Retrieval results:")
for i, doc in enumerate(retrieved, 1):
    print(f"{i}. [Score: {doc['score']:.3f}] {doc['document']}")

# Prompt for answer generation
prompt = rag.generate_answer(query, retrieved)
print(f"\nGenerated prompt:\n{prompt}")

print("\n‚úì RAG system retrieves relevant knowledge and uses it for answer generation")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== RAG System ===
Knowledge base: 8 items

Query: Tell me about Transformer

Retrieval results:
1. [Score: 0.712] Transformer is a revolutionary architecture introduced in 2017.
2. [Score: 0.623] BERT is a text understanding model using bidirectional Transformers.
3. [Score: 0.589] GPT is a Decoder-only Transformer excellent at text generation.

Generated prompt:

Based on the following context, answer the question.

Context:
- Transformer is a revolutionary architecture introduced in 2017.
- BERT is a text understanding model using bidirectional Transformers.
- GPT is a Decoder-only Transformer excellent at text generation.

Question: Tell me about Transformer

Answer:
</code></pre>
</details>
<h3>Exercise 5 (Difficulty: hard)</h3>
<p>Explain the three steps of RLHF (SFT, Reward Model Training, PPO), their roles, and why they need to be performed in this order.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></p>
<ul>
<li><strong>Role</strong>: Fine-tune model with high-quality human-generated dialogue data</li>
<li><strong>Purpose</strong>: Adapt base model to dialogue tasks</li>
<li><strong>Data</strong>: Pairs of (prompt, ideal response)</li>
<li><strong>Necessity</strong>: Pre-trained models are not optimized for dialogue</li>
</ul>
<p><strong>Step 2: Reward Model Training</strong></p>
<ul>
<li><strong>Role</strong>: Train a reward model that learns human preferences</li>
<li><strong>Purpose</strong>: Enable automatic evaluation of "good response" vs "bad response"</li>
<li><strong>Data</strong>: Rankings of multiple responses to the same prompt</li>
<li><strong>Necessity</strong>: Provide reward signal needed for reinforcement learning</li>
</ul>
<p><strong>Loss Function</strong>:</p>
<p>$$
\mathcal{L}_{\text{reward}} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$</p>
<ul>
<li>$y_w$: Preferred response</li>
<li>$y_l$: Non-preferred response</li>
<li>$r(x, y)$: Reward score</li>
</ul>
<p><strong>Step 3: PPO (Proximal Policy Optimization)</strong></p>
<ul>
<li><strong>Role</strong>: Optimize model using reward model</li>
<li><strong>Purpose</strong>: Learn to generate responses aligned with human preferences</li>
<li><strong>Method</strong>: Reinforcement learning (PPO algorithm)</li>
<li><strong>Constraint</strong>: KL regularization to prevent large deviation from original model</li>
</ul>
<p><strong>Objective Function</strong>:</p>
<p>$$
\mathcal{L}^{\text{PPO}} = \mathbb{E}_{x,y \sim \pi_\theta} \left[ r(x, y) - \beta \cdot \text{KL}(\pi_\theta || \pi_{\text{SFT}}) \right]
$$</p>
<p><strong>Why This Order is Necessary</strong>:</p>
<ol>
<li><strong>SFT First</strong>:
<ul>
<li>Base model is unfamiliar with dialogue</li>
<li>Serves as initial policy for reinforcement learning</li>
<li>Also used for generating data for reward model training</li>
</ul></li>
<li><strong>Reward Model Second</strong>:
<ul>
<li>Provides reward signal needed for PPO</li>
<li>Learns from human ranking data</li>
<li>Evaluates responses generated by SFT model</li>
</ul></li>
<li><strong>PPO Last</strong>:
<ul>
<li>Cannot optimize without reward model</li>
<li>Uses SFT model as initial policy</li>
<li>KL regularization prevents large deviation from SFT</li>
</ul></li>
</ol>
<p><strong>Overall Flow</strong>:</p>
<div class="mermaid">
graph TD
    A[Pre-trained Model] --&gt; B[SFT]
    B --&gt; C[SFT Model]
    C --&gt; D[Response Generation]
    D --&gt; E[Human Ranking]
    E --&gt; F[Reward Model Training]
    F --&gt; G[Reward Model]
    C --&gt; H[PPO]
    G --&gt; H
    H --&gt; I[Aligned Model<br/>ChatGPT]
</div>
<p><strong>Effects</strong>:</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Performance Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base Model</td>
<td>Dialogue quality: Low</td>
</tr>
<tr>
<td>SFT</td>
<td>Dialogue quality: Medium (follows instructions)</td>
</tr>
<tr>
<td>RLHF (PPO)</td>
<td>Dialogue quality: High (aligned with human preferences)</td>
</tr>
</tbody>
</table>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Vaswani, A., et al. (2017). <em>Attention is All You Need</em>. NeurIPS.</li>
<li>Radford, A., et al. (2019). <em>Language Models are Unsupervised Multitask Learners</em> (GPT-2). OpenAI.</li>
<li>Brown, T., et al. (2020). <em>Language Models are Few-Shot Learners</em> (GPT-3). NeurIPS.</li>
<li>Ouyang, L., et al. (2022). <em>Training language models to follow instructions with human feedback</em>. NeurIPS.</li>
<li>Hu, E. J., et al. (2021). <em>LoRA: Low-Rank Adaptation of Large Language Models</em>. ICLR.</li>
<li>Wei, J., et al. (2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. NeurIPS.</li>
<li>Touvron, H., et al. (2023). <em>LLaMA: Open and Efficient Foundation Language Models</em>. arXiv.</li>
<li>Touvron, H., et al. (2023). <em>Llama 2: Open Foundation and Fine-Tuned Chat Models</em>. arXiv.</li>
<li>Lewis, P., et al. (2020). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>. NeurIPS.</li>
<li>Dettmers, T., et al. (2023). <em>QLoRA: Efficient Finetuning of Quantized LLMs</em>. arXiv.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter3-transformers.html">‚Üê Previous: Transformer Architecture</a>
<a class="nav-button" href="index.html">Series Index</a>
</div>
</i};></p></main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, or operational safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
