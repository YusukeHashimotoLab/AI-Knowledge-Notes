<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2 Chapterï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç† - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/nlp-introduction/index.html">Nlp</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2 Chapterï¼šæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†</h1>
            <p class="subtitle">RNNã‹ã‚‰Attentionã¾ã§ - ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ·±å±¤å­¦ç¿’</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 35-40 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate</span>
                <span class="meta-item">ğŸ’» Code Examples: 10</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… RNNã®åŸºæœ¬æ§‹é€ ã¨è‡ªç„¶è¨€èªå‡¦ç†ã¸ã®å¿œç”¨ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… LSTM/GRUã§é•·æœŸä¾å­˜é–¢ä¿‚ã‚’æ‰±ãˆã‚‹</li>
<li>âœ… Seq2Seqãƒ¢ãƒ‡ãƒ«ã§æ©Ÿæ¢°ç¿»è¨³ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åŸç†ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… PyTorchã§å®Œå…¨ãªæ·±å±¤å­¦ç¿’NLPãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 RNNã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†</h2>

<h3>RNNã®åŸºæœ¬æ§‹é€ </h3>

<p><strong>RNNï¼ˆRecurrent Neural Networkï¼šå†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰</strong>ã¯ã€ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ãŸã‚ã®æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<blockquote>
<p>RNNã¯éš ã‚ŒçŠ¶æ…‹ã‚’æŒã¡ã€å‰ã®æ™‚åˆ»ã®æƒ…å ±ã‚’æ¬¡ã®æ™‚åˆ»ã«ä¼ãˆã‚‹ã“ã¨ã§ã€æ–‡è„ˆã‚’ç†è§£ã—ã¾ã™ã€‚</p>
</blockquote>

<h3>RNNã®æ•°å¼</h3>

<p>æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹éš ã‚ŒçŠ¶æ…‹ $h_t$ ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

<p>$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
$$</p>

<p>$$
y_t = W_{hy} h_t + b_y
$$</p>

<ul>
<li>$x_t$: æ™‚åˆ» $t$ ã®å…¥åŠ›</li>
<li>$h_t$: æ™‚åˆ» $t$ ã®éš ã‚ŒçŠ¶æ…‹</li>
<li>$y_t$: æ™‚åˆ» $t$ ã®å‡ºåŠ›</li>
<li>$W_{hh}, W_{xh}, W_{hy}$: é‡ã¿è¡Œåˆ—</li>
<li>$b_h, b_y$: ãƒã‚¤ã‚¢ã‚¹</li>
</ul>

<div class="mermaid">
graph LR
    X1[x1] --> H1[h1]
    H1 --> Y1[y1]
    H1 --> H2[h2]
    X2[x2] --> H2
    H2 --> Y2[y2]
    H2 --> H3[h3]
    X3[x3] --> H3
    H3 --> Y3[y3]
    H3 --> H4[...]

    style H1 fill:#e3f2fd
    style H2 fill:#e3f2fd
    style H3 fill:#e3f2fd
    style Y1 fill:#c8e6c9
    style Y2 fill:#c8e6c9
    style Y3 fill:#c8e6c9
</div>

<h3>PyTorchã«ã‚ˆã‚‹åŸºæœ¬çš„ãªRNNå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

# ç°¡å˜ãªRNNã®å®Ÿè£…
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size

        # RNNå±¤
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        # å‡ºåŠ›å±¤
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x: (batch_size, seq_len, input_size)
        # h0: (1, batch_size, hidden_size)
        h0 = torch.zeros(1, x.size(0), self.hidden_size)

        # RNN forward
        out, hn = self.rnn(x, h0)
        # out: (batch_size, seq_len, hidden_size)

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        out = self.fc(out[:, -1, :])
        return out

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
input_size = 10   # å…¥åŠ›ã®æ¬¡å…ƒï¼ˆä¾‹ï¼šå˜èªåŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒï¼‰
hidden_size = 20  # éš ã‚Œå±¤ã®æ¬¡å…ƒ
output_size = 2   # å‡ºåŠ›ã®æ¬¡å…ƒï¼ˆä¾‹ï¼š2ã‚¯ãƒ©ã‚¹ classificationï¼‰

model = SimpleRNN(input_size, hidden_size, output_size)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
batch_size = 3
seq_len = 5
x = torch.randn(batch_size, seq_len, input_size)

# Forward pass
output = model(x)
print(f"å…¥åŠ›å½¢çŠ¶: {x.shape}")
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
print(f"\nå‡ºåŠ›:\n{output}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å…¥åŠ›å½¢çŠ¶: torch.Size([3, 5, 10])
å‡ºåŠ›å½¢çŠ¶: torch.Size([3, 2])

å‡ºåŠ›:
tensor([[-0.1234,  0.5678],
        [ 0.2345, -0.3456],
        [-0.4567,  0.6789]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>

<h3>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ä¾‹</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# æ–‡å­—Levelã®RNN
class CharRNN(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super(CharRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        # x: (batch_size, seq_len)
        x = self.embedding(x)  # (batch_size, seq_len, embed_size)

        if hidden is None:
            out, hidden = self.rnn(x)
        else:
            out, hidden = self.rnn(x, hidden)

        out = self.fc(out)  # (batch_size, seq_len, vocab_size)
        return out, hidden

# ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
text = "hello world"
chars = sorted(list(set(text)))
char_to_idx = {ch: i for i, ch in enumerate(chars)}
idx_to_char = {i: ch for i, ch in enumerate(chars)}

vocab_size = len(chars)
print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
print(f"æ–‡å­— â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {char_to_idx}")

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
text_encoded = [char_to_idx[ch] for ch in text]
print(f"\nã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {text_encoded}")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = CharRNN(vocab_size=vocab_size, embed_size=16, hidden_size=32)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆæ¬¡ã®æ–‡å­—ã‚’äºˆæ¸¬ï¼‰
seq_len = 3
X, Y = [], []
for i in range(len(text_encoded) - seq_len):
    X.append(text_encoded[i:i+seq_len])
    Y.append(text_encoded[i+1:i+seq_len+1])

X = torch.tensor(X)
Y = torch.tensor(Y)

print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿:")
print(f"X shape: {X.shape}, Y shape: {Y.shape}")
print(f"æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ« - å…¥åŠ›: {X[0]}, å‡ºåŠ›: {Y[0]}")

# ç°¡å˜ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    output, _ = model(X)
    # output: (batch, seq_len, vocab_size)
    # Y: (batch, seq_len)

    loss = criterion(output.reshape(-1, vocab_size), Y.reshape(-1))
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

print("\nè¨“ç·´å®Œäº†ï¼")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>èªå½™ã‚µã‚¤ã‚º: 8
æ–‡å­— â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {' ': 0, 'd': 1, 'e': 2, 'h': 3, 'l': 4, 'o': 5, 'r': 6, 'w': 7}

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: [3, 2, 4, 4, 5, 0, 7, 5, 6, 4, 1]

è¨“ç·´ãƒ‡ãƒ¼ã‚¿:
X shape: torch.Size([8, 3]), Y shape: torch.Size([8, 3])
æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ« - å…¥åŠ›: tensor([3, 2, 4]), å‡ºåŠ›: tensor([2, 4, 4])

Epoch [20/100], Loss: 1.4567
Epoch [40/100], Loss: 0.8901
Epoch [60/100], Loss: 0.4234
Epoch [80/100], Loss: 0.2123
Epoch [100/100], Loss: 0.1234

è¨“ç·´å®Œäº†ï¼
</code></pre>

<h3>RNNã®å•é¡Œç‚¹</h3>

<table>
<thead>
<tr>
<th>å•é¡Œ</th>
<th>èª¬æ˜</th>
<th>å½±éŸ¿</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å‹¾é…æ¶ˆå¤±</strong></td>
<td>é•·ã„ç³»åˆ—ã§å‹¾é…ãŒ0ã«è¿‘ã¥ã</td>
<td>é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ããªã„</td>
</tr>
<tr>
<td><strong>å‹¾é…çˆ†ç™º</strong></td>
<td>å‹¾é…ãŒç™ºæ•£ã™ã‚‹</td>
<td>å­¦ç¿’ãŒä¸å®‰å®š</td>
</tr>
<tr>
<td><strong>çŸ­æœŸè¨˜æ†¶</strong></td>
<td>é ã„éå»ã®æƒ…å ±ã‚’å¿˜ã‚Œã‚‹</td>
<td>æ–‡è„ˆç†è§£ãŒä¸å minutes</td>
</tr>
</tbody>
</table>

<hr>

<h2>2.2 LSTM & GRU</h2>

<h3>LSTMï¼ˆLong Short-Term Memoryï¼‰</h3>

<p><strong>LSTM</strong>ã¯ã€RNNã®å‹¾é…æ¶ˆå¤±å•é¡Œã‚’è§£æ±ºã—ã€é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚</p>

<h4>LSTMã®ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹</h4>

<p>LSTMã¯3ã¤ã®ã‚²ãƒ¼ãƒˆã§æƒ…å ±ã®æµã‚Œã‚’åˆ¶å¾¡ã—ã¾ã™ï¼š</p>

<ol>
<li><strong>å¿˜å´ã‚²ãƒ¼ãƒˆï¼ˆForget Gateï¼‰</strong>: éå»ã®æƒ…å ±ã‚’ã©ã‚Œã ã‘å¿˜ã‚Œã‚‹ã‹</li>
<li><strong>å…¥åŠ›ã‚²ãƒ¼ãƒˆï¼ˆInput Gateï¼‰</strong>: æ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘è¿½åŠ ã™ã‚‹ã‹</li>
<li><strong>å‡ºåŠ›ã‚²ãƒ¼ãƒˆï¼ˆOutput Gateï¼‰</strong>: éš ã‚ŒçŠ¶æ…‹ã¨ã—ã¦ä½•ã‚’å‡ºåŠ›ã™ã‚‹ã‹</li>
</ol>

<h4>LSTMã®æ•°å¼</h4>

<p>$$
\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(å¿˜å´ã‚²ãƒ¼ãƒˆ)} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(å…¥åŠ›ã‚²ãƒ¼ãƒˆ)} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad \text{(å€™è£œã‚»ãƒ«çŠ¶æ…‹)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°)} \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(å‡ºåŠ›ã‚²ãƒ¼ãƒˆ)} \\
h_t &= o_t \odot \tanh(C_t) \quad \text{(éš ã‚ŒçŠ¶æ…‹)}
\end{align}
$$</p>

<ul>
<li>$\sigma$: ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°</li>
<li>$\odot$: è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰</li>
<li>$C_t$: ã‚»ãƒ«çŠ¶æ…‹</li>
</ul>

<div class="mermaid">
graph TD
    A[å…¥åŠ› x_t] --> B{å¿˜å´ã‚²ãƒ¼ãƒˆ}
    A --> C{å…¥åŠ›ã‚²ãƒ¼ãƒˆ}
    A --> D{å‡ºåŠ›ã‚²ãƒ¼ãƒˆ}
    E[ã‚»ãƒ«çŠ¶æ…‹ C_t-1] --> B
    B --> F[Ã—]
    C --> G[Ã—]
    H[å€™è£œã‚»ãƒ«çŠ¶æ…‹] --> G
    F --> I[+]
    G --> I
    I --> J[ã‚»ãƒ«çŠ¶æ…‹ C_t]
    J --> D
    D --> K[éš ã‚ŒçŠ¶æ…‹ h_t]

    style B fill:#ffebee
    style C fill:#e3f2fd
    style D fill:#e8f5e9
    style J fill:#fff3e0
    style K fill:#f3e5f5
</div>

<h3>PyTorchã§ã®LSTMå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import numpy as np

# æ„Ÿæƒ… minutesæã®ãŸã‚ã®LSTMãƒ¢ãƒ‡ãƒ«
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers=1):
        super(SentimentLSTM, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,
                           batch_first=True, dropout=0.2 if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch_size, seq_len, embed_size)

        # LSTM forward
        lstm_out, (hn, cn) = self.lstm(embedded)
        # lstm_out: (batch_size, seq_len, hidden_size)

        # æœ€å¾Œã®æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        out = self.dropout(lstm_out[:, -1, :])
        out = self.fc(out)

        return out

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ… minutesæï¼‰
sentences = [
    "this movie is great",
    "i love this film",
    "amazing acting and story",
    "best movie ever",
    "this is terrible",
    "worst movie i have seen",
    "i hate this film",
    "boring and dull"
]

labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1: positive, 0: negative

# ç°¡å˜ãªèªå½™ã®æ§‹ç¯‰
words = set(" ".join(sentences).split())
word_to_idx = {word: i+1 for i, word in enumerate(words)}  # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ç”¨
word_to_idx['<PAD>'] = 0

vocab_size = len(word_to_idx)
print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
print(f"å˜èª â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆä¸€éƒ¨ï¼‰: {dict(list(word_to_idx.items())[:5])}")

# æ–‡ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã«å¤‰æ›
def encode_sentence(sentence, word_to_idx, max_len=10):
    tokens = sentence.split()
    encoded = [word_to_idx.get(word, 0) for word in tokens]
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    if len(encoded) < max_len:
        encoded += [0] * (max_len - len(encoded))
    else:
        encoded = encoded[:max_len]
    return encoded

max_len = 10
X = [encode_sentence(s, word_to_idx, max_len) for s in sentences]
X = torch.tensor(X)
y = torch.tensor(labels)

print(f"\nãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:")
print(f"X: {X.shape}, y: {y.shape}")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´
model = SentimentLSTM(vocab_size=vocab_size, embed_size=32,
                     hidden_size=64, num_classes=2, num_layers=2)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è¨“ç·´
num_epochs = 200
model.train()

for epoch in range(num_epochs):
    optimizer.zero_grad()

    outputs = model(X)
    loss = criterion(outputs, y)

    loss.backward()
    optimizer.step()

    if (epoch + 1) % 50 == 0:
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == y).sum().item() / len(y)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}")

# ãƒ†ã‚¹ãƒˆ
model.eval()
test_sentences = [
    "i love this amazing movie",
    "this is the worst film"
]

with torch.no_grad():
    for sent in test_sentences:
        encoded = encode_sentence(sent, word_to_idx, max_len)
        x_test = torch.tensor([encoded])
        output = model(x_test)
        _, pred = torch.max(output, 1)
        sentiment = "Positive" if pred.item() == 1 else "Negative"
        print(f"\næ–‡: '{sent}'")
        print(f"äºˆæ¸¬: {sentiment}")
        print(f"ç¢ºç‡: {torch.softmax(output, dim=1).numpy()}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>èªå½™ã‚µã‚¤ã‚º: 24
å˜èª â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆä¸€éƒ¨ï¼‰: {'this': 1, 'movie': 2, 'is': 3, 'great': 4, 'i': 5}

ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:
X: torch.Size([8, 10]), y: torch.Size([8])

Epoch [50/200], Loss: 0.5234, Accuracy: 0.7500
Epoch [100/200], Loss: 0.2156, Accuracy: 1.0000
Epoch [150/200], Loss: 0.0987, Accuracy: 1.0000
Epoch [200/200], Loss: 0.0456, Accuracy: 1.0000

æ–‡: 'i love this amazing movie'
äºˆæ¸¬: Positive
ç¢ºç‡: [[0.0234 0.9766]]

æ–‡: 'this is the worst film'
äºˆæ¸¬: Negative
ç¢ºç‡: [[0.9823 0.0177]]
</code></pre>

<h3>GRUï¼ˆGated Recurrent Unitï¼‰</h3>

<p><strong>GRU</strong>ã¯ã€LSTMã‚’ç°¡ç•¥åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã€ã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åŒç­‰ã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚</p>

<h4>GRUã®æ•°å¼</h4>

<p>$$
\begin{align}
r_t &= \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ)} \\
z_t &= \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(æ›´æ–°ã‚²ãƒ¼ãƒˆ)} \\
\tilde{h}_t &= \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t]) \quad \text{(å€™è£œéš ã‚ŒçŠ¶æ…‹)} \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad \text{(éš ã‚ŒçŠ¶æ…‹)}
\end{align}
$$</p>

<h3>PyTorchã§ã®GRUå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# GRUãƒ¢ãƒ‡ãƒ«
class TextClassifierGRU(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):
        super(TextClassifierGRU, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, hn = self.gru(embedded)

        # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        out = self.fc(hn.squeeze(0))
        return out

# ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
lstm_model = SentimentLSTM(vocab_size=100, embed_size=32,
                          hidden_size=64, num_classes=2)
gru_model = TextClassifierGRU(vocab_size=100, embed_size=32,
                             hidden_size=64, num_classes=2)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ
lstm_params = sum(p.numel() for p in lstm_model.parameters())
gru_params = sum(p.numel() for p in gru_model.parameters())

print("=== LSTM vs GRU ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ ===")
print(f"LSTM: {lstm_params:,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"GRU:  {gru_params:,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
print(f"å‰Šæ¸›ç‡: {(1 - gru_params/lstm_params)*100:.1f}%")

# æ¨è«–é€Ÿåº¦ã®æ¯”è¼ƒ
x = torch.randint(0, 100, (32, 20))  # (batch_size=32, seq_len=20)

import time

# LSTM
start = time.time()
for _ in range(100):
    _ = lstm_model(x)
lstm_time = time.time() - start

# GRU
start = time.time()
for _ in range(100):
    _ = gru_model(x)
gru_time = time.time() - start

print(f"\n=== æ¨è«–é€Ÿåº¦æ¯”è¼ƒï¼ˆ100å›å®Ÿè¡Œï¼‰===")
print(f"LSTM: {lstm_time:.4f}ç§’")
print(f"GRU:  {gru_time:.4f}ç§’")
print(f"é«˜é€ŸåŒ–: {(lstm_time/gru_time - 1)*100:.1f}%")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== LSTM vs GRU ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ ===
LSTM: 37,954 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GRU:  28,866 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
å‰Šæ¸›ç‡: 23.9%

=== æ¨è«–é€Ÿåº¦æ¯”è¼ƒï¼ˆ100å›å®Ÿè¡Œï¼‰===
LSTM: 0.1234ç§’
GRU:  0.0987ç§’
é«˜é€ŸåŒ–: 25.0%
</code></pre>

<h3>LSTM vs GRU æ¯”è¼ƒè¡¨</h3>

<table>
<thead>
<tr>
<th>ç‰¹å¾´</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ã‚²ãƒ¼ãƒˆæ•°</strong></td>
<td>3ï¼ˆå¿˜å´ã€å…¥åŠ›ã€å‡ºåŠ›ï¼‰</td>
<td>2ï¼ˆãƒªã‚»ãƒƒãƒˆã€æ›´æ–°ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></td>
<td>å¤šã„</td>
<td>å°‘ãªã„ï¼ˆç´„25%å‰Šæ¸›ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>é«˜ã„</td>
<td>ä½ã„</td>
</tr>
<tr>
<td><strong>è¡¨ç¾åŠ›</strong></td>
<td>é«˜ã„</td>
<td>ã‚„ã‚„ä½ã„</td>
</tr>
<tr>
<td><strong>å­¦ç¿’é€Ÿåº¦</strong></td>
<td>é…ã„</td>
<td>é€Ÿã„</td>
</tr>
<tr>
<td><strong>Recommendedç”¨é€”</strong></td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯</td>
<td>ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€é«˜é€ŸåŒ–ãŒå¿…è¦</td>
</tr>
</tbody>
</table>

<hr>

<h2>2.3 Seq2Seqãƒ¢ãƒ‡ãƒ«</h2>

<h3>Seq2Seqï¼ˆSequence-to-Sequenceï¼‰ã¨ã¯</h3>

<p><strong>Seq2Seq</strong>ã¯ã€å¯å¤‰é•·ã®å…¥åŠ›ç³»åˆ—ã‚’å¯å¤‰é•·ã®å‡ºåŠ›ç³»åˆ—ã«å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<blockquote>
<p>æ©Ÿæ¢°ç¿»è¨³ã€è¦ç´„ã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€å¤šãã®NLPã‚¿ã‚¹ã‚¯ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚</p>
</blockquote>

<h3>Seq2Seqã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<p>Seq2Seqã¯2ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<ol>
<li><strong>Encoderï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰</strong>: å…¥åŠ›ç³»åˆ—ã‚’å›ºå®šé•·ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®</li>
<li><strong>Decoderï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ï¼‰</strong>: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰å‡ºåŠ›ç³»åˆ—ã‚’ç”Ÿæˆ</li>
</ol>

<div class="mermaid">
graph LR
    A[å…¥åŠ›ç³»åˆ—] --> B[Encoder]
    B --> C[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«]
    C --> D[Decoder]
    D --> E[å‡ºåŠ›ç³»åˆ—]

    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#e8f5e9
</div>

<h3>PyTorchã§ã®Seq2Seqå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import random

# Encoderã‚¯ãƒ©ã‚¹
class Encoder(nn.Module):
    def __init__(self, input_size, embed_size, hidden_size, num_layers=1):
        super(Encoder, self).__init__()

        self.embedding = nn.Embedding(input_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)

    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)
        # embedded: (batch_size, seq_len, embed_size)

        outputs, (hidden, cell) = self.lstm(embedded)
        # outputs: (batch_size, seq_len, hidden_size)
        # hidden: (num_layers, batch_size, hidden_size)

        return hidden, cell

# Decoderã‚¯ãƒ©ã‚¹
class Decoder(nn.Module):
    def __init__(self, output_size, embed_size, hidden_size, num_layers=1):
        super(Decoder, self).__init__()

        self.embedding = nn.Embedding(output_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden, cell):
        # x: (batch_size, 1)
        embedded = self.embedding(x)
        # embedded: (batch_size, 1, embed_size)

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: (batch_size, 1, hidden_size)

        prediction = self.fc(output.squeeze(1))
        # prediction: (batch_size, output_size)

        return prediction, hidden, cell

# Seq2Seqãƒ¢ãƒ‡ãƒ«
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        # source: (batch_size, src_seq_len)
        # target: (batch_size, tgt_seq_len)

        batch_size = source.size(0)
        target_len = target.size(1)
        target_vocab_size = self.decoder.fc.out_features

        # å‡ºåŠ›ã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«
        outputs = torch.zeros(batch_size, target_len, target_vocab_size)

        # Encoderã§å…¥åŠ›ã‚’å‡¦ç†
        hidden, cell = self.encoder(source)

        # Decoderã®æœ€åˆã®å…¥åŠ›ï¼ˆ<SOS>ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        decoder_input = target[:, 0].unsqueeze(1)

        for t in range(1, target_len):
            # Decoderã§1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬
            output, hidden, cell = self.decoder(decoder_input, hidden, cell)
            outputs[:, t, :] = output

            # Teacher forcing: ãƒ©ãƒ³ãƒ€ãƒ ã«æ­£è§£ã‚’ä½¿ã†ã‹äºˆæ¸¬ã‚’ä½¿ã†ã‹æ±ºå®š
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1).unsqueeze(1)
            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1

        return outputs

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
input_vocab_size = 100   # å…¥åŠ›èªå½™ã‚µã‚¤ã‚º
output_vocab_size = 100  # å‡ºåŠ›èªå½™ã‚µã‚¤ã‚º
embed_size = 128
hidden_size = 256

encoder = Encoder(input_vocab_size, embed_size, hidden_size)
decoder = Decoder(output_vocab_size, embed_size, hidden_size)
model = Seq2Seq(encoder, decoder)

print("=== Seq2Seqãƒ¢ãƒ‡ãƒ« ===")
print(f"Encoderãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in encoder.parameters()):,}")
print(f"Decoderãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in decoder.parameters()):,}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in model.parameters()):,}")

# ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
batch_size = 2
src_seq_len = 5
tgt_seq_len = 6

source = torch.randint(0, input_vocab_size, (batch_size, src_seq_len))
target = torch.randint(0, output_vocab_size, (batch_size, tgt_seq_len))

with torch.no_grad():
    output = model(source, target, teacher_forcing_ratio=0.0)
    print(f"\nå…¥åŠ›å½¢çŠ¶: {source.shape}")
    print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Seq2Seqãƒ¢ãƒ‡ãƒ« ===
Encoderãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 275,456
Decoderãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 301,156
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 576,612

å…¥åŠ›å½¢çŠ¶: torch.Size([2, 5])
å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 6, 100])
</code></pre>

<h3>Teacher Forcing</h3>

<p><strong>Teacher Forcing</strong>ã¯ã€è¨“ç·´æ™‚ã«ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ã¨ã—ã¦å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®äºˆæ¸¬ã§ã¯ãªãã€æ­£è§£ã‚’ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>ãƒ¡ãƒªãƒƒãƒˆ</th>
<th>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Teacher Forcing</strong></td>
<td>å­¦ç¿’ãŒé€Ÿãå®‰å®š</td>
<td>è¨“ç·´ã¨æ¨è«–ã®ã‚®ãƒ£ãƒƒãƒ—ï¼ˆExposure Biasï¼‰</td>
</tr>
<tr>
<td><strong>Free Running</strong></td>
<td>æ¨è«–ã¨åŒã˜æ¡ä»¶</td>
<td>å­¦ç¿’ãŒä¸å®‰å®šã€é…ã„</td>
</tr>
<tr>
<td><strong>Scheduled Sampling</strong></td>
<td>ä¸¡æ–¹ã®ãƒãƒ©ãƒ³ã‚¹</td>
<td>ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå¿…è¦</td>
</tr>
</tbody>
</table>

<h3>æ©Ÿæ¢°ç¿»è¨³ã®ç°¡å˜ãªä¾‹</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# ç°¡å˜ãªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ï¼ˆè‹±èªâ†’æ—¥æœ¬èªï¼‰
en_sentences = [
    "i am a student",
    "he is a teacher",
    "she likes cats",
    "we study english"
]

ja_sentences = [
    "<SOS> ç§ ã¯ å­¦ç”Ÿ ã§ã™ <EOS>",
    "<SOS> å½¼ ã¯ æ•™å¸« ã§ã™ <EOS>",
    "<SOS> å½¼å¥³ ã¯ çŒ« ãŒ å¥½ã ã§ã™ <EOS>",
    "<SOS> ç§ãŸã¡ ã¯ è‹±èª ã‚’ å‹‰å¼· ã—ã¾ã™ <EOS>"
]

# èªå½™ã®æ§‹ç¯‰
en_words = set(" ".join(en_sentences).split())
ja_words = set(" ".join(ja_sentences).split())

en_vocab = {word: i+1 for i, word in enumerate(en_words)}
ja_vocab = {word: i+1 for i, word in enumerate(ja_words)}
ja_vocab['<PAD>'] = 0

en_vocab_size = len(en_vocab) + 1
ja_vocab_size = len(ja_vocab) + 1

print(f"è‹±èªèªå½™ã‚µã‚¤ã‚º: {en_vocab_size}")
print(f"æ—¥æœ¬èªèªå½™ã‚µã‚¤ã‚º: {ja_vocab_size}")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
def encode(sentence, vocab, max_len):
    tokens = sentence.split()
    encoded = [vocab.get(word, 0) for word in tokens]
    if len(encoded) < max_len:
        encoded += [0] * (max_len - len(encoded))
    else:
        encoded = encoded[:max_len]
    return encoded

en_max_len = 5
ja_max_len = 7

X = torch.tensor([encode(s, en_vocab, en_max_len) for s in en_sentences])
y = torch.tensor([encode(s, ja_vocab, ja_max_len) for s in ja_sentences])

print(f"\nãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: X={X.shape}, y={y.shape}")

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´
encoder = Encoder(en_vocab_size, 64, 128)
decoder = Decoder(ja_vocab_size, 64, 128)
model = Seq2Seq(encoder, decoder)

criterion = nn.CrossEntropyLoss(ignore_index=0)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç„¡è¦–
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è¨“ç·´
num_epochs = 500
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    output = model(X, y, teacher_forcing_ratio=0.5)
    # output: (batch_size, seq_len, vocab_size)

    output = output[:, 1:, :].reshape(-1, ja_vocab_size)
    target = y[:, 1:].reshape(-1)

    loss = criterion(output, target)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

print("\nè¨“ç·´å®Œäº†ï¼")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>è‹±èªèªå½™ã‚µã‚¤ã‚º: 13
æ—¥æœ¬èªèªå½™ã‚µã‚¤ã‚º: 17

ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: X=torch.Size([4, 5]), y=torch.Size([4, 7])

Epoch [100/500], Loss: 1.2345
Epoch [200/500], Loss: 0.5678
Epoch [300/500], Loss: 0.2345
Epoch [400/500], Loss: 0.1234
Epoch [500/500], Loss: 0.0678

è¨“ç·´å®Œäº†ï¼
</code></pre>

<hr>

<h2>2.4 Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ </h2>

<h3>Attentionã®å¿…è¦æ€§</h3>

<p>Seq2Seqã®å•é¡Œç‚¹ï¼š</p>
<ul>
<li>é•·ã„å…¥åŠ›ç³»åˆ—ã‚’å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã™ã‚‹ã¨æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹</li>
<li>å…¥åŠ›ã®å…¨ã¦ã®éƒ¨ minutesãŒå‡ºåŠ›ã«ç­‰ã—ãé‡è¦ã¨ã¯é™ã‚‰ãªã„</li>
</ul>

<blockquote>
<p><strong>Attention</strong>ã¯ã€å‡ºåŠ›ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã§å…¥åŠ›ã®é‡è¦ãªéƒ¨ minutesã«æ³¨ç›®ã™ã‚‹ã“ã¨ã§ã€ã“ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚</p>
</blockquote>

<h3>Attentionã®ä»•çµ„ã¿</h3>

<p>Attentionã¯ä»¥ä¸‹ã®3ã‚¹ãƒ†ãƒƒãƒ—ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

<ol>
<li><strong>ã‚¹ã‚³ã‚¢è¨ˆç®—</strong>: ãƒ‡ã‚³ãƒ¼ãƒ€ã®éš ã‚ŒçŠ¶æ…‹ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å…¨å‡ºåŠ›ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—</li>
<li><strong>é‡ã¿ã®æ­£è¦åŒ–</strong>: ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’è¨ˆç®—</li>
<li><strong>ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ</strong>: é‡ã¿ä»˜ãå’Œã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ</li>
</ol>

<h3>Bahdanau Attention</h3>

<p>$$
\begin{align}
\text{score}(h_t, \bar{h}_s) &= v^T \tanh(W_1 h_t + W_2 \bar{h}_s) \\
\alpha_{ts} &= \frac{\exp(\text{score}(h_t, \bar{h}_s))}{\sum_{s'} \exp(\text{score}(h_t, \bar{h}_{s'}))} \\
c_t &= \sum_s \alpha_{ts} \bar{h}_s
\end{align}
$$</p>

<ul>
<li>$h_t$: ãƒ‡ã‚³ãƒ¼ãƒ€ã®æ™‚åˆ» $t$ ã®éš ã‚ŒçŠ¶æ…‹</li>
<li>$\bar{h}_s$: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®æ™‚åˆ» $s$ ã®å‡ºåŠ›</li>
<li>$\alpha_{ts}$: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿</li>
<li>$c_t$: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«</li>
</ul>

<div class="mermaid">
graph TD
    A[Encoderå‡ºåŠ›] --> B[ã‚¹ã‚³ã‚¢è¨ˆç®—]
    C[Decoderéš ã‚ŒçŠ¶æ…‹] --> B
    B --> D[Softmax]
    D --> E[ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿]
    E --> F[é‡ã¿ä»˜ãå’Œ]
    A --> F
    F --> G[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«]

    style B fill:#e3f2fd
    style D fill:#fff3e0
    style E fill:#ffebee
    style G fill:#e8f5e9
</div>

<h3>PyTorchã§ã®Attentionå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()

        self.W1 = nn.Linear(hidden_size, hidden_size)
        self.W2 = nn.Linear(hidden_size, hidden_size)
        self.V = nn.Linear(hidden_size, 1)

    def forward(self, decoder_hidden, encoder_outputs):
        # decoder_hidden: (batch_size, hidden_size)
        # encoder_outputs: (batch_size, seq_len, hidden_size)

        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)

        # decoder_hiddenã‚’æ‹¡å¼µ
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        # (batch_size, seq_len, hidden_size)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        energy = torch.tanh(self.W1(decoder_hidden) + self.W2(encoder_outputs))
        # (batch_size, seq_len, hidden_size)

        attention_scores = self.V(energy).squeeze(2)
        # (batch_size, seq_len)

        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®è¨ˆç®—ï¼ˆsoftmaxï¼‰
        attention_weights = F.softmax(attention_scores, dim=1)
        # (batch_size, seq_len)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        # (batch_size, 1, hidden_size)

        return context_vector.squeeze(1), attention_weights

# Attentionã¤ãDecoder
class AttentionDecoder(nn.Module):
    def __init__(self, output_size, embed_size, hidden_size):
        super(AttentionDecoder, self).__init__()

        self.embedding = nn.Embedding(output_size, embed_size)
        self.attention = BahdanauAttention(hidden_size)
        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden, cell, encoder_outputs):
        # x: (batch_size, 1)
        embedded = self.embedding(x)
        # embedded: (batch_size, 1, embed_size)

        # Attentionã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        context, attention_weights = self.attention(hidden[-1], encoder_outputs)
        # context: (batch_size, hidden_size)

        # åŸ‹ã‚è¾¼ã¿ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        lstm_input = torch.cat([embedded.squeeze(1), context], dim=1).unsqueeze(1)
        # (batch_size, 1, embed_size + hidden_size)

        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
        prediction = self.fc(output.squeeze(1))

        return prediction, hidden, cell, attention_weights

# Attentionã¤ãSeq2Seq
class Seq2SeqWithAttention(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2SeqWithAttention, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        batch_size = source.size(0)
        target_len = target.size(1)
        target_vocab_size = self.decoder.fc.out_features

        outputs = torch.zeros(batch_size, target_len, target_vocab_size)

        # Encoderã§å‡¦ç†
        encoder_outputs, (hidden, cell) = self.encoder(source)

        decoder_input = target[:, 0].unsqueeze(1)

        all_attention_weights = []

        for t in range(1, target_len):
            output, hidden, cell, attention_weights = self.decoder(
                decoder_input, hidden, cell, encoder_outputs
            )
            outputs[:, t, :] = output
            all_attention_weights.append(attention_weights)

            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1).unsqueeze(1)
            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1

        return outputs, all_attention_weights

# Encoderã‚’ä¿®æ­£ï¼ˆå‡ºåŠ›ã‚‚è¿”ã™ï¼‰
class EncoderWithOutputs(nn.Module):
    def __init__(self, input_size, embed_size, hidden_size):
        super(EncoderWithOutputs, self).__init__()
        self.embedding = nn.Embedding(input_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)

    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded)
        return outputs, (hidden, cell)

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
input_vocab_size = 100
output_vocab_size = 100
embed_size = 128
hidden_size = 256

encoder = EncoderWithOutputs(input_vocab_size, embed_size, hidden_size)
decoder = AttentionDecoder(output_vocab_size, embed_size, hidden_size)
model = Seq2SeqWithAttention(encoder, decoder)

print("=== Seq2Seq with Attention ===")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in model.parameters()):,}")

# ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
source = torch.randint(0, input_vocab_size, (2, 5))
target = torch.randint(0, output_vocab_size, (2, 6))

with torch.no_grad():
    output, attention_weights = model(source, target, teacher_forcing_ratio=0.0)
    print(f"\nå‡ºåŠ›å½¢çŠ¶: {output.shape}")
    print(f"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿æ•°: {len(attention_weights)}")
    print(f"å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿å½¢çŠ¶: {attention_weights[0].shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Seq2Seq with Attention ===
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 609,124

å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 6, 100])
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿æ•°: 5
å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿å½¢çŠ¶: torch.Size([2, 5])
</code></pre>

<h3>ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®å¯è¦–åŒ–ç”¨ã‚µãƒ³ãƒ—ãƒ«
def visualize_attention(attention_weights, source_tokens, target_tokens):
    """
    ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–

    Parameters:
    - attention_weights: (target_len, source_len)
    - source_tokens: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    - target_tokens: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    fig, ax = plt.subplots(figsize=(10, 8))

    sns.heatmap(attention_weights,
                xticklabels=source_tokens,
                yticklabels=target_tokens,
                cmap='YlOrRd',
                annot=True,
                fmt='.2f',
                cbar_kws={'label': 'Attention Weight'},
                ax=ax)

    ax.set_xlabel('Source (English)', fontsize=12)
    ax.set_ylabel('Target (Japanese)', fontsize=12)
    ax.set_title('Attention Weights Visualization', fontsize=14, fontweight='bold')

    plt.tight_layout()
    plt.show()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
source_tokens = ['I', 'love', 'natural', 'language', 'processing']
target_tokens = ['ç§', 'ã¯', 'è‡ªç„¶', 'è¨€èª', 'å‡¦ç†', 'ãŒ', 'å¥½ã', 'ã§ã™']

# ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ï¼ˆå®Ÿéš›ã¯å­¦ç¿’ã•ã‚ŒãŸã‚‚ã®ï¼‰
np.random.seed(42)
attention_weights = np.random.rand(len(target_tokens), len(source_tokens))
# è¡Œã”ã¨ã«æ­£è¦åŒ–ï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹ï¼‰
attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)

print("=== ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ ===")
print(f"å½¢çŠ¶: {attention_weights.shape}")
print(f"\næœ€åˆã®3å˜èªã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿:")
print(attention_weights[:3])

# å¯è¦–åŒ–
visualize_attention(attention_weights, source_tokens, target_tokens)
</code></pre>

<hr>

<h2>2.5 Embeddingå±¤ã®æ´»ç”¨</h2>

<h3>Embeddingå±¤ã¨ã¯</h3>

<p><strong>Embeddingå±¤</strong>ã¯ã€å˜èªã‚’å¯†ãªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«å¤‰æ›ã—ã¾ã™ã€‚</p>

<p>$$
\text{Embedding}: \text{å˜èªID} \rightarrow \mathbb{R}^d
$$</p>

<ul>
<li>$d$: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆé€šå¸¸50ã€œ300ï¼‰</li>
</ul>

<h3>PyTorchã§ã®Embeddingå±¤</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# Embeddingå±¤ã®åŸºæœ¬
vocab_size = 1000  # èªå½™ã‚µã‚¤ã‚º
embed_dim = 128    # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ

embedding = nn.Embedding(vocab_size, embed_dim)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
num_params = vocab_size * embed_dim
print(f"=== Embeddingå±¤ ===")
print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,}")

# ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›
input_ids = torch.tensor([[1, 2, 3, 4],
                         [5, 6, 7, 8]])
# (batch_size=2, seq_len=4)

embedded = embedding(input_ids)
print(f"\nå…¥åŠ›å½¢çŠ¶: {input_ids.shape}")
print(f"åŸ‹ã‚è¾¼ã¿å¾Œã®å½¢çŠ¶: {embedded.shape}")
print(f"\næœ€åˆã®å˜èªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¸€éƒ¨ï¼‰:")
print(embedded[0, 0, :10])
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Embeddingå±¤ ===
èªå½™ã‚µã‚¤ã‚º: 1000
åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 128
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 128,000

å…¥åŠ›å½¢çŠ¶: torch.Size([2, 4])
åŸ‹ã‚è¾¼ã¿å¾Œã®å½¢çŠ¶: torch.Size([2, 4, 128])

æœ€åˆã®å˜èªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¸€éƒ¨ï¼‰:
tensor([-0.1234,  0.5678, -0.9012,  0.3456, -0.7890,  0.1234, -0.5678,  0.9012,
        -0.3456,  0.7890], grad_fn=&lt;SliceBackward0&gt;)
</code></pre>

<h3>äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ã®åˆ©ç”¨</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

# äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# å®Ÿéš›ã«ã¯Word2Vecã€GloVeã€fastTextãªã©ã‚’ä½¿ç”¨
vocab_size = 1000
embed_dim = 100

# ãƒ©ãƒ³ãƒ€ãƒ ãªäº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã¯å­¦ç¿’æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
pretrained_embeddings = torch.randn(vocab_size, embed_dim)

# Embeddingå±¤ã«äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
embedding = nn.Embedding(vocab_size, embed_dim)
embedding.weight = nn.Parameter(pretrained_embeddings)

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: åŸ‹ã‚è¾¼ã¿ã‚’å›ºå®šï¼ˆfine-tuningã—ãªã„ï¼‰
embedding.weight.requires_grad = False
print("=== äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆå›ºå®šï¼‰===")
print(f"å­¦ç¿’å¯èƒ½: {embedding.weight.requires_grad}")

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: åŸ‹ã‚è¾¼ã¿ã‚’fine-tuning
embedding.weight.requires_grad = True
print(f"\n=== äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆfine-tuningï¼‰===")
print(f"å­¦ç¿’å¯èƒ½: {embedding.weight.requires_grad}")

# ãƒ¢ãƒ‡ãƒ«ã§ã®ä½¿ç”¨ä¾‹
class TextClassifierWithPretrainedEmbedding(nn.Module):
    def __init__(self, pretrained_embeddings, hidden_size, num_classes, freeze_embedding=True):
        super(TextClassifierWithPretrainedEmbedding, self).__init__()

        vocab_size, embed_dim = pretrained_embeddings.shape

        # äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embedding.weight = nn.Parameter(pretrained_embeddings)
        self.embedding.weight.requires_grad = not freeze_embedding

        # LSTMå±¤
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        out = self.fc(lstm_out[:, -1, :])
        return out

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = TextClassifierWithPretrainedEmbedding(
    pretrained_embeddings,
    hidden_size=128,
    num_classes=2,
    freeze_embedding=True
)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ ===")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
print(f"å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params - trainable_params:,}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆå›ºå®šï¼‰===
å­¦ç¿’å¯èƒ½: False

=== äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆfine-tuningï¼‰===
å­¦ç¿’å¯èƒ½: True

=== ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ ===
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 230,018
å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 130,018
å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 100,000
</code></pre>

<h3>æ–‡å­—Levelãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# æ–‡å­—Levelã®RNNãƒ¢ãƒ‡ãƒ«
class CharLevelRNN(nn.Module):
    def __init__(self, num_chars, embed_size, hidden_size, num_layers=2):
        super(CharLevelRNN, self).__init__()

        self.embedding = nn.Embedding(num_chars, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, num_chars)

    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        out = self.fc(lstm_out)
        return out

# æ–‡å­—ã®èªå½™
chars = "abcdefghijklmnopqrstuvwxyz "
char_to_idx = {ch: i for i, ch in enumerate(chars)}
idx_to_char = {i: ch for i, ch in enumerate(chars)}

num_chars = len(chars)

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = CharLevelRNN(num_chars, embed_size=32, hidden_size=64, num_layers=2)

print(f"=== æ–‡å­—Levelãƒ¢ãƒ‡ãƒ« ===")
print(f"æ–‡å­—æ•°: {num_chars}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in model.parameters()):,}")

# ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
text = "hello world"
encoded = [char_to_idx[ch] for ch in text]
print(f"\nãƒ†ã‚­ã‚¹ãƒˆ: '{text}'")
print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: {encoded}")

# ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
x = torch.tensor([encoded])
with torch.no_grad():
    output = model(x)
    print(f"\nå‡ºåŠ›å½¢çŠ¶: {output.shape}")

    # å„ä½ç½®ã§ã®æœ€ã‚‚ç¢ºç‡ã®é«˜ã„æ–‡å­—
    predicted_indices = output.argmax(dim=2).squeeze(0)
    predicted_text = ''.join([idx_to_char[idx.item()] for idx in predicted_indices])
    print(f"äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè¨“ç·´å‰ï¼‰: '{predicted_text}'")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== æ–‡å­—Levelãƒ¢ãƒ‡ãƒ« ===
æ–‡å­—æ•°: 27
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 24,091

ãƒ†ã‚­ã‚¹ãƒˆ: 'hello world'
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: [7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]

å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 11, 27])
äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè¨“ç·´å‰ï¼‰: 'aaaaaaaaaaa'
</code></pre>

<h3>Embeddingå±¤ã®æ¯”è¼ƒ</h3>

<table>
<thead>
<tr>
<th>æ‰‹æ³•</th>
<th>ãƒ¡ãƒªãƒƒãƒˆ</th>
<th>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ</th>
<th>Recommendedç”¨é€”</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–</strong></td>
<td>ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã€æŸ”è»Ÿ</td>
<td>å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦</td>
<td>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆå›ºå®šï¼‰</strong></td>
<td>å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§OK</td>
<td>ã‚¿ã‚¹ã‚¯é©å¿œæ€§ä½ã„</td>
<td>å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€æ±ç”¨ã‚¿ã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆfine-tuningï¼‰</strong></td>
<td>ä¸¡æ–¹ã®ãƒãƒ©ãƒ³ã‚¹</td>
<td>éå­¦ç¿’ã®ãƒªã‚¹ã‚¯</td>
<td>ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€ç‰¹å®šã‚¿ã‚¹ã‚¯</td>
</tr>
<tr>
<td><strong>æ–‡å­—Level</strong></td>
<td>æœªçŸ¥èªã«å¯¾å¿œã€èªå½™å°</td>
<td>ç³»åˆ—ãŒé•·ããªã‚‹</td>
<td>å½¢æ…‹ç´ è§£æãŒå›°é›£ãªè¨€èª</td>
</tr>
</tbody>
</table>

<hr>

<h2>2.6 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>RNNã®åŸºç¤</strong></p>
<ul>
<li>ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«é©ã—ãŸæ§‹é€ </li>
<li>éš ã‚ŒçŠ¶æ…‹ã§éå»ã®æƒ…å ±ã‚’ä¿æŒ</li>
<li>å‹¾é…æ¶ˆå¤±/çˆ†ç™ºã®å•é¡Œ</li>
</ul></li>

<li><p><strong>LSTM & GRU</strong></p>
<ul>
<li>ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§é•·æœŸä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’</li>
<li>LSTMã¯3ã‚²ãƒ¼ãƒˆã€GRUã¯2ã‚²ãƒ¼ãƒˆ</li>
<li>GRUã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãé«˜é€Ÿ</li>
</ul></li>

<li><p><strong>Seq2Seqãƒ¢ãƒ‡ãƒ«</strong></p>
<ul>
<li>Encoder-Decoderã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
<li>æ©Ÿæ¢°ç¿»è¨³ã€è¦ç´„ãªã©ã«å¿œç”¨</li>
<li>Teacher Forcingã§å­¦ç¿’ã‚’å®‰å®šåŒ–</li>
</ul></li>

<li><p><strong>Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ </strong></p>
<ul>
<li>å…¥åŠ›ã®é‡è¦ãªéƒ¨ minutesã«æ³¨ç›®</li>
<li>é•·ã„ç³»åˆ—ã§ã‚‚æ€§èƒ½å‘ä¸Š</li>
<li>è§£é‡ˆå¯èƒ½æ€§ã®å‘ä¸Š</li>
</ul></li>

<li><p><strong>Embeddingå±¤</strong></p>
<ul>
<li>å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›</li>
<li>äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ã®æ´»ç”¨</li>
<li>æ–‡å­—Levelãƒ¢ãƒ‡ãƒ«ã®åˆ©ç‚¹</li>
</ul></li>
</ol>

<h3>æ·±å±¤å­¦ç¿’NLPã®é€²åŒ–</h3>

<div class="mermaid">
graph LR
    A[RNN] --> B[LSTM/GRU]
    B --> C[Seq2Seq]
    C --> D[Attention]
    D --> E[Transformer]
    E --> F[BERT/GPT]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#c8e6c9
</div>

<h3>Next Chapterã¸</h3>

<p>Chapter 3 Chapterã§ã¯ã€<strong>Transformerã¨äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>Self-Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ </li>
<li>Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
<li>BERTã€GPTã®ä»•çµ„ã¿</li>
<li>Transfer Learningã®å®Ÿè·µ</li>
<li>Fine-tuningãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>RNNã¨LSTMã®ä¸»ãªé•ã„ã‚’3ã¤æŒ™ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<ol>
<li><p><strong>æ§‹é€ ã®è¤‡é›‘ã•</strong></p>
<ul>
<li>RNN: ã‚·ãƒ³ãƒ—ãƒ«ãªå†å¸°æ§‹é€ ã€1ã¤ã®éš ã‚ŒçŠ¶æ…‹ã®ã¿</li>
<li>LSTM: ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã‚’æŒã¡ã€éš ã‚ŒçŠ¶æ…‹ã¨ã‚»ãƒ«çŠ¶æ…‹ã®2ã¤ã‚’ä¿æŒ</li>
</ul></li>

<li><p><strong>é•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’èƒ½åŠ›</strong></p>
<ul>
<li>RNN: å‹¾é…æ¶ˆå¤±å•é¡Œã«ã‚ˆã‚Šã€é•·ã„ç³»åˆ—ã§å­¦ç¿’ãŒå›°é›£</li>
<li>LSTM: ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã«ã‚ˆã‚Šé•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹æœçš„ã«å­¦ç¿’å¯èƒ½</li>
</ul></li>

<li><p><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</strong></p>
<ul>
<li>RNN: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼ˆé«˜é€Ÿã ãŒè¡¨ç¾åŠ›ãŒé™å®šçš„ï¼‰</li>
<li>LSTM: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ï¼ˆç´„4å€ã€é«˜ã„è¡¨ç¾åŠ›ï¼‰</li>
</ul></li>
</ol>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ã€ç°¡å˜ãªLSTMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèªã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python"># è¦ä»¶:
# - vocab_size = 50
# - embed_size = 32
# - hidden_size = 64
# - num_classes = 3
# - å…¥åŠ›: (batch_size=4, seq_len=10)ã®æ•´æ•°ãƒ†ãƒ³ã‚½ãƒ«
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import torch.nn as nn

# LSTMãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
class SimpleLSTM(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):
        super(SimpleLSTM, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)
        # embedded: (batch_size, seq_len, embed_size)

        lstm_out, (hn, cn) = self.lstm(embedded)
        # lstm_out: (batch_size, seq_len, hidden_size)

        # æœ€å¾Œã®æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        out = self.fc(lstm_out[:, -1, :])
        # out: (batch_size, num_classes)

        return out

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
vocab_size = 50
embed_size = 32
hidden_size = 64
num_classes = 3

model = SimpleLSTM(vocab_size, embed_size, hidden_size, num_classes)

print("=== LSTMãƒ¢ãƒ‡ãƒ« ===")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in model.parameters()):,}")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
batch_size = 4
seq_len = 10
x = torch.randint(0, vocab_size, (batch_size, seq_len))

print(f"\nå…¥åŠ›å½¢çŠ¶: {x.shape}")

# Forward pass
with torch.no_grad():
    output = model(x)
    print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    print(f"\nå‡ºåŠ›:\n{output}")

    # äºˆæ¸¬ã‚¯ãƒ©ã‚¹
    predicted = output.argmax(dim=1)
    print(f"\näºˆæ¸¬ã‚¯ãƒ©ã‚¹: {predicted}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== LSTMãƒ¢ãƒ‡ãƒ« ===
ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 26,563

å…¥åŠ›å½¢çŠ¶: torch.Size([4, 10])
å‡ºåŠ›å½¢çŠ¶: torch.Size([4, 3])

å‡ºåŠ›:
tensor([[-0.1234,  0.5678, -0.2345],
        [ 0.3456, -0.6789,  0.1234],
        [-0.4567,  0.2345, -0.8901],
        [ 0.6789, -0.1234,  0.4567]])

äºˆæ¸¬ã‚¯ãƒ©ã‚¹: tensor([1, 0, 1, 0])
</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>Teacher Forcingã¨ã¯ä½•ã‹èª¬æ˜ã—ã€ãã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>

<p><strong>Teacher Forcingã¨ã¯</strong>ï¼š</p>
<p>Seq2Seqãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ™‚ã«ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å…¥åŠ›ã¨ã—ã¦ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®äºˆæ¸¬å€¤ã§ã¯ãªãã€æ­£è§£ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚</p>

<p><strong>ãƒ¡ãƒªãƒƒãƒˆ</strong>ï¼š</p>
<ol>
<li><strong>å­¦ç¿’ã®å®‰å®šåŒ–</strong>: æ­£ã—ã„å…¥åŠ›ã‚’ä½¿ã†ãŸã‚ã€å­¦ç¿’ãŒå®‰å®šã—åæŸãŒé€Ÿã„</li>
<li><strong>å‹¾é…ã®ä¼æ’­</strong>: èª¤ã£ãŸäºˆæ¸¬ã®é€£é–ã‚’é˜²ãã€åŠ¹æœçš„ãªå‹¾é…ä¼æ’­ãŒå¯èƒ½</li>
<li><strong>è¨“ç·´ hoursã®çŸ­ç¸®</strong>: åæŸãŒæ—©ã„ãŸã‚ã€è¨“ç·´ hoursãŒçŸ­ããªã‚‹</li>
</ol>

<p><strong>ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ</strong>ï¼š</p>
<ol>
<li><strong>Exposure Bias</strong>: è¨“ç·´ã¨æ¨è«–ã®æ¡ä»¶ãŒç•°ãªã‚‹ãŸã‚ã€æ¨è«–æ™‚ã«èª¤å·®ãŒè“„ç©ã—ã‚„ã™ã„</li>
<li><strong>éå­¦ç¿’ã®ãƒªã‚¹ã‚¯</strong>: æ­£è§£ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ã—ã™ãã€æ±åŒ–æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§</li>
<li><strong>ã‚¨ãƒ©ãƒ¼ä¼æ’­ã¸ã®è„†å¼±æ€§</strong>: æ¨è«–æ™‚ã«æœ€åˆã®äºˆæ¸¬ã‚’èª¤ã‚‹ã¨ã€ãã®å¾Œã®äºˆæ¸¬ã‚‚é€£é–çš„ã«æ‚ªåŒ–</li>
</ol>

<p><strong>å¯¾ç­–</strong>ï¼š</p>
<ul>
<li><strong>Scheduled Sampling</strong>: è¨“ç·´ã®é€²è¡Œã«å¿œã˜ã¦ã€Teacher Forcingã®ä½¿ç”¨ç‡ã‚’å¾ã€…ã«æ¸›ã‚‰ã™</li>
<li><strong>Mixed Training</strong>: ãƒ©ãƒ³ãƒ€ãƒ ã«æ­£è§£ã¨äºˆæ¸¬ã‚’ä½¿ã„ minutesã‘ã‚‹ï¼ˆteacher_forcing_ratio=0.5ãªã©ï¼‰</li>
</ul>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Bahdanau Attentionã‚’å®Ÿè£…ã—ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã¨ãƒ‡ã‚³ãƒ¼ãƒ€éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()

        # ãƒ‡ã‚³ãƒ¼ãƒ€éš ã‚ŒçŠ¶æ…‹ã®å¤‰æ›
        self.W1 = nn.Linear(hidden_size, hidden_size)
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å¤‰æ›
        self.W2 = nn.Linear(hidden_size, hidden_size)
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ç”¨
        self.V = nn.Linear(hidden_size, 1)

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: (batch_size, hidden_size)
            encoder_outputs: (batch_size, seq_len, hidden_size)

        Returns:
            context_vector: (batch_size, hidden_size)
            attention_weights: (batch_size, seq_len)
        """
        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)

        # decoder_hiddenã‚’å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ä½ç½®ã«å¯¾ã—ã¦ã‚³ãƒ”ãƒ¼
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        # (batch_size, seq_len, hidden_size)

        # ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—: tanh(W1*decoder + W2*encoder)
        energy = torch.tanh(self.W1(decoder_hidden) + self.W2(encoder_outputs))
        # (batch_size, seq_len, hidden_size)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—: V^T * energy
        attention_scores = self.V(energy).squeeze(2)
        # (batch_size, seq_len)

        # Softmaxã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’è¨ˆç®—
        attention_weights = F.softmax(attention_scores, dim=1)
        # (batch_size, seq_len)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®é‡ã¿ä»˜ãå’Œ
        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        # (batch_size, 1, hidden_size)
        context_vector = context_vector.squeeze(1)
        # (batch_size, hidden_size)

        return context_vector, attention_weights

# ãƒ†ã‚¹ãƒˆ
batch_size = 2
seq_len = 5
hidden_size = 64

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)
decoder_hidden = torch.randn(batch_size, hidden_size)

# Attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
attention = BahdanauAttention(hidden_size)

# Forward pass
context, weights = attention(decoder_hidden, encoder_outputs)

print("=== Bahdanau Attention ===")
print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›å½¢çŠ¶: {encoder_outputs.shape}")
print(f"ãƒ‡ã‚³ãƒ¼ãƒ€éš ã‚ŒçŠ¶æ…‹å½¢çŠ¶: {decoder_hidden.shape}")
print(f"\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: {context.shape}")
print(f"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿å½¢çŠ¶: {weights.shape}")

print(f"\næœ€åˆã®ãƒãƒƒãƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿:")
print(weights[0])
print(f"åˆè¨ˆ: {weights[0].sum():.4f}ï¼ˆ1.0ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰")

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.bar(range(seq_len), weights[0].detach().numpy())
plt.xlabel('Encoder Position')
plt.ylabel('Attention Weight')
plt.title('Attention Weights (Batch 1)')
plt.ylim(0, 1)
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.imshow(weights.detach().numpy(), cmap='YlOrRd', aspect='auto')
plt.colorbar(label='Attention Weight')
plt.xlabel('Encoder Position')
plt.ylabel('Batch')
plt.title('Attention Weights Heatmap')

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Bahdanau Attention ===
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 5, 64])
ãƒ‡ã‚³ãƒ¼ãƒ€éš ã‚ŒçŠ¶æ…‹å½¢çŠ¶: torch.Size([2, 64])

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: torch.Size([2, 64])
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿å½¢çŠ¶: torch.Size([2, 5])

æœ€åˆã®ãƒãƒƒãƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿:
tensor([0.2134, 0.1987, 0.2345, 0.1876, 0.1658])
åˆè¨ˆ: 1.0000ï¼ˆ1.0ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
</code></pre>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã™ã‚‹åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚ã©ã®ã‚ˆã†ãªå ´åˆã«ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
torch.manual_seed(42)

vocab_size = 100
embed_dim = 50

# ã‚µãƒ³ãƒ—ãƒ«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆå°è¦æ¨¡ï¼‰
num_samples = 50
seq_len = 10

X_train = torch.randint(0, vocab_size, (num_samples, seq_len))
y_train = torch.randint(0, 2, (num_samples,))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_test = torch.randint(0, vocab_size, (20, seq_len))
y_test = torch.randint(0, 2, (20,))

# äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
pretrained_embeddings = torch.randn(vocab_size, embed_dim)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes,
                 pretrained=None, freeze=False):
        super(TextClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim)

        if pretrained is not None:
            self.embedding.weight = nn.Parameter(pretrained)
            self.embedding.weight.requires_grad = not freeze

        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        out = self.fc(lstm_out[:, -1, :])
        return out

# è¨“ç·´é–¢æ•°
def train_model(model, X, y, epochs=100, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)

    losses = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        output = model(X)
        loss = criterion(output, y)

        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    return losses

# è©•ä¾¡é–¢æ•°
def evaluate_model(model, X, y):
    model.eval()
    with torch.no_grad():
        output = model(X)
        _, predicted = torch.max(output, 1)
        accuracy = (predicted == y).sum().item() / len(y)
    return accuracy

# å®Ÿé¨“1: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–
print("=== å®Ÿé¨“1: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–åŸ‹ã‚è¾¼ã¿ ===")
model_random = TextClassifier(vocab_size, embed_dim, 64, 2)
losses_random = train_model(model_random, X_train, y_train)
acc_random = evaluate_model(model_random, X_test, y_test)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {acc_random:.4f}")

# å®Ÿé¨“2: äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆå›ºå®šï¼‰
print("\n=== å®Ÿé¨“2: äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆå›ºå®šï¼‰===")
model_pretrained_frozen = TextClassifier(vocab_size, embed_dim, 64, 2,
                                        pretrained_embeddings, freeze=True)
losses_frozen = train_model(model_pretrained_frozen, X_train, y_train)
acc_frozen = evaluate_model(model_pretrained_frozen, X_test, y_test)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {acc_frozen:.4f}")

# å®Ÿé¨“3: äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆfine-tuningï¼‰
print("\n=== å®Ÿé¨“3: äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆfine-tuningï¼‰===")
model_pretrained_ft = TextClassifier(vocab_size, embed_dim, 64, 2,
                                    pretrained_embeddings, freeze=False)
losses_ft = train_model(model_pretrained_ft, X_train, y_train)
acc_ft = evaluate_model(model_pretrained_ft, X_test, y_test)
print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {acc_ft:.4f}")

# çµæœã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Lossæ›²ç·š
axes[0].plot(losses_random, label='Random', alpha=0.7)
axes[0].plot(losses_frozen, label='Pretrained (Frozen)', alpha=0.7)
axes[0].plot(losses_ft, label='Pretrained (Fine-tuning)', alpha=0.7)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training Loss Comparison')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ç²¾åº¦æ¯”è¼ƒ
methods = ['Random', 'Frozen', 'Fine-tuning']
accuracies = [acc_random, acc_frozen, acc_ft]
axes[1].bar(methods, accuracies, color=['#3182ce', '#f59e0b', '#10b981'])
axes[1].set_ylabel('Test Accuracy')
axes[1].set_title('Test Accuracy Comparison')
axes[1].set_ylim(0, 1)
axes[1].grid(True, alpha=0.3, axis='y')

for i, acc in enumerate(accuracies):
    axes[1].text(i, acc + 0.02, f'{acc:.4f}', ha='center', fontsize=10)

plt.tight_layout()
plt.show()

# è€ƒå¯Ÿ
print("\n=== è€ƒå¯Ÿ ===")
print("\nã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã€‘")
print("- äº‹å‰å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ï¼ˆå›ºå®šã¾ãŸã¯ fine-tuningï¼‰ãŒæœ‰åˆ©")
print("- ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã¯éå­¦ç¿’ã—ã‚„ã™ãã€æ±åŒ–æ€§èƒ½ãŒä½ã„")

print("\nã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã€‘")
print("- ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ã‚‚ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’å¯èƒ½")
print("- fine-tuningãŒæœ€ã‚‚é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹å¯èƒ½æ€§")

print("\nã€Recommendedæˆ¦ç•¥ã€‘")
print("ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„: äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆå›ºå®šï¼‰ > äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆfine-tuningï¼‰ > ãƒ©ãƒ³ãƒ€ãƒ ")
print("ãƒ‡ãƒ¼ã‚¿é‡ãŒä¸­ç¨‹åº¦: äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆfine-tuningï¼‰ > äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆå›ºå®šï¼‰ â‰ˆ ãƒ©ãƒ³ãƒ€ãƒ ")
print("ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„: äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆfine-tuningï¼‰ â‰ˆ ãƒ©ãƒ³ãƒ€ãƒ  > äº‹å‰å­¦ç¿’æ¸ˆã¿ï¼ˆå›ºå®šï¼‰")
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
<li>Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, 9(8), 1735-1780.</li>
<li>Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. <em>EMNLP 2014</em>.</li>
<li>Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. <em>ICLR 2015</em>.</li>
<li>Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. <em>EMNLP 2015</em>.</li>
<li>Goldberg, Y. (2017). <em>Neural Network Methods for Natural Language Processing</em>. Morgan & Claypool Publishers.</li>
</ol>

<div class="navigation">
    <a href="chapter1-text-preprocessing.html" class="nav-button">â† Previous Chapter: ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†åŸºç¤</a>
    <a href="chapter3-transformers.html" class="nav-button">Next Chapter: Transformerã¨äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ« â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
