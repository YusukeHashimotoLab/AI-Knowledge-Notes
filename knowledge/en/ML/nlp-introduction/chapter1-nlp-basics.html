<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: NLP Basics - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/nlp-introduction/index.html">NLP</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 1: NLP Basics</h1>
            <p class="subtitle">Fundamental Technologies in Natural Language Processing - From Text Preprocessing to Word Embeddings</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 30-35 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
                <span class="meta-item">üíª Code Examples: 10</span>
                <span class="meta-item">üìù Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand and implement various text preprocessing techniques</li>
<li>‚úÖ Master different types of tokenization and their use cases</li>
<li>‚úÖ Implement numerical representations of words (Bag of Words, TF-IDF)</li>
<li>‚úÖ Understand the principles of Word2Vec, GloVe, and FastText</li>
<li>‚úÖ Learn about unique challenges and solutions for Japanese NLP</li>
<li>‚úÖ Implement basic NLP tasks</li>
</ul>

<hr>

<h2>1.1 Text Preprocessing</h2>

<h3>What is Text Preprocessing?</h3>
<p><strong>Text Preprocessing</strong> is the process of converting raw text data into a format that machine learning models can process.</p>

<blockquote>
<p>"80% of NLP is preprocessing" - the quality of preprocessing significantly determines final model performance.</p>
</blockquote>

<h3>Overview of Text Preprocessing</h3>

<div class="mermaid">
graph TD
    A[Raw Text] --> B[Cleaning]
    B --> C[Tokenization]
    C --> D[Normalization]
    D --> E[Stopword Removal]
    E --> F[Stemming/Lemmatization]
    F --> G[Vectorization]
    G --> H[Model Input]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#e1f5fe
    style H fill:#c8e6c9
</div>

<h3>1.1.1 Tokenization</h3>

<p><strong>Tokenization</strong> is the process of splitting text into meaningful units (tokens).</p>

<h4>Word-Level Tokenization</h4>

<pre><code class="language-python">import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Download for the first time
# nltk.download('punkt')

text = """Natural Language Processing (NLP) is a field of AI.
It helps computers understand human language."""

# Sentence splitting
sentences = sent_tokenize(text)
print("=== Sentence Splitting ===")
for i, sent in enumerate(sentences, 1):
    print(f"{i}. {sent}")

# Word splitting
words = word_tokenize(text)
print("\n=== Word Tokenization ===")
print(f"Token count: {len(words)}")
print(f"First 10 tokens: {words[:10]}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Sentence Splitting ===
1. Natural Language Processing (NLP) is a field of AI.
2. It helps computers understand human language.

=== Word Tokenization ===
Token count: 20
First 10 tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of']
</code></pre>

<h4>Subword Tokenization</h4>

<pre><code class="language-python">from transformers import BertTokenizer

# BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "Tokenization is fundamental for NLP preprocessing."

# Tokenization
tokens = tokenizer.tokenize(text)
print("=== Subword Tokenization (BERT) ===")
print(f"Tokens: {tokens}")

# Convert to IDs
token_ids = tokenizer.encode(text, add_special_tokens=True)
print(f"\nToken IDs: {token_ids}")

# Decode
decoded = tokenizer.decode(token_ids)
print(f"Decoded: {decoded}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Subword Tokenization (BERT) ===
Tokens: ['token', '##ization', 'is', 'fundamental', 'for', 'nl', '##p', 'pre', '##processing', '.']

Token IDs: [101, 19204, 3989, 2003, 8148, 2005, 17953, 2243, 3653, 6693, 1012, 102]
Decoded: [CLS] tokenization is fundamental for nlp preprocessing. [SEP]
</code></pre>

<h4>Character-Level Tokenization</h4>

<pre><code class="language-python">text = "Hello, NLP!"

# Character-level tokenization
char_tokens = list(text)
print("=== Character-Level Tokenization ===")
print(f"Tokens: {char_tokens}")
print(f"Token count: {len(char_tokens)}")

# Unique characters
unique_chars = sorted(set(char_tokens))
print(f"Unique character count: {len(unique_chars)}")
print(f"Vocabulary: {unique_chars}")
</code></pre>

<h3>Comparison of Tokenization Methods</h3>

<table>
<thead>
<tr>
<th>Method</th>
<th>Granularity</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Word-Level</strong></td>
<td>Words</td>
<td>Easy to interpret</td>
<td>Large vocabulary, OOV problem</td>
<td>Traditional NLP</td>
</tr>
<tr>
<td><strong>Subword</strong></td>
<td>Word parts</td>
<td>Handles OOV, vocabulary compression</td>
<td>Somewhat complex</td>
<td>Modern Transformers</td>
</tr>
<tr>
<td><strong>Character-Level</strong></td>
<td>Characters</td>
<td>Minimal vocabulary, no OOV</td>
<td>Increased sequence length</td>
<td>Language modeling</td>
</tr>
</tbody>
</table>

<h3>1.1.2 Normalization and Standardization</h3>

<pre><code class="language-python">import re
import string

def normalize_text(text):
    """Text normalization"""
    # Lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)

    # Remove mentions
    text = re.sub(r'@\w+', '', text)

    # Remove hashtags
    text = re.sub(r'#\w+', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Multiple spaces to one
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Test
raw_text = """Check out https://example.com! @user tweeted #NLP is AMAZING!!
Contact us at 123-456-7890."""

normalized = normalize_text(raw_text)

print("=== Text Normalization ===")
print(f"Original text:\n{raw_text}\n")
print(f"After normalization:\n{normalized}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Text Normalization ===
Original text:
Check out https://example.com! @user tweeted #NLP is AMAZING!!
Contact us at 123-456-7890.

After normalization:
check out tweeted is amazing contact us at
</code></pre>

<h3>1.1.3 Stopword Removal</h3>

<p><strong>Stopwords</strong> are frequently occurring words that are not semantically important (like "the", "is", "a").</p>

<pre><code class="language-python">import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# nltk.download('stopwords')

text = "Natural language processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans."

# Tokenization
tokens = word_tokenize(text.lower())

# English stopwords
stop_words = set(stopwords.words('english'))

# Stopword removal
filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

print("=== Stopword Removal ===")
print(f"Original token count: {len(tokens)}")
print(f"Original tokens: {tokens[:15]}")
print(f"\nToken count after removal: {len(filtered_tokens)}")
print(f"Tokens after removal: {filtered_tokens}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Stopword Removal ===
Original token count: 21
Original tokens: ['natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between']

Token count after removal: 10
Tokens after removal: ['natural', 'language', 'processing', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans']
</code></pre>

<h3>1.1.4 Stemming and Lemmatization</h3>

<p><strong>Stemming</strong>: Convert words to their stem (rule-based)<br>
<strong>Lemmatization</strong>: Convert words to their dictionary form (dictionary-based)</p>

<pre><code class="language-python">from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# nltk.download('wordnet')
# nltk.download('omw-1.4')

# Stemming
stemmer = PorterStemmer()

# Lemmatization
lemmatizer = WordNetLemmatizer()

words = ["running", "runs", "ran", "easily", "fairly", "better", "worse"]

print("=== Stemming vs Lemmatization ===")
print(f"{'Word':<15} {'Stemming':<15} {'Lemmatization':<15}")
print("-" * 45)

for word in words:
    stemmed = stemmer.stem(word)
    lemmatized = lemmatizer.lemmatize(word, pos='v')  # Process as verb
    print(f"{word:<15} {stemmed:<15} {lemmatized:<15}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Stemming vs Lemmatization ===
Word             Stemming       Lemmatization
---------------------------------------------
running         run             run
runs            run             run
ran             ran             run
easily          easili          easily
fairly          fairli          fairly
better          better          better
worse           wors            worse
</code></pre>

<blockquote>
<p><strong>Selection Guidelines</strong>: Stemming is fast but rough. Lemmatization is accurate but slow. Choose based on task requirements.</p>
</blockquote>

<hr>

<h2>1.2 Word Representations</h2>

<h3>1.2.1 One-Hot Encoding</h3>

<p><strong>One-Hot Encoding</strong> represents each word as a vector with vocabulary-size dimensions, where only the corresponding position is 1 and all others are 0.</p>

<pre><code class="language-python">import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

sentences = ["I love NLP", "NLP is amazing", "I love AI"]

# Collect all words
words = ' '.join(sentences).lower().split()
unique_words = sorted(set(words))

print("=== One-Hot Encoding ===")
print(f"Vocabulary: {unique_words}")
print(f"Vocabulary size: {len(unique_words)}")

# Create One-Hot Encoding
word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

def one_hot_encode(word, vocab_size):
    """Convert word to One-Hot vector"""
    vector = np.zeros(vocab_size)
    if word in word_to_idx:
        vector[word_to_idx[word]] = 1
    return vector

# One-Hot representation of each word
print("\nOne-Hot representation of words:")
for word in ["nlp", "love", "ai"]:
    vector = one_hot_encode(word, len(unique_words))
    print(f"{word}: {vector}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== One-Hot Encoding ===
Vocabulary: ['ai', 'amazing', 'i', 'is', 'love', 'nlp']
Vocabulary size: 6

One-Hot representation of words:
nlp: [0. 0. 0. 0. 0. 1.]
love: [0. 0. 0. 0. 1. 0.]
ai: [1. 0. 0. 0. 0. 0.]
</code></pre>

<blockquote>
<p><strong>Issues</strong>: Vectors become huge as vocabulary grows (curse of dimensionality), cannot express semantic relationships between words.</p>
</blockquote>

<h3>1.2.2 Bag of Words (BoW)</h3>

<p><strong>Bag of Words</strong> represents documents as word occurrence frequency vectors.</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "I love machine learning",
    "I love deep learning",
    "Deep learning is powerful",
    "Machine learning is interesting"
]

# BoW vectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

# Vocabulary
vocab = vectorizer.get_feature_names_out()

print("=== Bag of Words ===")
print(f"Vocabulary: {vocab}")
print(f"Vocabulary size: {len(vocab)}")
print(f"\nDocument-term matrix:")
print(X.toarray())

# Representation for each document
import pandas as pd
df = pd.DataFrame(X.toarray(), columns=vocab)
print("\nDocument-term matrix (DataFrame):")
print(df)
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Bag of Words ===
Vocabulary: ['deep' 'interesting' 'is' 'learning' 'love' 'machine' 'powerful']
Vocabulary size: 7

Document-term matrix:
[[0 0 0 1 1 1 0]
 [1 0 0 1 1 0 0]
 [1 0 1 1 0 0 1]
 [0 1 1 1 0 1 0]]

Document-term matrix (DataFrame):
   deep  interesting  is  learning  love  machine  powerful
0     0            0   0         1     1        1         0
1     1            0   0         1     1        0         0
2     1            0   1         1     0        0         1
3     0            1   1         1     0        1         0
</code></pre>

<h3>1.2.3 TF-IDF</h3>

<p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> is a method for evaluating word importance.</p>

<p>$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$</p>

<p>$$
\text{IDF}(t) = \log\left(\frac{N}{df(t)}\right)
$$</p>

<ul>
<li>$\text{TF}(t, d)$: Frequency of word $t$ in document $d$</li>
<li>$N$: Total number of documents</li>
<li>$df(t)$: Number of documents containing word $t$</li>
</ul>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are animals",
    "The mat is on the floor"
]

# TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(corpus)

# Vocabulary
vocab = tfidf_vectorizer.get_feature_names_out()

print("=== TF-IDF ===")
print(f"Vocabulary: {vocab}")
print(f"\nTF-IDF matrix:")
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vocab)
print(df_tfidf.round(3))

# Most important words (document 0)
doc_idx = 0
feature_scores = list(zip(vocab, X_tfidf.toarray()[doc_idx]))
sorted_scores = sorted(feature_scores, key=lambda x: x[1], reverse=True)

print(f"\nTop 3 important words in document {doc_idx}:")
for word, score in sorted_scores[:3]:
    print(f"  {word}: {score:.3f}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== TF-IDF ===
Vocabulary: ['and' 'animals' 'are' 'cat' 'cats' 'dog' 'dogs' 'floor' 'is' 'log' 'mat' 'on' 'sat' 'the']

TF-IDF matrix:
   and  animals   are   cat  cats   dog  dogs  floor    is   log   mat    on   sat   the
0  0.00    0.000  0.00  0.48  0.00  0.00  0.00   0.00  0.00  0.00  0.48  0.35  0.35  0.58
1  0.00    0.000  0.00  0.00  0.00  0.50  0.00   0.00  0.00  0.50  0.00  0.36  0.36  0.60
2  0.41    0.410  0.41  0.00  0.31  0.00  0.31   0.00  0.00  0.00  0.00  0.00  0.00  0.52
3  0.00    0.000  0.00  0.00  0.00  0.00  0.00   0.50  0.50  0.00  0.38  0.28  0.00  0.55

Top 3 important words in document 0:
  the: 0.576
  cat: 0.478
  mat: 0.478
</code></pre>

<h3>1.2.4 N-gram Models</h3>

<p><strong>N-grams</strong> are combinations of N consecutive words.</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

text = ["Natural language processing is fun"]

# Unigram (1-gram)
unigram_vec = CountVectorizer(ngram_range=(1, 1))
unigrams = unigram_vec.fit_transform(text)
print("=== Unigram (1-gram) ===")
print(unigram_vec.get_feature_names_out())

# Bigram (2-gram)
bigram_vec = CountVectorizer(ngram_range=(2, 2))
bigrams = bigram_vec.fit_transform(text)
print("\n=== Bigram (2-gram) ===")
print(bigram_vec.get_feature_names_out())

# Trigram (3-gram)
trigram_vec = CountVectorizer(ngram_range=(3, 3))
trigrams = trigram_vec.fit_transform(text)
print("\n=== Trigram (3-gram) ===")
print(trigram_vec.get_feature_names_out())

# 1-gram to 3-gram
combined_vec = CountVectorizer(ngram_range=(1, 3))
combined = combined_vec.fit_transform(text)
print(f"\n=== Combined (1-3 gram) ===")
print(f"Total feature count: {len(combined_vec.get_feature_names_out())}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Unigram (1-gram) ===
['fun' 'is' 'language' 'natural' 'processing']

=== Bigram (2-gram) ===
['is fun' 'language processing' 'natural language' 'processing is']

=== Trigram (3-gram) ===
['language processing is' 'natural language processing' 'processing is fun']

=== Combined (1-3 gram) ===
Total feature count: 12
</code></pre>

<hr>

<h2>1.3 Word Embeddings</h2>

<h3>What are Word Embeddings?</h3>

<p><strong>Word Embeddings</strong> are methods that represent words as low-dimensional dense vectors. Semantically similar words are placed close together.</p>

<div class="mermaid">
graph LR
    A[One-Hot<br>Sparse, High-dim] --> B[Word2Vec<br>Dense, Low-dim]
    A --> C[GloVe<br>Dense, Low-dim]
    A --> D[FastText<br>Dense, Low-dim]

    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#e3f2fd
    style D fill:#fff3e0
</div>

<h3>1.3.1 Word2Vec</h3>

<p><strong>Word2Vec</strong> is a method that learns distributed representations of words from large corpora. There are two architectures:</p>

<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong>: Predict center word from surrounding words</li>
<li><strong>Skip-gram</strong>: Predict surrounding words from center word</li>
</ul>

<pre><code class="language-python">from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import numpy as np

# Sample corpus
corpus = [
    "Natural language processing with deep learning",
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks",
    "Neural networks are inspired by biological neurons",
    "Natural language understanding requires context",
    "Context is important in language processing"
]

# Tokenization
tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]

# Train Word2Vec model
# Skip-gram model (sg=1), CBOW (sg=0)
model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,  # Embedding dimension
    window=5,         # Context window
    min_count=1,      # Minimum occurrence count
    sg=1,             # Skip-gram
    workers=4
)

print("=== Word2Vec ===")
print(f"Vocabulary size: {len(model.wv)}")
print(f"Embedding dimension: {model.wv.vector_size}")

# Get word vector
word = "learning"
vector = model.wv[word]
print(f"\nVector for '{word}' (first 10 dimensions):")
print(vector[:10])

# Find similar words
similar_words = model.wv.most_similar("learning", topn=5)
print(f"\nWords similar to '{word}':")
for word, similarity in similar_words:
    print(f"  {word}: {similarity:.3f}")

# Word similarity
similarity = model.wv.similarity("neural", "networks")
print(f"\nSimilarity between 'neural' and 'networks': {similarity:.3f}")

# Word arithmetic (King - Man + Woman ‚âà Queen example)
# Simple example: deep - neural + machine
try:
    result = model.wv.most_similar(
        positive=['deep', 'machine'],
        negative=['neural'],
        topn=3
    )
    print("\nWord arithmetic (deep - neural + machine):")
    for word, score in result:
        print(f"  {word}: {score:.3f}")
except:
    print("\nWord arithmetic: Insufficient data")
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== Word2Vec ===
Vocabulary size: 27
Embedding dimension: 100

Vector for 'learning' (first 10 dimensions):
[-0.00234  0.00891 -0.00156  0.00423 -0.00678  0.00234  0.00567 -0.00123  0.00789 -0.00345]

Words similar to 'learning':
  deep: 0.876
  neural: 0.823
  processing: 0.791
  networks: 0.765
  natural: 0.734

Similarity between 'neural' and 'networks': 0.892
</code></pre>

<h3>1.3.2 GloVe (Global Vectors)</h3>

<p><strong>GloVe</strong> learns embeddings using word co-occurrence statistics. Unlike Word2Vec, it leverages global co-occurrence information.</p>

<pre><code class="language-python">import gensim.downloader as api
import numpy as np

# Load pre-trained GloVe model (downloads on first run)
print("Downloading GloVe model...")
glove_model = api.load("glove-wiki-gigaword-100")

print("\n=== GloVe (Pre-trained) ===")
print(f"Vocabulary size: {len(glove_model)}")
print(f"Embedding dimension: {glove_model.vector_size}")

# Word vector
word = "computer"
vector = glove_model[word]
print(f"\nVector for '{word}' (first 10 dimensions):")
print(vector[:10])

# Similar words
similar_words = glove_model.most_similar(word, topn=5)
print(f"\nWords similar to '{word}':")
for w, sim in similar_words:
    print(f"  {w}: {sim:.3f}")

# Famous word arithmetic: King - Man + Woman ‚âà Queen
result = glove_model.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=5
)
print("\nWord arithmetic (King - Man + Woman):")
for w, sim in result:
    print(f"  {w}: {sim:.3f}")

# Similarity calculation
pairs = [
    ("good", "bad"),
    ("good", "excellent"),
    ("cat", "dog"),
    ("cat", "car")
]
print("\nWord pair similarities:")
for w1, w2 in pairs:
    sim = glove_model.similarity(w1, w2)
    print(f"  {w1} - {w2}: {sim:.3f}")
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== GloVe (Pre-trained) ===
Vocabulary size: 400000
Embedding dimension: 100

Vector for 'computer' (first 10 dimensions):
[ 0.45893  0.19521 -0.23456  0.67234 -0.34521  0.12345  0.89012 -0.45678  0.23456 -0.78901]

Words similar to 'computer':
  computers: 0.887
  software: 0.756
  hardware: 0.734
  pc: 0.712
  system: 0.689

Word arithmetic (King - Man + Woman):
  queen: 0.768
  monarch: 0.654
  princess: 0.621
  crown: 0.598
  prince: 0.587

Word pair similarities:
  good - bad: 0.523
  good - excellent: 0.791
  cat - dog: 0.821
  cat - car: 0.234
</code></pre>

<h3>1.3.3 FastText</h3>

<p><strong>FastText</strong> is a word embedding that can handle out-of-vocabulary (OOV) words by using subword information.</p>

<pre><code class="language-python">from gensim.models import FastText

# Sample corpus
sentences = [
    ["machine", "learning", "is", "awesome"],
    ["deep", "learning", "with", "neural", "networks"],
    ["natural", "language", "processing"],
    ["fasttext", "handles", "unknown", "words"]
]

# Train FastText model
ft_model = FastText(
    sentences=sentences,
    vector_size=100,
    window=3,
    min_count=1,
    sg=1  # Skip-gram
)

print("=== FastText ===")
print(f"Vocabulary size: {len(ft_model.wv)}")

# Word in training data
word = "learning"
vector = ft_model.wv[word]
print(f"\nVector for '{word}' (first 5 dimensions):")
print(vector[:5])

# Unknown word (OOV) can still get vector
unknown_word = "machinelearning"  # Not in training data
try:
    unknown_vector = ft_model.wv[unknown_word]
    print(f"\nVector for unknown word '{unknown_word}' (first 5 dimensions):")
    print(unknown_vector[:5])
    print("‚úì FastText can handle unknown words")
except:
    print(f"\nUnknown word '{unknown_word}' cannot be vectorized")

# Similar words
similar = ft_model.wv.most_similar("learning", topn=3)
print(f"\nWords similar to '{word}':")
for w, sim in similar:
    print(f"  {w}: {sim:.3f}")
</code></pre>

<h3>Word2Vec vs GloVe vs FastText</h3>

<table>
<thead>
<tr>
<th>Method</th>
<th>Learning Approach</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>OOV Support</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Word2Vec</strong></td>
<td>Local co-occurrence (window)</td>
<td>Fast, efficient</td>
<td>Ignores global statistics</td>
<td>No</td>
</tr>
<tr>
<td><strong>GloVe</strong></td>
<td>Global co-occurrence matrix</td>
<td>Utilizes global statistics</td>
<td>Somewhat slower</td>
<td>No</td>
</tr>
<tr>
<td><strong>FastText</strong></td>
<td>Subword information</td>
<td>OOV support, morphological info</td>
<td>Somewhat complex</td>
<td>Yes</td>
</tr>
</tbody>
</table>

<hr>

<h2>1.4 Japanese NLP</h2>

<h3>Characteristics and Challenges of Japanese</h3>

<p>Japanese differs from English in the following ways:</p>

<ul>
<li>No spaces between words (word segmentation required)</li>
<li>Multiple character types (hiragana, katakana, kanji, romaji)</li>
<li>Orthographic variations with same meaning (e.g., "„Ç≥„É≥„Éî„É•„Éº„Çø" "„Ç≥„É≥„Éî„É•„Éº„Çø„Éº")</li>
<li>Context-dependent meaning determination</li>
</ul>

<h3>1.4.1 Morphological Analysis with MeCab</h3>

<pre><code class="language-python">import MeCab

# Initialize MeCab
mecab = MeCab.Tagger()

text = "Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØ‰∫∫Â∑•Áü•ËÉΩ„ÅÆ‰∏ÄÂàÜÈáé„Åß„Åô„ÄÇ"

# Morphological analysis
print("=== MeCab Morphological Analysis ===")
print(mecab.parse(text))

# Word segmentation
mecab_wakati = MeCab.Tagger("-Owakati")
wakati_text = mecab_wakati.parse(text).strip()
print(f"Word segmentation: {wakati_text}")

# Extract part-of-speech information
node = mecab.parseToNode(text)
words = []
pos_tags = []

while node:
    features = node.feature.split(',')
    if node.surface:
        words.append(node.surface)
        pos_tags.append(features[0])
    node = node.next

print("\nWords and part-of-speech:")
for word, pos in zip(words, pos_tags):
    print(f"  {word}: {pos}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== MeCab Morphological Analysis ===
Ëá™ÁÑ∂	ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Ëá™ÁÑ∂,„Ç∑„Çº„É≥,„Ç∑„Çº„É≥
Ë®ÄË™û	ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Ë®ÄË™û,„Ç≤„É≥„Ç¥,„Ç≤„É≥„Ç¥
Âá¶ÁêÜ	ÂêçË©û,„ÇµÂ§âÊé•Á∂ö,*,*,*,*,Âá¶ÁêÜ,„Ç∑„Éß„É™,„Ç∑„Éß„É™
„ÅØ	Âä©Ë©û,‰øÇÂä©Ë©û,*,*,*,*,„ÅØ,„Éè,„ÉØ
‰∫∫Â∑•	ÂêçË©û,‰∏ÄËà¨,*,*,*,*,‰∫∫Â∑•,„Ç∏„É≥„Ç≥„Ç¶,„Ç∏„É≥„Ç≥„Éº
Áü•ËÉΩ	ÂêçË©û,‰∏ÄËà¨,*,*,*,*,Áü•ËÉΩ,„ÉÅ„Éé„Ç¶,„ÉÅ„Éé„Éº
„ÅÆ	Âä©Ë©û,ÈÄ£‰ΩìÂåñ,*,*,*,*,„ÅÆ,„Éé,„Éé
‰∏Ä	ÂêçË©û,Êï∞,*,*,*,*,‰∏Ä,„Ç§„ÉÅ,„Ç§„ÉÅ
ÂàÜÈáé	ÂêçË©û,‰∏ÄËà¨,*,*,*,*,ÂàÜÈáé,„Éñ„É≥„É§,„Éñ„É≥„É§
„Åß„Åô	Âä©ÂãïË©û,*,*,*,ÁâπÊÆä„Éª„Éá„Çπ,Âü∫Êú¨ÂΩ¢,„Åß„Åô,„Éá„Çπ,„Éá„Çπ
„ÄÇ	Ë®òÂè∑,Âè•ÁÇπ,*,*,*,*,„ÄÇ,„ÄÇ,„ÄÇ
EOS

Word segmentation: Ëá™ÁÑ∂ Ë®ÄË™û Âá¶ÁêÜ „ÅØ ‰∫∫Â∑• Áü•ËÉΩ „ÅÆ ‰∏Ä ÂàÜÈáé „Åß„Åô „ÄÇ

Words and part-of-speech:
  Ëá™ÁÑ∂: ÂêçË©û
  Ë®ÄË™û: ÂêçË©û
  Âá¶ÁêÜ: ÂêçË©û
  „ÅØ: Âä©Ë©û
  ‰∫∫Â∑•: ÂêçË©û
  Áü•ËÉΩ: ÂêçË©û
  „ÅÆ: Âä©Ë©û
  ‰∏Ä: ÂêçË©û
  ÂàÜÈáé: ÂêçË©û
  „Åß„Åô: Âä©ÂãïË©û
  „ÄÇ: Ë®òÂè∑
</code></pre>

<h3>1.4.2 Morphological Analysis with SudachiPy</h3>

<p><strong>SudachiPy</strong> provides multiple split modes (A: short unit, B: medium unit, C: long unit).</p>

<pre><code class="language-python">from sudachipy import tokenizer
from sudachipy import dictionary

# Initialize Sudachi
tokenizer_obj = dictionary.Dictionary().create()

text = "Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫„Å´Ë°å„Åç„Åæ„Åó„Åü„ÄÇ"

print("=== SudachiPy Split Mode Comparison ===\n")

# Mode A (short unit)
mode_a = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.A)
print("Mode A (short unit):")
print([m.surface() for m in mode_a])

# Mode B (medium unit)
mode_b = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.B)
print("\nMode B (medium unit):")
print([m.surface() for m in mode_b])

# Mode C (long unit)
mode_c = tokenizer_obj.tokenize(text, tokenizer.Tokenizer.SplitMode.C)
print("\nMode C (long unit):")
print([m.surface() for m in mode_c])

# Detailed information
print("\nDetailed information (Mode B):")
for token in mode_b:
    print(f"  Surface form: {token.surface()}")
    print(f"  Dictionary form: {token.dictionary_form()}")
    print(f"  Part-of-speech: {token.part_of_speech()[0]}")
    print(f"  Reading: {token.reading_form()}")
    print()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== SudachiPy Split Mode Comparison ===

Mode A (short unit):
['Êù±‰∫¨', 'ÈÉΩ', 'Ê∏ãË∞∑', 'Âå∫', '„Å´', 'Ë°å„Åç', '„Åæ„Åó', '„Åü', '„ÄÇ']

Mode B (medium unit):
['Êù±‰∫¨ÈÉΩ', 'Ê∏ãË∞∑Âå∫', '„Å´', 'Ë°å„Åè', '„Åü', '„ÄÇ']

Mode C (long unit):
['Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫', '„Å´', 'Ë°å„Åè', '„Åü', '„ÄÇ']

Detailed information (Mode B):
  Surface form: Êù±‰∫¨ÈÉΩ
  Dictionary form: Êù±‰∫¨ÈÉΩ
  Part-of-speech: ÂêçË©û
  Reading: „Éà„Ç¶„Ç≠„Éß„Ç¶„Éà
  ...
</code></pre>

<h3>1.4.3 Japanese Text Normalization</h3>

<pre><code class="language-python">import unicodedata

def normalize_japanese(text):
    """Japanese text normalization"""
    # Unicode normalization (NFKC: compatible characters to standard form)
    text = unicodedata.normalize('NFKC', text)

    # Full-width alphanumerics to half-width
    text = text.translate(str.maketrans(
        'ÔºêÔºëÔºíÔºìÔºîÔºïÔºñÔºóÔºòÔºôÔº°Ôº¢Ôº£Ôº§Ôº•Ôº¶ÔºßÔº®Ôº©Ôº™Ôº´Ôº¨Ôº≠ÔºÆÔºØÔº∞Ôº±Ôº≤Ôº≥Ôº¥ÔºµÔº∂Ôº∑Ôº∏ÔºπÔº∫ÔΩÅÔΩÇÔΩÉÔΩÑÔΩÖÔΩÜÔΩáÔΩàÔΩâÔΩäÔΩãÔΩåÔΩçÔΩéÔΩèÔΩêÔΩëÔΩíÔΩìÔΩîÔΩïÔΩñÔΩóÔΩòÔΩôÔΩö',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
    ))

    # Unify long vowel marks
    text = text.replace('„Éº', '')

    return text

# Test
texts = [
    "„Ç≥„É≥„Éî„É•„Éº„Çø",
    "„Ç≥„É≥„Éî„É•„Éº„Çø„Éº",
    "Ôº°Ôº©ÊäÄË°ì",
    "AIÊäÄË°ì",
    "ÔºëÔºíÔºìÔºîÔºï"
]

print("=== Japanese Normalization ===")
for original in texts:
    normalized = normalize_japanese(original)
    print(f"{original} ‚Üí {normalized}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Japanese Normalization ===
„Ç≥„É≥„Éî„É•„Éº„Çø ‚Üí „Ç≥„É≥„Éî„É•„Éº„Çø
„Ç≥„É≥„Éî„É•„Éº„Çø„Éº ‚Üí „Ç≥„É≥„Éî„É•„Éº„Çø
Ôº°Ôº©ÊäÄË°ì ‚Üí AIÊäÄË°ì
AIÊäÄË°ì ‚Üí AIÊäÄË°ì
ÔºëÔºíÔºìÔºîÔºï ‚Üí 12345
</code></pre>

<hr>

<h2>1.5 Basic NLP Tasks</h2>

<h3>1.5.1 Document Classification</h3>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Sample data
texts = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Natural language processing analyzes text",
    "Computer vision recognizes images",
    "Reinforcement learning learns from rewards",
    "Supervised learning uses labeled data",
    "Unsupervised learning finds patterns",
    "NLP understands human language",
    "CNN is used for image classification",
    "RNN is good for sequence data"
]

labels = [
    "ML", "DL", "NLP", "CV", "RL",
    "ML", "ML", "NLP", "CV", "DL"
]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.3, random_state=42
)

# TF-IDF vectorization
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Naive Bayes classifier
classifier = MultinomialNB()
classifier.fit(X_train_tfidf, y_train)

# Prediction
y_pred = classifier.predict(X_test_tfidf)

# Evaluation
print("=== Document Classification ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"\nClassification report:")
print(classification_report(y_test, y_pred))

# Classify new documents
new_texts = [
    "Neural networks are powerful",
    "Text mining extracts information"
]
new_tfidf = vectorizer.transform(new_texts)
predictions = classifier.predict(new_tfidf)

print("\nClassification of new documents:")
for text, pred in zip(new_texts, predictions):
    print(f"  '{text}' ‚Üí {pred}")
</code></pre>

<h3>1.5.2 Similarity Calculation</h3>

<pre><code class="language-python">from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "Machine learning is fun",
    "Deep learning is exciting",
    "Natural language processing is interesting",
    "I love pizza and pasta",
    "Python is a great programming language"
]

# TF-IDF vectorization
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Calculate cosine similarity
similarity_matrix = cosine_similarity(tfidf_matrix)

print("=== Cosine Similarity Between Documents ===\n")
print("Similarity matrix:")
import pandas as pd
df_sim = pd.DataFrame(
    similarity_matrix,
    index=[f"Doc{i}" for i in range(len(documents))],
    columns=[f"Doc{i}" for i in range(len(documents))]
)
print(df_sim.round(3))

# Most similar document pairs
print("\nMost similar document for each document:")
for i, doc in enumerate(documents):
    # Exclude itself
    similarities = similarity_matrix[i].copy()
    similarities[i] = -1
    most_similar_idx = similarities.argmax()
    print(f"Doc{i}: '{doc[:30]}...'")
    print(f"  ‚Üí Doc{most_similar_idx}: '{documents[most_similar_idx][:30]}...' (similarity: {similarities[most_similar_idx]:.3f})\n")
</code></pre>

<h3>1.5.3 Text Clustering</h3>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

documents = [
    "Machine learning algorithms are powerful",
    "Deep learning uses neural networks",
    "Supervised learning needs labeled data",
    "Pizza is delicious food",
    "I love eating pasta",
    "Italian cuisine is amazing",
    "Python is a programming language",
    "JavaScript is used for web development",
    "Java is object-oriented"
]

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=20)
X = vectorizer.fit_transform(documents)

# K-Means clustering
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(X)

print("=== Text Clustering ===\n")
print(f"Number of clusters: {n_clusters}\n")

# Display documents by cluster
for i in range(n_clusters):
    print(f"Cluster {i}:")
    cluster_docs = [doc for doc, cluster in zip(documents, clusters) if cluster == i]
    for doc in cluster_docs:
        print(f"  - {doc}")
    print()

# Words close to cluster centers
feature_names = vectorizer.get_feature_names_out()
print("Characteristic words for each cluster (top 5):")
for i in range(n_clusters):
    center = kmeans.cluster_centers_[i]
    top_indices = center.argsort()[-5:][::-1]
    top_words = [feature_names[idx] for idx in top_indices]
    print(f"Cluster {i}: {', '.join(top_words)}")
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== Text Clustering ===

Number of clusters: 3

Cluster 0:
  - Machine learning algorithms are powerful
  - Deep learning uses neural networks
  - Supervised learning needs labeled data

Cluster 1:
  - Pizza is delicious food
  - I love eating pasta
  - Italian cuisine is amazing

Cluster 2:
  - Python is a programming language
  - JavaScript is used for web development
  - Java is object-oriented

Characteristic words for each cluster (top 5):
Cluster 0: learning, neural, deep, machine, supervised
Cluster 1: italian, food, pizza, pasta, cuisine
Cluster 2: programming, language, python, java, javascript
</code></pre>

<hr>

<h2>1.6 Chapter Summary</h2>

<h3>What We Learned</h3>

<ol>
<li><p><strong>Text Preprocessing</strong></p>
<ul>
<li>Tokenization (word, subword, character-level)</li>
<li>Normalization and standardization</li>
<li>Stopword removal</li>
<li>Stemming and lemmatization</li>
</ul></li>

<li><p><strong>Word Representations</strong></p>
<ul>
<li>One-Hot Encoding: Simple but high-dimensional</li>
<li>Bag of Words: Frequency-based</li>
<li>TF-IDF: Considers word importance</li>
<li>N-grams: Word combinations</li>
</ul></li>

<li><p><strong>Word Embeddings</strong></p>
<ul>
<li>Word2Vec: Learn from local co-occurrence</li>
<li>GloVe: Utilize global co-occurrence statistics</li>
<li>FastText: Handle OOV with subword information</li>
</ul></li>

<li><p><strong>Japanese NLP</strong></p>
<ul>
<li>MeCab: Fast morphological analysis</li>
<li>SudachiPy: Flexible split modes</li>
<li>Unicode normalization and orthographic variation handling</li>
</ul></li>

<li><p><strong>Basic NLP Tasks</strong></p>
<ul>
<li>Document classification</li>
<li>Similarity calculation</li>
<li>Text clustering</li>
</ul></li>
</ol>

<h3>Method Selection Guidelines</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small-scale document classification</td>
<td>TF-IDF + Linear model</td>
<td>Simple, fast</td>
</tr>
<tr>
<td>Semantic similarity</td>
<td>Word Embeddings</td>
<td>Captures meaning</td>
</tr>
<tr>
<td>OOV handling</td>
<td>FastText, Subword</td>
<td>Uses morphological info</td>
</tr>
<tr>
<td>Large-scale data</td>
<td>Pre-trained models</td>
<td>Transfer learning</td>
</tr>
<tr>
<td>Japanese processing</td>
<td>MeCab/SudachiPy + Normalization</td>
<td>Handles language characteristics</td>
</tr>
</tbody>
</table>

<h3>Next Chapter</h3>

<p>In Chapter 2, we will learn about <strong>Sequence Models and RNN</strong>:</p>
<ul>
<li>Recurrent Neural Networks (RNN)</li>
<li>LSTM (Long Short-Term Memory)</li>
<li>GRU (Gated Recurrent Unit)</li>
<li>Sequence data modeling</li>
<li>Text generation</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain the differences between stemming and lemmatization, and describe their respective advantages and disadvantages.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>

<p><strong>Stemming</strong>:</p>
<ul>
<li>Definition: Rule-based conversion of words to stems</li>
<li>Example: "running" ‚Üí "run", "studies" ‚Üí "studi"</li>
<li>Advantages: Fast, simple implementation</li>
<li>Disadvantages: Stem may not be an actual word, excessive or insufficient trimming may occur</li>
</ul>

<p><strong>Lemmatization</strong>:</p>
<ul>
<li>Definition: Dictionary-based conversion of words to base form (lemma)</li>
<li>Example: "running" ‚Üí "run", "better" ‚Üí "good"</li>
<li>Advantages: Accurate, always returns valid words</li>
<li>Disadvantages: Slow, requires dictionary, may need part-of-speech information</li>
</ul>

<p><strong>Selection Guide</strong>:</p>
<ul>
<li>Speed priority, rough processing: Stemming</li>
<li>Accuracy priority, meaning preservation: Lemmatization</li>
</ul>

</details>

<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Calculate TF-IDF for the following text and identify the most important words.</p>

<pre><code class="language-python">documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets"
]
</code></pre>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets"
]

# TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Feature names
feature_names = vectorizer.get_feature_names_out()

# Display as DataFrame
df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=feature_names
)

print("=== TF-IDF Matrix ===")
print(df.round(3))

# Most important word for each document
print("\nMost important word for each document:")
for i, doc in enumerate(documents):
    scores = tfidf_matrix[i].toarray()[0]
    top_idx = scores.argmax()
    top_word = feature_names[top_idx]
    top_score = scores[top_idx]
    print(f"Document {i}: '{doc}'")
    print(f"  Most important word: '{top_word}' (score: {top_score:.3f})")

    # Top 3
    top_3_indices = scores.argsort()[-3:][::-1]
    print("  Top 3:")
    for idx in top_3_indices:
        if scores[idx] > 0:
            print(f"    {feature_names[idx]}: {scores[idx]:.3f}")
    print()
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== TF-IDF Matrix ===
   and   are   cat  cats   dog  dogs   log   mat    on  pets   sat   the
0  0.00  0.00  0.48  0.00  0.00  0.00  0.00  0.48  0.35  0.00  0.35  0.58
1  0.00  0.00  0.00  0.00  0.50  0.00  0.50  0.00  0.36  0.00  0.36  0.60
2  0.41  0.41  0.00  0.31  0.00  0.31  0.00  0.00  0.00  0.41  0.00  0.52

Most important word for each document:
Document 0: 'The cat sat on the mat'
  Most important word: 'the' (score: 0.576)
  Top 3:
    the: 0.576
    cat: 0.478
    mat: 0.478

Document 1: 'The dog sat on the log'
  Most important word: 'the' (score: 0.596)
  Top 3:
    the: 0.596
    log: 0.496
    dog: 0.496

Document 2: 'Cats and dogs are pets'
  Most important word: 'the' (score: 0.524)
  Top 3:
    the: 0.524
    and: 0.412
    pets: 0.412
</code></pre>

</details>

<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Explain the differences between Word2Vec's two architectures (CBOW and Skip-gram), and describe in what situations each is effective.</p>

<details>
<summary>Sample Answer</summary>

<p><strong>Answer</strong>:</p>

<p><strong>CBOW (Continuous Bag of Words)</strong>:</p>
<ul>
<li>Mechanism: Predict center word from surrounding words</li>
<li>Input: Average of surrounding word vectors</li>
<li>Output: Center word</li>
<li>Advantages: Fast, effective with small data</li>
<li>Disadvantages: Weak learning for infrequent words</li>
</ul>

<p><strong>Skip-gram</strong>:</p>
<ul>
<li>Mechanism: Predict surrounding words from center word</li>
<li>Input: Center word</li>
<li>Output: Surrounding words (multiple)</li>
<li>Advantages: Good at learning rare words, high-quality embeddings</li>
<li>Disadvantages: Higher computational cost</li>
</ul>

<p><strong>Selection Guide</strong>:</p>

<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small corpus</td>
<td>CBOW</td>
</tr>
<tr>
<td>Large corpus</td>
<td>Skip-gram</td>
</tr>
<tr>
<td>Speed priority</td>
<td>CBOW</td>
</tr>
<tr>
<td>Quality priority</td>
<td>Skip-gram</td>
</tr>
<tr>
<td>Frequent words focus</td>
<td>CBOW</td>
</tr>
<tr>
<td>Rare words important</td>
<td>Skip-gram</td>
</tr>
</tbody>
</table>

</details>

<h3>Problem 4 (Difficulty: Hard)</h3>
<p>Perform morphological analysis on the Japanese text "Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫„ÅßAIÈñãÁô∫„ÇíË°å„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ" using MeCab, extract only nouns, and write code to perform document classification using TF-IDF vectorization.</p>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">import MeCab
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# Initialize MeCab
mecab = MeCab.Tagger()

def extract_nouns(text):
    """Extract only nouns"""
    node = mecab.parseToNode(text)
    nouns = []
    while node:
        features = node.feature.split(',')
        # If part-of-speech is noun
        if features[0] == 'ÂêçË©û' and node.surface:
            nouns.append(node.surface)
        node = node.next
    return ' '.join(nouns)

# Sample data
texts = [
    "Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫„ÅßAIÈñãÁô∫„ÇíË°å„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    "Â§ßÈò™Â∫ú„Åß„É≠„Éú„ÉÉ„ÉàÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    "Ê©üÊ¢∞Â≠¶Áøí„ÅÆÂãâÂº∑„ÇíÊù±‰∫¨„Åß„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    "‰∫∫Â∑•Áü•ËÉΩ„ÅÆÈñãÁô∫„ÅØÂ§ßÈò™„ÅßÈÄ≤„ÇÅ„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
]

labels = ["tech", "robot", "ml", "ai"]

print("=== Japanese Text Preprocessing and Classification ===\n")

# Noun extraction
processed_texts = []
for text in texts:
    nouns = extract_nouns(text)
    print(f"Original: {text}")
    print(f"Nouns: {nouns}\n")
    processed_texts.append(nouns)

# TF-IDF vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(processed_texts)

print("Vocabulary:")
print(vectorizer.get_feature_names_out())

print("\nTF-IDF matrix:")
import pandas as pd
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(df.round(3))

# Train classifier
classifier = MultinomialNB()
classifier.fit(X, labels)

# Classify new text
new_text = "Êù±‰∫¨„ÅßAIÊäÄË°ì„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
new_nouns = extract_nouns(new_text)
new_vector = vectorizer.transform([new_nouns])
prediction = classifier.predict(new_vector)

print(f"\nNew text: {new_text}")
print(f"Extracted nouns: {new_nouns}")
print(f"Classification result: {prediction[0]}")
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== Japanese Text Preprocessing and Classification ===

Original: Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫„ÅßAIÈñãÁô∫„ÇíË°å„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Nouns: Êù±‰∫¨ ÈÉΩ Ê∏ãË∞∑ Âå∫ AI ÈñãÁô∫

Original: Â§ßÈò™Â∫ú„Åß„É≠„Éú„ÉÉ„ÉàÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Nouns: Â§ßÈò™ Â∫ú „É≠„Éú„ÉÉ„Éà Á†îÁ©∂

Original: Ê©üÊ¢∞Â≠¶Áøí„ÅÆÂãâÂº∑„ÇíÊù±‰∫¨„Åß„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Nouns: Ê©üÊ¢∞ Â≠¶Áøí ÂãâÂº∑ Êù±‰∫¨

Original: ‰∫∫Â∑•Áü•ËÉΩ„ÅÆÈñãÁô∫„ÅØÂ§ßÈò™„ÅßÈÄ≤„ÇÅ„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Nouns: ‰∫∫Â∑• Áü•ËÉΩ ÈñãÁô∫ Â§ßÈò™

Vocabulary:
['ai' '„É≠„Éú„ÉÉ„Éà' '‰∫∫Â∑•' 'ÂãâÂº∑' 'Â§ßÈò™' 'Â≠¶Áøí' 'Â∫ú' 'Êù±‰∫¨' 'Ê©üÊ¢∞' 'Ê∏ãË∞∑' 'Áü•ËÉΩ' 'Á†îÁ©∂' 'ÈñãÁô∫']

TF-IDF matrix:
   ai  „É≠„Éú„ÉÉ„Éà  ‰∫∫Â∑•  ÂãâÂº∑  Â§ßÈò™  Â≠¶Áøí   Â∫ú  Êù±‰∫¨  Ê©üÊ¢∞  Ê∏ãË∞∑  Áü•ËÉΩ  Á†îÁ©∂  ÈñãÁô∫
0  0.45   0.0  0.0  0.0  0.0  0.0  0.0  0.36  0.0  0.45  0.0  0.0  0.36
1  0.00   0.52  0.0  0.0  0.40  0.0  0.52  0.00  0.0  0.00  0.0  0.52  0.00
2  0.00   0.00  0.0  0.48  0.00  0.48  0.00  0.37  0.48  0.00  0.0  0.00  0.00
3  0.00   0.00  0.48  0.0  0.37  0.00  0.00  0.00  0.0  0.00  0.48  0.00  0.37

New text: Êù±‰∫¨„ÅßAIÊäÄË°ì„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
Extracted nouns: Êù±‰∫¨ AI ÊäÄË°ì Á†îÁ©∂
Classification result: tech
</code></pre>

</details>

<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Calculate the cosine similarity between two sentences using (1) Bag of Words and (2) Word2Vec embedding average vectors, and compare the results.</p>

<details>
<summary>Sample Answer</summary>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import numpy as np

# Two sentences
sentence1 = "I love machine learning"
sentence2 = "I enjoy deep learning"

print("=== Cosine Similarity Comparison ===\n")
print(f"Sentence 1: {sentence1}")
print(f"Sentence 2: {sentence2}\n")

# ========================================
# (1) Bag of Words similarity
# ========================================
vectorizer = CountVectorizer()
bow_vectors = vectorizer.fit_transform([sentence1, sentence2])
bow_similarity = cosine_similarity(bow_vectors[0], bow_vectors[1])[0][0]

print("=== (1) Bag of Words ===")
print(f"Vocabulary: {vectorizer.get_feature_names_out()}")
print(f"Sentence 1 BoW: {bow_vectors[0].toarray()}")
print(f"Sentence 2 BoW: {bow_vectors[1].toarray()}")
print(f"Cosine similarity: {bow_similarity:.3f}\n")

# ========================================
# (2) Word2Vec embedding average vectors
# ========================================
# Prepare corpus (larger corpus needed in practice)
corpus = [
    "I love machine learning",
    "I enjoy deep learning",
    "Machine learning is fun",
    "Deep learning uses neural networks",
    "I love deep neural networks"
]
tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]

# Train Word2Vec model
w2v_model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1
)

def sentence_vector(sentence, model):
    """Sentence vector representation (average of word vectors)"""
    words = word_tokenize(sentence.lower())
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

# Calculate sentence vectors
vec1 = sentence_vector(sentence1, w2v_model)
vec2 = sentence_vector(sentence2, w2v_model)

# Cosine similarity
w2v_similarity = cosine_similarity([vec1], [vec2])[0][0]

print("=== (2) Word2Vec ===")
print(f"Sentence 1 vector (first 5 dimensions): {vec1[:5]}")
print(f"Sentence 2 vector (first 5 dimensions): {vec2[:5]}")
print(f"Cosine similarity: {w2v_similarity:.3f}\n")

# ========================================
# Comparison
# ========================================
print("=== Comparison ===")
print(f"BoW similarity: {bow_similarity:.3f}")
print(f"Word2Vec similarity: {w2v_similarity:.3f}")
print(f"\nDifference: {abs(w2v_similarity - bow_similarity):.3f}")

print("\nAnalysis:")
print("- BoW only considers common words ('I', 'learning')")
print("- Word2Vec captures semantic similarity ('love' ‚âà 'enjoy', 'machine' ‚âà 'deep')")
print("- Word2Vec typically returns more semantically accurate similarity")
</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code>=== Cosine Similarity Comparison ===

Sentence 1: I love machine learning
Sentence 2: I enjoy deep learning

=== (1) Bag of Words ===
Vocabulary: ['deep' 'enjoy' 'learning' 'love' 'machine']
Sentence 1 BoW: [[0 0 1 1 1]]
Sentence 2 BoW: [[1 1 1 0 0]]
Cosine similarity: 0.333

=== (2) Word2Vec ===
Sentence 1 vector (first 5 dimensions): [-0.00123  0.00456 -0.00789  0.00234  0.00567]
Sentence 2 vector (first 5 dimensions): [-0.00098  0.00423 -0.00712  0.00198  0.00534]
Cosine similarity: 0.876

=== Comparison ===
BoW similarity: 0.333
Word2Vec similarity: 0.876

Difference: 0.543

Analysis:
- BoW only considers common words ('I', 'learning')
- Word2Vec captures semantic similarity ('love' ‚âà 'enjoy', 'machine' ‚âà 'deep')
- Word2Vec typically returns more semantically accurate similarity
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em> (3rd ed.). Stanford University.</li>
<li>Eisenstein, J. (2019). <em>Introduction to Natural Language Processing</em>. MIT Press.</li>
<li>Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." <em>arXiv:1301.3781</em>.</li>
<li>Pennington, J., Socher, R., & Manning, C. D. (2014). "GloVe: Global Vectors for Word Representation." <em>EMNLP</em>.</li>
<li>Bojanowski, P., et al. (2017). "Enriching Word Vectors with Subword Information." <em>TACL</em>.</li>
<li>Kudo, T., & Shindo, H. (2018). <em>Theory and Implementation of Morphological Analysis</em>. Kindai Kagaku-sha.</li>
</ol>

<div class="navigation">
    <a href="index.html" class="nav-button">‚Üê Series Index</a>
    <a href="chapter2-sequence-models.html" class="nav-button">Next Chapter: Sequence Models and RNN ‚Üí</a>
</div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content is subject to change, update, or discontinuation without notice.</li>
            <li>The copyright and license of this content are subject to the terms specified (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
