<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Transformer & BERT - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/nlp-introduction/index.html">Nlp</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Transformer & BERT</h1>
            <p class="subtitle">æ³¨æ„æ©Ÿæ§‹ã‹ã‚‰äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¾ã§ - è‡ªç„¶è¨€èªå‡¦ç†ã®é©å‘½</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 35-40 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate to Advanced</span>
                <span class="meta-item">ğŸ’» Code Examples: 10</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Self-Attentionã¨Multi-Head Attentionã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Positional Encodingã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… BERTã®äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹</li>
<li>âœ… HuggingFace Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã“ãªã›ã‚‹</li>
<li>âœ… æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿå‹™ã§æ´»ç”¨ã§ãã‚‹</li>
</ul>

<hr>

<h2>3.1 Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>Transformerã®èª•ç”Ÿ</h3>
<p><strong>Transformer</strong>ã¯ã€2017å¹´ã«GoogleãŒç™ºè¡¨ã—ãŸã€ŒAttention is All You Needã€è«–æ–‡ã§ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚RNNã‚„LSTMã‚’ä½¿ã‚ãšã€<strong>Self-Attentionæ©Ÿæ§‹</strong>ã®ã¿ã§ç³»åˆ—å‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</p>

<blockquote>
<p>ã€ŒRNNã®é€æ¬¡å‡¦ç†ã‚’æ’é™¤ã—ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚ã‚’ä¸¦åˆ—è¨ˆç®—ã™ã‚‹ã€</p>
</blockquote>

<h3>Transformerã®åˆ©ç‚¹</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>RNN/LSTM</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ä¸¦åˆ—åŒ–</strong></td>
<td>é€æ¬¡å‡¦ç†ï¼ˆé…ã„ï¼‰</td>
<td>å®Œå…¨ä¸¦åˆ—ï¼ˆé€Ÿã„ï¼‰</td>
</tr>
<tr>
<td><strong>é•·è·é›¢ä¾å­˜</strong></td>
<td>å‹¾é…æ¶ˆå¤±ã§å›°é›£</td>
<td>ç›´æ¥æ¥ç¶šã§å®¹æ˜“</td>
</tr>
<tr>
<td><strong>è¨ˆç®—è¤‡é›‘åº¦</strong></td>
<td>O(n)</td>
<td>O(nÂ²)</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§</strong></td>
<td>ä½ã„</td>
<td>Attentionå¯è¦–åŒ–ã§é«˜ã„</td>
</tr>
</tbody>
</table>

<h3>å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3>

<div class="mermaid">
graph TB
    A[å…¥åŠ›æ–‡] --> B[Input Embedding]
    B --> C[Positional Encoding]
    C --> D[Encoder Stack]
    D --> E[Decoder Stack]
    E --> F[Linear + Softmax]
    F --> G[å‡ºåŠ›æ–‡]

    D --> |Context| E

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
    style G fill:#e0f2f1
</div>

<hr>

<h2>3.2 Self-Attentionæ©Ÿæ§‹</h2>

<h3>Self-Attentionã®åŸç†</h3>

<p><strong>Self-Attentionï¼ˆè‡ªå·±æ³¨æ„ï¼‰</strong>ã¯ã€å…¥åŠ›ç³»åˆ—å†…ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»–ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã®é–¢ä¿‚ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿæ§‹ã§ã™ã€‚</p>

<p>3ã¤ã®é‡ã¿è¡Œåˆ—ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š</p>
<ul>
<li>$\mathbf{W}_Q$: Queryï¼ˆã‚¯ã‚¨ãƒªï¼‰è¡Œåˆ—</li>
<li>$\mathbf{W}_K$: Keyï¼ˆã‚­ãƒ¼ï¼‰è¡Œåˆ—</li>
<li>$\mathbf{W}_V$: Valueï¼ˆå€¤ï¼‰è¡Œåˆ—</li>
</ul>

<h3>è¨ˆç®—æ‰‹é †</h3>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—1: Queryã€Keyã€Valueã‚’è¨ˆç®—</strong></p>

<p>$$
\mathbf{Q} = \mathbf{X}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V
$$</p>

<p><strong>ã‚¹ãƒ†ãƒƒãƒ—2: Attention Scoreã‚’è¨ˆç®—</strong></p>

<p>$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$</p>

<ul>
<li>$d_k$: Keyã®æ¬¡å…ƒæ•°ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼‰</li>
</ul>

<h3>å®Ÿè£…ä¾‹ï¼šScaled Dot-Product Attention</h3>

<pre><code class="language-python">import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention

    Args:
        Q: Queryè¡Œåˆ— (batch_size, seq_len, d_k)
        K: Keyè¡Œåˆ— (batch_size, seq_len, d_k)
        V: Valueè¡Œåˆ— (batch_size, seq_len, d_v)
        mask: ãƒã‚¹ã‚¯ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)

    Returns:
        output: Attentioné©ç”¨å¾Œã®å‡ºåŠ›
        attention_weights: Attentioné‡ã¿
    """
    d_k = Q.shape[-1]

    # Attention Scoreè¨ˆç®—
    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)

    # ãƒã‚¹ã‚¯é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if mask is not None:
        scores = scores + (mask * -1e9)

    # Softmax
    attention_weights = softmax(scores, axis=-1)

    # é‡ã¿ä»˜ãå’Œ
    output = np.matmul(attention_weights, V)

    return output, attention_weights

def softmax(x, axis=-1):
    """Softmaxé–¢æ•°"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# ä½¿ç”¨ä¾‹
batch_size, seq_len, d_model = 2, 5, 64
Q = np.random.randn(batch_size, seq_len, d_model)
K = np.random.randn(batch_size, seq_len, d_model)
V = np.random.randn(batch_size, seq_len, d_model)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
print(f"Attentioné‡ã¿å½¢çŠ¶: {weights.shape}")
print(f"\nAttentioné‡ã¿ï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ï¼‰:\n{weights[0]}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å‡ºåŠ›å½¢çŠ¶: (2, 5, 64)
Attentioné‡ã¿å½¢çŠ¶: (2, 5, 5)

Attentioné‡ã¿ï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ï¼‰:
[[0.21 0.19 0.20 0.18 0.22]
 [0.20 0.21 0.19 0.20 0.20]
 [0.19 0.20 0.21 0.20 0.20]
 [0.20 0.20 0.19 0.21 0.20]
 [0.22 0.18 0.20 0.19 0.21]]
</code></pre>

<h3>PyTorchã«ã‚ˆã‚‹å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k

    def forward(self, Q, K, V, mask=None):
        # Attention Score
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))

        # ãƒã‚¹ã‚¯é©ç”¨
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # é‡ã¿ä»˜ãå’Œ
        output = torch.matmul(attention_weights, V)

        return output, attention_weights

# ä½¿ç”¨ä¾‹
d_model = 64
attention = ScaledDotProductAttention(d_k=d_model)

Q = torch.randn(2, 5, d_model)
K = torch.randn(2, 5, d_model)
V = torch.randn(2, 5, d_model)

output, weights = attention(Q, K, V)
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
print(f"Attentioné‡ã¿å½¢çŠ¶: {weights.shape}")
</code></pre>

<hr>

<h2>3.3 Multi-Head Attention</h2>

<h3>æ¦‚è¦</h3>

<p><strong>Multi-Head Attention</strong>ã¯ã€è¤‡æ•°ã®Attention headã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã€ç•°ãªã‚‹è¡¨ç¾éƒ¨ minutesç©ºé–“ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã¾ã™ã€‚</p>

<div class="mermaid">
graph LR
    A[å…¥åŠ› X] --> B1[Head 1]
    A --> B2[Head 2]
    A --> B3[Head 3]
    A --> B4[Head h]

    B1 --> C[Concat]
    B2 --> C
    B3 --> C
    B4 --> C

    C --> D[Linear]
    D --> E[å‡ºåŠ›]

    style A fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style B4 fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#e0f2f1
</div>

<h3>æ•°å¼</h3>

<p>$$
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O
$$</p>

<p>å„headã¯ï¼š</p>

<p>$$
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
$$</p>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        """
        Multi-Head Attention

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
            num_heads: Attention headã®æ•°
        """
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linearå±¤
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(self.d_k)

    def split_heads(self, x, batch_size):
        """è¤‡æ•°headã« minuteså‰²"""
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # Linearå¤‰æ›
        Q = self.W_Q(Q)
        K = self.W_K(K)
        V = self.W_V(V)

        # è¤‡æ•°headã« minuteså‰²
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)

        # Attentioné©ç”¨
        output, attention_weights = self.attention(Q, K, V, mask)

        # Concat
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, -1, self.d_model)

        # æœ€çµ‚Linearå±¤
        output = self.W_O(output)

        return output, attention_weights

# ä½¿ç”¨ä¾‹
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = mha(x, x, x)
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
print(f"Attentioné‡ã¿å½¢çŠ¶: {weights.shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 10, 512])
Attentioné‡ã¿å½¢çŠ¶: torch.Size([2, 8, 10, 10])
</code></pre>

<hr>

<h2>3.4 Positional Encoding</h2>

<h3>å¿…è¦æ€§</h3>

<p>Self-Attentionã¯é †åºæƒ…å ±ã‚’æŒãŸãªã„ãŸã‚ã€<strong>Positional Encodingï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰</strong>ã§ç³»åˆ—ã®ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã™ã€‚</p>

<h3>Sinusoidal Positional Encoding</h3>

<p>$$
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>

<p>$$
\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>

<ul>
<li>$pos$: ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®</li>
<li>$i$: æ¬¡å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹</li>
</ul>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        """
        Positional Encoding

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
            max_len: æœ€å¤§ç³»åˆ—é•·
        """
        super().__init__()

        # Positional Encodingã‚’äº‹å‰è¨ˆç®—
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]

# ä½¿ç”¨ä¾‹
d_model = 128
max_len = 100

pe_layer = PositionalEncoding(d_model, max_len)
x = torch.randn(2, 50, d_model)
output = pe_layer(x)

print(f"å…¥åŠ›å½¢çŠ¶: {x.shape}")
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")

# Positional Encodingã®å¯è¦–åŒ–
pe_matrix = pe_layer.pe[0, :max_len, :].numpy()

plt.figure(figsize=(12, 6))
plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')
plt.xlabel('Position', fontsize=12)
plt.ylabel('Dimension', fontsize=12)
plt.title('Positional Encoding (Sinusoidal)', fontsize=14)
plt.colorbar()
plt.tight_layout()
plt.show()
</code></pre>

<hr>

<h2>3.5 Feed-Forward Networks</h2>

<h3>Position-wise Feed-Forward Networks</h3>

<p>å„ä½ç½®ã«ç‹¬ç«‹ã«é©ç”¨ã•ã‚Œã‚‹2å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ï¼š</p>

<p>$$
\text{FFN}(x) = \max(0, x\mathbf{W}_1 + b_1)\mathbf{W}_2 + b_2
$$</p>

<h3>å®Ÿè£…ä¾‹</h3>

<pre><code class="language-python">class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        Position-wise Feed-Forward Networks

        Args:
            d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
            d_ff: ä¸­é–“å±¤ã®æ¬¡å…ƒæ•°ï¼ˆé€šå¸¸ 4 * d_modelï¼‰
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()

    def forward(self, x):
        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)

        # (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)
        x = self.linear2(x)
        return x

# ä½¿ç”¨ä¾‹
d_model = 512
d_ff = 2048

ffn = PositionwiseFeedForward(d_model, d_ff)
x = torch.randn(2, 10, d_model)

output = ffn(x)
print(f"å…¥åŠ›å½¢çŠ¶: {x.shape}")
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
</code></pre>

<hr>

<h2>3.6 BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰</h2>

<h3>BERTã®ç‰¹å¾´</h3>

<p><strong>BERT</strong>ã¯ã€2018å¹´ã«GoogleãŒç™ºè¡¨ã—ãŸåŒæ–¹å‘ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<blockquote>
<p>ã€Œå·¦ã‹ã‚‰å³ã ã‘ã§ãªãã€åŒæ–¹å‘ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹ã€</p>
</blockquote>

<div class="mermaid">
graph LR
    A[å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹] --> B[äº‹å‰å­¦ç¿’]
    B --> C[BERT Base/Large]
    C --> D1[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:  classification]
    C --> D2[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: NER]
    C --> D3[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: QA]
    C --> D4[ç‰¹å¾´æŠ½å‡º]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
    style D4 fill:#e8f5e9
</div>

<h3>BERTã®ãƒ¢ãƒ‡ãƒ«æ§‹æˆ</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>å±¤æ•°</th>
<th>Hidden Size</th>
<th>Attention Heads</th>
<th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT-Base</strong></td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr>
<td><strong>BERT-Large</strong></td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>

<h3>äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯</h3>

<h4>1. Masked Language Modeling (MLM)</h4>

<p>å…¥åŠ›ã®15%ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ï¼ˆ[MASK]ï¼‰ã—ã€äºˆæ¸¬ã—ã¾ã™ã€‚</p>

<pre><code>å…¥åŠ›:  The [MASK] is beautiful today.
ç›®æ¨™:  The weather is beautiful today.
</code></pre>

<h4>2. Next Sentence Prediction (NSP)</h4>

<p>2ã¤ã®æ–‡ãŒé€£ç¶šã—ã¦ã„ã‚‹ã‹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚</p>

<pre><code>æ–‡A: The cat sat on the mat.
æ–‡B: It was very comfortable.
ãƒ©ãƒ™ãƒ«: IsNext (1)

æ–‡A: The cat sat on the mat.
æ–‡B: The economy is growing.
ãƒ©ãƒ™ãƒ«: NotNext (0)
</code></pre>

<h3>HuggingFace Transformersã§å§‹ã‚ã‚‹BERT</h3>

<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

print("ãƒˆãƒ¼ã‚¯ãƒ³:", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))
print("å…¥åŠ›ID:", inputs['input_ids'])
print("Attention Mask:", inputs['attention_mask'])

# ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–
with torch.no_grad():
    outputs = model(**inputs)

# å‡ºåŠ›
last_hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)
pooler_output = outputs.pooler_output  # (batch_size, hidden_size) [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›

print(f"\nLast Hidden Stateså½¢çŠ¶: {last_hidden_states.shape}")
print(f"Pooler Outputå½¢çŠ¶: {pooler_output.shape}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>ãƒˆãƒ¼ã‚¯ãƒ³: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']
å…¥åŠ›ID: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])

Last Hidden Stateså½¢çŠ¶: torch.Size([1, 8, 768])
Pooler Outputå½¢çŠ¶: torch.Size([1, 768])
</code></pre>

<hr>

<h2>3.7 BERTã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h2>

<h3>ãƒ†ã‚­ã‚¹ãƒˆ classificationã‚¿ã‚¹ã‚¯</h3>

<pre><code class="language-python">from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹: IMDbæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
dataset = load_dataset('imdb')

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# å°è¦æ¨¡ã‚µãƒ–ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))
eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(200))

# è©•ä¾¡é–¢æ•°
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {'accuracy': acc, 'f1': f1}

# å­¦ç¿’è¨­å®š
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_strategy='epoch',
)

# Trainerã®åˆæœŸåŒ–
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
trainer.train()

# è©•ä¾¡
results = trainer.evaluate()
print(f"\nè©•ä¾¡çµæœ: {results}")
</code></pre>

<h3>Named Entity Recognition (NER)</h3>

<pre><code class="language-python">from transformers import BertForTokenClassification, pipeline

# NERç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
ner_pipeline = pipeline('ner', model=model_name, tokenizer=model_name)

# ãƒ†ã‚­ã‚¹ãƒˆã®å›ºæœ‰è¡¨ç¾æŠ½å‡º
text = "Apple Inc. is looking at buying U.K. startup for $1 billion. Tim Cook is the CEO."
results = ner_pipeline(text)

print("å›ºæœ‰è¡¨ç¾æŠ½å‡ºçµæœ:")
for entity in results:
    print(f"{entity['word']}: {entity['entity']} (ä¿¡é ¼åº¦: {entity['score']:.4f})")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>å›ºæœ‰è¡¨ç¾æŠ½å‡ºçµæœ:
Apple: B-ORG (ä¿¡é ¼åº¦: 0.9987)
Inc: I-ORG (ä¿¡é ¼åº¦: 0.9983)
U: B-LOC (ä¿¡é ¼åº¦: 0.9976)
K: I-LOC (ä¿¡é ¼åº¦: 0.9945)
Tim: B-PER (ä¿¡é ¼åº¦: 0.9995)
Cook: I-PER (ä¿¡é ¼åº¦: 0.9993)
CEO: B-MISC (ä¿¡é ¼åº¦: 0.8734)
</code></pre>

<h3>Question Answering</h3>

<pre><code class="language-python">from transformers import pipeline

# QAç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è³ªå•
context = """
The Transformer is a deep learning model introduced in 2017, used primarily in the field of
natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are
designed to handle sequential data, such as natural language, for tasks such as translation
and text summarization. However, unlike RNNs, Transformers do not require that the sequential
data be processed in order.
"""

question = "When was the Transformer introduced?"

# è³ªå•å¿œç­”
result = qa_pipeline(question=question, context=context)

print(f"è³ªå•: {question}")
print(f"å›ç­”: {result['answer']}")
print(f"ä¿¡é ¼åº¦: {result['score']:.4f}")
print(f"é–‹å§‹ä½ç½®: {result['start']}, çµ‚äº†ä½ç½®: {result['end']}")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>è³ªå•: When was the Transformer introduced?
å›ç­”: 2017
ä¿¡é ¼åº¦: 0.9812
é–‹å§‹ä½ç½®: 50, çµ‚äº†ä½ç½®: 54
</code></pre>

<hr>

<h2>3.8 æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«</h2>

<h3>ä»£è¡¨çš„ãªæ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«</h3>

<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«</th>
<th>æä¾›å…ƒ</th>
<th>ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>æ±åŒ—å¤§BERT</strong></td>
<td>æ±åŒ—å¤§å­¦</td>
<td>MeCab + WordPiece</td>
<td>æ—¥æœ¬èªWikipediaã§å­¦ç¿’</td>
</tr>
<tr>
<td><strong>äº¬éƒ½å¤§BERT</strong></td>
<td>äº¬éƒ½å¤§å­¦</td>
<td>Juman++ + WordPiece</td>
<td>é«˜å“è³ªãªå½¢æ…‹ç´ è§£æ</td>
</tr>
<tr>
<td><strong>NICT BERT</strong></td>
<td>NICT</td>
<td>SentencePiece</td>
<td>å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹</td>
</tr>
<tr>
<td><strong>æ—©ç¨²ç”°RoBERTa</strong></td>
<td>æ—©ç¨²ç”°å¤§å­¦</td>
<td>SentencePiece</td>
<td>RoBERTaï¼ˆNSPãªã—ï¼‰</td>
</tr>
</tbody>
</table>

<h3>æ±åŒ—å¤§BERTã®ä½¿ç”¨ä¾‹</h3>

<pre><code class="language-python">from transformers import BertJapaneseTokenizer, BertModel
import torch

# æ±åŒ—å¤§BERTã®ãƒ­ãƒ¼ãƒ‰
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ
text = "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®é‡è¦ãª minutesé‡ã§ã™ã€‚"

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
inputs = tokenizer(text, return_tensors='pt')
print("ãƒˆãƒ¼ã‚¯ãƒ³:", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))

# æ¨è«–
with torch.no_grad():
    outputs = model(**inputs)

print(f"\nLast Hidden Stateså½¢çŠ¶: {outputs.last_hidden_state.shape}")
print(f"Pooler Outputå½¢çŠ¶: {outputs.pooler_output.shape}")
</code></pre>

<h3>æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ classification</h3>

<pre><code class="language-python">from transformers import BertForSequenceClassification, BertJapaneseTokenizer
import torch
import torch.nn.functional as F

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ­ãƒ¼ãƒ‰
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)

# æ„Ÿæƒ… minutesæç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆä¾‹: ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# ãƒ†ã‚­ã‚¹ãƒˆä¾‹
texts = [
    "ã“ã®æ˜ ç”»ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã‹ã£ãŸï¼",
    "å…¨ãé¢ç™½ããªãã¦ hoursã®ç„¡é§„ã ã£ãŸã€‚",
    "æ™®é€šã®ä½œå“ã§ç‰¹ã«å°è±¡ã«æ®‹ã‚‰ãªã‹ã£ãŸã€‚"
]

# æ¨è«–
model.eval()
for text in texts:
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)

    predicted_class = torch.argmax(probs, dim=-1).item()
    confidence = probs[0][predicted_class].item()

    label = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if predicted_class == 1 else "ãƒã‚¬ãƒ†ã‚£ãƒ–"
    print(f"\nãƒ†ã‚­ã‚¹ãƒˆ: {text}")
    print(f"äºˆæ¸¬: {label} (ä¿¡é ¼åº¦: {confidence:.4f})")
</code></pre>

<hr>

<h2>3.9 BERTã®å¿œç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>Feature Extractionï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰</h3>

<p>BERTã‚’å›ºå®šã®ç‰¹å¾´æŠ½å‡ºå™¨ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚</p>

<pre><code class="language-python">from transformers import BertModel, BertTokenizer
import torch
import numpy as np

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰

def get_sentence_embedding(text, pooling='mean'):
    """
    æ–‡ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        pooling: ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³• ('mean', 'max', 'cls')

    Returns:
        embedding: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    """
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    # Last Hidden States
    last_hidden = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)

    if pooling == 'mean':
        # Mean pooling
        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size()).float()
        sum_embeddings = torch.sum(last_hidden * mask, 1)
        sum_mask = torch.clamp(mask.sum(1), min=1e-9)
        embedding = sum_embeddings / sum_mask
    elif pooling == 'max':
        # Max pooling
        embedding = torch.max(last_hidden, 1)[0]
    elif pooling == 'cls':
        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³
        embedding = outputs.pooler_output

    return embedding.squeeze().numpy()

# ä½¿ç”¨ä¾‹
texts = [
    "Natural language processing is fascinating.",
    "I love machine learning.",
    "The weather is nice today."
]

embeddings = [get_sentence_embedding(text, pooling='mean') for text in texts]

# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(embeddings)

print("ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—:")
print(similarity_matrix)
print(f"\næ–‡1ã¨æ–‡2ã®é¡ä¼¼åº¦: {similarity_matrix[0, 1]:.4f}")
print(f"æ–‡1ã¨æ–‡3ã®é¡ä¼¼åº¦: {similarity_matrix[0, 2]:.4f}")
</code></pre>

<h3>Sentence Embeddingsï¼ˆæ–‡åŸ‹ã‚è¾¼ã¿ï¼‰</h3>

<p>Sentence-BERTã‚’ä½¿ç”¨ã—ãŸé«˜å“è³ªãªæ–‡åŸ‹ã‚è¾¼ã¿ï¼š</p>

<pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Sentence-BERTãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# æ–‡ã®ãƒªã‚¹ãƒˆ
sentences = [
    "The cat sits on the mat.",
    "A feline rests on a rug.",
    "The dog plays in the park.",
    "Machine learning is a subset of artificial intelligence.",
    "Deep learning uses neural networks."
]

# åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—
embeddings = model.encode(sentences)

print(f"åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å½¢çŠ¶: {embeddings.shape}")

# é¡ä¼¼åº¦è¨ˆç®—
similarity = cosine_similarity(embeddings)

print("\næ–‡ã®é¡ä¼¼åº¦è¡Œåˆ—:")
for i, sent in enumerate(sentences):
    print(f"\n{i}: {sent}")

print("\né¡ä¼¼åº¦:")
for i in range(len(sentences)):
    for j in range(i+1, len(sentences)):
        print(f"æ–‡{i} ã¨ æ–‡{j}: {similarity[i, j]:.4f}")
</code></pre>

<h3>Domain Adaptationï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œï¼‰</h3>

<p>ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã§è¿½åŠ ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã„ã¾ã™ï¼š</p>

<pre><code class="language-python">from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
from datasets import Dataset

# åŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ä¾‹
domain_texts = [
    "æ‚£ learnerã®è¡€åœ§ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚",
    "ç³–å°¿ç—…ã®æ²»ç™‚ã«ã¯é£Ÿäº‹ç™‚æ³•ãŒé‡è¦ã§ã™ã€‚",
    "ã“ã®è–¬å‰¤ã¯å‰¯ä½œç”¨ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚",
    # ... å¤§é‡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ†ã‚­ã‚¹ãƒˆ
]

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset = Dataset.from_dict({'text': domain_texts})

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')
model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])

# Data Collatorï¼ˆMLMç”¨ï¼‰
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,
    mlm_probability=0.15
)

# å­¦ç¿’è¨­å®š
training_args = TrainingArguments(
    output_dir='./domain_adapted_bert',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
    learning_rate=5e-5,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# è¿½åŠ ã®äº‹å‰å­¦ç¿’
# trainer.train()  # ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ã—ã¦å®Ÿè¡Œ

print("ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™å®Œäº†")
</code></pre>

<hr>

<h2>3.10 æœ¬ Chapterã®Summary</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

<ol>
<li><p><strong>Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong></p>
<ul>
<li>Self-Attentionæ©Ÿæ§‹ã®åŸç†ã¨å®Ÿè£…</li>
<li>Multi-Head Attentionã§è¤‡æ•°ã®è¡¨ç¾ã‚’å­¦ç¿’</li>
<li>Positional Encodingã§ä½ç½®æƒ…å ±ã‚’ä»˜ä¸</li>
<li>Feed-Forward Networksã§éç·šå½¢å¤‰æ›</li>
</ul></li>
<li><p><strong>BERTã®åŸºç¤</strong></p>
<ul>
<li>åŒæ–¹å‘äº‹å‰å­¦ç¿’ã®é‡è¦æ€§</li>
<li>MLMã¨NSPã‚¿ã‚¹ã‚¯</li>
<li>HuggingFace Transformersã®ä½¿ã„æ–¹</li>
</ul></li>
<li><p><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong></p>
<ul>
<li>ãƒ†ã‚­ã‚¹ãƒˆ classificationã€NERã€QAã‚¿ã‚¹ã‚¯</li>
<li>æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨</li>
</ul></li>
<li><p><strong>å¿œç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong></p>
<ul>
<li>ç‰¹å¾´æŠ½å‡ºã¨æ–‡åŸ‹ã‚è¾¼ã¿</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</li>
<li>å®Ÿå‹™ã§ã®æ´»ç”¨æ–¹æ³•</li>
</ul></li>
</ol>

<h3>Next Chapterã¸</h3>

<p>Chapter 4 Chapterã§ã¯ã€<strong>BERTã®ç™ºå±•ãƒ¢ãƒ‡ãƒ«</strong>ã‚’å­¦ã³ã¾ã™ï¼š</p>
<ul>
<li>RoBERTaã€ALBERTã€DistilBERT</li>
<li>GPTç³»åˆ—ï¼ˆGPT-2ã€GPT-3ï¼‰</li>
<li>T5ã€BARTï¼ˆSeq2Seqãƒ¢ãƒ‡ãƒ«ï¼‰</li>
<li>æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«</li>
</ul>

<hr>

<h2>Exercises</h2>

<h3>å•é¡Œ1ï¼ˆDifficultyï¼šeasyï¼‰</h3>
<p>Self-Attentionã¨Cross-Attentionã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è§£ç­”</strong>ï¼š</p>
<ul>
<li><strong>Self-Attention</strong>: Queryã€Keyã€ValueãŒã™ã¹ã¦åŒã˜å…¥åŠ›ç³»åˆ—ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ã€‚å…¥åŠ›ç³»åˆ—å†…ã®å„è¦ç´ é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã€‚</li>
<li><strong>Cross-Attention</strong>: Queryã¨Key/ValueãŒç•°ãªã‚‹ç³»åˆ—ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ï¼ˆä¾‹: Decoderã®Queryã¨Encoderã®Key/Valueï¼‰ã€‚ç•°ãªã‚‹ç³»åˆ—é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã€‚</li>
</ul>

<p><strong>ä½¿ç”¨å ´é¢</strong>ï¼š</p>
<ul>
<li>Self-Attention: BERTã®Encoderã€æ–‡å†…ã®å˜èªé–“ä¾å­˜é–¢ä¿‚</li>
<li>Cross-Attention: æ©Ÿæ¢°ç¿»è¨³ã®Decoderã€åŸæ–‡ã¨è¨³æ–‡ã®å¯¾å¿œé–¢ä¿‚</li>
</ul>

</details>

<h3>å•é¡Œ2ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>Positional EncodingãŒãªãœå¿…è¦ã‹ã€ã¾ãŸãªãœSinusoidalé–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>å¿…è¦æ€§</strong>ï¼š</p>
<ul>
<li>Self-Attentionã¯é †åºæƒ…å ±ã‚’æŒãŸãªã„ï¼ˆé †åˆ—ä¸å¤‰æ€§ï¼‰</li>
<li>"cat sat on mat"ã¨"mat on sat cat"ãŒåŒºåˆ¥ã§ããªã„</li>
<li>ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ç³»åˆ—ã®é †åºã‚’ä¿æŒ</li>
</ul>

<p><strong>Sinusoidalé–¢æ•°ã‚’ä½¿ã†ç†ç”±</strong>ï¼š</p>
<ol>
<li><strong>å›ºå®šé•·ç³»åˆ—ã¸ã®å¯¾å¿œ</strong>: å­¦ç¿’æ™‚ã«è¦‹ã¦ã„ãªã„é•·ã•ã®ç³»åˆ—ã«ã‚‚å¯¾å¿œå¯èƒ½</li>
<li><strong>ç›¸å¯¾ä½ç½®ã®è¡¨ç¾</strong>: $\text{PE}_{pos+k}$ã¯$\text{PE}_{pos}$ã®ç·šå½¢å¤‰æ›ã§è¡¨ç¾å¯èƒ½</li>
<li><strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦</strong>: å­¦ç¿’ä¸è¦ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„</li>
<li><strong>å‘¨æœŸæ€§</strong>: ç•°ãªã‚‹å‘¨æ³¢æ•°ã§çŸ­æœŸãƒ»é•·æœŸã®ä½ç½®é–¢ä¿‚ã‚’æ‰ãˆã‚‹</li>
</ol>

</details>

<h3>å•é¡Œ3ï¼ˆDifficultyï¼šmediumï¼‰</h3>
<p>BERTã®MLMï¼ˆMasked Language Modelingï¼‰ã«ãŠã„ã¦ã€ãªãœå…¥åŠ›ã®15%ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ã™ã‚‹ã®ã‹ã€ãã®è¨­è¨ˆç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>15%ã¨ã„ã†å‰²åˆã®ç†ç”±</strong>ï¼š</p>
<ul>
<li><strong>ãƒãƒ©ãƒ³ã‚¹</strong>: ä½ã™ãã‚‹ã¨å­¦ç¿’ãŒé…ã„ã€é«˜ã™ãã‚‹ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸è¶³</li>
<li><strong>å®Ÿé¨“çš„æœ€é©å€¤</strong>: BERTã®è«–æ–‡ã§æ§˜ã€…ãªå‰²åˆã‚’è©¦ã—ãŸçµæœ</li>
</ul>

<p><strong>ãƒã‚¹ã‚¯ã®å†…è¨³</strong>ï¼š</p>
<ul>
<li>80%: [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›</li>
<li>10%: ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›</li>
<li>10%: å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¾ã¾</li>
</ul>

<p><strong>ã“ã®è¨­è¨ˆã®ç†ç”±</strong>ï¼š</p>
<ol>
<li>80%ãŒ[MASK]: ä¸»è¦ãªå­¦ç¿’ã‚¿ã‚¹ã‚¯</li>
<li>10%ãŒãƒ©ãƒ³ãƒ€ãƒ : ãƒ¢ãƒ‡ãƒ«ãŒ[MASK]ã ã‘ã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«ã™ã‚‹</li>
<li>10%ãŒå…ƒã®ã¾ã¾: å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ã‚‚å­¦ç¿’ã™ã‚‹</li>
</ol>

<p>ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ãŒç¾ã‚Œãªãã¦ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ãŒé©åˆ‡ã«å‹•ä½œã—ã¾ã™ã€‚</p>

</details>

<h3>å•é¡Œ4ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>Multi-Head Attentionã§è¤‡æ•°ã®headã‚’ä½¿ç”¨ã™ã‚‹åˆ©ç‚¹ã‚’ã€å˜ä¸€ã®Attentionã¨ã®æ¯”è¼ƒã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€headæ•°ãŒå¤šã™ãã‚‹å ´åˆã®å•é¡Œç‚¹ã‚‚è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<details>
<summary>è§£ç­”ä¾‹</summary>

<p><strong>è¤‡æ•°headã®åˆ©ç‚¹</strong>ï¼š</p>
<ol>
<li><p><strong>ç•°ãªã‚‹è¡¨ç¾éƒ¨ minutesç©ºé–“</strong></p>
<ul>
<li>å„headãŒç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚ã‚’å­¦ç¿’</li>
<li>ä¾‹: æ§‹æ–‡çš„é–¢ä¿‚ã€æ„å‘³çš„é–¢ä¿‚ã€é•·è·é›¢ä¾å­˜ãªã©</li>
</ul></li>
<li><p><strong>ä¸¦åˆ—è¨ˆç®—</strong></p>
<ul>
<li>è¤‡æ•°headã‚’åŒæ™‚ã«è¨ˆç®—å¯èƒ½</li>
<li>GPUã§ã®åŠ¹ç‡çš„ãªå‡¦ç†</li>
</ul></li>
<li><p><strong>å†—é•·æ€§ã¨ãƒ­ãƒã‚¹ãƒˆæ€§</strong></p>
<ul>
<li>ä¸€éƒ¨ã®headãŒå¤±æ•—ã—ã¦ã‚‚ä»–ã®headãŒè£œå®Œ</li>
<li>å¤šæ§˜ãªæƒ…å ±ã‚’æ•æ‰</li>
</ul></li>
<li><p><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ</strong></p>
<ul>
<li>è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰ã®æƒ…å ±ã‚’çµ±åˆ</li>
<li>ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ã‚’å­¦ç¿’</li>
</ul></li>
</ol>

<p><strong>headæ•°ãŒå¤šã™ãã‚‹å ´åˆã®å•é¡Œ</strong>ï¼š</p>
<ol>
<li><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ </strong>: ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®— hoursã®å¢—åŠ </li>
<li><strong>éå­¦ç¿’ãƒªã‚¹ã‚¯</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å¢—åŠ ã«ã‚ˆã‚‹éå­¦ç¿’</li>
<li><strong>å†—é•·æ€§</strong>: ä¼¼ãŸã‚ˆã†ãªå½¹å‰²ã®headãŒå¢—ãˆã‚‹</li>
<li><strong>æœ€é©åŒ–ã®å›°é›£æ€§</strong>: å¤šæ•°ã®headã®èª¿æ•´ãŒé›£ã—ã„</li>
</ol>

<p><strong>å®Ÿå‹™ã§ã®Recommended</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º</th>
<th>Recommendedheadæ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small (d=256)</td>
<td>4-8</td>
</tr>
<tr>
<td>Base (d=512-768)</td>
<td>8-12</td>
</tr>
<tr>
<td>Large (d=1024)</td>
<td>12-16</td>
</tr>
</tbody>
</table>

</details>

<h3>å•é¡Œ5ï¼ˆDifficultyï¼šhardï¼‰</h3>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Œæˆã•ã›ã€BERTã‚’ä½¿ã£ãŸæ„Ÿæƒ… minutesæãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã¯è‡ª minutesã§ç”¨æ„ã™ã‚‹ã‹ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import torch

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…
class SentimentDataset(Dataset):
    # ã“ã“ã«å®Ÿè£…
    pass

# å­¦ç¿’é–¢æ•°ã‚’å®Ÿè£…
def train_model(model, train_loader, optimizer, device):
    # ã“ã“ã«å®Ÿè£…
    pass

# è©•ä¾¡é–¢æ•°ã‚’å®Ÿè£…
def evaluate_model(model, test_loader, device):
    # ã“ã“ã«å®Ÿè£…
    pass
</code></pre>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
train_texts = [
    "ã“ã®å•†å“ã¯ç´ æ™´ã‚‰ã—ã„ï¼",
    "æœ€æ‚ªã®ä½“é¨“ã§ã—ãŸã€‚",
    "æ™®é€šã§ã™ã€‚",
    "ã¨ã¦ã‚‚æº€è¶³ã—ã¦ã„ã¾ã™ã€‚",
    "äºŒåº¦ã¨è²·ã„ã¾ã›ã‚“ã€‚",
]
train_labels = [1, 0, 1, 1, 0]  # 1: ãƒã‚¸ãƒ†ã‚£ãƒ–, 0: ãƒã‚¬ãƒ†ã‚£ãƒ–

test_texts = [
    "è‰¯ã„å•†å“ã ã¨æ€ã„ã¾ã™ã€‚",
    "æœŸå¾…å¤–ã‚Œã§ã—ãŸã€‚"
]
test_labels = [1, 0]

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# å­¦ç¿’é–¢æ•°
def train_model(model, train_loader, optimizer, device, epochs=3):
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# è©•ä¾¡é–¢æ•°
def evaluate_model(model, test_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)

            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    print(f"\nç²¾åº¦: {accuracy:.4f}")
    print("\n classificationãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(true_labels, predictions, target_names=['ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒã‚¸ãƒ†ã‚£ãƒ–']))

    return accuracy

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    model.to(device)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)
    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

    # å­¦ç¿’
    print("å­¦ç¿’é–‹å§‹...")
    train_model(model, train_loader, optimizer, device, epochs=3)

    # è©•ä¾¡
    print("\nè©•ä¾¡é–‹å§‹...")
    evaluate_model(model, test_loader, device)

if __name__ == '__main__':
    main()
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>å­¦ç¿’é–‹å§‹...
Epoch 1/3, Loss: 0.6234
Epoch 2/3, Loss: 0.4521
Epoch 3/3, Loss: 0.3012

è©•ä¾¡é–‹å§‹...
ç²¾åº¦: 1.0000

 classificationãƒ¬ãƒãƒ¼ãƒˆ:
              precision    recall  f1-score   support

  ãƒã‚¬ãƒ†ã‚£ãƒ–       1.00      1.00      1.00         1
ãƒã‚¸ãƒ†ã‚£ãƒ–       1.00      1.00      1.00         1

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2
</code></pre>

</details>

<hr>

<h2>References</h2>

<ol>
<li>Vaswani, A., et al. (2017). <em>Attention Is All You Need</em>. NeurIPS.</li>
<li>Devlin, J., et al. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. NAACL.</li>
<li>Liu, Y., et al. (2019). <em>RoBERTa: A Robustly Optimized BERT Pretraining Approach</em>. arXiv.</li>
<li>Lan, Z., et al. (2020). <em>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</em>. ICLR.</li>
<li>Sanh, V., et al. (2019). <em>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</em>. NeurIPS Workshop.</li>
<li>HuggingFace Transformers Documentation. <a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a></li>
<li>æ±åŒ—å¤§å­¦BERTãƒ¢ãƒ‡ãƒ«. <a href="https://github.com/cl-tohoku/bert-japanese">https://github.com/cl-tohoku/bert-japanese</a></li>
</ol>

<div class="navigation">
    <a href="chapter2-word-embeddings.html" class="nav-button">â† Previous Chapter: å˜èªåŸ‹ã‚è¾¼ã¿</a>
    <a href="chapter4-advanced-transformers.html" class="nav-button">Next Chapter: ç™ºå±•çš„Transformerãƒ¢ãƒ‡ãƒ« â†’</a>
</div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>ä½œæˆ learner</strong>: AI Terakoya Content Team</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-21</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
