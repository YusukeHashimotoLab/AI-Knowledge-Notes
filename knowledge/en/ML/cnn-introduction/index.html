<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Convolutional Neural Network (CNN) Introduction Series - Complete Guide">
    <title>Convolutional Neural Network (CNN) Introduction Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "⚠️";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">›</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">›</span><span class="breadcrumb-current">CNN</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>Convolutional Neural Network (CNN) Introduction Series v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">From Image Recognition Fundamentals to Transfer Learning and Object Detection</p>
            <div class="meta">
                <span>Reading Time: 100-120 minutes</span>
                <span>Level: Intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Systematically master the most important architecture for image recognition</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is a practical educational content consisting of 5 chapters that allows you to learn Convolutional Neural Networks (CNN) from fundamentals progressively.</p>

        <p><strong>CNN</strong> is the most important deep learning architecture for computer vision tasks such as image recognition, object detection, and segmentation. By mastering local feature extraction through convolutional layers, dimensionality reduction through pooling layers, and efficient model construction techniques through transfer learning, you can build practical image recognition systems. We provide systematic knowledge from basic CNN mechanisms to modern architectures like ResNet and EfficientNet, and object detection with YOLO.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>From Fundamentals to Applications: Systematic learning from convolution principles to object detection</li>
            <li>Implementation-Focused: Over 40 executable PyTorch code examples and practical techniques</li>
            <li>Intuitive Understanding: Understand operational principles through filter and feature map visualization</li>
            <li>PyTorch Full Compliance: Latest implementation methods using industry-standard framework</li>
            <li>Transfer Learning Practice: Efficient development methods using pre-trained models</li>
        </ul>

        <p><strong>Total Learning Time</strong>: 100-120 minutes (including code execution and exercises)</p>

        <h2 id="learning">How to Learn</h2>

        <h3>Recommended Learning Order</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: CNN Fundamentals and Convolutional Layers] --> B[Chapter 2: Pooling Layers and CNN Architectures]
    B --> C[Chapter 3: Transfer Learning and Fine-Tuning]
    C --> D[Chapter 4: Data Augmentation and Model Optimization]
    D --> E[Chapter 5: Object Detection Introduction]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>For Beginners (No CNN knowledge):</strong><br>
        - Chapter 1 → Chapter 2 → Chapter 3 → Chapter 4 → Chapter 5 (All chapters recommended)<br>
        - Duration: 100-120 minutes</p>

        <p><strong>For Intermediate Learners (Deep learning experience):</strong><br>
        - Chapter 2 → Chapter 3 → Chapter 4 → Chapter 5<br>
        - Duration: 80-90 minutes</p>

        <p><strong>Topic-Specific Enhancement:</strong><br>
        - Transfer Learning: Chapter 3 (intensive study)<br>
        - Data Augmentation: Chapter 4 (intensive study)<br>
        - Object Detection: Chapter 5 (intensive study)<br>
        - Duration: 20-25 minutes per chapter</p>

        <h2 id="chapters">Chapter Details</h2>

        <h3><a href="./chapter1-cnn-basics.html">Chapter 1: CNN Fundamentals and Convolutional Layers</a></h3>
        <p><strong>Difficulty</strong>: Beginner to Intermediate<br>
        <strong>Reading Time</strong>: 20-25 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Principles of Convolution Operations</strong> - Understanding kernels, strides, and padding</li>
            <li><strong>Filters and Feature Maps</strong> - Mechanisms of edge detection and texture extraction</li>
            <li><strong>Channels and Dimensions</strong> - RGB image processing and multi-channel convolution</li>
            <li><strong>Convolutional Layer Implementation</strong> - Conv2D implementation and visualization with PyTorch</li>
            <li><strong>Receptive Field Concept</strong> - Stacking convolutional layers and field of view expansion</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>Understand the mathematical principles of convolution operations</li>
            <li>Explain the mechanism of feature extraction by filters</li>
            <li>Understand the effects of padding and stride</li>
            <li>Implement Conv2D with PyTorch</li>
            <li>Visualize and interpret feature maps</li>
        </ul>

        <p><strong><a href="./chapter1-cnn-basics.html">Read Chapter 1 →</a></strong></p>

        <hr>

        <h3><a href="./chapter2-pooling-architectures.html">Chapter 2: Pooling Layers and CNN Architectures</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 20-25 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Role of Pooling Layers</strong> - Max Pooling, Average Pooling, and dimensionality reduction</li>
            <li><strong>LeNet and AlexNet</strong> - Features and implementation of early CNN architectures</li>
            <li><strong>VGGNet</strong> - Design philosophy of stacking small filters</li>
            <li><strong>ResNet</strong> - Deep networks and solving gradient vanishing with residual connections</li>
            <li><strong>EfficientNet</strong> - Efficient scaling methods</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>Understand the roles and types of pooling layers</li>
            <li>Explain features of representative CNN architectures</li>
            <li>Understand the importance of ResNet's residual connections</li>
            <li>Implement VGG/ResNet with PyTorch</li>
            <li>Understand criteria for architecture selection</li>
        </ul>

        <p><strong><a href="./chapter2-pooling-architectures.html">Read Chapter 2 →</a></strong></p>

        <hr>

        <h3><a href="./chapter3-transfer-learning.html">Chapter 3: Transfer Learning and Fine-Tuning</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 20-25 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Principles of Transfer Learning</strong> - Utilizing ImageNet pre-trained models</li>
            <li><strong>Feature Extraction Approach</strong> - Fast learning with frozen layers</li>
            <li><strong>Fine-Tuning</strong> - Gradual layer unfreezing and training</li>
            <li><strong>Using timm Library</strong> - Hundreds of pre-trained models</li>
            <li><strong>Domain Adaptation</strong> - Strategies for applying to different datasets</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>Understand the benefits and principles of transfer learning</li>
            <li>Load pre-trained models with torchvision</li>
            <li>Distinguish between feature extraction and fine-tuning</li>
            <li>Utilize latest models with timm</li>
            <li>Design transfer learning strategies based on data size</li>
        </ul>

        <p><strong><a href="./chapter3-transfer-learning.html">Read Chapter 3 →</a></strong></p>

        <hr>

        <h3><a href="./chapter4-data-augmentation.html">Chapter 4: Data Augmentation and Model Optimization</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 20-25 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Basic Data Augmentation</strong> - Rotation, flipping, cropping, and color transformation</li>
            <li><strong>Advanced Augmentation Methods</strong> - Mixup, CutMix, and RandAugment</li>
            <li><strong>Regularization Techniques</strong> - Dropout, Batch Normalization, and Weight Decay</li>
            <li><strong>Mixed Precision Training</strong> - Acceleration and memory reduction with FP16</li>
            <li><strong>Learning Rate Scheduling</strong> - Cosine Annealing and Warmup</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>Implement data augmentation with torchvision.transforms</li>
            <li>Understand the effects of Mixup/CutMix</li>
            <li>Apply regularization methods appropriately</li>
            <li>Accelerate with Mixed Precision training</li>
            <li>Use learning rate schedulers effectively</li>
        </ul>

        <p><strong><a href="./chapter4-data-augmentation.html">Read Chapter 4 →</a></strong></p>

        <hr>

        <h3><a href="./chapter5-object-detection.html">Chapter 5: Object Detection Introduction</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Object Detection Fundamentals</strong> - Bounding Box, IoU, and Non-Maximum Suppression</li>
            <li><strong>YOLO Architecture</strong> - One-stage detection mechanism and implementation</li>
            <li><strong>Faster R-CNN</strong> - Two-stage detection and Region Proposal Network</li>
            <li><strong>Detection Evaluation Metrics</strong> - mAP and Precision-Recall curves</li>
            <li><strong>Practical Object Detection</strong> - Integration with OpenCV and real-time inference</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>Understand basic concepts of object detection</li>
            <li>Explain IoU and NMS mechanisms</li>
            <li>Implement object detection with YOLO</li>
            <li>Evaluate detection performance with mAP</li>
            <li>Perform real-time detection in coordination with OpenCV</li>
        </ul>

        <p><strong><a href="./chapter5-object-detection.html">Read Chapter 5 →</a></strong></p>

        <hr>

        <h2 id="outcomes">Overall Learning Outcomes</h2>

        <p>Upon completing this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>Explain the principles of CNN convolution operations and pooling</li>
            <li>Understand the features and evolution of representative CNN architectures</li>
            <li>Explain the mechanisms and benefits of transfer learning</li>
            <li>Understand the effects of data augmentation and regularization</li>
            <li>Explain basic object detection algorithms</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>Implement CNNs with PyTorch</li>
            <li>Execute transfer learning with pre-trained models</li>
            <li>Build data augmentation pipelines</li>
            <li>Accelerate training with Mixed Precision</li>
            <li>Build object detection systems with YOLO</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>Select appropriate architectures for new image classification tasks</li>
            <li>Design transfer learning strategies based on data size</li>
            <li>Optimize models while preventing overfitting</li>
            <li>Build real-time inference systems</li>
        </ul>

        <hr>

        <h2 id="prerequisites">Prerequisites</h2>

        <p>To effectively learn this series, the following knowledge is desirable:</p>

        <h3>Required (Must Have)</h3>
        <ul>
            <li><strong>Python Fundamentals</strong>: Variables, functions, classes, loops, and conditionals</li>
            <li><strong>NumPy Fundamentals</strong>: Array operations, broadcasting, and basic mathematical functions</li>
            <li><strong>Deep Learning Fundamentals</strong>: Neural networks, backpropagation, and gradient descent</li>
            <li><strong>PyTorch Fundamentals</strong>: Tensor operations, nn.Module, Dataset and DataLoader</li>
            <li><strong>Linear Algebra Fundamentals</strong>: Matrix operations, dot products, and shape transformations</li>
        </ul>

        <h3>Recommended (Nice to Have)</h3>
        <ul>
            <li><strong>Image Processing Fundamentals</strong>: Pixels, channels, and image formats</li>
            <li><strong>Optimization Algorithms</strong>: Adam, SGD, and learning rate scheduling</li>
            <li><strong>Matplotlib/PIL</strong>: Image loading and visualization</li>
            <li><strong>GPU Environment</strong>: Basic understanding of CUDA</li>
        </ul>

        <p><strong>Recommended Prior Learning</strong>:</p>
        <ul>
            <!-- Content in preparation <li>Deep Learning Fundamentals Series - Neural network basics</li>
            <li>PyTorch Introduction Series - Basic PyTorch operations</li> -->
        </ul>

        <hr>

        <h2 id="tech">Technologies and Tools Used</h2>

        <h3>Main Libraries</h3>
        <ul>
            <li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
            <li><strong>torchvision 0.15+</strong> - Image processing and model library</li>
            <li><strong>timm 0.9+</strong> - PyTorch Image Models, pre-trained model collection</li>
            <li><strong>OpenCV 4.8+</strong> - Image processing and object detection</li>
            <li><strong>NumPy 1.24+</strong> - Numerical computation</li>
            <li><strong>Matplotlib 3.7+</strong> - Visualization</li>
            <li><strong>Pillow 10.0+</strong> - Image loading and conversion</li>
        </ul>

        <h3>Development Environment</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - Programming language</li>
            <li><strong>Jupyter Notebook / Lab</strong> - Interactive development environment</li>
            <li><strong>Google Colab</strong> - GPU environment (free to use)</li>
            <li><strong>CUDA 11.8+ / cuDNN</strong> - GPU acceleration (recommended)</li>
        </ul>

        <h3>Datasets</h3>
        <ul>
            <li><strong>CIFAR-10/100</strong> - Image classification fundamentals</li>
            <li><strong>ImageNet</strong> - Large-scale image classification (pre-training)</li>
            <li><strong>COCO</strong> - Object detection and segmentation</li>
        </ul>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Ready to begin? Start with Chapter 1 and master CNN technology!</p>

        <p><strong><a href="./chapter1-cnn-basics.html">Chapter 1: CNN Fundamentals and Convolutional Layers →</a></strong></p>

        <hr>

        <h2 id="next">Next Steps</h2>

        <p>After completing this series, we recommend progressing to the following topics:</p>

        <h3>Advanced Learning</h3>
        <ul>
            <li><strong>Segmentation</strong>: U-Net, Mask R-CNN, and semantic segmentation</li>
            <li><strong>Vision Transformer</strong>: ViT, Swin Transformer, and attention mechanisms</li>
            <li><strong>Image Generation</strong>: GAN, VAE, and Diffusion Models</li>
            <li><strong>Model Optimization</strong>: Quantization, pruning, and Knowledge Distillation</li>
        </ul>

        <h3>Related Series</h3>
        <ul>
            <li><a href="../deep-learning-advanced/">Advanced Deep Learning</a> - Attention, Transformer, and latest architectures</li>
            <li><a href="../computer-vision/">Computer Vision Applications</a> - Segmentation and pose estimation</li>
            <li><a href="../model-deployment/">Model Deployment</a> - ONNX, TensorRT, and production deployment</li>
        </ul>

        <h3>Practical Projects</h3>
        <ul>
            <li>Image Classification System - Classify custom datasets with transfer learning</li>
            <li>Face Recognition App - Real-time face detection and authentication</li>
            <li>Medical Image Diagnosis - Anomaly detection in X-ray images</li>
            <li>Autonomous Driving Simulation - Object detection and lane recognition</li>
        </ul>

        <hr>

        <p><strong>Update History</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 first edition released</li>
        </ul>

        <hr>

        <p><strong>Your CNN learning journey starts here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
