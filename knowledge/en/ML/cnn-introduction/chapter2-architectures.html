<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Pooling Layers and CNN Architectures - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/cnn-introduction/index.html">CNN</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Pooling Layers and CNN Architectures</h1>
            <p class="subtitle">Understanding the Evolution and Design Principles of Representative CNN Models</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
                <span class="meta-item">üíª Code Examples: 10</span>
                <span class="meta-item">üìù Exercises: 5</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the role and types of pooling layers (Max Pooling, Average Pooling)</li>
<li>‚úÖ Explain dimensionality reduction and acquisition of translation invariance through pooling</li>
<li>‚úÖ Grasp the evolution of representative CNN architectures from LeNet-5 to ResNet</li>
<li>‚úÖ Understand the principles and effects of Batch Normalization</li>
<li>‚úÖ Implement overfitting prevention techniques using Dropout</li>
<li>‚úÖ Understand the importance of Skip Connections (Residual Connections)</li>
<li>‚úÖ Build a practical image classification model on the CIFAR-10 dataset</li>
</ul>

<hr>

<h2>2.1 The Role of Pooling Layers</h2>

<h3>What is Pooling?</h3>

<p>A <strong>Pooling Layer</strong> is a layer that performs <strong>spatial downsampling</strong> of the output from convolutional layers. Its main purposes are the following three:</p>

<ul>
<li><strong>Dimensionality reduction</strong>: Reduces the size of feature maps, decreasing computational cost and memory usage</li>
<li><strong>Translation invariance</strong>: Acquires robustness to minor positional changes in features</li>
<li><strong>Expanding receptive field</strong>: Integrates information from a wider range</li>
</ul>

<blockquote>
<p>"Pooling is an operation that preserves important features of an image while discarding unnecessary details"</p>
</blockquote>

<h3>Max Pooling vs Average Pooling</h3>

<p>There are mainly two types of pooling:</p>

<table>
<thead>
<tr>
<th>Type</th>
<th>Operation</th>
<th>Characteristics</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Max Pooling</strong></td>
<td>Takes the maximum value in the region</td>
<td>Preserves the strongest features</td>
<td>Object detection, general image classification</td>
</tr>
<tr>
<td><strong>Average Pooling</strong></td>
<td>Takes the average value in the region</td>
<td>Preserves overall features</td>
<td>Global Average Pooling, segmentation</td>
</tr>
</tbody>
</table>

<div class="mermaid">
graph LR
    A["Input Feature Map<br/>4√ó4"] --> B["Max Pooling<br/>2√ó2, stride=2"]
    A --> C["Average Pooling<br/>2√ó2, stride=2"]

    B --> D["Output<br/>2√ó2<br/>Max values retained"]
    C --> E["Output<br/>2√ó2<br/>Average values retained"]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
</div>

<h3>Example of Max Pooling Operation</h3>

<pre><code class="language-python">import numpy as np
import torch
import torch.nn as nn

# Input data (1 channel, 4√ó4)
input_data = torch.tensor([[
    [1.0, 3.0, 2.0, 4.0],
    [5.0, 6.0, 1.0, 2.0],
    [7.0, 2.0, 8.0, 3.0],
    [1.0, 4.0, 6.0, 9.0]
]], dtype=torch.float32).unsqueeze(0)  # (1, 1, 4, 4)

print("Input Feature Map:")
print(input_data.squeeze().numpy())

# Max Pooling (2√ó2, stride=2)
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
output_max = max_pool(input_data)

print("\nMax Pooling (2√ó2) Output:")
print(output_max.squeeze().numpy())

# Average Pooling (2√ó2, stride=2)
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
output_avg = avg_pool(input_data)

print("\nAverage Pooling (2√ó2) Output:")
print(output_avg.squeeze().numpy())

# Manual calculation verification (top-left region)
print("\nManual calculation (top-left 2√ó2 region):")
region = input_data[0, 0, 0:2, 0:2].numpy()
print(f"Region: \n{region}")
print(f"Max: {region.max()}")
print(f"Average: {region.mean()}")
</code></pre>

<h3>Effect of Pooling: Translation Invariance</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# Original feature map
original = torch.tensor([[
    [0, 0, 1, 0],
    [0, 1, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 4, 4)

# Slightly shifted feature map
shifted = torch.tensor([[
    [0, 1, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 4, 4)

max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

print("Max Pooling of original feature map:")
print(max_pool(original).squeeze().numpy())

print("\nMax Pooling of shifted feature map:")
print(max_pool(shifted).squeeze().numpy())

print("\n‚Üí Even with minor positional changes, Max Pooling output has 1 appearing in the same region")
print("  This is 'translation invariance'")
</code></pre>

<h3>Pooling Parameters</h3>

<ul>
<li><strong>kernel_size</strong>: Size of the pooling region (typically 2√ó2 or 3√ó3)</li>
<li><strong>stride</strong>: Sliding width (typically the same as kernel_size, with no overlap)</li>
<li><strong>padding</strong>: Zero padding (typically 0)</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

# Comparison of different pooling configurations
input_data = torch.randn(1, 1, 8, 8)  # (batch, channels, height, width)

# Configuration 1: 2√ó2, stride=2 (standard)
pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
output1 = pool1(input_data)

# Configuration 2: 3√ó3, stride=2 (with overlap)
pool2 = nn.MaxPool2d(kernel_size=3, stride=2)
output2 = pool2(input_data)

# Configuration 3: 2√ó2, stride=1 (high overlap)
pool3 = nn.MaxPool2d(kernel_size=2, stride=1)
output3 = pool3(input_data)

print(f"Input size: {input_data.shape}")
print(f"2√ó2, stride=2 output size: {output1.shape}")
print(f"3√ó3, stride=2 output size: {output2.shape}")
print(f"2√ó2, stride=1 output size: {output3.shape}")

# Calculate dimensionality reduction rate
reduction1 = (input_data.numel() - output1.numel()) / input_data.numel() * 100
reduction2 = (input_data.numel() - output2.numel()) / input_data.numel() * 100

print(f"\n2√ó2, stride=2 dimensionality reduction rate: {reduction1:.1f}%")
print(f"3√ó3, stride=2 dimensionality reduction rate: {reduction2:.1f}%")
</code></pre>

<h3>Global Average Pooling</h3>

<p><strong>Global Average Pooling (GAP)</strong> is a special type of pooling that takes the average of the entire feature map. In modern CNNs, it is increasingly used as a replacement for the final fully connected layer.</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# Input: (batch_size, channels, height, width)
input_features = torch.randn(2, 512, 7, 7)  # 2 samples, 512 channels, 7√ó7

# Global Average Pooling
gap = nn.AdaptiveAvgPool2d((1, 1))  # Specify output size as (1, 1)
output = gap(input_features)

print(f"Input size: {input_features.shape}")
print(f"GAP output size: {output.shape}")

# Flatten
output_flat = output.view(output.size(0), -1)
print(f"After flattening: {output_flat.shape}")

# Benefits of GAP
print("\nAdvantages of Global Average Pooling:")
print("1. Zero parameters (compared to fully connected layers)")
print("2. Independent of input size (works with any size)")
print("3. Reduced risk of overfitting")
print("4. Spatial average of each channel = intensity of the concept that channel represents")
</code></pre>

<hr>

<h2>2.2 Representative CNN Architectures</h2>

<h3>Evolution of CNNs: Historical Overview</h3>

<div class="mermaid">
graph LR
    A[LeNet-5<br/>1998] --> B[AlexNet<br/>2012]
    B --> C[VGGNet<br/>2014]
    C --> D[GoogLeNet<br/>2014]
    D --> E[ResNet<br/>2015]
    E --> F[DenseNet<br/>2017]
    F --> G[EfficientNet<br/>2019]
    G --> H[Vision Transformer<br/>2020+]

    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style F fill:#03a9f4
    style G fill:#039be5
    style H fill:#0288d1
</div>

<h3>LeNet-5 (1998): The Origin of CNNs</h3>

<p><strong>LeNet-5</strong> was developed by Yann LeCun for handwritten digit recognition (MNIST). It is the foundational architecture of modern CNNs.</p>

<table>
<thead>
<tr>
<th>Layer</th>
<th>Output Size</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td>1√ó28√ó28</td>
<td>-</td>
</tr>
<tr>
<td>Conv1 (5√ó5, 6ch)</td>
<td>6√ó24√ó24</td>
<td>156</td>
</tr>
<tr>
<td>AvgPool (2√ó2)</td>
<td>6√ó12√ó12</td>
<td>0</td>
</tr>
<tr>
<td>Conv2 (5√ó5, 16ch)</td>
<td>16√ó8√ó8</td>
<td>2,416</td>
</tr>
<tr>
<td>AvgPool (2√ó2)</td>
<td>16√ó4√ó4</td>
<td>0</td>
</tr>
<tr>
<td>FC1 (120)</td>
<td>120</td>
<td>30,840</td>
</tr>
<tr>
<td>FC2 (84)</td>
<td>84</td>
<td>10,164</td>
</tr>
<tr>
<td>FC3 (10)</td>
<td>10</td>
<td>850</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()

        # Feature extraction layers
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)    # 28√ó28 ‚Üí 24√ó24
        self.pool1 = nn.AvgPool2d(kernel_size=2)        # 24√ó24 ‚Üí 12√ó12
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)   # 12√ó12 ‚Üí 8√ó8
        self.pool2 = nn.AvgPool2d(kernel_size=2)        # 8√ó8 ‚Üí 4√ó4

        # Classification layers
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

    def forward(self, x):
        # Feature extraction
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        # Flatten
        x = x.view(x.size(0), -1)

        # Classification
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

# Create model and summary
model = LeNet5(num_classes=10)
print(model)

# Calculate parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Test run
x = torch.randn(1, 1, 28, 28)
output = model(x)
print(f"\nInput size: {x.shape}")
print(f"Output size: {output.shape}")
</code></pre>

<h3>AlexNet (2012): The Dawn of Deep Learning</h3>

<p><strong>AlexNet</strong> demonstrated overwhelming performance in the 2012 ImageNet competition and sparked the deep learning boom.</p>

<p>Key features:</p>
<ul>
<li>Use of <strong>ReLU activation function</strong> (learns faster than Sigmoid)</li>
<li><strong>Dropout</strong> for overfitting prevention</li>
<li>Utilization of <strong>Data Augmentation</strong></li>
<li>Leveraging <strong>GPU parallel processing</strong></li>
<li><strong>Local Response Normalization</strong> (not commonly used today)</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()

        # Feature extraction layers
        self.features = nn.Sequential(
            # Conv1: 96 filters, 11√ó11, stride=4
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv2: 256 filters, 5√ó5
            nn.Conv2d(96, 256, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv3: 384 filters, 3√ó3
            nn.Conv2d(256, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv4: 384 filters, 3√ó3
            nn.Conv2d(384, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv5: 256 filters, 3√ó3
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        # Classification layers
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.classifier(x)
        return x

# Create model
model = AlexNet(num_classes=1000)

# Parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"AlexNet total parameters: {total_params:,}")

# Check layer sizes
x = torch.randn(1, 3, 224, 224)
print(f"\nInput: {x.shape}")

for i, layer in enumerate(model.features):
    x = layer(x)
    if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)):
        print(f"Layer {i} ({layer.__class__.__name__}): {x.shape}")
</code></pre>

<h3>VGGNet (2014): The Aesthetics of Simplicity</h3>

<p><strong>VGGNet</strong> demonstrated the effectiveness of deep networks with a simple design that repeatedly uses small 3√ó3 filters.</p>

<p>Design principles:</p>
<ul>
<li>Uses <strong>only 3√ó3 filters</strong> (stacking small filters is more efficient)</li>
<li>Gradually halves size with <strong>2√ó2 Max Pooling</strong></li>
<li><strong>Doubles channel count</strong> while going deeper (64 ‚Üí 128 ‚Üí 256 ‚Üí 512)</li>
<li>VGG-16 (16 layers) and VGG-19 (19 layers) are famous</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn

class VGGBlock(nn.Module):
    """Basic VGG block: repeat Conv ‚Üí ReLU"""
    def __init__(self, in_channels, out_channels, num_convs):
        super(VGGBlock, self).__init__()

        layers = []
        for i in range(num_convs):
            layers.append(nn.Conv2d(
                in_channels if i == 0 else out_channels,
                out_channels,
                kernel_size=3,
                padding=1
            ))
            layers.append(nn.ReLU(inplace=True))

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()

        # Feature extraction part
        self.features = nn.Sequential(
            VGGBlock(3, 64, 2),      # Block 1: 64 channels, 2 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(64, 128, 2),    # Block 2: 128 channels, 2 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(128, 256, 3),   # Block 3: 256 channels, 3 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(256, 512, 3),   # Block 4: 512 channels, 3 convs
            nn.MaxPool2d(2, 2),

            VGGBlock(512, 512, 3),   # Block 5: 512 channels, 3 convs
            nn.MaxPool2d(2, 2),
        )

        # Classification part
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Create model
model = VGG16(num_classes=1000)
total_params = sum(p.numel() for p in model.parameters())
print(f"VGG-16 total parameters: {total_params:,}")

# Why stack 3√ó3 filters twice?
print("\nTwo 3√ó3 filters vs one 5√ó5 filter:")
print("Receptive field: Same 5√ó5")
print("Parameters: 3√ó3√ó2 = 18 < 5√ó5 = 25")
print("Non-linearity: 2 ReLUs > 1 ReLU (higher expressiveness)")
</code></pre>

<h3>ResNet (2015): The Revolution of Skip Connections</h3>

<p><strong>ResNet (Residual Network)</strong> introduced <strong>Skip Connections (residual connections)</strong>, enabling the training of very deep networks (over 100 layers).</p>

<p>Problem: Deeper networks should perform better, but in practice the <strong>vanishing gradient problem</strong> makes training difficult.</p>

<p>Solution: Introduce <strong>Residual Blocks</strong></p>

<div class="mermaid">
graph TD
    A["Input x"] --> B["Conv + ReLU"]
    B --> C["Conv"]
    A --> D["Identity<br/>(as is)"]
    C --> E["Addition +"]
    D --> E
    E --> F["ReLU"]
    F --> G["Output"]

    style A fill:#e1f5ff
    style D fill:#fff9c4
    style E fill:#c8e6c9
    style G fill:#4fc3f7
</div>

<p>Mathematical expression:</p>
$$
\mathbf{y} = F(\mathbf{x}) + \mathbf{x}
$$

<p>Where $F(\mathbf{x})$ is the residual function (the part to be learned) and $\mathbf{x}$ is the shortcut connection.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """Basic ResNet block"""
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()

        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Shortcut path (adjustment when input/output channels differ)
        self.downsample = downsample

    def forward(self, x):
        identity = x

        # Main path
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # Shortcut connection
        if self.downsample is not None:
            identity = self.downsample(x)

        # Addition
        out += identity
        out = F.relu(out)

        return out

class SimpleResNet(nn.Module):
    """Simplified ResNet (for CIFAR-10)"""
    def __init__(self, num_classes=10):
        super(SimpleResNet, self).__init__()

        # Initial layer
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # Residual blocks
        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)
        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)
        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)

        # Classification layer
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels),
            )

        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))

        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# Create model
model = SimpleResNet(num_classes=10)
total_params = sum(p.numel() for p in model.parameters())
print(f"ResNet total parameters: {total_params:,}")

# Visualize Skip Connection effect
x = torch.randn(1, 3, 32, 32)
output = model(x)
print(f"\nInput: {x.shape} ‚Üí Output: {output.shape}")
</code></pre>

<h3>Why Skip Connections are Effective</h3>

<details>
<summary><strong>Theoretical Background of Skip Connections</strong></summary>

<p>In conventional networks, deepening causes the vanishing gradient problem:</p>

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

<p>With deep layers, $\frac{\partial y}{\partial x}$ is multiplied many times, causing gradients to vanish.</p>

<p>With Skip Connections:</p>

$$
\frac{\partial}{\partial x}(F(x) + x) = \frac{\partial F(x)}{\partial x} + 1
$$

<p>Because of the "+1" term, gradients always flow!</p>

<p>Furthermore, the network only needs to "learn the identity mapping" ‚Üí learning becomes easier.</p>

</details>

<hr>

<h2>2.3 Batch Normalization</h2>

<h3>What is Batch Normalization?</h3>

<p><strong>Batch Normalization (BN)</strong> is a technique that normalizes the output of each mini-batch to stabilize learning.</p>

<p>Normalize the output of each layer to mean 0 and variance 1 across the mini-batch:</p>

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

$$
y_i = \gamma \hat{x}_i + \beta
$$

<p>Where:</p>
<ul>
<li>$\mu_B, \sigma_B^2$: Mean and variance of the mini-batch</li>
<li>$\gamma, \beta$: Learnable parameters (scale and shift)</li>
<li>$\epsilon$: Small value for numerical stability (e.g., 1e-5)</li>
</ul>

<h3>Effects of Batch Normalization</h3>

<ul>
<li><strong>Faster learning</strong>: Can use larger learning rates</li>
<li><strong>Gradient stabilization</strong>: Suppresses Internal Covariate Shift</li>
<li><strong>Regularization effect</strong>: Reduces need for Dropout</li>
<li><strong>Reduced dependency on initialization</strong>: Weight initialization becomes easier</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlockWithoutBN(nn.Module):
    """Without Batch Normalization"""
    def __init__(self, in_channels, out_channels):
        super(ConvBlockWithoutBN, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x):
        return F.relu(self.conv(x))

class ConvBlockWithBN(nn.Module):
    """With Batch Normalization"""
    def __init__(self, in_channels, out_channels):
        super(ConvBlockWithBN, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        return F.relu(self.bn(self.conv(x)))

# Comparison experiment
x = torch.randn(32, 3, 32, 32)  # Batch size 32

# Without BN
block_without_bn = ConvBlockWithoutBN(3, 64)
output_without_bn = block_without_bn(x)

# With BN
block_with_bn = ConvBlockWithBN(3, 64)
output_with_bn = block_with_bn(x)

print("=== Effect of Batch Normalization ===")
print(f"Without BN - Mean: {output_without_bn.mean():.4f}, Std: {output_without_bn.std():.4f}")
print(f"With BN - Mean: {output_with_bn.mean():.4f}, Std: {output_with_bn.std():.4f}")

# Visualize distribution
print("\nStatistics per channel (with BN):")
for i in range(min(5, output_with_bn.size(1))):
    channel_data = output_with_bn[:, i, :, :]
    print(f"  Channel {i}: Mean={channel_data.mean():.4f}, Std={channel_data.std():.4f}")
</code></pre>

<h3>Placement of Batch Normalization</h3>

<p>BN is typically placed in the order: <strong>Conv ‚Üí BN ‚Üí Activation</strong>:</p>

<pre><code class="language-python">import torch.nn as nn

# Recommended order
class StandardConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(StandardConvBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)    # 1. Convolution
        x = self.bn(x)      # 2. Batch Normalization
        x = self.relu(x)    # 3. Activation function
        return x

# Note: Specify bias=False in Conv
# Reason: BN makes the mean 0, so bias is unnecessary
block = StandardConvBlock(3, 64)
print("Conv ‚Üí BN ‚Üí ReLU order")
print(block)
</code></pre>

<hr>

<h2>2.4 Overfitting Prevention with Dropout</h2>

<h3>What is Dropout?</h3>

<p><strong>Dropout</strong> is a regularization technique that prevents overfitting by randomly disabling (dropping out) neurons during training.</p>

<ul>
<li>During training: Randomly set neurons to 0 with probability $p$</li>
<li>During testing: Use all neurons (with scaling)</li>
</ul>

<div class="mermaid">
graph TD
    A["During Training"] --> B["All Neurons"]
    B --> C["Drop 50% Randomly"]
    C --> D["Learn with Remaining 50%"]

    E["During Testing"] --> F["Use All Neurons"]
    F --> G["Scale Weights by 0.5"]

    style A fill:#e1f5ff
    style E fill:#c8e6c9
    style D fill:#b3e5fc
    style G fill:#81d4fa
</div>

<h3>Why is Dropout Effective?</h3>

<ul>
<li><strong>Ensemble effect</strong>: Train different sub-networks each time ‚Üí similar to averaging multiple models</li>
<li><strong>Prevent co-adaptation</strong>: Learn robust representations that don't depend on specific neurons</li>
<li><strong>Regularization</strong>: Suppresses model complexity</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Verify Dropout behavior
x = torch.ones(4, 10)  # Tensor of all 1s

dropout = nn.Dropout(p=0.5)  # 50% probability of dropping

# Training mode
dropout.train()
print("=== Training Mode (Dropout Enabled) ===")
for i in range(3):
    output = dropout(x)
    print(f"Trial {i+1}: {output[0, :5].numpy()}")  # Display first 5 elements

# Evaluation mode
dropout.eval()
print("\n=== Evaluation Mode (Dropout Disabled) ===")
output = dropout(x)
print(f"Output: {output[0, :5].numpy()}")
</code></pre>

<h3>How to Use Dropout in CNNs</h3>

<p>In CNNs, Dropout is typically placed <strong>before fully connected layers</strong>. It's not commonly used in convolutional layers.</p>

<pre><code class="language-python">import torch.nn as nn

class CNNWithDropout(nn.Module):
    def __init__(self, num_classes=10):
        super(CNNWithDropout, self).__init__()

        # Convolutional layers (without Dropout)
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        # Fully connected layers (with Dropout)
        self.classifier = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),  # Dropout

            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),  # Dropout

            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

model = CNNWithDropout(num_classes=10)
print(model)

# Experiment with Dropout effect
model.train()
x = torch.randn(2, 3, 32, 32)
output1 = model(x)
output2 = model(x)
print(f"\nTraining mode: Same input produces different outputs = {not torch.allclose(output1, output2)}")

model.eval()
output3 = model(x)
output4 = model(x)
print(f"Evaluation mode: Same input produces same output = {torch.allclose(output3, output4)}")
</code></pre>

<h3>Dropout vs Batch Normalization</h3>

<table>
<thead>
<tr>
<th>Item</th>
<th>Dropout</th>
<th>Batch Normalization</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Main Purpose</strong></td>
<td>Overfitting prevention</td>
<td>Learning stabilization & acceleration</td>
</tr>
<tr>
<td><strong>Where Used</strong></td>
<td>Fully connected layers</td>
<td>Convolutional layers</td>
</tr>
<tr>
<td><strong>Train/Test</strong></td>
<td>Different behavior</td>
<td>Different behavior</td>
</tr>
<tr>
<td><strong>Combined Use</strong></td>
<td>Possible (though BN may make it unnecessary)</td>
<td>-</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Modern Best Practice</strong>: Use Batch Normalization in convolutional layers and Dropout in fully connected layers (as needed). However, due to BN's regularization effect, Dropout often becomes unnecessary.</p>
</blockquote>

<hr>

<h2>2.5 Practical: CIFAR-10 Image Classification</h2>

<h3>CIFAR-10 Dataset</h3>

<p><strong>CIFAR-10</strong> is a dataset consisting of 32√ó32 color images (60,000 total) in 10 classes:</p>

<ul>
<li>Classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck</li>
<li>Training data: 50,000 images</li>
<li>Test data: 10,000 images</li>
</ul>

<h3>Complete CNN Classifier Implementation</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Data augmentation and normalization
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# Load dataset
train_dataset = datasets.CIFAR10(root='./data', train=True,
                                 download=True, transform=transform_train)
test_dataset = datasets.CIFAR10(root='./data', train=False,
                                download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)

print(f"Training data: {len(train_dataset)} images")
print(f"Test data: {len(test_dataset)} images")

# Class names
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')
</code></pre>

<h3>Modern CNN Architecture</h3>

<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

class CIFAR10Net(nn.Module):
    """Modern CNN for CIFAR-10"""
    def __init__(self, num_classes=10):
        super(CIFAR10Net, self).__init__()

        # Block 1
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # Block 2
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(128)

        # Block 3
        self.conv3 = nn.Conv2d(128, 256, 3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 256, 3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(256)

        # Block 4
        self.conv5 = nn.Conv2d(256, 512, 3, padding=1, bias=False)
        self.bn5 = nn.BatchNorm2d(512)
        self.conv6 = nn.Conv2d(512, 512, 3, padding=1, bias=False)
        self.bn6 = nn.BatchNorm2d(512)

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1, 1))

        # Classification layer
        self.fc = nn.Linear(512, num_classes)

        # Dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Block 1
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2)

        # Block 2
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2)

        # Block 3
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.max_pool2d(x, 2)

        # Block 4
        x = F.relu(self.bn5(self.conv5(x)))
        x = F.relu(self.bn6(self.conv6(x)))

        # Global Average Pooling
        x = self.gap(x)
        x = x.view(x.size(0), -1)

        # Dropout + classification
        x = self.dropout(x)
        x = self.fc(x)

        return x

# Create model
model = CIFAR10Net(num_classes=10).to(device)
print(model)

# Parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters: {total_params:,}")
</code></pre>

<h3>Training Loop</h3>

<pre><code class="language-python">import torch.optim as optim

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Forward
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def test_epoch(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

# Execute training
num_epochs = 50
best_acc = 0

print("\nStarting training...")
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = test_epoch(model, test_loader, criterion, device)

    scheduler.step()

    if (epoch + 1) % 5 == 0 or epoch == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

    # Save best model
    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_cifar10_model.pth')

print(f"\nTraining complete! Best accuracy: {best_acc:.2f}%")
</code></pre>

<h3>Prediction and Visualization</h3>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def imshow(img, title=None):
    """Helper function for image display"""
    img = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.4914, 0.4822, 0.4465])
    std = np.array([0.2023, 0.1994, 0.2010])
    img = std * img + mean
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')

# Get samples from test data
dataiter = iter(test_loader)
images, labels = next(dataiter)
images, labels = images.to(device), labels.to(device)

# Prediction
model.eval()
with torch.no_grad():
    outputs = model(images)
    _, predicted = outputs.max(1)

# Display first 8 images
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
for i, ax in enumerate(axes.flat):
    imshow(images[i].cpu(), title=f"True: {classes[labels[i]]}\nPred: {classes[predicted[i]]}")
    ax.imshow(images[i].cpu().numpy().transpose((1, 2, 0)))

plt.tight_layout()
plt.savefig('cifar10_predictions.png', dpi=150, bbox_inches='tight')
print("Saved prediction results: cifar10_predictions.png")
</code></pre>

<hr>

<h2>2.6 Overview of Modern Architectures</h2>

<h3>EfficientNet (2019): Efficient Scaling</h3>

<p><strong>EfficientNet</strong> proposed a method to <strong>scale network depth, width, and resolution in a balanced way</strong>.</p>

<p>Compound Scaling:</p>
$$
\text{depth} = \alpha^\phi, \quad \text{width} = \beta^\phi, \quad \text{resolution} = \gamma^\phi
$$

<p>Constraint: $\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$</p>

<ul>
<li>Achieves high accuracy with fewer parameters</li>
<li>Uses Mobile Inverted Bottleneck Convolution (MBConv)</li>
<li>Variations from EfficientNet-B0 to B7 for accuracy and size trade-offs</li>
</ul>

<h3>Vision Transformer (2020+): Surpassing CNNs</h3>

<p><strong>Vision Transformer (ViT)</strong> is a new approach that <strong>divides images into patches</strong> and processes them with Transformers.</p>

<div class="mermaid">
graph LR
    A["Image<br/>224√ó224"] --> B["Patch Division<br/>16√ó16 patches"]
    B --> C["Linear Projection"]
    C --> D["Transformer Encoder"]
    D --> E["Classification Head"]
    E --> F["Class Prediction"]

    style A fill:#e1f5ff
    style D fill:#b3e5fc
    style F fill:#4fc3f7
</div>

<p>Features:</p>
<ul>
<li>Abandons CNN's inductive biases (locality, translation invariance)</li>
<li>Surpasses CNNs on large-scale data</li>
<li>Captures relationships across the entire image with Self-Attention</li>
<li>Likely to become mainstream in the future</li>
</ul>

<h3>Architecture Selection Guidelines</h3>

<table>
<thead>
<tr>
<th>Architecture</th>
<th>Features</th>
<th>Recommended Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LeNet-5</strong></td>
<td>Simple, lightweight</td>
<td>MNIST, educational purposes</td>
</tr>
<tr>
<td><strong>VGGNet</strong></td>
<td>Easy-to-understand structure</td>
<td>Transfer learning base, education</td>
</tr>
<tr>
<td><strong>ResNet</strong></td>
<td>Deep network, stable</td>
<td>General image classification, standard choice</td>
</tr>
<tr>
<td><strong>EfficientNet</strong></td>
<td>Efficient, high accuracy</td>
<td>Resource constraints, mobile</td>
</tr>
<tr>
<td><strong>Vision Transformer</strong></td>
<td>State-of-the-art, large-scale data</td>
<td>Large datasets, research</td>
</tr>
</tbody>
</table>

<hr>

<h2>Exercises</h2>

<details>
<summary><strong>Exercise 1: Effects of Pooling Layers</strong></summary>

<p>Apply Max Pooling and Average Pooling to the same input and verify the differences in output. For what types of image features is each advantageous?</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# Create feature map with edges
edge_feature = torch.tensor([[
    [0, 0, 0, 0, 0, 0],
    [0, 1, 1, 1, 1, 0],
    [0, 1, 0, 0, 1, 0],
    [0, 1, 0, 0, 1, 0],
    [0, 1, 1, 1, 1, 0],
    [0, 0, 0, 0, 0, 0]
]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

# TODO: Apply Max Pooling and Average Pooling and compare results
# Hint: Use nn.MaxPool2d and nn.AvgPool2d
</code></pre>

</details>

<details>
<summary><strong>Exercise 2: Skip Connection in Residual Block</strong></summary>

<p>Compare Residual blocks with and without Skip Connections to verify differences in gradient flow.</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: Implement blocks with and without Skip Connection
# Hint: Same structure, only difference is presence of Skip Connection
# To compare gradients, check grad attribute after backward()
</code></pre>

</details>

<details>
<summary><strong>Exercise 3: Effect of Batch Normalization</strong></summary>

<p>Train the same network with and without Batch Normalization and compare convergence speed and final accuracy.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# TODO: Create models with and without BN
# TODO: Train on MNIST or CIFAR-10
# TODO: Compare training curves
</code></pre>

</details>

<details>
<summary><strong>Exercise 4: Regularization Effect of Dropout</strong></summary>

<p>Vary Dropout probability (p=0.0, 0.3, 0.5, 0.7) and investigate the impact on overfitting.</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: Train models with different Dropout probabilities
# TODO: Compare difference between training and test errors (degree of overfitting)
# Which Dropout probability is optimal?
</code></pre>

</details>

<details>
<summary><strong>Exercise 5: Architecture Comparison on CIFAR-10</strong></summary>

<p>Compare three architectures on CIFAR-10: LeNet-5, VGG-style, and ResNet-style.</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: Implement three different architectures
# TODO: Compare performance with same training settings
# TODO: Record parameter count, training time, and accuracy

# Evaluation metrics:
# - Test accuracy
# - Parameter count
# - Training time (per epoch)
# - Epochs until convergence
</code></pre>

</details>

<hr>

<h2>Summary</h2>

<p>In this chapter, we learned about pooling layers and representative CNN architectures.</p>

<h3>Key Points</h3>

<ul>
<li><strong>Pooling layers</strong>: Provide dimensionality reduction and translation invariance. Choosing between Max Pooling and Average Pooling</li>
<li><strong>LeNet-5</strong>: Foundation of CNNs. Basic structure of Conv ‚Üí Pool ‚Üí FC</li>
<li><strong>AlexNet</strong>: Utilization of ReLU, Dropout, and Data Augmentation</li>
<li><strong>VGGNet</strong>: Simple design with repeated 3√ó3 filters</li>
<li><strong>ResNet</strong>: Solved vanishing gradients with Skip Connections, enabling networks over 100 layers</li>
<li><strong>Batch Normalization</strong>: Stabilizes and accelerates learning. Conv ‚Üí BN ‚Üí ReLU order</li>
<li><strong>Dropout</strong>: Prevents overfitting. Used in fully connected layers</li>
<li><strong>Global Average Pooling</strong>: Reduces parameters, independent of input size</li>
</ul>

<h3>Next Steps</h3>

<p>In the next chapter, we will learn about <strong>Transfer Learning</strong> and <strong>Fine-tuning</strong>. We will master practical techniques for building high-accuracy models with limited data by leveraging pre-trained models.</p>

<div class="navigation">
    <a href="chapter1-cnn-basics.html" class="nav-button">‚Üê Chapter 1: CNN Basics</a>
    <a href="chapter3-transfer-learning.html" class="nav-button">Chapter 3: Transfer Learning and Fine-tuning ‚Üí</a>
</div>

</main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
