<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Fundamentals of CNN and Convolutional Layers - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Fundamentals of CNN and Convolutional Layers - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/cnn-introduction/index.html">CNN</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/cnn-introduction/chapter1-cnn-basics.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Fundamentals of CNN and Convolutional Layers</h1>
<p class="subtitle">Revolution in Image Recognition - Understanding the Basic Principles of Convolutional Neural Networks</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
<span class="meta-item">üíª Code Examples: 11</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Fundamentals of CNN and Convolutional Layers, which forms the foundation of this area. You will learn mathematical definition, roles of stride, and concepts of feature maps.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the challenges of traditional image recognition methods and the advantages of CNNs</li>
<li>‚úÖ Master the mathematical definition and computational process of convolution operations</li>
<li>‚úÖ Understand the roles of stride, padding, and kernel size</li>
<li>‚úÖ Explain the concepts of feature maps and receptive fields</li>
<li>‚úÖ Implement Conv2d layers in PyTorch and calculate parameters</li>
<li>‚úÖ Understand filter visualization and feature extraction mechanisms</li>
</ul>
<hr/>
<h2>1.1 Challenges in Image Recognition and the Emergence of CNNs</h2>
<h3>Limitations of Traditional Image Recognition Methods</h3>
<p>When using <strong>Fully Connected Networks</strong> for image recognition, serious problems arise.</p>
<blockquote>
<p>"Images are two-dimensional data with spatial structure. Ignoring this structure leads to an explosion of parameters and overfitting."</p>
</blockquote>
<h4>Problem 1: Explosion of Parameter Count</h4>
<p>For example, when inputting a 224√ó224 pixel color image (RGB) to a fully connected layer:</p>
<ul>
<li>Input dimensions: $224 \times 224 \times 3 = 150,528$</li>
<li>If hidden layer has 1,000 neurons: $150,528 \times 1,000 = 150,528,000$ parameters</li>
<li>That's over 150 million parameters in just the first layer!</li>
</ul>
<h4>Problem 2: Lack of Translation Invariance</h4>
<p>In fully connected layers, even a slight change in object position within an image is treated as a completely different input.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: In fully connected layers, even a slight change in object po

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# Simple example: Representing "cat" features (ears) in a 5√ó5 image
original = np.zeros((5, 5))
original[0, 1] = 1  # Left ear
original[0, 3] = 1  # Right ear
original[2, 2] = 1  # Nose

# Shifted 1 pixel to the right
shifted = np.zeros((5, 5))
shifted[0, 2] = 1  # Left ear
shifted[0, 4] = 1  # Right ear
shifted[2, 3] = 1  # Nose

print("Original image flattened:", original.flatten())
print("Shifted image flattened:", shifted.flatten())
print(f"Euclidean distance: {np.linalg.norm(original.flatten() - shifted.flatten()):.2f}")

# In fully connected layers, these two are treated as completely different inputs
print("\nConclusion: Fully connected layers cannot handle minor positional changes")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Original image flattened: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Shifted image flattened: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Euclidean distance: 2.45

Conclusion: Fully connected layers cannot handle minor positional changes
</code></pre>
<h3>Three Important Properties of CNNs</h3>
<p><strong>Convolutional Neural Networks (CNNs)</strong> leverage the spatial structure of images with the following properties:</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Explanation</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Local Connectivity</strong></td>
<td>Each neuron connects only to a small region of the input</td>
<td>Reduction in parameter count</td>
</tr>
<tr>
<td><strong>Weight Sharing</strong></td>
<td>Same filter used across the entire image</td>
<td>Acquisition of translation invariance</td>
</tr>
<tr>
<td><strong>Hierarchical Feature Learning</strong></td>
<td>Progressively extracts low-level to high-level features</td>
<td>Complex pattern recognition</td>
</tr>
</tbody>
</table>
<h3>Overall Structure of CNNs</h3>
<div class="mermaid">
graph LR
    A[Input Image<br/>28√ó28√ó1] --&gt; B[Conv Layer<br/>26√ó26√ó32]
    B --&gt; C[Activation<br/>ReLU]
    C --&gt; D[Pooling<br/>13√ó13√ó32]
    D --&gt; E[Conv Layer<br/>11√ó11√ó64]
    E --&gt; F[Activation<br/>ReLU]
    F --&gt; G[Pooling<br/>5√ó5√ó64]
    G --&gt; H[Flatten<br/>1600]
    H --&gt; I[FC Layer<br/>128]
    I --&gt; J[Output Layer<br/>10 classes]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#fce4ec
    style E fill:#fff3e0
    style F fill:#e8f5e9
    style G fill:#fce4ec
    style H fill:#f3e5f5
    style I fill:#fff9c4
    style J fill:#ffebee
</div>
<hr/>
<h2>1.2 Fundamentals of Convolution Operations</h2>
<h3>What is Convolution?</h3>
<p><strong>Convolution</strong> is an operation where a filter (kernel) is slid across an image while performing element-wise multiplication and summation.</p>
<h4>Mathematical Definition</h4>
<p>Two-dimensional discrete convolution is defined as follows:</p>

$$
(I * K)(i, j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m, n)
$$

<p>Where:</p>
<ul>
<li>$I$: Input image</li>
<li>$K$: Kernel or Filter</li>
<li>$(i, j)$: Output position</li>
<li>$(m, n)$: Position within kernel</li>
</ul>
<h4>Concrete Example: Convolution with 3√ó3 Kernel</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Concrete Example: Convolution with 3√ó3 Kernel

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: 5-10 seconds
Dependencies: None
"""

import numpy as np

# Input image (5√ó5)
image = np.array([
    [1, 2, 3, 0, 1],
    [4, 5, 6, 1, 2],
    [7, 8, 9, 2, 3],
    [1, 2, 3, 4, 5],
    [2, 3, 4, 5, 6]
])

# Edge detection kernel (3√ó3)
kernel = np.array([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
])

def manual_convolution(image, kernel):
    """
    Manually execute convolution operation
    """
    img_h, img_w = image.shape
    ker_h, ker_w = kernel.shape

    # Calculate output size
    out_h = img_h - ker_h + 1
    out_w = img_w - ker_w + 1

    output = np.zeros((out_h, out_w))

    # Convolution operation
    for i in range(out_h):
        for j in range(out_w):
            # Extract image region
            region = image[i:i+ker_h, j:j+ker_w]
            # Sum of element-wise products
            output[i, j] = np.sum(region * kernel)

    return output

# Execute convolution
result = manual_convolution(image, kernel)

print("Input image (5√ó5):")
print(image)
print("\nKernel (3√ó3, edge detection):")
print(kernel)
print("\nOutput (3√ó3):")
print(result)

# Detailed calculation example (top-left position)
print("\n=== Calculation Example (Position [0, 0]) ===")
region = image[0:3, 0:3]
print("Image region:")
print(region)
print("\nKernel:")
print(kernel)
print("\nElement-wise product:")
print(region * kernel)
print(f"\nSum: {np.sum(region * kernel)}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Input image (5√ó5):
[[1 2 3 0 1]
 [4 5 6 1 2]
 [7 8 9 2 3]
 [1 2 3 4 5]
 [2 3 4 5 6]]

Kernel (3√ó3, edge detection):
[[-1 -1 -1]
 [-1  8 -1]
 [-1 -1 -1]]

Output (3√ó3):
[[-13. -15. -12.]
 [ -8.  -9.  -6.]
 [ -5.  -6.   0.]]

=== Calculation Example (Position [0, 0]) ===
Image region:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Kernel:
[[-1 -1 -1]
 [-1  8 -1]
 [-1 -1 -1]]

Element-wise product:
[[-1 -2 -3]
 [-4 40 -6]
 [-7 -8 -9]]

Sum: -13
</code></pre>
<h3>Filters and Kernels</h3>
<p><strong>Kernel</strong> and <strong>Filter</strong> are often used interchangeably, but strictly speaking:</p>
<ul>
<li><strong>Kernel</strong>: A 2D weight array (e.g., 3√ó3 matrix)</li>
<li><strong>Filter</strong>: A collection of kernels across all channels (e.g., 3√ó3√ó3 filter for RGB images)</li>
</ul>
<h4>Examples of Representative Kernels</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - scipy&gt;=1.11.0

"""
Example: Examples of Representative Kernels

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import matplotlib.pyplot as plt
from scipy import signal

# Define various kernels
kernels = {
    "Identity": np.array([[0, 0, 0],
                       [0, 1, 0],
                       [0, 0, 0]]),

    "Edge Detection (Vertical)": np.array([[-1, 0, 1],
                              [-2, 0, 2],
                              [-1, 0, 1]]),  # Sobel filter

    "Edge Detection (Horizontal)": np.array([[-1, -2, -1],
                              [ 0,  0,  0],
                              [ 1,  2,  1]]),

    "Smoothing (blur)": np.array([[1, 1, 1],
                            [1, 1, 1],
                            [1, 1, 1]]) / 9,

    "Sharpening": np.array([[ 0, -1,  0],
                         [-1,  5, -1],
                         [ 0, -1,  0]])
}

# Test image (simple pattern)
test_image = np.array([
    [0, 0, 0, 0, 0, 0, 0],
    [0, 255, 255, 255, 255, 0, 0],
    [0, 255, 0, 0, 255, 0, 0],
    [0, 255, 0, 0, 255, 0, 0],
    [0, 255, 255, 255, 255, 0, 0],
    [0, 0, 0, 0, 0, 0, 0]
], dtype=float)

# Visualize effects of each kernel
fig, axes = plt.subplots(2, 3, figsize=(12, 8))
axes = axes.flatten()

axes[0].imshow(test_image, cmap='gray')
axes[0].set_title('Original Image')
axes[0].axis('off')

for idx, (name, kernel) in enumerate(kernels.items(), 1):
    result = signal.correlate2d(test_image, kernel, mode='same', boundary='symm')
    axes[idx].imshow(result, cmap='gray')
    axes[idx].set_title(name)
    axes[idx].axis('off')

plt.tight_layout()
print("Visualized kernel effects")
</code></pre>
<h3>Stride and Padding</h3>
<h4>Stride</h4>
<p><strong>Stride</strong> is the step size when moving the kernel.</p>
<ul>
<li>Stride = 1: Kernel moves 1 pixel at a time (standard)</li>
<li>Stride = 2: Kernel moves 2 pixels at a time (output size halved)</li>
</ul>
<p>Output size calculation formula:</p>

$$
\text{Output Size} = \left\lfloor \frac{\text{Input Size} - \text{Kernel Size}}{\text{Stride}} \right\rfloor + 1
$$

<h4>Padding</h4>
<p><strong>Padding</strong> is the operation of adding values (typically 0) around the input image.</p>
<table>
<thead>
<tr>
<th>Padding Type</th>
<th>Explanation</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Valid</strong></td>
<td>No padding</td>
<td>When reducing output size</td>
</tr>
<tr>
<td><strong>Same</strong></td>
<td>Adjusted so output size = input size</td>
<td>When maintaining spatial size</td>
</tr>
<tr>
<td><strong>Full</strong></td>
<td>So that entire kernel overlaps with image</td>
<td>When maximizing use of boundary information</td>
</tr>
</tbody>
</table>
<p>Padding amount calculation for Same padding:</p>

$$
\text{Padding} = \frac{\text{Kernel Size} - 1}{2}
$$

<pre><code class="language-python">def calculate_output_size(input_size, kernel_size, stride, padding):
    """
    Calculate output size after convolution operation

    Parameters:
    -----------
    input_size : int
        Input height or width
    kernel_size : int
        Kernel height or width
    stride : int
        Stride
    padding : int
        Padding amount

    Returns:
    --------
    int : Output size
    """
    return (input_size + 2 * padding - kernel_size) // stride + 1

# Calculate output sizes for various configurations
print("=== Output Size Calculation Examples ===\n")

configurations = [
    (28, 3, 1, 0, "Valid (no padding)"),
    (28, 3, 1, 1, "Same (maintain size)"),
    (28, 5, 2, 2, "Stride 2, Padding 2"),
    (32, 3, 1, 1, "32√ó32 image, 3√ó3 kernel"),
]

for input_size, kernel_size, stride, padding, description in configurations:
    output_size = calculate_output_size(input_size, kernel_size, stride, padding)
    print(f"{description}")
    print(f"  Input: {input_size}√ó{input_size}")
    print(f"  Kernel: {kernel_size}√ó{kernel_size}, Stride: {stride}, Padding: {padding}")
    print(f"  ‚Üí Output: {output_size}√ó{output_size}\n")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Output Size Calculation Examples ===

Valid (no padding)
  Input: 28√ó28
  Kernel: 3√ó3, Stride: 1, Padding: 0
  ‚Üí Output: 26√ó26

Same (maintain size)
  Input: 28√ó28
  Kernel: 3√ó3, Stride: 1, Padding: 1
  ‚Üí Output: 28√ó28

Stride 2, Padding 2
  Input: 28√ó28
  Kernel: 5√ó5, Stride: 2, Padding: 2
  ‚Üí Output: 14√ó14

32√ó32 image, 3√ó3 kernel
  Input: 32√ó32
  Kernel: 3√ó3, Stride: 1, Padding: 1
  ‚Üí Output: 32√ó32
</code></pre>
<hr/>
<h2>1.3 Feature Maps and Receptive Fields</h2>
<h3>Feature Maps</h3>
<p><strong>Feature maps</strong> are the output results of convolution operations. Each filter detects different features (edges, textures, etc.) and generates respective feature maps.</p>
<ul>
<li>Number of input channels = $C_{in}$</li>
<li>Number of output channels = $C_{out}$ (number of filters)</li>
<li>Size of each filter = $K \times K \times C_{in}$</li>
</ul>
<h4>Multi-Channel Convolution Calculation</h4>
<p>For color images (RGB, 3 channels):</p>

$$
\text{Output}(i, j) = \sum_{c=1}^{3} \sum_{m}\sum_{n} I_c(i+m, j+n) \cdot K_c(m, n) + b
$$

<p>Where $b$ is the bias term.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Where $b$ is the bias term.

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn

# RGB image (batch size 1, 3 channels, 28√ó28)
input_image = torch.randn(1, 3, 28, 28)

# Define convolutional layer
# Input: 3 channels (RGB)
# Output: 16 channels (16 feature maps)
# Kernel size: 3√ó3
conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)

# Forward pass
output = conv_layer(input_image)

print(f"Input size: {input_image.shape}")
print(f"  ‚Üí [Batch, Channel, Height, Width] = [1, 3, 28, 28]")
print(f"\nConvolutional layer parameters:")
print(f"  Input channels: 3")
print(f"  Output channels: 16")
print(f"  Kernel size: 3√ó3")
print(f"  Padding: 1 (Same padding)")
print(f"\nOutput size: {output.shape}")
print(f"  ‚Üí [Batch, Channel, Height, Width] = [1, 16, 28, 28]")

# Calculate parameter count
weight_params = 3 * 16 * 3 * 3  # in_ch √ó out_ch √ó k_h √ó k_w
bias_params = 16  # One per output channel
total_params = weight_params + bias_params

print(f"\nParameter count:")
print(f"  Weights: {weight_params:,} (= 3 √ó 16 √ó 3 √ó 3)")
print(f"  Bias: {bias_params}")
print(f"  Total: {total_params:,}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Input size: torch.Size([1, 3, 28, 28])
  ‚Üí [Batch, Channel, Height, Width] = [1, 3, 28, 28]

Convolutional layer parameters:
  Input channels: 3
  Output channels: 16
  Kernel size: 3√ó3
  Padding: 1 (Same padding)

Output size: torch.Size([1, 16, 28, 28])
  ‚Üí [Batch, Channel, Height, Width] = [1, 16, 28, 28]

Parameter count:
  Weights: 432 (= 3 √ó 16 √ó 3 √ó 3)
  Bias: 16
  Total: 448
</code></pre>
<h3>Receptive Field</h3>
<p>The <strong>receptive field</strong> is the region of the input image that a particular output neuron "sees". In CNNs, the receptive field expands as layers are stacked.</p>
<h4>Receptive Field Size Calculation</h4>
<p>Receptive field size $R$ calculation formula (with stride 1 and padding):</p>

$$
R_l = R_{l-1} + (K_l - 1)
$$

<p>Where:</p>
<ul>
<li>$R_l$: Receptive field size at layer $l$</li>
<li>$K_l$: Kernel size at layer $l$</li>
<li>$R_0 = 1$ (input layer)</li>
</ul>
<pre><code class="language-python">def calculate_receptive_field(layers_info):
    """
    Calculate CNN receptive field size

    Parameters:
    -----------
    layers_info : list of tuples
        List of (kernel_size, stride) for each layer

    Returns:
    --------
    list : Receptive field size for each layer
    """
    receptive_fields = [1]  # Input layer

    for kernel_size, stride in layers_info:
        # Simplified calculation (for stride 1)
        rf = receptive_fields[-1] + (kernel_size - 1)
        receptive_fields.append(rf)

    return receptive_fields

# VGG-style network configuration
vgg_layers = [
    (3, 1),  # Conv1
    (3, 1),  # Conv2
    (2, 2),  # MaxPool
    (3, 1),  # Conv3
    (3, 1),  # Conv4
    (2, 2),  # MaxPool
]

receptive_fields = calculate_receptive_field(vgg_layers)

print("=== Receptive Field Expansion Process ===\n")
print("Layer                Receptive Field Size")
print("-" * 35)
print(f"Input layer            {receptive_fields[0]}√ó{receptive_fields[0]}")

layer_names = ["Conv1 (3√ó3)", "Conv2 (3√ó3)", "MaxPool (2√ó2)",
               "Conv3 (3√ó3)", "Conv4 (3√ó3)", "MaxPool (2√ó2)"]

for i, name in enumerate(layer_names, 1):
    print(f"{name:18}{receptive_fields[i]:2}√ó{receptive_fields[i]:2}")

print(f"\nFinal receptive field: {receptive_fields[-1]}√ó{receptive_fields[-1]} pixels")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Receptive Field Expansion Process ===

Layer                Receptive Field Size
-----------------------------------
Input layer             1√ó1
Conv1 (3√ó3)        3√ó3
Conv2 (3√ó3)        5√ó5
MaxPool (2√ó2)      6√ó6
Conv3 (3√ó3)        8√ó8
Conv4 (3√ó3)       10√ó10
MaxPool (2√ó2)     11√ó11

Final receptive field: 11√ó11 pixels
</code></pre>
<h4>Receptive Field Visualization</h4>
<div class="mermaid">
graph TD
    subgraph "Input Image"
    A1[" "]
    A2[" "]
    A3[" "]
    A4[" "]
    A5[" "]
    end

    subgraph "Conv1: 3√ó3 Kernel"
    B1[Receptive field: 3√ó3]
    end

    subgraph "Conv2: 3√ó3 Kernel"
    C1[Receptive field: 5√ó5]
    end

    subgraph "Conv3: 3√ó3 Kernel"
    D1[Receptive field: 7√ó7]
    end

    A1 --&gt; B1
    A2 --&gt; B1
    A3 --&gt; B1
    B1 --&gt; C1
    C1 --&gt; D1

    style B1 fill:#fff3e0
    style C1 fill:#ffe0b2
    style D1 fill:#ffcc80
</div>
<blockquote>
<p><strong>Important</strong>: Deeper networks have larger receptive fields and can integrate information from wider areas. This is the source of deep learning's powerful feature extraction capability.</p>
</blockquote>
<hr/>
<h2>1.4 Implementing Convolutional Layers in PyTorch</h2>
<h3>Basic Usage of Conv2d</h3>
<p>In PyTorch, we use the <code>torch.nn.Conv2d</code> class to define convolutional layers.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: In PyTorch, we use thetorch.nn.Conv2dclass to define convolu

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn

# Basic Conv2d syntax
conv = nn.Conv2d(
    in_channels=3,      # Number of input channels (3 for RGB)
    out_channels=64,    # Number of output channels (number of filters)
    kernel_size=3,      # Kernel size (3√ó3)
    stride=1,           # Stride
    padding=1,          # Padding
    bias=True           # Whether to use bias term
)

# Dummy input (batch size 8, RGB image, 224√ó224)
x = torch.randn(8, 3, 224, 224)

# Forward pass
output = conv(x)

print("=== Conv2d Operation Verification ===\n")
print(f"Input size: {x.shape}")
print(f"  [Batch, Channel, Height, Width] = [{x.shape[0]}, {x.shape[1]}, {x.shape[2]}, {x.shape[3]}]")
print(f"\nOutput size: {output.shape}")
print(f"  [Batch, Channel, Height, Width] = [{output.shape[0]}, {output.shape[1]}, {output.shape[2]}, {output.shape[3]}]")

# Parameter details
print(f"\nParameter details:")
print(f"  Weight size: {conv.weight.shape}")
print(f"  ‚Üí [Output ch, Input ch, Height, Width] = [{conv.weight.shape[0]}, {conv.weight.shape[1]}, {conv.weight.shape[2]}, {conv.weight.shape[3]}]")
print(f"  Bias size: {conv.bias.shape}")
print(f"  ‚Üí [Output ch] = [{conv.bias.shape[0]}]")

# Parameter count
total_params = conv.weight.numel() + conv.bias.numel()
print(f"\nTotal parameter count: {total_params:,}")
print(f"  Formula: (3 √ó 64 √ó 3 √ó 3) + 64 = {total_params:,}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Conv2d Operation Verification ===

Input size: torch.Size([8, 3, 224, 224])
  [Batch, Channel, Height, Width] = [8, 3, 224, 224]

Output size: torch.Size([8, 64, 224, 224])
  [Batch, Channel, Height, Width] = [8, 64, 224, 224]

Parameter details:
  Weight size: torch.Size([64, 3, 3, 3])
  ‚Üí [Output ch, Input ch, Height, Width] = [64, 3, 3, 3]
  Bias size: torch.Size([64])
  ‚Üí [Output ch] = [64]

Total parameter count: 1,792
  Formula: (3 √ó 64 √ó 3 √ó 3) + 64 = 1,792
</code></pre>
<h3>Parameter Count Calculation Formula</h3>
<p>The parameter count for convolutional layers is calculated by the following formula:</p>

$$
\text{Parameter Count} = (C_{in} \times K_h \times K_w \times C_{out}) + C_{out}
$$

<p>Where:</p>
<ul>
<li>$C_{in}$: Number of input channels</li>
<li>$C_{out}$: Number of output channels (number of filters)</li>
<li>$K_h, K_w$: Kernel height and width</li>
<li>The last $C_{out}$ is for bias terms</li>
</ul>
<pre><code class="language-python">def calculate_conv_params(in_channels, out_channels, kernel_size, bias=True):
    """
    Calculate parameter count for convolutional layer
    """
    if isinstance(kernel_size, int):
        kernel_h = kernel_w = kernel_size
    else:
        kernel_h, kernel_w = kernel_size

    weight_params = in_channels * out_channels * kernel_h * kernel_w
    bias_params = out_channels if bias else 0

    return weight_params + bias_params

# Calculate parameter counts for various configurations
print("=== Convolutional Layer Parameter Count Comparison ===\n")

configs = [
    (3, 32, 3, "Layer 1 (RGB ‚Üí 32 channels)"),
    (32, 64, 3, "Layer 2 (32 ‚Üí 64 channels)"),
    (64, 128, 3, "Layer 3 (64 ‚Üí 128 channels)"),
    (128, 256, 3, "Layer 4 (128 ‚Üí 256 channels)"),
    (3, 64, 7, "Large kernel (7√ó7)"),
    (512, 512, 3, "Deep layer (512 ‚Üí 512 channels)"),
]

for in_ch, out_ch, k_size, description in configs:
    params = calculate_conv_params(in_ch, out_ch, k_size)
    print(f"{description}")
    print(f"  Configuration: {in_ch}ch ‚Üí {out_ch}ch, Kernel{k_size}√ó{k_size}")
    print(f"  Parameter count: {params:,}\n")

# Comparison with fully connected layer
print("=== Comparison with Fully Connected Layer ===\n")
fc_input = 224 * 224 * 3
fc_output = 1000
fc_params = fc_input * fc_output + fc_output

print(f"Fully connected layer (224√ó224√ó3 ‚Üí 1000):")
print(f"  Parameter count: {fc_params:,}")

conv_params = calculate_conv_params(3, 64, 3)
print(f"\nConvolutional layer (3ch ‚Üí 64ch, 3√ó3):")
print(f"  Parameter count: {conv_params:,}")
print(f"\nReduction rate: {(1 - conv_params/fc_params)*100:.2f}%")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Convolutional Layer Parameter Count Comparison ===

Layer 1 (RGB ‚Üí 32 channels)
  Configuration: 3ch ‚Üí 32ch, Kernel3√ó3
  Parameter count: 896

Layer 2 (32 ‚Üí 64 channels)
  Configuration: 32ch ‚Üí 64ch, Kernel3√ó3
  Parameter count: 18,496

Layer 3 (64 ‚Üí 128 channels)
  Configuration: 64ch ‚Üí 128ch, Kernel3√ó3
  Parameter count: 73,856

Layer 4 (128 ‚Üí 256 channels)
  Configuration: 128ch ‚Üí 256ch, Kernel3√ó3
  Parameter count: 295,168

Large kernel (7√ó7)
  Configuration: 3ch ‚Üí 64ch, Kernel7√ó7
  Parameter count: 9,472

Deep layer (512 ‚Üí 512 channels)
  Configuration: 512ch ‚Üí 512ch, Kernel3√ó3
  Parameter count: 2,359,808

=== Comparison with Fully Connected Layer ===

Fully connected layer (224√ó224√ó3 ‚Üí 1000):
  Parameter count: 150,529,000

Convolutional layer (3ch ‚Üí 64ch, 3√ó3):
  Parameter count: 1,792

Reduction rate: 100.00%
</code></pre>
<h3>Convolutional Filter Visualization</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Convolutional Filter Visualization

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import matplotlib.pyplot as plt
import torch.nn as nn

# Define convolutional layer
conv_layer = nn.Conv2d(1, 8, kernel_size=3, padding=1)

# Visualize trained filters (here using random initialization)
filters = conv_layer.weight.data.cpu().numpy()

# Display 8 filters in 2 rows √ó 4 columns
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
axes = axes.flatten()

for i in range(8):
    # filters[i, 0] is the i-th filter (1st channel)
    axes[i].imshow(filters[i, 0], cmap='gray')
    axes[i].set_title(f'Filter {i+1}')
    axes[i].axis('off')

plt.suptitle('Convolutional Filter Visualization (3√ó3 kernel)', fontsize=16)
plt.tight_layout()
print("Visualized filters (random initialization)")
print("After training, filters evolve to have features like edge detection and texture detection")
</code></pre>
<hr/>
<h2>1.5 Activation Function: ReLU</h2>
<h3>Why Activation Functions Are Needed</h3>
<p>Convolution operations are linear transformations. Without activation functions, stacking multiple layers would simply be a combination of linear transformations, unable to learn complex patterns.</p>
<blockquote>
<p>Activation functions introduce <strong>non-linearity</strong>, giving the network the ability to approximate complex functions.</p>
</blockquote>
<h3>ReLU (Rectified Linear Unit)</h3>
<p>The most commonly used activation function in CNNs is <strong>ReLU</strong>.</p>

$$
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \\
0 &amp; \text{if } x \leq 0
\end{cases}
$$

<h4>Advantages of ReLU</h4>
<table>
<thead>
<tr>
<th>Advantage</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Computational Efficiency</strong></td>
<td>Only simple max operation</td>
</tr>
<tr>
<td><strong>Mitigates Vanishing Gradients</strong></td>
<td>Gradient is 1 in positive region (better than Sigmoid or Tanh)</td>
</tr>
<tr>
<td><strong>Sparsity</strong></td>
<td>Setting negative values to 0 creates sparse representations</td>
</tr>
<tr>
<td><strong>Biological Plausibility</strong></td>
<td>Similar to neuron firing patterns</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Advantages of ReLU

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Comparison of various activation functions
x = np.linspace(-3, 3, 100)

# ReLU
relu = np.maximum(0, x)

# Sigmoid
sigmoid = 1 / (1 + np.exp(-x))

# Tanh
tanh = np.tanh(x)

# Leaky ReLU
leaky_relu = np.where(x &gt; 0, x, 0.1 * x)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].plot(x, relu, 'b-', linewidth=2)
axes[0, 0].set_title('ReLU: max(0, x)', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].axhline(y=0, color='k', linewidth=0.5)
axes[0, 0].axvline(x=0, color='k', linewidth=0.5)

axes[0, 1].plot(x, sigmoid, 'r-', linewidth=2)
axes[0, 1].set_title('Sigmoid: 1/(1+exp(-x))', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].axhline(y=0, color='k', linewidth=0.5)
axes[0, 1].axvline(x=0, color='k', linewidth=0.5)

axes[1, 0].plot(x, tanh, 'g-', linewidth=2)
axes[1, 0].set_title('Tanh: tanh(x)', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].axhline(y=0, color='k', linewidth=0.5)
axes[1, 0].axvline(x=0, color='k', linewidth=0.5)

axes[1, 1].plot(x, leaky_relu, 'm-', linewidth=2)
axes[1, 1].set_title('Leaky ReLU: max(0.1x, x)', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].axhline(y=0, color='k', linewidth=0.5)
axes[1, 1].axvline(x=0, color='k', linewidth=0.5)

plt.tight_layout()
print("Compared activation function shapes")

# Usage example in PyTorch
print("\n=== Using Activation Functions in PyTorch ===\n")

x_tensor = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])

relu_layer = nn.ReLU()
print(f"Input: {x_tensor.numpy()}")
print(f"ReLU: {relu_layer(x_tensor).numpy()}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Compared activation function shapes

=== Using Activation Functions in PyTorch ===

Input: [-2. -1.  0.  1.  2.]
ReLU: [0. 0. 0. 1. 2.]
</code></pre>
<h3>Conv + ReLU Pattern</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Conv + ReLU Pattern

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn

# Standard Conv-ReLU block
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ConvBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x

# Usage example
block = ConvBlock(3, 64)
x = torch.randn(1, 3, 224, 224)
output = block(x)

print(f"Input size: {x.shape}")
print(f"Output size: {output.shape}")
print(f"\nProcessing flow:")
print(f"  1. Conv2d(3 ‚Üí 64, 3√ó3) filtering")
print(f"  2. ReLU() non-linear transformation")
print(f"  ‚Üí Negative values in feature map become 0")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Input size: torch.Size([1, 3, 224, 224])
Output size: torch.Size([1, 64, 224, 224])

Processing flow:
  1. Conv2d(3 ‚Üí 64, 3√ó3) filtering
  2. ReLU() non-linear transformation
  ‚Üí Negative values in feature map become 0
</code></pre>
<hr/>
<h2>1.6 Practice: Handwritten Digit Recognition (MNIST)</h2>
<h3>Building a Simple CNN</h3>
<p>We will implement a basic CNN to classify the MNIST dataset (28√ó28 grayscale handwritten digit images).</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - torchvision&gt;=0.15.0

"""
Example: We will implement a basic CNN to classify the MNIST dataset 

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Simple CNN model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # Convolutional layer 1: 1ch ‚Üí 32ch, 3√ó3 kernel
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        # Convolutional layer 2: 32ch ‚Üí 64ch, 3√ó3 kernel
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        # Fully connected layers
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        # Others
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # Conv1 ‚Üí ReLU ‚Üí MaxPool
        x = self.pool(F.relu(self.conv1(x)))  # 28√ó28 ‚Üí 14√ó14
        # Conv2 ‚Üí ReLU ‚Üí MaxPool
        x = self.pool(F.relu(self.conv2(x)))  # 14√ó14 ‚Üí 7√ó7
        # Flatten
        x = x.view(-1, 64 * 7 * 7)
        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Model instantiation
model = SimpleCNN()

# Display model structure
print("=== SimpleCNN Architecture ===\n")
print(model)

# Calculate parameter count
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Parameter count details for each layer
print("\n=== Parameter Count per Layer ===")
for name, param in model.named_parameters():
    print(f"{name:20} {str(list(param.shape)):30} {param.numel():&gt;10,} params")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== SimpleCNN Architecture ===

SimpleCNN(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc1): Linear(in_features=3136, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout(p=0.25, inplace=False)
)

Total parameters: 421,066
Trainable parameters: 421,066

=== Parameter Count per Layer ===
conv1.weight         [32, 1, 3, 3]                         288 params
conv1.bias           [32]                                   32 params
conv2.weight         [64, 32, 3, 3]                     18,432 params
conv2.bias           [64]                                   64 params
fc1.weight           [128, 3136]                       401,408 params
fc1.bias             [128]                                 128 params
fc2.weight           [10, 128]                           1,280 params
fc2.bias             [10]                                   10 params
</code></pre>
<h3>Data Preparation and Training</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Data Preparation and Training

Purpose: Demonstrate optimization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch.optim as optim

# Data preprocessing
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std
])

# Load datasets (download only on first run)
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# Training setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleCNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training function
def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

    avg_loss = running_loss / len(train_loader)
    accuracy = 100. * correct / total
    return avg_loss, accuracy

# Evaluation function
def evaluate(model, test_loader, criterion, device):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    return test_loss, accuracy

# Execute training (simplified version: 3 epochs)
print("\n=== Training Started ===\n")
num_epochs = 3

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"  Test Loss:  {test_loss:.4f}, Test Acc:  {test_acc:.2f}%\n")

print("Training complete!")
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>=== Training Started ===

Epoch 1/3
  Train Loss: 0.2145, Train Acc: 93.52%
  Test Loss:  0.0789, Test Acc:  97.56%

Epoch 2/3
  Train Loss: 0.0701, Train Acc: 97.89%
  Test Loss:  0.0512, Test Acc:  98.34%

Epoch 3/3
  Train Loss: 0.0512, Train Acc: 98.42%
  Test Loss:  0.0401, Test Acc:  98.67%

Training complete!
</code></pre>
<h3>Visualizing Trained Filters</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Visualizing Trained Filters

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import matplotlib.pyplot as plt
import numpy as np

# Visualize first layer convolutional filters
conv1_weights = model.conv1.weight.data.cpu().numpy()

# Display first 16 filters
fig, axes = plt.subplots(4, 8, figsize=(16, 8))
axes = axes.flatten()

for i in range(min(32, len(axes))):
    axes[i].imshow(conv1_weights[i, 0], cmap='viridis')
    axes[i].set_title(f'Filter {i+1}', fontsize=9)
    axes[i].axis('off')

plt.suptitle('Trained Convolutional Filters (Layer 1, 32 out of 32)', fontsize=16)
plt.tight_layout()
print("Visualized trained filters")
print("Each filter has learned to detect different features such as edges, curves, and corners")
</code></pre>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the fundamentals of CNNs and convolutional layers.</p>
<h3>Key Points</h3>
<p><strong>1. Local connectivity and weight sharing</strong> ‚Äî Enable CNNs to drastically reduce parameter count compared to fully connected networks.</p>
<p><strong>2. Convolution operations</strong> ‚Äî Extract features by sliding filters across images, detecting edges, textures, and patterns.</p>
<p><strong>3. Stride and padding</strong> ‚Äî Control output size and spatial dimension management.</p>
<p><strong>4. Receptive field</strong> ‚Äî Expands with each layer, integrating information from progressively wider areas.</p>
<p><strong>5. ReLU activation function</strong> ‚Äî Introduces non-linearity, enabling the network to learn complex patterns.</p>
<h3>Preview of Next Chapter</h3>
<p>Chapter 2 will cover pooling layers (MaxPooling and AveragePooling), Batch Normalization techniques, regularization with Dropout, and representative CNN architectures including VGG and ResNet.</p>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Exercise 1: Output Size Calculation</strong></summary>
<p><strong>Problem</strong>: Calculate the output size for the following convolutional layer.</p>
<ul>
<li>Input: 64√ó64√ó3</li>
<li>Kernel: 5√ó5</li>
<li>Stride: 2</li>
<li>Padding: 2</li>
<li>Output channels: 128</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code># Calculate output size
output_h = (64 + 2*2 - 5) // 2 + 1 = 32
output_w = (64 + 2*2 - 5) // 2 + 1 = 32

# Answer: 32√ó32√ó128
</code></pre>
</details>
<details>
<summary><strong>Exercise 2: Parameter Count Calculation</strong></summary>
<p><strong>Problem</strong>: Calculate the parameter count for the following CNN.</p>
<ul>
<li>Conv1: 3ch ‚Üí 64ch, 7√ó7 kernel</li>
<li>Conv2: 64ch ‚Üí 128ch, 3√ó3 kernel</li>
<li>Conv3: 128ch ‚Üí 256ch, 3√ó3 kernel</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code># Conv1 parameters
conv1_params = (3 * 64 * 7 * 7) + 64 = 9,472

# Conv2 parameters
conv2_params = (64 * 128 * 3 * 3) + 128 = 73,856

# Conv3 parameters
conv3_params = (128 * 256 * 3 * 3) + 256 = 295,168

# Total
total_params = 9,472 + 73,856 + 295,168 = 378,496
</code></pre>
</details>
<details>
<summary><strong>Exercise 3: Receptive Field Calculation</strong></summary>
<p><strong>Problem</strong>: Calculate the final receptive field size for a CNN with the following configuration (all with stride 1 and padding).</p>
<ul>
<li>Conv1: 3√ó3 kernel</li>
<li>Conv2: 3√ó3 kernel</li>
<li>Conv3: 3√ó3 kernel</li>
<li>Conv4: 3√ó3 kernel</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code># Receptive field calculation
# R_0 = 1 (input)
# R_1 = 1 + (3-1) = 3
# R_2 = 3 + (3-1) = 5
# R_3 = 5 + (3-1) = 7
# R_4 = 7 + (3-1) = 9

# Answer: 9√ó9 pixels
</code></pre>
</details>
<details>
<summary><strong>Exercise 4: Custom CNN Implementation</strong></summary>
<p><strong>Problem</strong>: Implement a CNN in PyTorch with the following specifications.</p>
<ul>
<li>Input: 32√ó32√ó3 (CIFAR-10 format)</li>
<li>Conv1: 32 filters, 3√ó3 kernel, ReLU</li>
<li>MaxPool: 2√ó2</li>
<li>Conv2: 64 filters, 3√ó3 kernel, ReLU</li>
<li>MaxPool: 2√ó2</li>
<li>Fully connected: 10 class classification</li>
</ul>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch.nn as nn
import torch.nn.functional as F

class CustomCNN(nn.Module):
    def __init__(self):
        super(CustomCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(64 * 8 * 8, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # 32√ó32 ‚Üí 16√ó16
        x = self.pool(F.relu(self.conv2(x)))  # 16√ó16 ‚Üí 8√ó8
        x = x.view(-1, 64 * 8 * 8)
        x = self.fc(x)
        return x
</code></pre>
</details>
<details>
<summary><strong>Exercise 5: Comparison of Fully Connected Layers and CNNs</strong></summary>
<p><strong>Problem</strong>: For a 224√ó224√ó3 image input, compare parameter counts for the following two approaches.</p>
<ul>
<li>Approach 1: Fully connected layer (input ‚Üí 1000 units)</li>
<li>Approach 2: 3 Conv layers (3ch‚Üí64ch, 3√ó3 kernel)</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code># Approach 1: Fully connected layer
fc_params = (224 * 224 * 3 * 1000) + 1000 = 150,529,000

# Approach 2: CNN (3 layers)
conv1_params = (3 * 64 * 3 * 3) + 64 = 1,792
conv2_params = (64 * 64 * 3 * 3) + 64 = 36,928
conv3_params = (64 * 64 * 3 * 3) + 64 = 36,928
cnn_total = 1,792 + 36,928 + 36,928 = 75,648

# Reduction rate
reduction = (1 - 75,648/150,529,000) * 100 = 99.95%

# CNNs require only 0.05% of the parameters of fully connected layers!
</code></pre>
</details>
<hr/>
<div class="navigation">
<a class="nav-button" href="../index.html">üìö Return to Course Index</a>
<a class="nav-button" href="chapter2-architectures.html">Next Chapter: CNN Architectures ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
