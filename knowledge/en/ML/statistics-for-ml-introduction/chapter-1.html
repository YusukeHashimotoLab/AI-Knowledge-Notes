<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta content="Chapter 1: Descriptive Statistics and Probability Basics - AI Terakoya" name="description"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 1: Descriptive Statistics and Probability Basics - AI Terakoya</title>

  <style>
    :root {
      --color-primary: #2c3e50;
      --color-primary-dark: #1a252f;
      --color-accent: #7b2cbf;
      --color-accent-light: #9d4edd;
      --color-text: #2d3748;
      --color-text-light: #4a5568;
      --color-bg: #ffffff;
      --color-bg-alt: #f7fafc;
      --color-border: #e2e8f0;
      --color-code-bg: #f8f9fa;
      --color-link: #3182ce;
      --color-link-hover: #2c5aa0;

      --spacing-xs: 0.5rem;
      --spacing-sm: 1rem;
      --spacing-md: 1.5rem;
      --spacing-lg: 2rem;
      --spacing-xl: 3rem;

      --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

      --border-radius: 8px;
      --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: var(--font-body);
      line-height: 1.7;
      color: var(--color-text);
      background-color: var(--color-bg);
      font-size: 16px;
    }

    header {
      background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
      color: white;
      padding: var(--spacing-xl) var(--spacing-md);
      margin-bottom: var(--spacing-xl);
      box-shadow: var(--box-shadow);
    }

    .header-content {
      max-width: 900px;
      margin: 0 auto;
    }

    h1 {
      font-size: 2rem;
      font-weight: 700;
      margin-bottom: var(--spacing-sm);
      line-height: 1.2;
    }

    .subtitle {
      font-size: 1.1rem;
      opacity: 0.95;
      font-weight: 400;
      margin-bottom: var(--spacing-md);
    }

    .meta {
      display: flex;
      flex-wrap: wrap;
      gap: var(--spacing-md);
      font-size: 0.9rem;
      opacity: 0.9;
    }

    .meta-item {
      display: flex;
      align-items: center;
      gap: 0.3rem;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 0 var(--spacing-md) var(--spacing-xl);
    }

    h2 {
      font-size: 1.75rem;
      color: var(--color-primary);
      margin-top: var(--spacing-xl);
      margin-bottom: var(--spacing-md);
      padding-bottom: var(--spacing-xs);
      border-bottom: 3px solid var(--color-accent);
    }

    h3 {
      font-size: 1.4rem;
      color: var(--color-primary);
      margin-top: var(--spacing-lg);
      margin-bottom: var(--spacing-sm);
    }

    h4 {
      font-size: 1.1rem;
      color: var(--color-primary-dark);
      margin-top: var(--spacing-md);
      margin-bottom: var(--spacing-sm);
    }

    p {
      margin-bottom: var(--spacing-md);
      color: var(--color-text);
    }

    a {
      color: var(--color-link);
      text-decoration: none;
      transition: color 0.2s;
    }

    a:hover {
      color: var(--color-link-hover);
      text-decoration: underline;
    }

    ul, ol {
      margin-left: var(--spacing-lg);
      margin-bottom: var(--spacing-md);
    }

    li {
      margin-bottom: var(--spacing-xs);
      color: var(--color-text);
    }

    pre {
      background-color: var(--color-code-bg);
      border: 1px solid var(--color-border);
      border-radius: var(--border-radius);
      padding: var(--spacing-md);
      overflow-x: auto;
      margin-bottom: var(--spacing-md);
    }

    code {
      font-family: var(--font-mono);
      font-size: 0.9em;
    }

    pre code {
      background: none;
      padding: 0;
    }

    p code {
      background-color: var(--color-code-bg);
      padding: 0.2em 0.4em;
      border-radius: 3px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: var(--spacing-md);
    }

    th, td {
      padding: var(--spacing-sm);
      text-align: left;
      border: 1px solid var(--color-border);
    }

    th {
      background-color: var(--color-primary);
      color: white;
      font-weight: 600;
    }

    tr:nth-child(even) {
      background-color: var(--color-bg-alt);
    }

    blockquote {
      border-left: 4px solid var(--color-accent);
      padding-left: var(--spacing-md);
      margin: var(--spacing-md) 0;
      font-style: italic;
      color: var(--color-text-light);
    }

    .info-box {
      background-color: #e3f2fd;
      border-left: 4px solid #2196f3;
      padding: var(--spacing-md);
      margin-bottom: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    .warning-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: var(--spacing-md);
      margin-bottom: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    .example-box {
      background-color: #f3e5f5;
      border-left: 4px solid #9c27b0;
      padding: var(--spacing-md);
      margin-bottom: var(--spacing-md);
      border-radius: var(--border-radius);
    }

    .nav-buttons {
      display: flex;
      justify-content: space-between;
      margin-top: var(--spacing-xl);
      gap: var(--spacing-sm);
    }

    .nav-button {
      padding: var(--spacing-sm) var(--spacing-md);
      background-color: var(--color-accent);
      color: white;
      border-radius: var(--border-radius);
      text-decoration: none;
      transition: all 0.3s;
      font-weight: 600;
    }

    .nav-button:hover {
      background-color: var(--color-accent-light);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(123, 44, 191, 0.3);
    }

    .breadcrumb {
      background: #f7fafc;
      padding: 0.75rem 1rem;
      border-bottom: 1px solid #e2e8f0;
      font-size: 0.9rem;
    }

    .breadcrumb-content {
      max-width: 900px;
      margin: 0 auto;
      display: flex;
      align-items: center;
      flex-wrap: wrap;
      gap: 0.5rem;
    }

    .breadcrumb a {
      color: #667eea;
      text-decoration: none;
    }

    .breadcrumb a:hover {
      color: #764ba2;
      text-decoration: underline;
    }

    .breadcrumb-separator {
      color: #a0aec0;
    }

    .breadcrumb-current {
      color: #4a5568;
      font-weight: 500;
    }

    .locale-switcher {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.5rem 1rem;
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      border-radius: 6px;
      font-size: 0.9rem;
      margin-bottom: 1rem;
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
    }

    .current-locale {
      font-weight: 600;
      color: #7b2cbf;
    }

    .locale-separator {
      color: #adb5bd;
    }

    .locale-link {
      color: #f093fb;
      text-decoration: none;
      font-weight: 500;
      padding: 0.2rem 0.5rem;
      border-radius: 4px;
    }

    .locale-link:hover {
      background: rgba(240, 147, 251, 0.1);
      color: #d07be8;
    }

    footer {
      margin-top: var(--spacing-xl);
      padding: var(--spacing-lg) 0;
      border-top: 2px solid var(--color-border);
      text-align: center;
      color: var(--color-text-light);
    }

    @media (max-width: 768px) {
      h1 {
        font-size: 1.6rem;
      }

      h2 {
        font-size: 1.4rem;
      }

      .container {
        padding: 0 var(--spacing-sm) var(--spacing-lg);
      }

      pre {
        padding: var(--spacing-sm);
        font-size: 0.85rem;
      }
    }
  </style>

  <!-- MathJax for mathematical formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      }
    };
  </script>
</head>
<body>
  <div class="locale-switcher">
    <span class="current-locale">üåê EN</span>
    <span class="locale-separator">|</span>
    <a href="../../../jp/ML/statistics-for-ml-introduction/chapter-1.html" class="locale-link">üáØüáµ JP</a>
  </div>

  <nav class="breadcrumb">
    <div class="breadcrumb-content">
      <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span>
      <a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span>
      <a href="./index.html">Statistics Introduction</a><span class="breadcrumb-separator">‚Ä∫</span>
      <span class="breadcrumb-current">Chapter 1</span>
    </div>
  </nav>

  <header>
    <div class="header-content">
      <h1>Chapter 1: Descriptive Statistics and Probability Basics</h1>
      <p class="subtitle">Capturing Data Characteristics and Quantifying Uncertainty</p>
      <div class="meta">
        <span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
        <span class="meta-item">üìä Difficulty: Beginner</span>
        <span class="meta-item">üíª Code Examples: 8</span>
      </div>
    </div>
  </header>

  <main class="container">
    <h2>Introduction</h2>

    <p>Statistics is the discipline of extracting meaningful information from data and making rational decisions under uncertainty. In machine learning, statistical knowledge is essential for understanding data characteristics, evaluating model performance, and quantifying prediction uncertainty.</p>

    <p>In this chapter, we will learn about <strong>descriptive statistics</strong> and <strong>probability theory</strong>, which form the foundation of statistics. In descriptive statistics, we will learn how to express the central tendency (mean, median) and spread (variance, standard deviation) of data numerically. In probability theory, we will master how to mathematically handle uncertain events.</p>

    <div class="info-box">
      <strong>üí° What You'll Learn in This Chapter</strong>
      <ul>
        <li>Summarizing data characteristics with numerical indicators (mean, variance, quartiles, etc.)</li>
        <li>Visualizing data with appropriate graphs (histograms, box plots, etc.)</li>
        <li>Basic probability calculations and conditional probability</li>
        <li>Understanding and applying Bayes' theorem</li>
        <li>Mathematical definitions of expected value and variance</li>
      </ul>
    </div>

    <h2>1. Fundamentals of Descriptive Statistics</h2>

    <p>Descriptive Statistics is a method for summarizing and expressing data characteristics in an easily understandable form. By representing large amounts of data with a few numerical indicators, we can grasp the overall picture of the data.</p>

    <h3>1.1 Measures of Central Tendency</h3>

    <p>These are indicators that show where the "center" of the data is.</p>

    <h4>Mean</h4>

    <p>The sum of all data values divided by the number of data points, the most basic measure of central tendency.</p>

    <p>Mathematical expression:</p>
    <p>$$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$$</p>

    <p>Where $n$ is the number of data points and $x_i$ is the $i$-th data value.</p>

    <h4>Median</h4>

    <p>The middle value when data is arranged in ascending order. A robust measure that is less affected by outliers.</p>

    <h4>Mode</h4>

    <p>The value that appears most frequently in the data. Can also be applied to categorical data.</p>

    <div class="example-box">
      <strong>üìù Example: Student Test Scores</strong>
      <p>Test scores of 5 students: 65, 70, 75, 80, 95 points</p>
      <ul>
        <li>Mean: $(65+70+75+80+95)/5 = 77$ points</li>
        <li>Median: 75 points (middle value)</li>
      </ul>
      <p>If an extreme value (e.g., 10 points) is included, the mean changes significantly, but the median remains relatively stable.</p>
    </div>

    <h3>1.2 Measures of Spread</h3>

    <p>These are indicators that show how much the data is scattered.</p>

    <h4>Variance</h4>

    <p>The average of the squared differences between each data value and the mean.</p>

    <p>$$\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2$$</p>

    <div class="warning-box">
      <strong>‚ö†Ô∏è Note: Sample Variance vs Population Variance</strong>
      <p>When estimating population variance from a sample, divide by $n-1$ instead of $n$ (unbiased estimator):</p>
      <p>$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$</p>
    </div>

    <h4>Standard Deviation</h4>

    <p>The square root of variance. Can express spread in the same units as the original data.</p>

    <p>$$\sigma = \sqrt{\sigma^2}$$</p>

    <h4>Quartiles and Percentiles</h4>

    <p>Indicators that show position when data is ordered.</p>

    <ul>
      <li><strong>First Quartile (Q1)</strong>: Position at 25% of the data</li>
      <li><strong>Second Quartile (Q2)</strong>: Median (50% position)</li>
      <li><strong>Third Quartile (Q3)</strong>: Position at 75% of the data</li>
      <li><strong>Interquartile Range (IQR)</strong>: $Q3 - Q1$ (range of the middle 50% of data)</li>
    </ul>

    <h3>1.3 Python Implementation</h3>

    <p>Let's calculate descriptive statistics using NumPy and SciPy.</p>

    <pre><code>import numpy as np
from scipy import stats

# Sample data: Student test scores
scores = np.array([65, 70, 72, 75, 78, 80, 82, 85, 88, 95, 98])

# Measures of central tendency
mean = np.mean(scores)
median = np.median(scores)
mode_result = stats.mode(scores, keepdims=True)
mode = mode_result.mode[0] if len(mode_result.mode) > 0 else None

print(f"Mean: {mean:.2f}")
print(f"Median: {median:.2f}")
print(f"Mode: {mode}")

# Measures of spread
variance = np.var(scores)  # Population variance
std_dev = np.std(scores)   # Population standard deviation
sample_variance = np.var(scores, ddof=1)  # Sample variance (unbiased estimator)
sample_std = np.std(scores, ddof=1)       # Sample standard deviation

print(f"\nPopulation Variance: {variance:.2f}")
print(f"Population Standard Deviation: {std_dev:.2f}")
print(f"Sample Variance: {sample_variance:.2f}")
print(f"Sample Standard Deviation: {sample_std:.2f}")

# Quartiles
q1 = np.percentile(scores, 25)
q2 = np.percentile(scores, 50)  # Median
q3 = np.percentile(scores, 75)
iqr = q3 - q1

print(f"\nFirst Quartile (Q1): {q1:.2f}")
print(f"Second Quartile (Q2): {q2:.2f}")
print(f"Third Quartile (Q3): {q3:.2f}")
print(f"Interquartile Range (IQR): {iqr:.2f}")</code></pre>

    <p><strong>Execution Result:</strong></p>
    <pre><code>Mean: 80.73
Median: 80.00
Mode: 65

Population Variance: 103.29
Population Standard Deviation: 10.16
Sample Variance: 113.62
Sample Standard Deviation: 10.66

First Quartile (Q1): 73.50
Second Quartile (Q2): 80.00
Third Quartile (Q3): 88.00
Interquartile Range (IQR): 14.50</code></pre>

    <h2>2. Data Visualization</h2>

    <p>Not only numerical indicators but also visualization through graphs is essential for understanding data.</p>

    <h3>2.1 Histogram</h3>

    <p>A graph that visually represents the distribution of data. Data is divided into classes (bins), and the frequency of each class is displayed as a bar graph.</p>

    <pre><code>import matplotlib.pyplot as plt
import numpy as np

# Generate sample data following a normal distribution
np.random.seed(42)
data = np.random.normal(loc=70, scale=10, size=1000)

# Draw histogram
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
plt.axvline(np.mean(data), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(data):.2f}')
plt.axvline(np.median(data), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(data):.2f}')
plt.xlabel('Value', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Histogram: Data Distribution', fontsize=14)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.show()</code></pre>

    <h3>2.2 Box Plot</h3>

    <p>A graph that visually represents quartiles and can also identify outliers.</p>

    <pre><code>import matplotlib.pyplot as plt
import numpy as np

# Multiple group data
np.random.seed(42)
group_a = np.random.normal(70, 10, 100)
group_b = np.random.normal(75, 8, 100)
group_c = np.random.normal(65, 12, 100)

data_groups = [group_a, group_b, group_c]

# Draw box plot
plt.figure(figsize=(10, 6))
bp = plt.boxplot(data_groups, labels=['Group A', 'Group B', 'Group C'],
                 patch_artist=True, notch=True)

# Customize colors
colors = ['lightblue', 'lightgreen', 'lightcoral']
for patch, color in zip(bp['boxes'], colors):
    patch.set_facecolor(color)

plt.ylabel('Score', fontsize=12)
plt.title('Box Plot: Comparison Between Groups', fontsize=14)
plt.grid(axis='y', alpha=0.3)
plt.show()</code></pre>

    <div class="info-box">
      <strong>üí° How to Read a Box Plot</strong>
      <ul>
        <li>Bottom of box: First quartile (Q1)</li>
        <li>Line in box: Median (Q2)</li>
        <li>Top of box: Third quartile (Q3)</li>
        <li>Whiskers: Data range (excluding outliers)</li>
        <li>Dots: Outliers</li>
      </ul>
    </div>

    <h3>2.3 Scatter Plot</h3>

    <p>A graph that visualizes the relationship between two variables.</p>

    <pre><code>import matplotlib.pyplot as plt
import numpy as np

# Generate correlated data
np.random.seed(42)
x = np.random.normal(50, 10, 100)
y = 2 * x + np.random.normal(0, 10, 100)  # y has positive correlation with x

# Draw scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(x, y, alpha=0.6, edgecolors='black', s=50)
plt.xlabel('Variable X', fontsize=12)
plt.ylabel('Variable Y', fontsize=12)
plt.title('Scatter Plot: Relationship Between Two Variables', fontsize=14)
plt.grid(alpha=0.3)

# Calculate and display correlation coefficient
correlation = np.corrcoef(x, y)[0, 1]
plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}',
         transform=plt.gca().transAxes, fontsize=12,
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.show()</code></pre>

    <h2>3. Probability Basics</h2>

    <p>Probability theory is a framework for mathematically handling uncertain events. In machine learning, probability theory is used to model data generation processes and quantify prediction uncertainty.</p>

    <h3>3.1 Definition and Axioms of Probability</h3>

    <p><strong>Probability</strong> is a numerical value from 0 to 1 that represents the likelihood of an event occurring.</p>

    <p><strong>Kolmogorov's Axioms</strong> (basic properties of probability):</p>
    <ol>
      <li><strong>Non-negativity</strong>: For all events $A$, $P(A) \geq 0$</li>
      <li><strong>Total Probability</strong>: The probability of the entire event space is 1, $P(\Omega) = 1$</li>
      <li><strong>Additivity</strong>: For mutually exclusive events $A$ and $B$, $P(A \cup B) = P(A) + P(B)$</li>
    </ol>

    <h4>Basic Probability Calculations</h4>

    <ul>
      <li><strong>Complement</strong>: $P(A^c) = 1 - P(A)$</li>
      <li><strong>Union</strong>: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$</li>
    </ul>

    <h3>3.2 Conditional Probability</h3>

    <p>The probability that event $A$ occurs given that event $B$ has occurred is called <strong>conditional probability</strong>, denoted as $P(A|B)$.</p>

    <p>$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$</p>

    <p>Where $P(B) > 0$.</p>

    <div class="example-box">
      <strong>üìù Example: Card Drawing</strong>
      <p>When drawing one card from a 52-card deck:</p>
      <ul>
        <li>Probability that the drawn card is a spade: $P(\text{Spade}) = 13/52 = 1/4$</li>
        <li>Probability that the drawn card is a face card (J, Q, K): $P(\text{Face card}) = 12/52 = 3/13$</li>
        <li>Probability that it is a face card given that it is a spade: $P(\text{Face card}|\text{Spade}) = 3/13$</li>
      </ul>
    </div>

    <h3>3.3 Bayes' Theorem</h3>

    <p><strong>Bayes' theorem</strong> is an important formula for reversing conditional probability. It plays a central role in machine learning, especially in Bayesian statistics.</p>

    <p>$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$</p>

    <p>Meaning of each term:</p>
    <ul>
      <li>$P(A|B)$: <strong>Posterior probability</strong> - Probability of $A$ after observing event $B$</li>
      <li>$P(B|A)$: <strong>Likelihood</strong> - Probability of observing $B$ when $A$ is true</li>
      <li>$P(A)$: <strong>Prior probability</strong> - Probability of $A$ before observation</li>
      <li>$P(B)$: <strong>Marginal probability (Evidence)</strong> - Overall probability of observing $B$</li>
    </ul>

    <h4>Application of Bayes' Theorem: Medical Diagnosis</h4>

    <div class="example-box">
      <strong>üìù Example: Disease Testing</strong>
      <p>For a rare disease:</p>
      <ul>
        <li>1% of the population has this disease: $P(\text{Disease}) = 0.01$</li>
        <li>Test sensitivity (correctly identifies diseased as positive): $P(\text{Positive}|\text{Disease}) = 0.99$</li>
        <li>Test false positive rate: $P(\text{Positive}|\text{Healthy}) = 0.05$</li>
      </ul>
      <p>If the test comes back positive, what is the probability of actually having the disease?</p>
    </div>

    <p>Applying Bayes' theorem:</p>

    <p>$$P(\text{Disease}|\text{Positive}) = \frac{P(\text{Positive}|\text{Disease}) \cdot P(\text{Disease})}{P(\text{Positive})}$$</p>

    <p>First, calculate $P(\text{Positive})$ (law of total probability):</p>

    <p>$$P(\text{Positive}) = P(\text{Positive}|\text{Disease})P(\text{Disease}) + P(\text{Positive}|\text{Healthy})P(\text{Healthy})$$</p>
    <p>$$= 0.99 \times 0.01 + 0.05 \times 0.99 = 0.0099 + 0.0495 = 0.0594$$</p>

    <p>Therefore:</p>

    <p>$$P(\text{Disease}|\text{Positive}) = \frac{0.99 \times 0.01}{0.0594} \approx 0.167$$</p>

    <p>In other words, even if the test is positive, the probability of actually having the disease is only about 16.7%. This is due to the low prevalence of the disease.</p>

    <h4>Python Implementation</h4>

    <pre><code>import numpy as np

def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):
    """
    Calculate Bayes' theorem

    Parameters:
    -----------
    p_a : float
        Prior probability P(A)
    p_b_given_a : float
        Likelihood P(B|A)
    p_b_given_not_a : float
        P(B|not A)

    Returns:
    --------
    float
        Posterior probability P(A|B)
    """
    # Calculate P(B) using law of total probability
    p_not_a = 1 - p_a
    p_b = p_b_given_a * p_a + p_b_given_not_a * p_not_a

    # Calculate P(A|B) using Bayes' theorem
    p_a_given_b = (p_b_given_a * p_a) / p_b

    return p_a_given_b, p_b

# Medical diagnosis example
p_disease = 0.01  # Prior probability of disease (prevalence)
p_positive_given_disease = 0.99  # Sensitivity (true positive rate)
p_positive_given_healthy = 0.05  # False positive rate

p_disease_given_positive, p_positive = bayes_theorem(
    p_disease,
    p_positive_given_disease,
    p_positive_given_healthy
)

print("=== Bayes' Theorem in Medical Diagnosis ===")
print(f"Disease prevalence: {p_disease * 100:.1f}%")
print(f"Test sensitivity: {p_positive_given_disease * 100:.1f}%")
print(f"False positive rate: {p_positive_given_healthy * 100:.1f}%")
print(f"\nProbability of positive result: {p_positive * 100:.2f}%")
print(f"Probability of actually having disease when positive: {p_disease_given_positive * 100:.2f}%")

# Compare with varying sensitivity
print("\n=== Comparison with Varying Sensitivity ===")
sensitivities = [0.90, 0.95, 0.99, 0.999]
for sens in sensitivities:
    prob, _ = bayes_theorem(p_disease, sens, p_positive_given_healthy)
    print(f"Sensitivity {sens*100:.1f}%: Disease probability when positive = {prob*100:.2f}%")</code></pre>

    <p><strong>Execution Result:</strong></p>
    <pre><code>=== Bayes' Theorem in Medical Diagnosis ===
Disease prevalence: 1.0%
Test sensitivity: 99.0%
False positive rate: 5.0%

Probability of positive result: 5.94%
Probability of actually having disease when positive: 16.64%

=== Comparison with Varying Sensitivity ===
Sensitivity 90.0%: Disease probability when positive = 15.38%
Sensitivity 95.0%: Disease probability when positive = 16.10%
Sensitivity 99.0%: Disease probability when positive = 16.64%
Sensitivity 99.9%: Disease probability when positive = 16.72%</code></pre>

    <h2>4. Expected Value and Variance</h2>

    <p>These are important concepts for expressing characteristics of random variables numerically.</p>

    <h3>4.1 Expected Value</h3>

    <p>The <strong>expected value</strong> represents the average value of a random variable.</p>

    <p>For <strong>discrete random variables</strong>:</p>
    <p>$$E[X] = \sum_{i} x_i \cdot P(X = x_i)$$</p>

    <p>For <strong>continuous random variables</strong>:</p>
    <p>$$E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx$$</p>

    <p>Where $f(x)$ is the probability density function.</p>

    <h4>Properties of Expected Value</h4>

    <ul>
      <li><strong>Linearity</strong>: $E[aX + b] = aE[X] + b$</li>
      <li><strong>Additivity</strong>: $E[X + Y] = E[X] + E[Y]$</li>
    </ul>

    <h3>4.2 Variance and Standard Deviation</h3>

    <p><strong>Variance</strong> represents how much a random variable is scattered from its expected value.</p>

    <p>$$\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$</p>

    <p><strong>Standard deviation</strong> is the square root of variance:</p>
    <p>$$\sigma = \sqrt{\text{Var}(X)}$$</p>

    <h4>Properties of Variance</h4>

    <ul>
      <li>$\text{Var}(aX + b) = a^2 \text{Var}(X)$</li>
      <li>If $X$ and $Y$ are independent: $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$</li>
    </ul>

    <h3>4.3 Python Implementation</h3>

    <pre><code>import numpy as np
import matplotlib.pyplot as plt

# Example of dice roll experiment
def dice_expectation():
    """Calculate expected value of dice roll"""
    outcomes = np.array([1, 2, 3, 4, 5, 6])
    probabilities = np.array([1/6] * 6)

    # Calculate expected value
    expectation = np.sum(outcomes * probabilities)

    # Calculate variance
    variance = np.sum((outcomes - expectation)**2 * probabilities)
    std_dev = np.sqrt(variance)

    print("=== Expected Value and Variance of Dice Roll ===")
    print(f"Expected value E[X]: {expectation:.4f}")
    print(f"Variance Var(X): {variance:.4f}")
    print(f"Standard deviation œÉ: {std_dev:.4f}")

    # Visualization
    plt.figure(figsize=(10, 6))
    plt.bar(outcomes, probabilities, edgecolor='black', alpha=0.7, color='skyblue')
    plt.axvline(expectation, color='red', linestyle='--', linewidth=2,
                label=f'Expected value: {expectation:.2f}')
    plt.xlabel('Outcome', fontsize=12)
    plt.ylabel('Probability', fontsize=12)
    plt.title('Probability Distribution of Dice Roll', fontsize=14)
    plt.legend()
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(outcomes)
    plt.show()

dice_expectation()

# Verification by simulation
print("\n=== Verification by Simulation ===")
n_trials = 10000
dice_rolls = np.random.randint(1, 7, n_trials)

empirical_mean = np.mean(dice_rolls)
empirical_variance = np.var(dice_rolls)
empirical_std = np.std(dice_rolls)

print(f"Number of simulations: {n_trials}")
print(f"Empirical mean: {empirical_mean:.4f}")
print(f"Empirical variance: {empirical_variance:.4f}")
print(f"Empirical standard deviation: {empirical_std:.4f}")
print(f"\nDifference from theoretical values:")
print(f"Difference in mean: {abs(empirical_mean - 3.5):.4f}")
print(f"Difference in variance: {abs(empirical_variance - 35/12):.4f}")</code></pre>

    <h2>5. Summary and Next Steps</h2>

    <p>In this chapter, we learned the basics of descriptive statistics and probability theory, which form the foundation of statistics.</p>

    <div class="info-box">
      <strong>‚úÖ What We Learned in This Chapter</strong>
      <ul>
        <li>Measures of central tendency (mean, median, mode) and spread (variance, standard deviation)</li>
        <li>Data visualization with histograms, box plots, and scatter plots</li>
        <li>Basic axioms of probability and conditional probability</li>
        <li>Theory and applications of Bayes' theorem</li>
        <li>Mathematical definitions and calculations of expected value and variance</li>
        <li>Implementation of statistical analysis using NumPy/SciPy/Matplotlib</li>
      </ul>
    </div>

    <div class="warning-box">
      <strong>üîë Key Points</strong>
      <ul>
        <li>Mean is susceptible to outliers, but median is robust</li>
        <li>When calculating sample variance, divide by $n-1$ (unbiased estimator)</li>
        <li>In Bayes' theorem, the prior probability greatly influences the posterior probability</li>
        <li>Expected value has linearity, and variance is squared with respect to linear transformations</li>
      </ul>
    </div>

    <h3>Next Steps</h3>

    <p>In the next chapter, we will learn about probability distributions. We will master the properties and applications of probability distributions frequently used in machine learning, such as normal distribution, binomial distribution, and Poisson distribution.</p>

    <div class="nav-buttons">
      <a href="./index.html" class="nav-button">‚Üê Back to Series Top</a>
      <span class="nav-button disabled">Chapter 2: Probability Distributions (Coming Soon) ‚Üí</span>
    </div>

    <h2>Practice Problems</h2>

    <details>
      <summary><strong>Problem 1: Calculating Descriptive Statistics</strong></summary>
      <p>Calculate the mean, median, variance, and standard deviation for the following dataset.</p>
      <p>Data: 12, 15, 18, 20, 22, 25, 28, 30, 35, 40</p>
      <pre><code>import numpy as np

data = np.array([12, 15, 18, 20, 22, 25, 28, 30, 35, 40])

mean = np.mean(data)
median = np.median(data)
variance = np.var(data, ddof=1)
std_dev = np.std(data, ddof=1)

print(f"Mean: {mean}")
print(f"Median: {median}")
print(f"Variance: {variance:.2f}")
print(f"Standard deviation: {std_dev:.2f}")</code></pre>
    </details>

    <details>
      <summary><strong>Problem 2: Applying Bayes' Theorem</strong></summary>
      <p>Consider a spam email filter. 10% of emails are spam, and the probability that an email contains the word "free" is 80% for spam emails and 5% for normal emails. Calculate the probability that an email containing the word "free" is spam.</p>
      <pre><code>def spam_filter_bayes(p_spam, p_free_given_spam, p_free_given_normal):
    p_normal = 1 - p_spam
    p_free = p_free_given_spam * p_spam + p_free_given_normal * p_normal
    p_spam_given_free = (p_free_given_spam * p_spam) / p_free
    return p_spam_given_free

# Parameters
p_spam = 0.10
p_free_given_spam = 0.80
p_free_given_normal = 0.05

result = spam_filter_bayes(p_spam, p_free_given_spam, p_free_given_normal)
print(f"Probability that email containing 'free' is spam: {result * 100:.2f}%")</code></pre>
    </details>

    <details>
      <summary><strong>Problem 3: Calculating Expected Value</strong></summary>
      <p>In a lottery game, buying a 1000 yen ticket gives a 10% chance of winning 5000 yen, a 5% chance of winning 10000 yen, and the rest is 0 yen. Calculate the expected value of this game and determine whether it is worth playing.</p>
      <pre><code>import numpy as np

# Outcomes and probabilities
outcomes = np.array([5000, 10000, 0])
probabilities = np.array([0.10, 0.05, 0.85])

# Calculate expected value
expected_value = np.sum(outcomes * probabilities)
net_expected_value = expected_value - 1000  # Subtract ticket cost

print(f"Expected value: {expected_value:.2f} yen")
print(f"Net expected value (after ticket cost): {net_expected_value:.2f} yen")

if net_expected_value > 0:
    print("It is worth playing in terms of expected value")
else:
    print("It is not worth playing in terms of expected value")</code></pre>
    </details>

  </main>

  <footer>
    <div class="container">
      <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
      <p>Licensed under CC BY 4.0</p>
    </div>
  </footer>
</body>
</html>
