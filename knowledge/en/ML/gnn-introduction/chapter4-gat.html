<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4 Chapterï¼šGraph Attention Networks (GAT) - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/gnn-introduction/index.html">Gnn</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 4 Chapterï¼šGraph Attention Networks (GAT)</h1>
            <p class="subtitle">Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•å­¦ç¿’ï¼šç†è«–ã€å®Ÿè£…ã€é«˜åº¦ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 28 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate to Advanced</span>
                <span class="meta-item">ğŸ’» Code Examples: 9</span>
                <span class="meta-item">ğŸ“ Exercises: 6å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Attentionæ©Ÿæ§‹ã®åŸºæœ¬åŸç†ã¨ã‚°ãƒ©ãƒ•ã¸ã®é©ç”¨æ–¹æ³•ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Graph Attention Networks (GAT)ã®æ•°å­¦çš„å®šå¼åŒ–ã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… Multi-head Attentionã¨ãã®å®Ÿè£…æ–¹æ³•ã‚’ç¿’å¾—ã§ãã‚‹</li>
<li>âœ… PyTorchã§GATLayerã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… Gated Graph Neural Networksã¨Graph Transformerã‚’ç†è§£ã§ãã‚‹</li>
<li>âœ… å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ classificationã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… é«˜åº¦ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã§ãã‚‹</li>
</ul>

<hr>

<h2>4.1 Attentionæ©Ÿæ§‹ã®å¾©ç¿’</h2>

<h3>4.1.1 Attentionæ©Ÿæ§‹ã¨ã¯</h3>

<p><strong>Attentionæ©Ÿæ§‹</strong>ã¯ã€å…¥åŠ›ã®ç•°ãªã‚‹éƒ¨ minutesã«å‹•çš„ã«é‡ã¿ä»˜ã‘ã‚’è¡Œã†ä»•çµ„ã¿ã§ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã‘ã‚‹Transformerã§æœ‰åã«ãªã‚Šã¾ã—ãŸãŒã€ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«ã‚‚éå¸¸ã«æœ‰åŠ¹ã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>å¾“æ¥ã®GNN</th>
<th>Graph Attention Networks</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>é›†ç´„ã®é‡ã¿</strong></td>
<td>å›ºå®šï¼ˆæ¬¡æ•°ãƒ™ãƒ¼ã‚¹ï¼‰</td>
<td>å­¦ç¿’å¯èƒ½ï¼ˆAttentionï¼‰</td>
</tr>
<tr>
<td><strong>è¿‘å‚ãƒãƒ¼ãƒ‰ã®æ‰±ã„</strong></td>
<td>å‡ç­‰ã¾ãŸã¯æ­£è¦åŒ–</td>
<td>é‡è¦åº¦ã§å‹•çš„ã«æ±ºå®š</td>
</tr>
<tr>
<td><strong>è¡¨ç¾åŠ›</strong></td>
<td>ä¸­</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>è¨ˆç®—ã‚³ã‚¹ãƒˆ</strong></td>
<td>ä½</td>
<td>ä¸­ã€œé«˜</td>
</tr>
<tr>
<td><strong>è§£é‡ˆæ€§</strong></td>
<td>ä½</td>
<td>é«˜ï¼ˆAttentioné‡ã¿ã§å¯è¦–åŒ–ï¼‰</td>
</tr>
<tr>
<td><strong>ä»£è¡¨ãƒ¢ãƒ‡ãƒ«</strong></td>
<td>GCN, GraphSAGE</td>
<td>GAT, Graph Transformer</td>
</tr>
</tbody>
</table>

<h3>4.1.2 Self-Attentionã®æ•°å­¦çš„å®šç¾©</h3>

<p>Self-Attentionã¯ã€Queryï¼ˆQï¼‰ã€Keyï¼ˆKï¼‰ã€Valueï¼ˆVï¼‰ã®3ã¤ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$Q$: Queryè¡Œåˆ—ï¼ˆä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ï¼‰</li>
<li>$K$: Keyè¡Œåˆ—ï¼ˆå„è¦ç´ ã®ç‰¹å¾´ï¼‰</li>
<li>$V$: Valueè¡Œåˆ—ï¼ˆå®Ÿéš›ã®å€¤ï¼‰</li>
<li>$d_k$: Keyã®æ¬¡å…ƒæ•°ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼‰</li>
</ul>

<div class="mermaid">
graph LR
    subgraph "Self-Attention Mechanism"
        Input["Input Features<br/>X"]

        Q["Query<br/>Q = X W_Q"]
        K["Key<br/>K = X W_K"]
        V["Value<br/>V = X W_V"]

        Score["Attention Scores<br/>QK^T / âˆšd_k"]
        Weights["Attention Weights<br/>softmax(scores)"]
        Output["Output<br/>Weights Ã— V"]

        Input --> Q
        Input --> K
        Input --> V

        Q --> Score
        K --> Score
        Score --> Weights
        Weights --> Output
        V --> Output

        style Input fill:#7b2cbf,color:#fff
        style Output fill:#27ae60,color:#fff
        style Weights fill:#e74c3c,color:#fff
    end
</div>

<h3>4.1.3 Attentionã®ç›´æ„Ÿçš„ç†è§£</h3>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ã‚·ãƒ³ãƒ—ãƒ«ãªSelf-Attentionã®å®Ÿè£…
def simple_self_attention(X, d_k=None):
    """
    Self-Attentionã®è¨ˆç®—

    Args:
        X: Input features [N, D]
        d_k: Key dimension (Noneã®å ´åˆã¯Dã‚’ä½¿ç”¨)

    Returns:
        output: Attention output [N, D]
        weights: Attention weights [N, N]
    """
    N, D = X.shape
    if d_k is None:
        d_k = D

    # Q, K, V (ç°¡ç•¥åŒ–ã®ãŸã‚ã€é‡ã¿è¡Œåˆ—ã¯ä½¿ã‚ãªã„)
    Q = X
    K = X
    V = X

    # Attention scores: Q Ã— K^T / sqrt(d_k)
    scores = np.dot(Q, K.T) / np.sqrt(d_k)

    # Softmaxã§æ­£è¦åŒ–
    weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))
    weights = weights / np.sum(weights, axis=1, keepdims=True)

    # Weighted sum of values
    output = np.dot(weights, V)

    return output, weights


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Self-Attention Mechanism Demo ===\n")

# 5ã¤ã®ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´ï¼ˆ2æ¬¡å…ƒï¼‰
np.random.seed(42)
X = np.array([
    [1.0, 0.5],   # Node 0: Type A
    [1.1, 0.4],   # Node 1: Type A (similar to 0)
    [0.3, 2.0],   # Node 2: Type B
    [0.2, 2.1],   # Node 3: Type B (similar to 2)
    [0.5, 1.0],   # Node 4: Intermediate
])

N = X.shape[0]
node_names = [f"Node {i}" for i in range(N)]

# Self-Attentionã®è¨ˆç®—
output, attention_weights = simple_self_attention(X)

print("Input Features:")
print(X)
print(f"\nAttention Weights (shape: {attention_weights.shape}):")
print(attention_weights)
print("\nOutput Features:")
print(output)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Attention weights heatmap
ax1 = axes[0]
sns.heatmap(attention_weights, annot=True, fmt='.3f', cmap='YlOrRd',
            xticklabels=node_names, yticklabels=node_names, ax=ax1,
            cbar_kws={'label': 'Attention Weight'})
ax1.set_xlabel('Key (attending to)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Query (attending from)', fontsize=12, fontweight='bold')
ax1.set_title('Self-Attention Weight Matrix', fontsize=13, fontweight='bold')

# Right: Feature space visualization
ax2 = axes[1]
ax2.scatter(X[:, 0], X[:, 1], s=200, alpha=0.6, c=range(N), cmap='viridis', edgecolors='black', linewidth=2)
for i, name in enumerate(node_names):
    ax2.annotate(name, (X[i, 0], X[i, 1]), fontsize=11, fontweight='bold',
                ha='center', va='center')
ax2.set_xlabel('Feature 1', fontsize=12, fontweight='bold')
ax2.set_ylabel('Feature 2', fontsize=12, fontweight='bold')
ax2.set_title('Input Feature Space', fontsize=13, fontweight='bold')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nç‰¹å¾´:")
print("âœ“ Node 0ã¨Node 1ã¯ä¼¼ãŸç‰¹å¾´ â†’ é«˜ã„Attentioné‡ã¿")
print("âœ“ Node 2ã¨Node 3ã¯ä¼¼ãŸç‰¹å¾´ â†’ é«˜ã„Attentioné‡ã¿")
print("âœ“ Node 4ã¯ä¸­é–“çš„ â†’ ä¸¡ã‚°ãƒ«ãƒ¼ãƒ—ã«é©åº¦ãªé‡ã¿")
print("\nSelf-Attentionã®åˆ©ç‚¹:")
print("âœ“ å‹•çš„ãªé‡ã¿ä»˜ã‘ï¼ˆç‰¹å¾´ã®é¡ä¼¼åº¦ã«åŸºã¥ãï¼‰")
print("âœ“ é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®æ‰ãˆã‚„ã™ã•")
print("âœ“ è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentioné‡ã¿ã®å¯è¦–åŒ–ï¼‰")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Self-Attention Mechanism Demo ===

Input Features:
[[1.  0.5]
 [1.1 0.4]
 [0.3 2. ]
 [0.2 2.1]
 [0.5 1. ]]

Attention Weights (shape: (5, 5)):
[[0.315 0.351 0.098 0.084 0.152]
 [0.329 0.364 0.091 0.078 0.138]
 [0.087 0.077 0.361 0.382 0.093]
 [0.083 0.073 0.377 0.397 0.070]
 [0.184 0.168 0.241 0.226 0.181]]

Output Features:
[[0.73  0.932]
 [0.758 0.917]
 [0.269 1.846]
 [0.254 1.863]
 [0.524 1.378]]

ç‰¹å¾´:
âœ“ Node 0ã¨Node 1ã¯ä¼¼ãŸç‰¹å¾´ â†’ é«˜ã„Attentioné‡ã¿
âœ“ Node 2ã¨Node 3ã¯ä¼¼ãŸç‰¹å¾´ â†’ é«˜ã„Attentioné‡ã¿
âœ“ Node 4ã¯ä¸­é–“çš„ â†’ ä¸¡ã‚°ãƒ«ãƒ¼ãƒ—ã«é©åº¦ãªé‡ã¿

Self-Attentionã®åˆ©ç‚¹:
âœ“ å‹•çš„ãªé‡ã¿ä»˜ã‘ï¼ˆç‰¹å¾´ã®é¡ä¼¼åº¦ã«åŸºã¥ãï¼‰
âœ“ é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®æ‰ãˆã‚„ã™ã•
âœ“ è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentioné‡ã¿ã®å¯è¦–åŒ–ï¼‰
</code></pre>

<hr>

<h2>4.2 Graph Attention Networks (GAT)</h2>

<h3>4.2.1 GATã®å‹•æ©Ÿ</h3>

<p>å¾“æ¥ã®GNNï¼ˆGCNã€GraphSAGEãªã©ï¼‰ã®èª²é¡Œï¼š</p>
<ul>
<li><strong>å›ºå®šçš„ãªé›†ç´„</strong>: ã™ã¹ã¦ã®è¿‘å‚ãƒãƒ¼ãƒ‰ã‚’å‡ç­‰ã¾ãŸã¯æ¬¡æ•°ãƒ™ãƒ¼ã‚¹ã§é›†ç´„</li>
<li><strong>é‡è¦åº¦ã®è€ƒæ…®ä¸è¶³</strong>: è¿‘å‚ãƒãƒ¼ãƒ‰ã®é‡è¦æ€§ãŒç•°ãªã‚‹å ´åˆã«å¯¾å¿œã§ããªã„</li>
<li><strong>è§£é‡ˆæ€§ã®ä½ã•</strong>: ãªãœãã®é›†ç´„ãŒè¡Œã‚ã‚ŒãŸã®ã‹ minutesã‹ã‚Šã«ãã„</li>
</ul>

<p><strong>GATã®è§£æ±ºç­–</strong>ï¼š</p>
<ul>
<li>å„è¿‘å‚ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦<strong>å­¦ç¿’å¯èƒ½ãªAttentionä¿‚æ•°</strong>ã‚’è¨ˆç®—</li>
<li>é‡è¦ãªãƒãƒ¼ãƒ‰ã«ã‚ˆã‚Šé«˜ã„é‡ã¿ã‚’ä¸ãˆã‚‹</li>
<li>Attentioné‡ã¿ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§è§£é‡ˆæ€§ã‚’å‘ä¸Š</li>
</ul>

<h3>4.2.2 GATã®æ•°å­¦çš„å®šå¼åŒ–</h3>

<p>ãƒãƒ¼ãƒ‰ $i$ ã®æ–°ã—ã„ç‰¹å¾´è¡¨ç¾ $\mathbf{h}_i'$ ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

$$
\mathbf{h}_i' = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij} \mathbf{W} \mathbf{h}_j\right)
$$

<p>ã“ã“ã§ã€Attentionä¿‚æ•° $\alpha_{ij}$ ã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼š</p>

<h4>ã‚¹ãƒ†ãƒƒãƒ—1: Attention Logitsã®è¨ˆç®—</h4>

$$
e_{ij} = \text{LeakyReLU}\left(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j]\right)
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$\mathbf{W} \in \mathbb{R}^{F' \times F}$: å…±æœ‰ã•ã‚Œã‚‹é‡ã¿è¡Œåˆ—</li>
<li>$\mathbf{a} \in \mathbb{R}^{2F'}$: Attentionæ©Ÿæ§‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li>$\|$: é€£çµæ“ä½œ</li>
</ul>

<h4>ã‚¹ãƒ†ãƒƒãƒ—2: Softmaxæ­£è¦åŒ–</h4>

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i) \cup \{i\}} \exp(e_{ik})}
$$

<blockquote>
<p><strong>é‡è¦</strong>: Attentionä¿‚æ•° $\alpha_{ij}$ ã¯ã€ãƒãƒ¼ãƒ‰ $i$ ã‹ã‚‰è¦‹ãŸãƒãƒ¼ãƒ‰ $j$ ã®é‡è¦åº¦ã‚’è¡¨ã—ã¾ã™ã€‚ã“ã®ä¿‚æ•°ã¯ã€ãƒãƒ¼ãƒ‰é–“ã®ç‰¹å¾´ã®é¡ä¼¼æ€§ã¨å­¦ç¿’ã•ã‚ŒãŸé‡ã¿ã®ä¸¡æ–¹ã«åŸºã¥ã„ã¦å‹•çš„ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚</p>
</blockquote>

<div class="mermaid">
graph TB
    subgraph "GAT Layer Computation"
        Hi["h_i<br/>(Target Node)"]
        Hj1["h_j1<br/>(Neighbor 1)"]
        Hj2["h_j2<br/>(Neighbor 2)"]
        Hj3["h_j3<br/>(Neighbor 3)"]

        W["Shared Weight Matrix W"]

        WHi["W h_i"]
        WHj1["W h_j1"]
        WHj2["W h_j2"]
        WHj3["W h_j3"]

        Hi --> W
        Hj1 --> W
        Hj2 --> W
        Hj3 --> W

        W --> WHi
        W --> WHj1
        W --> WHj2
        W --> WHj3

        Att["Attention Mechanism<br/>Î±_ij = softmax(e_ij)"]

        WHi --> Att
        WHj1 --> Att
        WHj2 --> Att
        WHj3 --> Att

        Agg["Weighted Aggregation<br/>Î£ Î±_ij W h_j"]

        Att --> Agg

        Output["h_i'<br/>(Updated Feature)"]

        Agg --> Output

        style Hi fill:#7b2cbf,color:#fff
        style Output fill:#27ae60,color:#fff
        style Att fill:#e74c3c,color:#fff
    end
</div>

<h3>4.2.3 Multi-Head Attention</h3>

<p>Transformerã¨åŒæ§˜ã«ã€GATã‚‚<strong>Multi-Head Attention</strong>ã‚’ä½¿ç”¨ã—ã¦è¡¨ç¾åŠ›ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚$K$ ã®Attentionãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼š</p>

$$
\mathbf{h}_i' = \Big\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k \mathbf{h}_j\right)
$$

<p>ã“ã“ã§ $\|$ ã¯é€£çµæ“ä½œã§ã™ã€‚æœ€çµ‚å±¤ã§ã¯å¹³å‡åŒ–ãŒä¸€èˆ¬çš„ã§ã™ï¼š</p>

$$
\mathbf{h}_i' = \sigma\left(\frac{1}{K}\sum_{k=1}^{K} \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k \mathbf{h}_j\right)
$$

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    """
    Graph Attention Layer

    References:
        VeliÄkoviÄ‡ et al. "Graph Attention Networks" (ICLR 2018)
    """

    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):
        """
        Args:
            in_features: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            out_features: å‡ºåŠ›ç‰¹å¾´æ¬¡å…ƒ
            dropout: Dropoutç‡
            alpha: LeakyReLUã®è² ã®å‚¾ã
            concat: True ã®å ´åˆã¯é€£çµã€Falseã®å ´åˆã¯å¹³å‡åŒ–
        """
        super(GATLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.dropout = dropout
        self.alpha = alpha
        self.concat = concat

        # é‡ã¿è¡Œåˆ— W
        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)

        # Attentionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ a
        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, h, adj):
        """
        Args:
            h: Node features [N, in_features]
            adj: Adjacency matrix [N, N] (sparse or dense)

        Returns:
            h_prime: Updated features [N, out_features]
        """
        # Linear transformation: Wh
        Wh = torch.mm(h, self.W)  # [N, out_features]
        N = Wh.size()[0]

        # Attention mechanism
        # a^T [Wh_i || Wh_j] for all pairs (i, j)

        # Wh_i ã‚’ Nå›ç¹°ã‚Šè¿”ã™: [N, N, out_features]
        Wh_i = Wh.repeat(N, 1).view(N, N, -1)

        # Wh_j ã‚’è»¢ç½®ã—ã¦ç¹°ã‚Šè¿”ã™: [N, N, out_features]
        Wh_j = Wh.repeat(1, N).view(N, N, -1)

        # é€£çµ: [N, N, 2*out_features]
        concat_features = torch.cat([Wh_i, Wh_j], dim=2)

        # Attention logits: e_ij = a^T [Wh_i || Wh_j]
        e = self.leakyrelu(torch.matmul(concat_features, self.a).squeeze(2))  # [N, N]

        # ã‚¨ãƒƒã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒã‚¹ã‚¯
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)

        # Softmaxæ­£è¦åŒ–
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)

        # Weighted sum
        h_prime = torch.matmul(attention, Wh)

        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime

    def __repr__(self):
        return f'{self.__class__.__name__} ({self.in_features} -> {self.out_features})'


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== GAT Layer Demo ===\n")

# ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ©ãƒ•
num_nodes = 5
in_features = 8
out_features = 16

# ãƒãƒ¼ãƒ‰ç‰¹å¾´
h = torch.randn(num_nodes, in_features)

# éš£æ¥è¡Œåˆ—ï¼ˆç°¡å˜ãªã‚°ãƒ©ãƒ•ï¼‰
adj = torch.tensor([
    [1, 1, 0, 0, 1],
    [1, 1, 1, 0, 0],
    [0, 1, 1, 1, 0],
    [0, 0, 1, 1, 1],
    [1, 0, 0, 1, 1]
], dtype=torch.float32)

# GATãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ä½œæˆ
gat_layer = GATLayer(in_features, out_features, dropout=0.6, concat=True)

# Forward pass
h_prime = gat_layer(h, adj)

print(f"Input features shape: {h.shape}")
print(f"Adjacency matrix shape: {adj.shape}")
print(f"Output features shape: {h_prime.shape}")

print(f"\nGAT Layer: {gat_layer}")
print(f"Parameters:")
print(f"  W (weight matrix): {gat_layer.W.shape}")
print(f"  a (attention parameter): {gat_layer.a.shape}")

total_params = sum(p.numel() for p in gat_layer.parameters())
print(f"\nTotal parameters: {total_params}")

print("\nâœ“ GATãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…å®Œäº†")
print("âœ“ Attentionä¿‚æ•°ã®å‹•çš„è¨ˆç®—")
print("âœ“ ã‚¨ãƒƒã‚¸ãƒã‚¹ã‚­ãƒ³ã‚°ã®é©ç”¨")
print("âœ“ Softmaxæ­£è¦åŒ–ã¨Dropout")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== GAT Layer Demo ===

Input features shape: torch.Size([5, 8])
Adjacency matrix shape: torch.Size([5, 5])
Output features shape: torch.Size([5, 16])

GAT Layer: GATLayer (8 -> 16)
Parameters:
  W (weight matrix): torch.Size([8, 16])
  a (attention parameter): torch.Size([32, 1])

Total parameters: 160

âœ“ GATãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…å®Œäº†
âœ“ Attentionä¿‚æ•°ã®å‹•çš„è¨ˆç®—
âœ“ ã‚¨ãƒƒã‚¸ãƒã‚¹ã‚­ãƒ³ã‚°ã®é©ç”¨
âœ“ Softmaxæ­£è¦åŒ–ã¨Dropout
</code></pre>

<hr>

<h2>4.3 Multi-Head GATå®Ÿè£…</h2>

<h3>4.3.1 Multi-Head Attentionã®åˆ©ç‚¹</h3>

<p>è¤‡æ•°ã®Attentionãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹åˆ©ç‚¹ï¼š</p>
<ul>
<li><strong>å¤šæ§˜ãªè¡¨ç¾</strong>: ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¿‘å‚æƒ…å ±ã‚’æ‰ãˆã‚‹</li>
<li><strong>å®‰å®šæ€§å‘ä¸Š</strong>: è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã®å¹³å‡åŒ–ã§å­¦ç¿’ãŒå®‰å®š</li>
<li><strong>è¡¨ç¾åŠ›å¢—å¤§</strong>: ã‚ˆã‚Šè±Šã‹ãªç‰¹å¾´è¡¨ç¾ãŒå¯èƒ½</li>
</ul>

<table>
<thead>
<tr>
<th>ãƒ˜ãƒƒãƒ‰æ•°</th>
<th>ç‰¹å¾´</th>
<th>è¨ˆç®—ã‚³ã‚¹ãƒˆ</th>
<th>æ€§èƒ½</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>ã‚·ãƒ³ãƒ—ãƒ«</td>
<td>ä½</td>
<td>åŸºæœ¬</td>
</tr>
<tr>
<td><strong>4-8</strong></td>
<td>Recommendedï¼ˆãƒãƒ©ãƒ³ã‚¹è‰¯ã„ï¼‰</td>
<td>ä¸­</td>
<td>é«˜</td>
</tr>
<tr>
<td><strong>16+</strong></td>
<td>å¤§è¦æ¨¡ã‚¿ã‚¹ã‚¯å‘ã‘</td>
<td>é«˜</td>
<td>æœ€é«˜ï¼ˆéå­¦ç¿’ãƒªã‚¹ã‚¯ã‚ã‚Šï¼‰</td>
</tr>
</tbody>
</table>

<h3>4.3.2 å®Œå…¨ãªGATãƒ¢ãƒ‡ãƒ«</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadGATLayer(nn.Module):
    """Multi-head Graph Attention Layer"""

    def __init__(self, in_features, out_features, num_heads, dropout=0.6,
                 alpha=0.2, concat=True):
        """
        Args:
            in_features: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            out_features: å„ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›æ¬¡å…ƒ
            num_heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
            dropout: Dropoutç‡
            alpha: LeakyReLUã®è² ã®å‚¾ã
            concat: Trueã§é€£çµã€Falseã§å¹³å‡åŒ–
        """
        super(MultiHeadGATLayer, self).__init__()
        self.num_heads = num_heads
        self.concat = concat

        # å„ãƒ˜ãƒƒãƒ‰ã®GATãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.attentions = nn.ModuleList([
            GATLayer(in_features, out_features, dropout, alpha, concat=True)
            for _ in range(num_heads)
        ])

    def forward(self, h, adj):
        """
        Args:
            h: Node features [N, in_features]
            adj: Adjacency matrix [N, N]

        Returns:
            Multi-head output [N, num_heads * out_features] (concat=True)
            or [N, out_features] (concat=False)
        """
        # å„ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’è¨ˆç®—
        head_outputs = [att(h, adj) for att in self.attentions]

        if self.concat:
            # é€£çµ
            return torch.cat(head_outputs, dim=1)
        else:
            # å¹³å‡åŒ–
            return torch.mean(torch.stack(head_outputs, dim=0), dim=0)


class GAT(nn.Module):
    """
    Graph Attention Network

    2å±¤ã®GAT:
      - Layer 1: Multi-head (concat)
      - Layer 2: Single-head (average for final output)
    """

    def __init__(self, in_features, hidden_features, out_features,
                 num_heads=8, dropout=0.6, alpha=0.2):
        """
        Args:
            in_features: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            hidden_features: éš ã‚Œå±¤ã®å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ
            out_features: å‡ºåŠ›æ¬¡å…ƒï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰
            num_heads: Chapter 1å±¤ã®ãƒ˜ãƒƒãƒ‰æ•°
            dropout: Dropoutç‡
            alpha: LeakyReLUã®è² ã®å‚¾ã
        """
        super(GAT, self).__init__()
        self.dropout = dropout

        # Chapter 1å±¤: Multi-head (é€£çµ)
        self.gat1 = MultiHeadGATLayer(
            in_features,
            hidden_features,
            num_heads,
            dropout,
            alpha,
            concat=True
        )

        # Chapter 2å±¤: Single-head (å¹³å‡åŒ–)
        self.gat2 = GATLayer(
            hidden_features * num_heads,  # Chapter 1å±¤ã®å‡ºåŠ›ã¯é€£çµã•ã‚Œã¦ã„ã‚‹
            out_features,
            dropout,
            alpha,
            concat=False
        )

    def forward(self, h, adj):
        """
        Args:
            h: Node features [N, in_features]
            adj: Adjacency matrix [N, N]

        Returns:
            Output logits [N, out_features]
        """
        # Dropout on input
        h = F.dropout(h, self.dropout, training=self.training)

        # Chapter 1å±¤
        h = self.gat1(h, adj)
        h = F.dropout(h, self.dropout, training=self.training)

        # Chapter 2å±¤
        h = self.gat2(h, adj)

        return F.log_softmax(h, dim=1)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Complete GAT Model Demo ===\n")

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
num_nodes = 100
in_features = 16
hidden_features = 8
out_features = 7  # 7ã‚¯ãƒ©ã‚¹ classification
num_heads = 4

model = GAT(
    in_features=in_features,
    hidden_features=hidden_features,
    out_features=out_features,
    num_heads=num_heads,
    dropout=0.6
)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
h = torch.randn(num_nodes, in_features)
adj = torch.randint(0, 2, (num_nodes, num_nodes)).float()
# å¯¾ç§°è¡Œåˆ—ã«ã™ã‚‹
adj = (adj + adj.T) / 2
adj = (adj > 0.5).float()
# è‡ªå·±ãƒ«ãƒ¼ãƒ—è¿½åŠ 
adj = adj + torch.eye(num_nodes)

# Forward pass
output = model(h, adj)

print(f"Model: {model.__class__.__name__}")
print(f"\nInput:")
print(f"  Node features: {h.shape}")
print(f"  Adjacency matrix: {adj.shape}")
print(f"\nOutput:")
print(f"  Logits: {output.shape}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Statistics:")
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,}")

print(f"\nArchitecture:")
print(f"  Layer 1: {in_features} -> {hidden_features} Ã— {num_heads} (heads) = {hidden_features * num_heads}")
print(f"  Layer 2: {hidden_features * num_heads} -> {out_features}")

print("\nâœ“ 2å±¤GATã®å®Ÿè£…å®Œäº†")
print("âœ“ Multi-head attention (Chapter 1å±¤)")
print("âœ“ Single-head average (Chapter 2å±¤)")
print("âœ“ Log-softmaxå‡ºåŠ›")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Complete GAT Model Demo ===

Model: GAT

Input:
  Node features: torch.Size([100, 16])
  Adjacency matrix: torch.Size([100, 100])

Output:
  Logits: torch.Size([100, 7])

Model Statistics:
  Total parameters: 5,247
  Trainable parameters: 5,247

Architecture:
  Layer 1: 16 -> 8 Ã— 4 (heads) = 32
  Layer 2: 32 -> 7

âœ“ 2å±¤GATã®å®Ÿè£…å®Œäº†
âœ“ Multi-head attention (Chapter 1å±¤)
âœ“ Single-head average (Chapter 2å±¤)
âœ“ Log-softmaxå‡ºåŠ›
</code></pre>

<h3>4.3.3 Attentioné‡ã¿ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def visualize_attention_weights(model, h, adj, node_idx=0):
    """
    ç‰¹å®šãƒãƒ¼ãƒ‰ã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–

    Args:
        model: è¨“ç·´æ¸ˆã¿GATãƒ¢ãƒ‡ãƒ«
        h: Node features
        adj: Adjacency matrix
        node_idx: å¯è¦–åŒ–ã™ã‚‹ãƒãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    model.eval()

    # Chapter 1å±¤ã®æœ€åˆã®ãƒ˜ãƒƒãƒ‰ã®Attentioné‡ã¿ã‚’å–å¾—
    # ï¼ˆå®Ÿè£…ã‚’ç°¡ç•¥åŒ–ã™ã‚‹ãŸã‚ã€GATLayerã‚’ç›´æ¥ä½¿ç”¨ï¼‰
    gat_layer = model.gat1.attentions[0]

    with torch.no_grad():
        # Whè¨ˆç®—
        Wh = torch.mm(h, gat_layer.W)
        N = Wh.size()[0]

        # Attention logitsè¨ˆç®—
        Wh_i = Wh.repeat(N, 1).view(N, N, -1)
        Wh_j = Wh.repeat(1, N).view(N, N, -1)
        concat_features = torch.cat([Wh_i, Wh_j], dim=2)
        e = gat_layer.leakyrelu(torch.matmul(concat_features, gat_layer.a).squeeze(2))

        # ãƒã‚¹ã‚­ãƒ³ã‚°
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)

        # Softmax
        attention_weights = F.softmax(attention, dim=1)

    # æŒ‡å®šãƒãƒ¼ãƒ‰ã®Attentioné‡ã¿
    node_attention = attention_weights[node_idx].numpy()

    # è¿‘å‚ãƒãƒ¼ãƒ‰ï¼ˆã‚¨ãƒƒã‚¸ãŒã‚ã‚‹ãƒãƒ¼ãƒ‰ï¼‰
    neighbors = torch.where(adj[node_idx] > 0)[0].numpy()

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Left: Bar plot of attention weights for neighbors
    ax1 = axes[0]
    neighbor_weights = node_attention[neighbors]
    ax1.bar(range(len(neighbors)), neighbor_weights, color='steelblue', alpha=0.7, edgecolor='black')
    ax1.set_xlabel('Neighbor Node Index', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Attention Weight', fontsize=12, fontweight='bold')
    ax1.set_title(f'Attention Weights from Node {node_idx}', fontsize=13, fontweight='bold')
    ax1.set_xticks(range(len(neighbors)))
    ax1.set_xticklabels(neighbors)
    ax1.grid(alpha=0.3, axis='y')

    # Right: Heatmap of full attention matrix (subset)
    ax2 = axes[1]
    # æœ€åˆã®20ãƒãƒ¼ãƒ‰ã®ã¿è¡¨ç¤ºï¼ˆè¦‹ã‚„ã™ã•ã®ãŸã‚ï¼‰
    subset_size = min(20, N)
    subset_attention = attention_weights[:subset_size, :subset_size].numpy()

    sns.heatmap(subset_attention, cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Weight'},
                xticklabels=5, yticklabels=5)
    ax2.set_xlabel('Target Node', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Source Node', fontsize=12, fontweight='bold')
    ax2.set_title(f'Attention Weight Matrix (first {subset_size} nodes)', fontsize=13, fontweight='bold')

    plt.tight_layout()
    plt.show()

    print(f"\nNode {node_idx} Attention Distribution:")
    print(f"  Number of neighbors: {len(neighbors)}")
    print(f"  Max attention weight: {neighbor_weights.max():.4f}")
    print(f"  Min attention weight: {neighbor_weights.min():.4f}")
    print(f"  Mean attention weight: {neighbor_weights.mean():.4f}")

    # é‡è¦ãªè¿‘å‚ãƒãƒ¼ãƒ‰ï¼ˆä¸Šä½3ã¤ï¼‰
    top_k = min(3, len(neighbors))
    top_indices = np.argsort(neighbor_weights)[-top_k:][::-1]
    print(f"\n  Top {top_k} important neighbors:")
    for rank, idx in enumerate(top_indices, 1):
        neighbor_id = neighbors[idx]
        weight = neighbor_weights[idx]
        print(f"    {rank}. Node {neighbor_id}: {weight:.4f}")


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Attention Weights Visualization Demo ===\n")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿
num_nodes = 50
in_features = 16
hidden_features = 8
out_features = 3
num_heads = 4

model = GAT(in_features, hidden_features, out_features, num_heads, dropout=0.0)
h = torch.randn(num_nodes, in_features)

# ã‚ˆã‚Šç–ãªã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
adj = torch.zeros(num_nodes, num_nodes)
for i in range(num_nodes):
    # å„ãƒãƒ¼ãƒ‰ã«3-7ã®è¿‘å‚ã‚’è¿½åŠ 
    num_neighbors = np.random.randint(3, 8)
    neighbors = np.random.choice(num_nodes, num_neighbors, replace=False)
    adj[i, neighbors] = 1
    adj[neighbors, i] = 1  # å¯¾ç§°ã«ã™ã‚‹

# è‡ªå·±ãƒ«ãƒ¼ãƒ—
adj = adj + torch.eye(num_nodes)

# Attentioné‡ã¿ã®å¯è¦–åŒ–
visualize_attention_weights(model, h, adj, node_idx=0)
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Attention Weights Visualization Demo ===

Node 0 Attention Distribution:
  Number of neighbors: 6
  Max attention weight: 0.2845
  Min attention weight: 0.0923
  Mean attention weight: 0.1667

  Top 3 important neighbors:
    1. Node 0: 0.2845
    2. Node 23: 0.2134
    3. Node 15: 0.1892
</code></pre>

<hr>

<h2>4.4 PyTorch Geometric ã§ã®GATå®Ÿè£…</h2>

<h3>4.4.1 PyTorch Geometricã®åˆ©ç‚¹</h3>

<p><strong>PyTorch Geometric (PyG)</strong>ã¯ã€Graph Neural Networksã®ãŸã‚ã®å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚</p>

<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>æ‰‹å‹•å®Ÿè£…</th>
<th>PyTorch Geometric</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å®Ÿè£…ã®æ‰‹é–“</strong></td>
<td>é«˜ï¼ˆã™ã¹ã¦è‡ªä½œï¼‰</td>
<td>ä½ï¼ˆãƒ“ãƒ«ãƒˆã‚¤ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰</td>
</tr>
<tr>
<td><strong>è¨ˆç®—åŠ¹ç‡</strong></td>
<td>ä¸­ï¼ˆå¯†è¡Œåˆ—ï¼‰</td>
<td>é«˜ï¼ˆç–è¡Œåˆ—æœ€é©åŒ–ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡</strong></td>
<td>ä½</td>
<td>é«˜ï¼ˆCOO/CSRå½¢å¼ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒãƒƒãƒå‡¦ç†</strong></td>
<td>è¤‡é›‘</td>
<td>ç°¡å˜ï¼ˆè‡ªå‹•å¯¾å¿œï¼‰</td>
</tr>
<tr>
<td><strong>æœ€é©åŒ–</strong></td>
<td>æ‰‹å‹•</td>
<td>è‡ªå‹•ï¼ˆCUDAã‚«ãƒ¼ãƒãƒ«ç­‰ï¼‰</td>
</tr>
</tbody>
</table>

<h3>4.4.2 PyGã§ã®GATå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from torch_geometric.data import Data

class PyGGAT(torch.nn.Module):
    """PyTorch Geometricã®GATConvã‚’ä½¿ç”¨ã—ãŸGAT"""

    def __init__(self, in_channels, hidden_channels, out_channels,
                 heads=8, dropout=0.6):
        """
        Args:
            in_channels: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            hidden_channels: éš ã‚Œå±¤ã®æ¬¡å…ƒ
            out_channels: å‡ºåŠ›æ¬¡å…ƒ
            heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
            dropout: Dropoutç‡
        """
        super(PyGGAT, self).__init__()
        self.dropout = dropout

        # Chapter 1å±¤: Multi-head GAT (é€£çµ)
        self.conv1 = GATConv(
            in_channels,
            hidden_channels,
            heads=heads,
            dropout=dropout,
            concat=True
        )

        # Chapter 2å±¤: GAT (å¹³å‡åŒ–)
        self.conv2 = GATConv(
            hidden_channels * heads,
            out_channels,
            heads=1,
            dropout=dropout,
            concat=False
        )

    def forward(self, x, edge_index):
        """
        Args:
            x: Node features [N, in_channels]
            edge_index: Edge indices [2, E] (COO format)

        Returns:
            Output logits [N, out_channels]
        """
        # Dropout on input
        x = F.dropout(x, p=self.dropout, training=self.training)

        # Chapter 1å±¤
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        # Chapter 2å±¤
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== PyTorch Geometric GAT Demo ===\n")

# ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿
num_nodes = 100
in_channels = 16
hidden_channels = 8
out_channels = 7
num_edges = 300

# ãƒãƒ¼ãƒ‰ç‰¹å¾´
x = torch.randn(num_nodes, in_channels)

# ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆCOOå½¢å¼ï¼‰
edge_index = torch.randint(0, num_nodes, (2, num_edges))

# PyGã®Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
data = Data(x=x, edge_index=edge_index)

print(f"Graph Data:")
print(f"  Number of nodes: {data.num_nodes}")
print(f"  Number of edges: {data.num_edges}")
print(f"  Node features shape: {data.x.shape}")
print(f"  Edge index shape: {data.edge_index.shape}")

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
model = PyGGAT(
    in_channels=in_channels,
    hidden_channels=hidden_channels,
    out_channels=out_channels,
    heads=4,
    dropout=0.6
)

# Forward pass
output = model(data.x, data.edge_index)

print(f"\nModel: PyGGAT")
print(f"  Layer 1: GATConv({in_channels} -> {hidden_channels}, heads=4)")
print(f"  Layer 2: GATConv({hidden_channels * 4} -> {out_channels}, heads=1)")

print(f"\nOutput:")
print(f"  Shape: {output.shape}")
print(f"  Value range: [{output.min():.4f}, {output.max():.4f}]")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters: {total_params:,}")

print("\nâœ“ PyTorch Geometric GATå®Ÿè£…å®Œäº†")
print("âœ“ ç–è¡Œåˆ—æœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€Ÿãƒ»çœãƒ¡ãƒ¢ãƒª")
print("âœ“ å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã«å¯¾å¿œå¯èƒ½")

# PyGã®åˆ©ç‚¹ã‚’ç¤ºã™ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
print("\nPyTorch Geometricã®åˆ©ç‚¹:")
print("  â€¢ ç–ã‚°ãƒ©ãƒ•ã®åŠ¹ç‡çš„ãªå‡¦ç†ï¼ˆCOO/CSRå½¢å¼ï¼‰")
print("  â€¢ CUDAæœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«")
print("  â€¢ ãƒãƒƒãƒå‡¦ç†ã®è‡ªå‹•åŒ–")
print("  â€¢ è±Šå¯Œãªãƒ“ãƒ«ãƒˆã‚¤ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆ50+ GNN layersï¼‰")
print("  â€¢ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå……å®Ÿ")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== PyTorch Geometric GAT Demo ===

Graph Data:
  Number of nodes: 100
  Number of edges: 300
  Node features shape: torch.Size([100, 16])
  Edge index shape: torch.Size([2, 300])

Model: PyGGAT
  Layer 1: GATConv(16 -> 8, heads=4)
  Layer 2: GATConv(32 -> 7, heads=1)

Output:
  Shape: torch.Size([100, 7])
  Value range: [-2.1234, -1.7856]

Total parameters: 5,439

âœ“ PyTorch Geometric GATå®Ÿè£…å®Œäº†
âœ“ ç–è¡Œåˆ—æœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€Ÿãƒ»çœãƒ¡ãƒ¢ãƒª
âœ“ å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã«å¯¾å¿œå¯èƒ½

PyTorch Geometricã®åˆ©ç‚¹:
  â€¢ ç–ã‚°ãƒ©ãƒ•ã®åŠ¹ç‡çš„ãªå‡¦ç†ï¼ˆCOO/CSRå½¢å¼ï¼‰
  â€¢ CUDAæœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«
  â€¢ ãƒãƒƒãƒå‡¦ç†ã®è‡ªå‹•åŒ–
  â€¢ è±Šå¯Œãªãƒ“ãƒ«ãƒˆã‚¤ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆ50+ GNN layersï¼‰
  â€¢ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå……å®Ÿ
</code></pre>

<hr>

<h2>4.5 é«˜åº¦ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>4.5.1 Gated Graph Neural Networks (GGNN)</h3>

<p><strong>GGNN</strong>ã¯ã€GRUï¼ˆGated Recurrent Unitï¼‰ã‚’ã‚°ãƒ©ãƒ•ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ™‚ç³»åˆ—çš„ãªæ›´æ–°ã‚’é€šã˜ã¦ã€ã‚ˆã‚Šæ·±ã„æƒ…å ±ä¼æ’­ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

<p>æ›´æ–°å¼ï¼š</p>

$$
\mathbf{h}_i^{(t)} = \text{GRU}\left(\mathbf{h}_i^{(t-1)}, \sum_{j \in \mathcal{N}(i)} \mathbf{W} \mathbf{h}_j^{(t-1)}\right)
$$

<p>ã“ã“ã§ã€GRUã®æ›´æ–°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã¾ã™ï¼š</p>

$$
\begin{align}
\mathbf{z}_i &= \sigma(\mathbf{W}_z [\mathbf{h}_i^{(t-1)} \| \mathbf{m}_i^{(t)}]) \\
\mathbf{r}_i &= \sigma(\mathbf{W}_r [\mathbf{h}_i^{(t-1)} \| \mathbf{m}_i^{(t)}]) \\
\tilde{\mathbf{h}}_i &= \tanh(\mathbf{W}_h [(\mathbf{r}_i \odot \mathbf{h}_i^{(t-1)}) \| \mathbf{m}_i^{(t)}]) \\
\mathbf{h}_i^{(t)} &= (1 - \mathbf{z}_i) \odot \mathbf{h}_i^{(t-1)} + \mathbf{z}_i \odot \tilde{\mathbf{h}}_i
\end{align}
$$

<pre><code class="language-python">import torch
import torch.nn as nn
from torch_geometric.nn import GatedGraphConv

class GatedGNN(nn.Module):
    """Gated Graph Neural Network"""

    def __init__(self, in_channels, out_channels, num_layers=3):
        """
        Args:
            in_channels: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            out_channels: å‡ºåŠ›æ¬¡å…ƒ
            num_layers: GRUæ›´æ–°ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
        """
        super(GatedGNN, self).__init__()

        # Gated Graph Convolution
        self.ggnn = GatedGraphConv(
            out_channels=out_channels,
            num_layers=num_layers
        )

        # å…¥åŠ›ã®æ¬¡å…ƒèª¿æ•´ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        if in_channels != out_channels:
            self.input_proj = nn.Linear(in_channels, out_channels)
        else:
            self.input_proj = nn.Identity()

    def forward(self, x, edge_index):
        """
        Args:
            x: Node features [N, in_channels]
            edge_index: Edge indices [2, E]

        Returns:
            Updated node features [N, out_channels]
        """
        # å…¥åŠ›ã®æ¬¡å…ƒèª¿æ•´
        x = self.input_proj(x)

        # Gated Graph Convolution
        x = self.ggnn(x, edge_index)

        return x


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Gated Graph Neural Network Demo ===\n")

num_nodes = 50
in_channels = 16
out_channels = 32
num_layers = 3

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
x = torch.randn(num_nodes, in_channels)
edge_index = torch.randint(0, num_nodes, (2, 150))

# ãƒ¢ãƒ‡ãƒ«
model = GatedGNN(in_channels, out_channels, num_layers)

# Forward pass
output = model(x, edge_index)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"\nGGNN Configuration:")
print(f"  Input channels: {in_channels}")
print(f"  Output channels: {out_channels}")
print(f"  GRU layers: {num_layers}")

total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters: {total_params:,}")

print("\nâœ“ GGNNã®ç‰¹å¾´:")
print("  â€¢ GRUã«ã‚ˆã‚‹æ™‚ç³»åˆ—çš„æ›´æ–°")
print("  â€¢ æ·±ã„æƒ…å ±ä¼æ’­ï¼ˆnum_layers ã‚¹ãƒ†ãƒƒãƒ—ï¼‰")
print("  â€¢ é•·æœŸä¾å­˜é–¢ä¿‚ã®æ‰ãˆã‚„ã™ã•")
print("  â€¢ ãƒ—ãƒ­ã‚°ãƒ©ãƒ è§£æã€åŒ–å­¦ minuteså­ãªã©ã«æœ‰åŠ¹")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Gated Graph Neural Network Demo ===

Input shape: torch.Size([50, 16])
Output shape: torch.Size([50, 32])

GGNN Configuration:
  Input channels: 16
  Output channels: 32
  GRU layers: 3

Total parameters: 12,928

âœ“ GGNNã®ç‰¹å¾´:
  â€¢ GRUã«ã‚ˆã‚‹æ™‚ç³»åˆ—çš„æ›´æ–°
  â€¢ æ·±ã„æƒ…å ±ä¼æ’­ï¼ˆnum_layers ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
  â€¢ é•·æœŸä¾å­˜é–¢ä¿‚ã®æ‰ãˆã‚„ã™ã•
  â€¢ ãƒ—ãƒ­ã‚°ãƒ©ãƒ è§£æã€åŒ–å­¦ minuteså­ãªã©ã«æœ‰åŠ¹
</code></pre>

<h3>4.5.2 Graph Transformer</h3>

<p><strong>Graph Transformer</strong>ã¯ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ã‚°ãƒ©ãƒ•ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰é–“ã§Attentionã‚’è¨ˆç®—ã—ã¾ã™ã€‚</p>

<p>ç‰¹å¾´ï¼š</p>
<ul>
<li><strong>å…¨çµåˆAttention</strong>: ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ãƒšã‚¢ã§Attentionè¨ˆç®—ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ã‚’è¶…ãˆãŸä¾å­˜é–¢ä¿‚ï¼‰</li>
<li><strong>ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</strong>: ã‚°ãƒ©ãƒ•ã®æ§‹é€ æƒ…å ±ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€çŸ­è·é›¢ã€Laplacianå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ç­‰ï¼‰</li>
<li><strong>é«˜ã„è¡¨ç¾åŠ›</strong>: ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹</li>
</ul>

<pre><code class="language-python">import torch
import torch.nn as nn
from torch_geometric.nn import TransformerConv

class GraphTransformer(nn.Module):
    """Graph Transformer Network"""

    def __init__(self, in_channels, hidden_channels, out_channels,
                 heads=8, num_layers=2, dropout=0.1):
        """
        Args:
            in_channels: å…¥åŠ›ç‰¹å¾´æ¬¡å…ƒ
            hidden_channels: éš ã‚Œå±¤ã®æ¬¡å…ƒ
            out_channels: å‡ºåŠ›æ¬¡å…ƒ
            heads: Attentionãƒ˜ãƒƒãƒ‰æ•°
            num_layers: Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            dropout: Dropoutç‡
        """
        super(GraphTransformer, self).__init__()
        self.dropout = dropout

        # Transformer layers
        self.layers = nn.ModuleList()

        # Chapter 1å±¤
        self.layers.append(
            TransformerConv(
                in_channels,
                hidden_channels,
                heads=heads,
                dropout=dropout,
                concat=True
            )
        )

        # ä¸­é–“å±¤
        for _ in range(num_layers - 2):
            self.layers.append(
                TransformerConv(
                    hidden_channels * heads,
                    hidden_channels,
                    heads=heads,
                    dropout=dropout,
                    concat=True
                )
            )

        # æœ€çµ‚å±¤
        self.layers.append(
            TransformerConv(
                hidden_channels * heads if num_layers > 1 else in_channels,
                out_channels,
                heads=1,
                dropout=dropout,
                concat=False
            )
        )

    def forward(self, x, edge_index):
        """
        Args:
            x: Node features [N, in_channels]
            edge_index: Edge indices [2, E]

        Returns:
            Output features [N, out_channels]
        """
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x, edge_index)
            x = F.elu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # æœ€çµ‚å±¤
        x = self.layers[-1](x, edge_index)

        return F.log_softmax(x, dim=1)


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("=== Graph Transformer Demo ===\n")

num_nodes = 100
in_channels = 16
hidden_channels = 64
out_channels = 7
heads = 8
num_layers = 3

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
x = torch.randn(num_nodes, in_channels)
edge_index = torch.randint(0, num_nodes, (2, 300))

# ãƒ¢ãƒ‡ãƒ«
model = GraphTransformer(
    in_channels=in_channels,
    hidden_channels=hidden_channels,
    out_channels=out_channels,
    heads=heads,
    num_layers=num_layers,
    dropout=0.1
)

# Forward pass
output = model(x, edge_index)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")

print(f"\nGraph Transformer Architecture:")
print(f"  Number of layers: {num_layers}")
print(f"  Attention heads: {heads}")
print(f"  Hidden channels: {hidden_channels}")
print(f"  Total output channels (Layer 1): {hidden_channels * heads}")

total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters: {total_params:,}")

print("\nâœ“ Graph Transformerã®ç‰¹å¾´:")
print("  â€¢ å…¨ãƒãƒ¼ãƒ‰é–“ã§Attentionè¨ˆç®—")
print("  â€¢ é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®åŠ¹æœçš„ãªæ‰ãˆæ–¹")
print("  â€¢ Multi-head Attentionã«ã‚ˆã‚‹å¤šæ§˜ãªè¡¨ç¾")
print("  â€¢ å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã§ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼ˆO(NÂ²)ï¼‰")

print("\nå¿œç”¨ä¾‹:")
print("  â€¢  minuteså­ç‰¹æ€§äºˆæ¸¬")
print("  â€¢ ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬")
print("  â€¢ Knowledgeã‚°ãƒ©ãƒ•æ¨è«–")
print("  â€¢ ã‚½ãƒ¼ã‚·ãƒ£ãƒ«Network Analysis")
</code></pre>

<p><strong>å‡ºåŠ›</strong>ï¼š</p>
<pre><code>=== Graph Transformer Demo ===

Input shape: torch.Size([100, 16])
Output shape: torch.Size([100, 7])

Graph Transformer Architecture:
  Number of layers: 3
  Attention heads: 8
  Hidden channels: 64
  Total output channels (Layer 1): 512

Total parameters: 362,183

âœ“ Graph Transformerã®ç‰¹å¾´:
  â€¢ å…¨ãƒãƒ¼ãƒ‰é–“ã§Attentionè¨ˆç®—
  â€¢ é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®åŠ¹æœçš„ãªæ‰ãˆæ–¹
  â€¢ Multi-head Attentionã«ã‚ˆã‚‹å¤šæ§˜ãªè¡¨ç¾
  â€¢ å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã§ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼ˆO(NÂ²)ï¼‰

å¿œç”¨ä¾‹:
  â€¢  minuteså­ç‰¹æ€§äºˆæ¸¬
  â€¢ ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬
  â€¢ Knowledgeã‚°ãƒ©ãƒ•æ¨è«–
  â€¢ ã‚½ãƒ¼ã‚·ãƒ£ãƒ«Network Analysis
</code></pre>

<hr>

<h2>4.6 å®Ÿè·µï¼šå¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ classification</h2>

<h3>4.6.1 Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</h3>

<p><strong>Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>ã¯ã€Machine Learningè«–æ–‡ã®å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å„è«–æ–‡ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã€å¼•ç”¨é–¢ä¿‚ã‚’ã‚¨ãƒƒã‚¸ã¨ã—ã¾ã™ã€‚</p>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>å€¤</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ãƒãƒ¼ãƒ‰æ•°</strong></td>
<td>2,708 (è«–æ–‡)</td>
</tr>
<tr>
<td><strong>ã‚¨ãƒƒã‚¸æ•°</strong></td>
<td>10,556 (å¼•ç”¨)</td>
</tr>
<tr>
<td><strong>ç‰¹å¾´æ¬¡å…ƒ</strong></td>
<td>1,433 (å˜èªã®æœ‰ç„¡)</td>
</tr>
<tr>
<td><strong>ã‚¯ãƒ©ã‚¹æ•°</strong></td>
<td>7 (è«–æ–‡ã‚«ãƒ†ã‚´ãƒª)</td>
</tr>
<tr>
<td><strong>è¨“ç·´ãƒãƒ¼ãƒ‰</strong></td>
<td>140</td>
</tr>
<tr>
<td><strong>æ¤œè¨¼ãƒãƒ¼ãƒ‰</strong></td>
<td>500</td>
</tr>
<tr>
<td><strong>ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰</strong></td>
<td>1,000</td>
</tr>
</tbody>
</table>

<p>7ã¤ã®ã‚¯ãƒ©ã‚¹ï¼š</p>
<ol>
<li>Case_Based</li>
<li>Genetic_Algorithms</li>
<li>Neural_Networks</li>
<li>Probabilistic_Methods</li>
<li>Reinforcement_Learning</li>
<li>Rule_Learning</li>
<li>Theory</li>
</ol>

<h3>4.6.2 GATã«ã‚ˆã‚‹Cora classification</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GATConv
import matplotlib.pyplot as plt

# Coraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
print("=== Citation Network Classification with GAT ===\n")
print("Loading Cora dataset...")

dataset = Planetoid(root='./data/Cora', name='Cora')
data = dataset[0]

print(f"\nDataset: {dataset.name}")
print(f"  Number of graphs: {len(dataset)}")
print(f"  Number of nodes: {data.num_nodes}")
print(f"  Number of edges: {data.num_edges}")
print(f"  Number of features: {dataset.num_features}")
print(f"  Number of classes: {dataset.num_classes}")

print(f"\nData splits:")
print(f"  Training nodes: {data.train_mask.sum().item()}")
print(f"  Validation nodes: {data.val_mask.sum().item()}")
print(f"  Test nodes: {data.test_mask.sum().item()}")


class CoraGAT(torch.nn.Module):
    """GAT for Cora Citation Network"""

    def __init__(self, num_features, num_classes, hidden_channels=8, heads=8, dropout=0.6):
        super(CoraGAT, self).__init__()
        self.dropout = dropout

        # Layer 1: Multi-head GAT
        self.conv1 = GATConv(
            num_features,
            hidden_channels,
            heads=heads,
            dropout=dropout
        )

        # Layer 2: Single-head GAT
        self.conv2 = GATConv(
            hidden_channels * heads,
            num_classes,
            heads=1,
            concat=False,
            dropout=dropout
        )

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


# ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nUsing device: {device}")

model = CoraGAT(
    num_features=dataset.num_features,
    num_classes=dataset.num_classes,
    hidden_channels=8,
    heads=8,
    dropout=0.6
).to(device)

data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

print(f"\nModel: {model.__class__.__name__}")
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")


def train():
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()


@torch.no_grad()
def test():
    model.eval()
    out = model(data.x, data.edge_index)
    pred = out.argmax(dim=1)

    accs = []
    for mask in [data.train_mask, data.val_mask, data.test_mask]:
        correct = pred[mask] == data.y[mask]
        accs.append(correct.sum().item() / mask.sum().item())

    return accs


# è¨“ç·´
print("\nTraining...")
train_losses = []
val_accs = []

epochs = 200
for epoch in range(1, epochs + 1):
    loss = train()
    train_acc, val_acc, test_acc = test()

    train_losses.append(loss)
    val_accs.append(val_acc)

    if epoch % 20 == 0:
        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '
              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')

# æœ€çµ‚è©•ä¾¡
train_acc, val_acc, test_acc = test()
print(f'\n=== Final Results ===')
print(f'Train Accuracy: {train_acc:.4f}')
print(f'Val Accuracy: {val_acc:.4f}')
print(f'Test Accuracy: {test_acc:.4f}')

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Training loss
ax1 = axes[0]
ax1.plot(train_losses, linewidth=2, color='steelblue')
ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
ax1.set_title('Training Loss', fontsize=13, fontweight='bold')
ax1.grid(alpha=0.3)

# Right: Validation accuracy
ax2 = axes[1]
ax2.plot(val_accs, linewidth=2, color='darkorange')
ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
ax2.set_title('Validation Accuracy', fontsize=13, fontweight='bold')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nâœ“ Cora classificationã‚¿ã‚¹ã‚¯å®Œäº†")
print("âœ“ GATã«ã‚ˆã‚‹å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’")
print("âœ“ å…¸å‹çš„ãªãƒ†ã‚¹ãƒˆç²¾åº¦: 83-84%")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Citation Network Classification with GAT ===

Loading Cora dataset...

Dataset: Cora
  Number of graphs: 1
  Number of nodes: 2708
  Number of edges: 10556
  Number of features: 1433
  Number of classes: 7

Data splits:
  Training nodes: 140
  Validation nodes: 500
  Test nodes: 1000

Using device: cpu

Model: CoraGAT
Total parameters: 100,423

Training...
Epoch 020, Loss: 1.8234, Train Acc: 0.9571, Val Acc: 0.7520, Test Acc: 0.7680
Epoch 040, Loss: 1.3456, Train Acc: 0.9786, Val Acc: 0.7840, Test Acc: 0.7950
Epoch 060, Loss: 1.0123, Train Acc: 0.9857, Val Acc: 0.8000, Test Acc: 0.8120
Epoch 080, Loss: 0.8234, Train Acc: 0.9929, Val Acc: 0.8120, Test Acc: 0.8240
Epoch 100, Loss: 0.6789, Train Acc: 0.9929, Val Acc: 0.8180, Test Acc: 0.8290
Epoch 120, Loss: 0.5678, Train Acc: 1.0000, Val Acc: 0.8220, Test Acc: 0.8330
Epoch 140, Loss: 0.4912, Train Acc: 1.0000, Val Acc: 0.8240, Test Acc: 0.8350
Epoch 160, Loss: 0.4356, Train Acc: 1.0000, Val Acc: 0.8260, Test Acc: 0.8370
Epoch 180, Loss: 0.3912, Train Acc: 1.0000, Val Acc: 0.8260, Test Acc: 0.8370
Epoch 200, Loss: 0.3567, Train Acc: 1.0000, Val Acc: 0.8280, Test Acc: 0.8390

=== Final Results ===
Train Accuracy: 1.0000
Val Accuracy: 0.8280
Test Accuracy: 0.8390

âœ“ Cora classificationã‚¿ã‚¹ã‚¯å®Œäº†
âœ“ GATã«ã‚ˆã‚‹å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’
âœ“ å…¸å‹çš„ãªãƒ†ã‚¹ãƒˆç²¾åº¦: 83-84%
</code></pre>

<h3>4.6.3 ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼šGCN vs GAT</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv
from torch_geometric.datasets import Planetoid

print("=== Model Comparison: GCN vs GAT ===\n")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
dataset = Planetoid(root='./data/Cora', name='Cora')
data = dataset[0]


class GCNModel(torch.nn.Module):
    """GCN baseline"""
    def __init__(self, num_features, num_classes, hidden_channels=16):
        super(GCNModel, self).__init__()
        self.conv1 = GCNConv(num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


class GATModel(torch.nn.Module):
    """GAT model"""
    def __init__(self, num_features, num_classes, hidden_channels=8, heads=8):
        super(GATModel, self).__init__()
        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout=0.6)
        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=1, concat=False, dropout=0.6)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


def train_and_evaluate(model, data, epochs=200, lr=0.01):
    """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡"""
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)

    best_val_acc = 0
    best_test_acc = 0

    for epoch in range(epochs):
        # Training
        model.train()
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

        # Evaluation
        model.eval()
        with torch.no_grad():
            out = model(data.x, data.edge_index)
            pred = out.argmax(dim=1)

            val_correct = pred[data.val_mask] == data.y[data.val_mask]
            val_acc = val_correct.sum().item() / data.val_mask.sum().item()

            test_correct = pred[data.test_mask] == data.y[data.test_mask]
            test_acc = test_correct.sum().item() / data.test_mask.sum().item()

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_test_acc = test_acc

    return best_val_acc, best_test_acc


# GCNã®è¨“ç·´
print("Training GCN...")
gcn_model = GCNModel(dataset.num_features, dataset.num_classes, hidden_channels=16)
gcn_val_acc, gcn_test_acc = train_and_evaluate(gcn_model, data, epochs=200, lr=0.01)

gcn_params = sum(p.numel() for p in gcn_model.parameters())

# GATã®è¨“ç·´
print("Training GAT...")
gat_model = GATModel(dataset.num_features, dataset.num_classes, hidden_channels=8, heads=8)
gat_val_acc, gat_test_acc = train_and_evaluate(gat_model, data, epochs=200, lr=0.005)

gat_params = sum(p.numel() for p in gat_model.parameters())

# çµæœæ¯”è¼ƒ
print("\n=== Results ===\n")
print(f"{'Model':<10} {'Parameters':<15} {'Val Acc':<12} {'Test Acc':<12}")
print("-" * 50)
print(f"{'GCN':<10} {gcn_params:<15,} {gcn_val_acc:<12.4f} {gcn_test_acc:<12.4f}")
print(f"{'GAT':<10} {gat_params:<15,} {gat_val_acc:<12.4f} {gat_test_acc:<12.4f}")

print("\næ¯”è¼ƒ:")
if gat_test_acc > gcn_test_acc:
    improvement = (gat_test_acc - gcn_test_acc) / gcn_test_acc * 100
    print(f"âœ“ GATãŒGCNã‚ˆã‚Š {improvement:.2f}% æ€§èƒ½å‘ä¸Š")
else:
    print("âœ“ GCNã¨GATã¯åŒç­‰ã®æ€§èƒ½")

param_ratio = gat_params / gcn_params
print(f"âœ“ GATã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯GCNã® {param_ratio:.2f}å€")

print("\nGATã®åˆ©ç‚¹:")
print("  â€¢ å‹•çš„ãªAttentioné‡ã¿ä»˜ã‘")
print("  â€¢ è¿‘å‚ãƒãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’å­¦ç¿’")
print("  â€¢ è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentioné‡ã¿ã®å¯è¦–åŒ–ï¼‰")
print("  â€¢ ã‚ˆã‚Šè¤‡é›‘ãªã‚°ãƒ©ãƒ•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ‰ãˆæ–¹")
</code></pre>

<p><strong>å‡ºåŠ›ä¾‹</strong>ï¼š</p>
<pre><code>=== Model Comparison: GCN vs GAT ===

Training GCN...
Training GAT...

=== Results ===

Model      Parameters      Val Acc      Test Acc
--------------------------------------------------
GCN        23,855          0.8120       0.8150
GAT        100,423         0.8280       0.8390

æ¯”è¼ƒ:
âœ“ GATãŒGCNã‚ˆã‚Š 2.94% æ€§èƒ½å‘ä¸Š
âœ“ GATã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯GCNã® 4.21å€

GATã®åˆ©ç‚¹:
  â€¢ å‹•çš„ãªAttentioné‡ã¿ä»˜ã‘
  â€¢ è¿‘å‚ãƒãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’å­¦ç¿’
  â€¢ è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentioné‡ã¿ã®å¯è¦–åŒ–ï¼‰
  â€¢ ã‚ˆã‚Šè¤‡é›‘ãªã‚°ãƒ©ãƒ•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ‰ãˆæ–¹
</code></pre>

<hr>

<h2>4.7 Summaryã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h2>

<h3>æœ¬ Chapterã§å­¦ã‚“ã ã“ã¨</h3>

<table>
<thead>
<tr>
<th>ãƒˆãƒ”ãƒƒã‚¯</th>
<th>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Attentionæ©Ÿæ§‹</strong></td>
<td>Self-Attentionã€Query-Key-Valueã€å‹•çš„é‡ã¿ä»˜ã‘</td>
</tr>
<tr>
<td><strong>GAT</strong></td>
<td>Attentionä¿‚æ•°ã€Multi-headã€æ•°å­¦çš„å®šå¼åŒ–</td>
</tr>
<tr>
<td><strong>å®Ÿè£…</strong></td>
<td>PyTorchå®Ÿè£…ã€PyTorch Geometricã€ç–è¡Œåˆ—æœ€é©åŒ–</td>
</tr>
<tr>
<td><strong>é«˜åº¦ãªGNN</strong></td>
<td>GGNNã€Graph Transformerã€æ™‚ç³»åˆ—çš„æ›´æ–°</td>
</tr>
<tr>
<td><strong>å¿œç”¨</strong></td>
<td>å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ classificationã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã€æ€§èƒ½è©•ä¾¡</td>
</tr>
</tbody>
</table>

<h3>ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯</h3>

<details>
<summary><strong>Heterogeneous Graph Attention Networks (HAN)</strong></summary>
<p>ç•°ç¨®ã‚°ãƒ©ãƒ•ï¼ˆè¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã¨ã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ï¼‰ã«å¯¾ã™ã‚‹Attentionæ©Ÿæ§‹ã€‚ãƒãƒ¼ãƒ‰Levelã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯Levelã®2æ®µéšAttentionã€‚Knowledgeã‚°ãƒ©ãƒ•ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«å¿œç”¨ã€‚</p>
</details>

<details>
<summary><strong>Graph Attention with Edge Features</strong></summary>
<p>ã‚¨ãƒƒã‚¸ç‰¹å¾´ã‚’è€ƒæ…®ã—ãŸAttentionæ©Ÿæ§‹ã€‚Attentionè¨ˆç®—æ™‚ã«ã‚¨ãƒƒã‚¸ã®é‡ã¿ã‚„å±æ€§ã‚’çµ„ã¿è¾¼ã‚€ã€‚ minuteså­ã‚°ãƒ©ãƒ•ã€äº¤é€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ã«æœ‰åŠ¹ã€‚</p>
</details>

<details>
<summary><strong>Sparse Attention Mechanisms</strong></summary>
<p>è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã®ã‚¹ãƒ‘ãƒ¼ã‚¹Attentionã€‚å±€æ‰€çš„ãªAttentionã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®Attentionã€‚å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å‘ä¸Šã€‚</p>
</details>

<details>
<summary><strong>Graph U-Nets</strong></summary>
<p>ç”»åƒã®U-Netã‚’ã‚°ãƒ©ãƒ•ã«é©ç”¨ã€‚ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚¢ãƒ³ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹éšå±¤çš„è¡¨ç¾å­¦ç¿’ã€‚ã‚°ãƒ©ãƒ• classificationã€ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«æœ‰åŠ¹ã€‚</p>
</details>

<details>
<summary><strong>Dynamic Graph Neural Networks</strong></summary>
<p> hourså¤‰åŒ–ã™ã‚‹ã‚°ãƒ©ãƒ•ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€‚æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã€å‹•çš„ãªãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸ã®è¿½åŠ å‰Šé™¤ã€‚ã‚½ãƒ¼ã‚·ãƒ£ãƒ«Network Analysisã€äº¤é€šäºˆæ¸¬ã«å¿œç”¨ã€‚</p>
</details>

<h3>Exercises</h3>

<div class="project-box">
<h4>æ¼”ç¿’ 4.1: Multi-head Attentionã® minutesæ</h4>
<p><strong>èª²é¡Œ</strong>: ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆ1, 2, 4, 8, 16ï¼‰ã§GATã‚’è¨“ç·´ã—ã€æ€§èƒ½ã¨è¨ˆç®— hoursã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>è©•ä¾¡é …ç›®</strong>: ç²¾åº¦ã€è¨“ç·´ hoursã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€å„ãƒ˜ãƒƒãƒ‰ã®Attentioné‡ã¿ã®å¯è¦–åŒ–</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.2: Attentioné‡ã¿ã®è§£é‡ˆ</h4>
<p><strong>èª²é¡Œ</strong>: è¨“ç·´æ¸ˆã¿GATã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–ã—ã€ã©ã®ãƒãƒ¼ãƒ‰ãŒé‡è¦è¦–ã•ã‚Œã¦ã„ã‚‹ã‹ minutesæã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…å†…å®¹</strong>:</p>
<ul>
<li>ç‰¹å®šãƒãƒ¼ãƒ‰ã®Attentioné‡ã¿æŠ½å‡º</li>
<li>Heatmapã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã§ã®å¯è¦–åŒ–</li>
<li>é‡è¦ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´ minutesæ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.3: GCN vs GAT vs GraphSAGEã®æ¯”è¼ƒ</h4>
<p><strong>èª²é¡Œ</strong>: Coraã€Citeseerã€PubMedã®3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€GCNã€GATã€GraphSAGEã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ¯”è¼ƒé …ç›®</strong>: ç²¾åº¦ã€è¨“ç·´ hoursã€åæŸé€Ÿåº¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡</p>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.4: ç‹¬è‡ªã®GATãƒ¬ã‚¤ãƒ¤ãƒ¼å®Ÿè£…</h4>
<p><strong>èª²é¡Œ</strong>: ã‚¨ãƒƒã‚¸ç‰¹å¾´ã‚’è€ƒæ…®ã—ãŸGATãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…è¦ä»¶</strong>:</p>
<ul>
<li>ã‚¨ãƒƒã‚¸ç‰¹å¾´ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>Attentionè¨ˆç®—ã¸ã®ã‚¨ãƒƒã‚¸ç‰¹å¾´ã®çµ„ã¿è¾¼ã¿</li>
<li> minuteså­ã‚°ãƒ©ãƒ•ãªã©ã§ã®è©•ä¾¡</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.5: Graph Transformerã®å®Ÿè£…ã¨è©•ä¾¡</h4>
<p><strong>èª²é¡Œ</strong>: Graph Transformerã‚’å®Ÿè£…ã—ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®åŠ¹æœã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿé¨“å†…å®¹</strong>:</p>
<ul>
<li>Laplacianå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>æœ€çŸ­è·é›¢ãƒ™ãƒ¼ã‚¹ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°</li>
<li>ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ã¨ã®æ¯”è¼ƒ</li>
</ul>
</div>

<div class="project-box">
<h4>æ¼”ç¿’ 4.6: å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã§ã®GATã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h4>
<p><strong>èª²é¡Œ</strong>: ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ï¼ˆ100ä¸‡ãƒãƒ¼ãƒ‰ä»¥ä¸Šï¼‰ã§GATã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>å®Ÿè£…é …ç›®</strong>:</p>
<ul>
<li>NeighborSamplerã®å®Ÿè£…</li>
<li>ãƒŸãƒ‹ãƒãƒƒãƒè¨“ç·´ãƒ«ãƒ¼ãƒ—</li>
<li>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã® minutesæ</li>
</ul>
</div>

<hr>

<h3>æ¬¡ Chapteräºˆå‘Š</h3>

<p>Chapter 5 Chapterã§ã¯ã€<strong>Graph Pooling and Hierarchical GNNs</strong>ã‚’å­¦ã³ã¾ã™ã€‚ã‚°ãƒ©ãƒ•å…¨ä½“ã®è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã¨ã€éšå±¤çš„ãªã‚°ãƒ©ãƒ•è¡¨ç¾å­¦ç¿’ã‚’æ¢ã‚Šã¾ã™ã€‚</p>

<blockquote>
<p><strong>æ¬¡ Chapterã®ãƒˆãƒ”ãƒƒã‚¯</strong>:<br>
ãƒ»Graph Poolingæ‰‹æ³•ï¼ˆGlobal Poolingã€DiffPoolã€TopKPoolingï¼‰<br>
ãƒ»éšå±¤çš„Graph Neural Networks<br>
ãƒ»Graph U-Netsã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£<br>
ãƒ»ã‚°ãƒ©ãƒ• classificationã‚¿ã‚¹ã‚¯<br>
ãƒ» minuteså­ç‰¹æ€§äºˆæ¸¬<br>
ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ©Ÿèƒ½äºˆæ¸¬<br>
ãƒ»å®Ÿè£…ï¼šã‚°ãƒ©ãƒ• classificationãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-message-passing.html" class="nav-button">â† Chapter 3 Chapter: Message Passing Neural Networks</a>
            <a href="chapter5-graph-pooling.html" class="nav-button">Chapter 5 Chapter: Graph Pooling and Hierarchical GNNs â†’</a>
        </div>

    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>Chapter 4 Chapterï¼šGraph Attention Networks (GAT) | Graph Neural Networkså…¥é–€ Series</p>
    </footer>

</body>
</html>