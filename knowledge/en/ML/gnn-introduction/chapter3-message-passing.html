<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Message Passing and GNN - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/gnn-introduction/index.html">Gnn</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 3: Message Passing and GNN</h1> <p class="subtitle">Generalized GNN Framework - GraphSAGE, GIN, PyTorch Geometric Implementation</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the basic structure of the message passing framework (Message, Aggregate, Update)</li>
<li>‚úÖ Master the mathematical formulation of generalized GNN (MPNN)</li>
<li>‚úÖ Implement GraphSAGE's sampling-based aggregation</li>
<li>‚úÖ Understand the characteristics of various aggregators (Mean, Pool, LSTM)</li>
<li>‚úÖ Understand the relationship between GIN (Graph Isomorphism Network) and WL test</li>
<li>‚úÖ Evaluate the expressive power of GNNs</li>
<li>‚úÖ Master efficient implementation methods with PyTorch Geometric</li>
<li>‚úÖ Implement graph classification tasks and batch processing</li>
</ul>
<hr/>
<h2>3.1 Message Passing Framework</h2>
<h3>Concept of Message Passing</h3>
<p><strong>Message Passing</strong> is a framework that describes information propagation in GNNs in a unified manner. It updates features by sending and receiving messages between nodes and aggregating them.</p>
<blockquote>
<p>"The message passing framework provides a unified way to describe any GNN architecture with three basic operations (Message, Aggregate, Update)"</p>
</blockquote>
<h3>Three Basic Operations</h3>
<p>Message passing consists of the following three steps:</p>
<div class="mermaid">
graph LR
    A[1. Message<br/>Message generation] --&gt; B[2. Aggregate<br/>Message aggregation]
 B --&gt;C[3. Update<br/>Featureupdate] 
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
</div>
<h4>Step 1: Message (Message Generation)</h4>
<p>Generate messages to be sent from neighboring nodes to the center node:</p>
<p>$$
\mathbf{m}_{j \to i}^{(k)} = \text{MESSAGE}^{(k)}\left(\mathbf{h}_i^{(k-1)}, \mathbf{h}_j^{(k-1)}, \mathbf{e}_{ji}\right)
$$</p>
<p>Where:</p>
<ul>
<li>$\mathbf{m}_{j \to i}^{(k)}$: Message from node $j$ to node $i$</li>
<li>$\mathbf{h}_i^{(k-1)}$: Previous layer features of receiving node $i$</li>
<li>$\mathbf{h}_j^{(k-1)}$: Previous layer features of sending node $j$</li>
<li>$\mathbf{e}_{ji}$: Edge $(j, i)$ features (optional)</li>
</ul>
<h4>Step 2: Aggregate (Message Aggregation)</h4>
<p>Aggregate all received messages:</p>
<p>$$
\mathbf{m}_i^{(k)} = \text{AGGREGATE}^{(k)}\left(\left\{\mathbf{m}_{j \to i}^{(k)} : j \in \mathcal{N}(i)\right\}\right)
$$</p>
<p>Representative aggregation functions:</p>
<ul>
<li><strong>Sum</strong>: $\text{AGGREGATE} = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
<li><strong>Mean</strong>: $\text{AGGREGATE} = \frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
<li><strong>Max</strong>: $\text{AGGREGATE} = \max_{j \in \mathcal{N}(i)} \mathbf{m}_{j \to i}$</li>
</ul>
<h4>Step 3: Update (Feature Update)</h4>
<p>Update features by combining aggregated messages with own information:</p>
<p>$$
\mathbf{h}_i^{(k)} = \text{UPDATE}^{(k)}\left(\mathbf{h}_i^{(k-1)}, \mathbf{m}_i^{(k)}\right)
$$</p>
<h3>Visualization of Message Passing</h3>
<div class="mermaid">
graph TB
    subgraph "Step1: Message"
        N1[Node v] --&gt; M1[m<sub>1‚Üív</sub>]
        N2[Node 1] --&gt; M1
        N3[Node 2] --&gt; M2[m<sub>2‚Üív</sub>]
        N4[Node 3] --&gt; M3[m<sub>3‚Üív</sub>]
    end

    subgraph "Step2: Aggregate"
        M1 --&gt; AGG[Œ£ / Mean / Max]
        M2 --&gt; AGG
        M3 --&gt; AGG
 AGG --&gt;AM[aggregationmessage]     end

    subgraph "Step3: Update"
        N1 --&gt; UPD[UPDATE Function]
        AM --&gt; UPD
        UPD --&gt; H[h<sub>v</sub><sup>(k)</sup>]
    end

    style M1 fill:#e3f2fd
    style M2 fill:#e3f2fd
    style M3 fill:#e3f2fd
    style AGG fill:#fff3e0
    style UPD fill:#e8f5e9
    style H fill:#c8e6c9
</div>
<h3>Implementation Example 1: Basic Message Passing Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("=== Message Passing Framework Basic Implementation ===\n")

class MessagePassingLayer(nn.Module):
    """Basic message passing layer"""

    def __init__(self, in_dim, out_dim, aggr='mean'):
        super(MessagePassingLayer, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.aggr = aggr

 # MessagefunctionÔºàLinear transformationÔºâ         self.message_nn = nn.Linear(in_dim, out_dim)

 # UpdatefunctionÔºàLinear transformation + activationÔºâ         self.update_nn = nn.Sequential(
            nn.Linear(in_dim + out_dim, out_dim),
            nn.ReLU()
        )

    def message(self, h_j):
        """Message generation"""
        return self.message_nn(h_j)

    def aggregate(self, messages, edge_index, num_nodes):
        """Message aggregation"""
 # edge_index[1]: receiving nodeindex         target_nodes = edge_index[1]

 # eachNode to messageaggregation         aggregated = torch.zeros(num_nodes, self.out_dim)

        if self.aggr == 'sum':
            aggregated.index_add_(0, target_nodes, messages)
        elif self.aggr == 'mean':
            aggregated.index_add_(0, target_nodes, messages)
            # Normalize by degree
            degree = torch.bincount(target_nodes, minlength=num_nodes).float()
            degree = degree.clamp(min=1).view(-1, 1)
            aggregated = aggregated / degree
        elif self.aggr == 'max':
            # Max pooling
            for i in range(num_nodes):
                mask = (target_nodes == i)
                if mask.any():
                    aggregated[i] = messages[mask].max(dim=0)[0]

        return aggregated

    def update(self, h_i, aggregated):
 """Featureupdate"""         combined = torch.cat([h_i, aggregated], dim=-1)
        return self.update_nn(combined)

    def forward(self, x, edge_index):
        """
        Args:
            x: NodeFeature [num_nodes, in_dim]
            edge_index: Edge index [2, num_edges]
        """
        num_nodes = x.size(0)

        # Step 1: Message
 # edge_index[0]: sending node  h_j = x[edge_index[0]] # sending nodeFeature         messages = self.message(h_j)

        # Step 2: Aggregate
        aggregated = self.aggregate(messages, edge_index, num_nodes)

        # Step 3: Update
        h_new = self.update(x, aggregated)

        return h_new


# test execution print("--- Creating Test Graph ---")
# 5NodeGraph num_nodes = 5
in_dim = 4
out_dim = 8

# NodeFeatureÔºàrandomly initializedÔºâ x = torch.randn(num_nodes, in_dim)
print(f"Node feature shape: {x.shape}")

# edge listÔºà0‚Üí1, 1‚Üí2, 2‚Üí3, 3‚Üí4, 1‚Üí3Ôºâ edge_index = torch.tensor([
 [0, 1, 2, 3, 1], # sending node  [1, 2, 3, 4, 3] # receiving node ], dtype=torch.long)
print(f"Edge index shape: {edge_index.shape}")
print(f"Number of edges: {edge_index.size(1)}\n")

# creating and executing message passing layer print("--- Message Passing with Each Aggregation Method ---")
for aggr in ['sum', 'mean', 'max']:
    print(f"\n{aggr.upper()} Aggregation:")
    mp_layer = MessagePassingLayer(in_dim, out_dim, aggr=aggr)
    h_new = mp_layer(x, edge_index)
    print(f"  Output shape: {h_new.shape}")
    print(f"  Output value range: [{h_new.min():.3f}, {h_new.max():.3f}]")
    print(f"  Output examples for each node:")
    for i in range(min(3, num_nodes)):
        print(f"    Node{i}: mean={h_new[i].mean():.3f}, std={h_new[i].std():.3f}")
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>=== Message Passing Framework Basic Implementation ===

--- Creating Test Graph ---
Node feature shape: torch.Size([5, 4])
Edge index shape: torch.Size([2, 5])
Number of edges: 5

--- Message Passing with Each Aggregation Method ---

SUM Aggregation:
  Output shape: torch.Size([5, 8])
  Output value range: [-1.234, 2.456]
  Output examples for each node:
    Node0: mean=0.123, std=0.876
    Node1: mean=0.234, std=0.945
    Node2: mean=-0.089, std=0.823

MEAN Aggregation:
  Output shape: torch.Size([5, 8])
  Output value range: [-0.987, 1.876]
  Output examples for each node:
    Node0: mean=0.098, std=0.734
    Node1: mean=0.187, std=0.812
    Node2: mean=-0.045, std=0.698

MAX Aggregation:
  Output shape: torch.Size([5, 8])
  Output value range: [-0.756, 2.123]
  Output examples for each node:
    Node0: mean=0.156, std=0.923
    Node1: mean=0.267, std=1.012
    Node2: mean=0.034, std=0.876
</code></pre>
<h3>Generalized GNN (MPNN)</h3>
<p><strong>Message Passing Neural Network (MPNN)</strong> is a framework that describes many GNN architectures in a unified manner.</p>
<p>General form of MPNN:</p>
<p>$$
\begin{align}
\mathbf{m}_i^{(k+1)} &amp;= \sum_{j \in \mathcal{N}(i)} M_k\left(\mathbf{h}_i^{(k)}, \mathbf{h}_j^{(k)}, \mathbf{e}_{ji}\right) \\
\mathbf{h}_i^{(k+1)} &amp;= U_k\left(\mathbf{h}_i^{(k)}, \mathbf{m}_i^{(k+1)}\right)
\end{align}
$$</p>
<p>MPNN representation of representative GNNs:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MESSAGE Function $M_k$</th>
<th>UPDATE Function $U_k$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GCN</strong></td>
<td>$\frac{1}{\sqrt{d_i d_j}} \mathbf{W}^{(k)} \mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{m}_i^{(k+1)})$</td>
</tr>
<tr>
<td><strong>GraphSAGE</strong></td>
<td>$\mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{W} \cdot [\mathbf{h}_i^{(k)} \| \text{AGG}(\mathbf{m}_i^{(k+1)})])$</td>
</tr>
<tr>
<td><strong>GAT</strong></td>
<td>$\alpha_{ij} \mathbf{W} \mathbf{h}_j^{(k)}$</td>
<td>$\sigma(\mathbf{m}_i^{(k+1)})$</td>
</tr>
<tr>
<td><strong>GIN</strong></td>
<td>$\mathbf{h}_j^{(k)}$</td>
<td>$\text{MLP}((1+\epsilon) \mathbf{h}_i^{(k)} + \mathbf{m}_i^{(k+1)})$</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.2 GraphSAGE</h2>
<h3>Overview of GraphSAGE</h3>
<p><strong>GraphSAGE (SAmple and aggreGatE)</strong> is a sampling-based GNN for large-scale graphs. Instead of using all neighbors, it samples and aggregates a fixed number of neighbors.</p>
<blockquote>
<p>"GraphSAGE enables mini-batch learning by sampling neighbors, achieving scalability to large-scale graphs"</p>
</blockquote>
<h3>Sampling-Based Aggregation</h3>
<p>Features of GraphSAGE:</p>
<ol>
<li><strong>Neighbor sampling</strong>Ôºörandomly sampling fixed number of neighbors from each node</li><li><strong>Various Aggregators</strong>: Aggregation functions such as Mean, Pool, LSTM</li>
<li><strong>Inductive learning</strong>Ôºöcan apply to nodes not seen during training</li></ol>
<div class="mermaid">
graph TB
    subgraph "Standard GNN (All Neighbors)"
 V1[centerNode] --&gt;N1[Neighbor1]         V1 --&gt; N2[Neighbor2]
        V1 --&gt; N3[Neighbor3]
        V1 --&gt; N4[Neighbor4]
        V1 --&gt; N5[Neighbor5]
        V1 --&gt; N6[Neighbor6]
    end

    subgraph "GraphSAGE (Sampling)"
 V2[centerNode] --&gt;S1[Sample1]         V2 --&gt; S2[Sample2]
        V2 --&gt; S3[Sample3]
        N7[Neighbor4] -.x.- V2
        N8[Neighbor5] -.x.- V2
        N9[Neighbor6] -.x.- V2
    end

    style V1 fill:#fff3e0
    style V2 fill:#fff3e0
    style S1 fill:#e3f2fd
    style S2 fill:#e3f2fd
    style S3 fill:#e3f2fd
</div>
<h3>GraphSAGE Algorithm</h3>
<p>Update equations for GraphSAGE:</p>
<p>$$
\begin{align}
\mathbf{h}_{\mathcal{N}(i)}^{(k)} &amp;= \text{AGGREGATE}_k\left(\left\{\mathbf{h}_j^{(k-1)}, \forall j \in \mathcal{S}_{\mathcal{N}(i)}\right\}\right) \\
\mathbf{h}_i^{(k)} &amp;= \sigma\left(\mathbf{W}^{(k)} \cdot \left[\mathbf{h}_i^{(k-1)} \| \mathbf{h}_{\mathcal{N}(i)}^{(k)}\right]\right) \\
\mathbf{h}_i^{(k)} &amp;= \frac{\mathbf{h}_i^{(k)}}{\|\mathbf{h}_i^{(k)}\|_2}
\end{align}
$$</p>
<p>Where:</p>
<ul>
<li>$\mathcal{S}_{\mathcal{N}(i)}$ÔºöNode$i$Neighbor from samplingpartset</li><li>$\|$: Feature concatenation</li>
<li>Final line: L2 normalization</li>
</ul>
<h3>Various Aggregators</h3>
<h4>1. Mean Aggregator</h4>
<p>$$
\text{AGGREGATE}_{\text{mean}} = \frac{1}{|\mathcal{S}_{\mathcal{N}(i)}|} \sum_{j \in \mathcal{S}_{\mathcal{N}(i)}} \mathbf{h}_j^{(k-1)}
$$</p>
<p>Feature: Simple and efficient, behaves similar to GCN</p>
<h4>2. Pool Aggregator</h4>
<p>$$
\text{AGGREGATE}_{\text{pool}} = \max\left(\left\{\sigma\left(\mathbf{W}_{\text{pool}} \mathbf{h}_j^{(k-1)} + \mathbf{b}\right), \forall j \in \mathcal{S}_{\mathcal{N}(i)}\right\}\right)
$$</p>
<p>Feature: Element-wise max-pooling, captures asymmetric neighbor information</p>
<h4>3. LSTM Aggregator</h4>
<p>$$
\text{AGGREGATE}_{\text{LSTM}} = \text{LSTM}\left(\left[\mathbf{h}_j^{(k-1)}, \forall j \in \pi(\mathcal{S}_{\mathcal{N}(i)})\right]\right)
$$</p>
<p>Where $\pi$ is a random permutation. Feature: High expressive power but requires attention to permutation dependency</p>
<h3>Implementation Example 2: GraphSAGE Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("\n=== GraphSAGE Implementation ===\n")

class SAGEConv(nn.Module):
    """GraphSAGE layer"""

    def __init__(self, in_dim, out_dim, aggr='mean'):
        super(SAGEConv, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.aggr = aggr

 # Linear transformationÔºàownFeature + NeighborFeatureconcatenationafterÔºâ         if aggr == 'lstm':
            self.lstm = nn.LSTM(in_dim, in_dim, batch_first=True)
            self.lin = nn.Linear(2 * in_dim, out_dim)
        elif aggr == 'pool':
            self.pool_nn = nn.Linear(in_dim, in_dim)
            self.lin = nn.Linear(2 * in_dim, out_dim)
        else:  # mean
            self.lin = nn.Linear(2 * in_dim, out_dim)

    def aggregate_mean(self, h_neighbors, edge_index, num_nodes):
        """Mean aggregation"""
        target_nodes = edge_index[1]
        aggregated = torch.zeros(num_nodes, self.in_dim)

        aggregated.index_add_(0, target_nodes, h_neighbors)
        degree = torch.bincount(target_nodes, minlength=num_nodes).float()
        degree = degree.clamp(min=1).view(-1, 1)

        return aggregated / degree

    def aggregate_pool(self, h_neighbors, edge_index, num_nodes):
        """Max-pooling aggregation"""
        target_nodes = edge_index[1]

 # eachNeighborFeaturetransformation         transformed = torch.relu(self.pool_nn(h_neighbors))

        # Max-pooling
        aggregated = torch.zeros(num_nodes, self.in_dim)
        for i in range(num_nodes):
            mask = (target_nodes == i)
            if mask.any():
                aggregated[i] = transformed[mask].max(dim=0)[0]

        return aggregated

    def aggregate_lstm(self, h_neighbors, edge_index, num_nodes):
        """LSTM aggregation"""
        target_nodes = edge_index[1]
        aggregated = torch.zeros(num_nodes, self.in_dim)

        for i in range(num_nodes):
            mask = (target_nodes == i)
            if mask.any():
 # input to LSTM in random order                 neighbors = h_neighbors[mask]
                perm = torch.randperm(neighbors.size(0))
                neighbors = neighbors[perm].unsqueeze(0)

                _, (h_n, _) = self.lstm(neighbors)
                aggregated[i] = h_n.squeeze(0)

        return aggregated

    def forward(self, x, edge_index):
        num_nodes = x.size(0)

 # Get neighbor features         h_neighbors = x[edge_index[0]]

 # aggregation         if self.aggr == 'mean':
            h_neigh = self.aggregate_mean(h_neighbors, edge_index, num_nodes)
        elif self.aggr == 'pool':
            h_neigh = self.aggregate_pool(h_neighbors, edge_index, num_nodes)
        elif self.aggr == 'lstm':
            h_neigh = self.aggregate_lstm(h_neighbors, edge_index, num_nodes)

 # ownFeature and concatenation         h_concat = torch.cat([x, h_neigh], dim=-1)

        # Linear transformation
        out = self.lin(h_concat)

 # L2normalization         out = F.normalize(out, p=2, dim=-1)

        return out


class GraphSAGE(nn.Module):
 """GraphSAGEModelÔºà2layerÔºâ""" 
    def __init__(self, in_dim, hidden_dim, out_dim, aggr='mean'):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_dim, hidden_dim, aggr)
        self.conv2 = SAGEConv(hidden_dim, out_dim, aggr)

    def forward(self, x, edge_index):
        # Layer 1
        h = self.conv1(x, edge_index)
        h = F.relu(h)
        h = F.dropout(h, p=0.5, training=self.training)

        # Layer 2
        h = self.conv2(h, edge_index)

        return h


# test execution print("--- Creating GraphSAGE Model ---") num_nodes = 10
in_dim = 8
hidden_dim = 16
out_dim = 4

x = torch.randn(num_nodes, in_dim)
edge_index = torch.tensor([
    [0, 1, 2, 3, 4, 1, 2, 5, 6, 7],
    [1, 2, 3, 4, 5, 0, 1, 6, 7, 8]
], dtype=torch.long)

print(f"Number of nodes: {num_nodes}") print(f"Input dimension: {in_dim}")
print(f"Hidden layer dimension: {hidden_dim}")
print(f"Output dimension: {out_dim}\n")

# eachAggregatortest for aggr in ['mean', 'pool', 'lstm']:
    print(f"--- {aggr.upper()} Aggregator ---")
    model = GraphSAGE(in_dim, hidden_dim, out_dim, aggr=aggr)
    model.eval()

    with torch.no_grad():
        out = model(x, edge_index)

    print(f"Output shape: {out.shape}")
    print(f"Output L2 norm: {out.norm(dim=-1)[:5].numpy()}")
    print(f"Output value range: [{out.min():.3f}, {out.max():.3f}]\n")
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== GraphSAGE Implementation ===

--- Creating GraphSAGE Model --- Number of nodes: 10 Input dimension: 8
Hidden layer dimension: 16
Output dimension: 4

--- MEAN Aggregator ---
Output shape: torch.Size([10, 4])
Output L2 norm: [1. 1. 1. 1. 1.]
Output value range: [-0.876, 0.923]

--- POOL Aggregator ---
Output shape: torch.Size([10, 4])
Output L2 norm: [1. 1. 1. 1. 1.]
Output value range: [-0.845, 0.891]

--- LSTM Aggregator ---
Output shape: torch.Size([10, 4])
Output L2 norm: [1. 1. 1. 1. 1.]
Output value range: [-0.912, 0.867]
</code></pre>
<hr/>
<h2>3.3 Graph Isomorphism Network (GIN)</h2>
<h3>Motivation for GIN: Improving Discriminative Power</h3>
<p><strong>Graph Isomorphism Network (GIN)</strong> is a GNN designed to have discriminative power equivalent to the Weisfeiler-Lehman (WL) test.</p>
<blockquote>
<p>"GIN has the maximum discriminative power theoretically achievable by GNNs. That is, graphs that GIN cannot distinguish cannot be distinguished by the WL test either"</p>
</blockquote>
<h3>Weisfeiler-Lehman (WL) Test</h3>
<p><strong>WL test</strong> is a heuristic algorithm for determining graph isomorphism. It can efficiently determine graph isomorphism in many cases.</p>
<p>WL test algorithm:</p>
<ol>
<li>assign initial labels to each node</li><li>for each node label, update with own label and neighbor labels multiple set</li><li>Hash the labels to create new labels</li>
<li>Repeat until convergence</li>
</ol>
<div class="mermaid">
graph TB
    subgraph "Iteration1"
        A1[1] --- B1[1]
        A1 --- C1[1]
        B1 --- C1
    end

    subgraph "Iteration2"
        A2[2] --- B2[3]
        A2 --- C2[3]
        B2 --- C2[2]
    end

    subgraph "Iteration3"
        A3[4] --- B3[5]
        A3 --- C3[5]
        B3 --- C3[4]
    end

    A1 --&gt; A2 --&gt; A3
    B1 --&gt; B2 --&gt; B3
    C1 --&gt; C2 --&gt; C3

    style A1 fill:#e3f2fd
    style A2 fill:#fff3e0
    style A3 fill:#e8f5e9
</div>
<h3>Formulation of GIN</h3>
<p>Update equation for GIN:</p>
<p>$$
\mathbf{h}_i^{(k)} = \text{MLP}^{(k)}\left(\left(1 + \epsilon^{(k)}\right) \cdot \mathbf{h}_i^{(k-1)} + \sum_{j \in \mathcal{N}(i)} \mathbf{h}_j^{(k-1)}\right)
$$</p>
<p>Important points:</p>
<ul>
<li><strong>Sum aggregation</strong>: The only injective aggregation function that can preserve multisets</li>
<li><strong>$(1 + \epsilon)$ coefficient</strong>: Distinguishes between own features and neighbor features</li>
<li><strong>MLP</strong>: Update function with sufficient expressive power</li>
</ul>
<h3>Why GIN Has the Highest Discriminative Power</h3>
<p>The discriminative power of GNNs has the following order:</p>
<p>$$
\text{Sum} &gt; \text{Mean} &gt; \text{Max}
$$</p>
<table>
<thead>
<tr>
<th>Aggregation Function</th>
<th>Multiset Preservation</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sum</strong></td>
<td>‚úÖ Injective (preserves multiplicity)</td>
<td>$\{1, 1, 2\} \to 4 \neq 3 \leftarrow \{1, 2\}$</td>
</tr>
<tr>
<td><strong>Mean</strong></td>
<td>‚ùå Information loss</td>
<td>$\{1, 1, 2\} \to 1.33 \neq 1.5 \leftarrow \{1, 2\}$</td>
</tr>
<tr>
<td><strong>Max</strong></td>
<td>‚ùå Preserves only maximum value</td>
<td>$\{1, 1, 2\} \to 2 = 2 \leftarrow \{1, 2\}$ ‚ö†Ô∏è</td>
</tr>
</tbody>
</table>
<h3>Implementation Example3: GINimplementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

print("\n=== Graph Isomorphism Network (GIN) Implementation ===\n")

class GINConv(nn.Module):
    """GIN layer"""

    def __init__(self, in_dim, out_dim, epsilon=0.0, train_eps=False):
        super(GINConv, self).__init__()

 # EpsilonÔºàlearnable optionÔºâ         if train_eps:
            self.epsilon = nn.Parameter(torch.Tensor([epsilon]))
        else:
            self.register_buffer('epsilon', torch.Tensor([epsilon]))

 # MLP (2layer)         self.mlp = nn.Sequential(
            nn.Linear(in_dim, 2 * out_dim),
            nn.BatchNorm1d(2 * out_dim),
            nn.ReLU(),
            nn.Linear(2 * out_dim, out_dim)
        )

    def forward(self, x, edge_index):
        num_nodes = x.size(0)

 # Sum aggregation         h_neighbors = x[edge_index[0]]
        target_nodes = edge_index[1]

        aggregated = torch.zeros_like(x)
        aggregated.index_add_(0, target_nodes, h_neighbors)

        # (1 + epsilon) * h_i + sum(h_j)
        out = (1 + self.epsilon) * x + aggregated

 # MLPapply         out = self.mlp(out)

        return out


class GIN(nn.Module):
 """GINModelÔºàGraphclassificationforÔºâ""" 
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3,
                 dropout=0.5, train_eps=False):
        super(GIN, self).__init__()

        self.num_layers = num_layers
        self.dropout = dropout

        # GIN layer
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # Layer 1
        self.convs.append(GINConv(in_dim, hidden_dim, train_eps=train_eps))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # Middle layers
        for _ in range(num_layers - 2):
            self.convs.append(GINConv(hidden_dim, hidden_dim, train_eps=train_eps))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # Final layer
        self.convs.append(GINConv(hidden_dim, hidden_dim, train_eps=train_eps))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

 # Graphlevelclassificationfor         self.graph_pred_linear = nn.Linear(hidden_dim, out_dim)

    def forward(self, x, edge_index, batch=None):
 # Nodelevelupdate         h = x
        for i in range(self.num_layers):
            h = self.convs[i](h, edge_index)
            h = self.batch_norms[i](h)
            h = F.relu(h)
            h = F.dropout(h, p=self.dropout, training=self.training)

 # Graph-level poolingÔºàmeanÔºâ         if batch is None:
 # singleGraphcase             h_graph = h.mean(dim=0, keepdim=True)
        else:
 # batchGraphcase             num_graphs = batch.max().item() + 1
            h_graph = torch.zeros(num_graphs, h.size(1))
            for i in range(num_graphs):
                mask = (batch == i)
                h_graph[i] = h[mask].mean(dim=0)

 # classification         out = self.graph_pred_linear(h_graph)

        return out


# test execution print("--- Creating GIN Model ---") in_dim = 10
hidden_dim = 32
out_dim = 5 # 5Classclassification num_layers = 3

model = GIN(in_dim, hidden_dim, out_dim, num_layers, train_eps=True)
print(f"Modelstructure:\n{model}\n") 
# singleGraphtest num_nodes = 20
x = torch.randn(num_nodes, in_dim)
edge_index = torch.randint(0, num_nodes, (2, 50))

print("--- Inference on Single Graph ---")
model.eval()
with torch.no_grad():
    out = model(x, edge_index)

print(f"Input number of nodes: {num_nodes}") print(f"Input feature dimension: {in_dim}")
print(f"Output shape: {out.shape}")
print(f"Output (logits): {out[0].numpy()}\n")

# batchGraphtest print("--- Inference on Batch Graphs ---")
# 3 graphsbatch processing x_batch = torch.randn(50, in_dim) # sum50Node edge_index_batch = torch.randint(0, 50, (2, 100))
batch = torch.tensor([0]*15 + [1]*20 + [2]*15)  # Graph1: 15Node, Graph2: 20Node, Graph3: 15Node

with torch.no_grad():
    out_batch = model(x_batch, edge_index_batch, batch)

print(f"Batch size: 3")
print(f"Total number of nodes: {x_batch.size(0)}") print(f"Output shape: {out_batch.shape}")
print(f"Predictions for each graph:")
for i in range(3):
    pred_class = out_batch[i].argmax().item()
    print(f"  Graph{i+1}: Class {pred_class} (score={out_batch[i, pred_class]:.3f})")
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== Graph Isomorphism Network (GIN) Implementation ===

--- Creating GIN Model --- Modelstructure: GIN(
  (convs): ModuleList(
    (0-2): 3 x GINConv(...)
  )
  (batch_norms): ModuleList(
    (0-2): 3 x BatchNorm1d(32, eps=1e-05, momentum=0.1)
  )
  (graph_pred_linear): Linear(in_features=32, out_features=5, bias=True)
)

--- Inference on Single Graph ---
Input number of nodes: 20 Input feature dimension: 10
Output shape: torch.Size([1, 5])
Output (logits): [-0.234  0.567  0.123 -0.456  0.891]

--- Inference on Batch Graphs ---
Batch size: 3
Total number of nodes: 50 Output shape: torch.Size([3, 5])
Predictions for each graph:
  Graph1: Class 4 (score=0.723)
  Graph2: Class 1 (score=0.845)
  Graph3: Class 3 (score=0.612)
</code></pre>
<h3>Comparing Discriminative Power of GIN and GCN</h3>
<p>following examples show graphs that GIN and GCN can distinguishÔºö</p>
<div class="mermaid">
graph LR
    subgraph "GraphA"
        A1((1)) --- A2((2))
        A2 --- A3((3))
        A3 --- A1
    end

    subgraph "GraphB"
        B1((1)) --- B2((2))
        B2 --- B3((3))
        B3 --- B4((4))
        B4 --- B1
    end

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style B4 fill:#fff3e0
</div>
<p>Results:</p>
<ul>
<li><strong>GIN</strong>Ôºö‚úÖ can distinguish Graph A and BÔºàNumber of nodesdifferentÔºâ</li><li><strong>GCN (Mean aggregation)</strong>Ôºö‚úÖ can distinguish Graph A and B</li></ul>
<p>more difficult examplesÔºàsame number of nodes, same degree distributionÔºâÔºö</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Discriminative Power</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GIN</strong></td>
<td>Equivalent to WL test</td>
<td>Sum aggregation + MLP preserves multisets</td>
</tr>
<tr>
<td><strong>GCN</strong></td>
<td>Weaker than WL test</td>
<td>Mean aggregation loses multiplicity information</td>
</tr>
<tr>
<td><strong>GAT</strong></td>
<td>Weaker than WL test</td>
<td>Information is smoothed by attention weights</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.4 Implementation with PyTorch Geometric</h2>
<h3>What is PyTorch Geometric (PyG)</h3>
<p><strong>PyTorch Geometric</strong>„ÄÅPyTorch library specialized for Graph Neural Networks„ÄÇefficient message passing„ÄÅrich pre-implemented layers„ÄÅprovides data loaders„ÄÇ</p>
<h3>Key Components of PyG</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>torch_geometric.data.Data</strong></td>
<td>Graphdatastructure</td><td><code>Data(x, edge_index)</code></td>
</tr>
<tr>
<td><strong>torch_geometric.nn.MessagePassing</strong></td>
<td>Message passing base class</td><td>Custom GNN layer implementation</td>
</tr>
<tr>
<td><strong>torch_geometric.nn.*Conv</strong></td>
<td>Pre-implemented GNN layers</td>
<td><code>GCNConv, SAGEConv, GINConv</code></td>
</tr>
<tr>
<td><strong>torch_geometric.datasets</strong></td>
<td>Benchmark datasets</td>
<td><code>Cora, MUTAG, QM9</code></td>
</tr>
<tr>
<td><strong>torch_geometric.loader.DataLoader</strong></td>
<td>Graphbatch processing</td><td>Mini-batch learning</td>
</tr>
</tbody>
</table>
<h3>Implementation Example4: PyGcustomGNN layer</h3>
<pre><code class="language-python"># Note: This example should be executed in an environment with PyTorch Geometric installed # pip install torch-geometric

print("\n=== PyTorch Geometric Custom GNN Layer ===\n")

# PyG imports (demo pseudo-code)
# from torch_geometric.nn import MessagePassing
# from torch_geometric.utils import add_self_loops, degree

# Pseudo-code for custom layer using MessagePassing base class class CustomGNNLayer:
    """
 Example of custom GNN layer inheriting from PyG MessagePassing 
 Override the following methods of MessagePassing classÔºö     - message(): Message generation
    - aggregate(): Message aggregation
 - update(): Nodeupdate     """

    def __init__(self, in_channels, out_channels):
        # super(CustomGNNLayer, self).__init__(aggr='add')
        self.in_channels = in_channels
        self.out_channels = out_channels
        # self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        """
        Args:
            x: [num_nodes, in_channels]
            edge_index: [2, num_edges]
        """
        # 1. Linear transformation
        # x = self.lin(x)

        # 2. Add self-loops
        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

 # 3. normalizationÔºàNormalize by degreeÔºâ         # row, col = edge_index
        # deg = degree(col, x.size(0), dtype=x.dtype)
        # deg_inv_sqrt = deg.pow(-0.5)
        # norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # 4. Start message passing
        # return self.propagate(edge_index, x=x, norm=norm)
        pass

    def message(self, x_j, norm):
        """
        Message generation

        Args:
 x_j: sending nodeFeature [num_edges, out_channels]             norm: Normalization coefficients [num_edges]
        """
        # return norm.view(-1, 1) * x_j
        pass

    def aggregate(self, inputs, index):
        """
 Message aggregationÔºàdefault 'add', no override neededÔºâ         """
        # return torch_scatter.scatter(inputs, index, dim=0, reduce='add')
        pass

    def update(self, aggr_out):
        """
 Nodeupdate 
        Args:
            aggr_out: Aggregated messages [num_nodes, out_channels]
        """
        # return aggr_out
        pass

print("--- PyG MessagePassingClassstructure ---") print("""
PyG's MessagePassing allows you to implement GNN layers as follows:

1. __init__: aggr='add'/'mean'/'max'Specify
2. forward: propagate()Start message passing by calling
3. message: x_j (sending node) used for message generation 4. aggregate: Automatically executed (method specified by aggr)
5. update: Post-aggregation processing (optional)

Advantage:
‚úÖ Efficient sparse tensor operations
‚úÖ GPU-optimized aggregation operations
‚úÖ Automatic batch processing
""")

print("\n--- PyG Data Structure ---")
print("""
from torch_geometric.data import Data

# Creating graph edge_index = torch.tensor([[0, 1, 1, 2],
                          [1, 0, 2, 1]], dtype=torch.long)
x = torch.tensor([[-1], [0], [1]], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)

Attributes:
- data.x: NodeFeaturematrix [num_nodes, num_features] - data.edge_index: Edge index [2, num_edges]
- data.edge_attr: Edge features (optional)
- data.y: labelsÔºànode level or graph levelÔºâ - data.num_nodes: Number of nodes """)
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== PyTorch Geometric Custom GNN Layer ===

--- PyG MessagePassingClassstructure --- 
PyG's MessagePassing allows you to implement GNN layers as follows:

1. __init__: aggr='add'/'mean'/'max'Specify
2. forward: propagate()Start message passing by calling
3. message: x_j (sending node) used for message generation 4. aggregate: Automatically executed (method specified by aggr)
5. update: Post-aggregation processing (optional)

Advantage:
‚úÖ Efficient sparse tensor operations
‚úÖ GPU-optimized aggregation operations
‚úÖ Automatic batch processing


--- PyG Data Structure ---

from torch_geometric.data import Data

# Creating graph edge_index = torch.tensor([[0, 1, 1, 2],
                          [1, 0, 2, 1]], dtype=torch.long)
x = torch.tensor([[-1], [0], [1]], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)

Attributes:
- data.x: NodeFeaturematrix [num_nodes, num_features] - data.edge_index: Edge index [2, num_edges]
- data.edge_attr: Edge features (optional)
- data.y: labelsÔºànode level or graph levelÔºâ - data.num_nodes: Number of nodes </code></pre>
<h3>Implementation Example5: Model using PyG pre-implemented layers</h3>
<pre><code class="language-python">import torch
import torch.nn.functional as F

print("\n=== Model using PyG pre-implemented layersÔºàpseudo-codeÔºâ ===\n") 
# Complete model example using PyG pre-implemented layersÔºàpseudo-codeÔºâ class GNNModel:
    """
    from torch_geometric.nn import GCNConv, SAGEConv, GINConv
    from torch_geometric.nn import global_mean_pool, global_max_pool

    class GNNModel(torch.nn.Module):
        def __init__(self, num_features, num_classes):
            super(GNNModel, self).__init__()

 # GCNlayer             self.conv1 = GCNConv(num_features, 64)
            self.conv2 = GCNConv(64, 64)
            self.conv3 = GCNConv(64, 64)

 # Graphlevelclassificationfor             self.lin = torch.nn.Linear(64, num_classes)

        def forward(self, data):
            x, edge_index, batch = data.x, data.edge_index, data.batch

 # GCNlayerapply             x = self.conv1(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, training=self.training)

            x = self.conv2(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, training=self.training)

            x = self.conv3(x, edge_index)

 # Graph-level pooling             x = global_mean_pool(x, batch)

 # classification             x = self.lin(x)

            return F.log_softmax(x, dim=1)
    """
    pass

print("--- Main GNN Layers Available in PyG ---\n")

layers_info = {
    "GCNConv": {
        "Description": "Graph Convolutional Network layer",
 "aggregation": "Mean (Sum with degree normalization)",         "Usage": "GCNConv(in_channels, out_channels)"
    },
    "SAGEConv": {
        "Description": "GraphSAGE layer",
 "aggregation": "Mean / LSTM / Max-pool",         "Usage": "SAGEConv(in_channels, out_channels, aggr='mean')"
    },
    "GINConv": {
        "Description": "Graph Isomorphism Network layer",
 "aggregation": "Sum",         "Usage": "GINConv(nn.Sequential(...))"
    },
    "GATConv": {
        "Description": "Graph Attention Network layer",
 "aggregation": "Attention-weighted Sum",         "Usage": "GATConv(in_channels, out_channels, heads=8)"
    },
    "GATv2Conv": {
        "Description": "GATv2 (dynamic attention)",
 "aggregation": "Improved Attention",         "Usage": "GATv2Conv(in_channels, out_channels, heads=8)"
    }
}

for layer_name, info in layers_info.items():
    print(f"{layer_name}:")
    print(f"  Description: {info['Description']}")
 print(f" Aggregation: {info['aggregation']}")     print(f"  Usage: {info['Usage']}\n")

print("--- Graph-level poolingfunction ---\n") 
pooling_info = {
 "global_mean_pool": "mean of all nodes",  "global_max_pool": "max value of all nodes",  "global_add_pool": "sum of all nodes",     "GlobalAttention": "Attention-weighted sum"
}

for func_name, desc in pooling_info.items():
    print(f"{func_name}: {desc}")
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== Model using PyG pre-implemented layersÔºàpseudo-codeÔºâ === 
--- Main GNN Layers Available in PyG ---

GCNConv:
  Description: Graph Convolutional Network layer
  Aggregation: Mean (Sum with degree normalization)
  Usage: GCNConv(in_channels, out_channels)

SAGEConv:
  Description: GraphSAGE layer
  Aggregation: Mean / LSTM / Max-pool
  Usage: SAGEConv(in_channels, out_channels, aggr='mean')

GINConv:
  Description: Graph Isomorphism Network layer
  Aggregation: Sum
  Usage: GINConv(nn.Sequential(...))

GATConv:
  Description: Graph Attention Network layer
  Aggregation: Attention-weighted Sum
  Usage: GATConv(in_channels, out_channels, heads=8)

GATv2Conv:
  Description: GATv2 (dynamic attention)
  Aggregation: Improved Attention
  Usage: GATv2Conv(in_channels, out_channels, heads=8)

--- Graph-level poolingfunction --- 
global_mean_pool: mean of all nodes global_max_pool: max value of all nodes global_add_pool: sum of all nodes GlobalAttention: Attention-weighted sum
</code></pre>
<hr/>
<h2>3.5 PracticeÔºöGraph classification task</h2>
<h3>Graphclassificationflow</h3>
<p>Graphclassification„ÄÅtask to classify entire graph into one class„ÄÇmolecular property prediction„ÄÅsocial network classificationand other applications„ÄÇ</p>
<div class="mermaid">
graph LR
 A[Input Graph] --&gt;B[GNN layer<br/>node-level feature extraction]  B --&gt;C[Graph Pooling<br/>graph-level representation]     C --&gt; D[MLP<br/>Classifier]
 D --&gt;E[Classprediction] 
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#f3e5f5
    style E fill:#e8f5e9
</div>
<h3>Batch Processing Mechanism</h3>
<p>for efficient processing of multiple graphs„ÄÅPyG uses unique batching methodÔºö</p>
<ol>
<li><strong>concatenate as one large graph</strong>Ôºöcombine multiple graphs as non-connected graph</li><li><strong>Batch vector</strong>Ôºörecord which graph each node belongs to</li><li><strong>Graph-level pooling</strong>Ôºöaggregate features for each graph using batch vector</li></ol>
<div class="mermaid">
graph TB
    subgraph "Graph1 (3Node)"
        A1((0)) --- A2((1))
        A2 --- A3((2))
    end

    subgraph "Graph2 (2Node)"
        B1((3)) --- B2((4))
    end

    subgraph "Batch Tensor"
        C[batch = 0,0,0,1,1]
    end

    A1 -.-&gt; C
    A2 -.-&gt; C
    A3 -.-&gt; C
    B1 -.-&gt; C
    B2 -.-&gt; C

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style C fill:#e8f5e9
</div>
<h3>Implementation Example6: Complete implementation of graph classification</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

print("\n=== Complete implementation of graph classification task ===\n") 
# simple graph dataset class SimpleGraphDataset(Dataset):
 """simple graph dataset""" 
    def __init__(self, num_graphs=100):
        self.num_graphs = num_graphs
        self.graphs = []

 # random graph generation         for i in range(num_graphs):
            num_nodes = torch.randint(10, 30, (1,)).item()
            num_edges = torch.randint(15, 50, (1,)).item()

 x = torch.randn(num_nodes, 8) # 8dimensionFeature             edge_index = torch.randint(0, num_nodes, (2, num_edges))

 # labelsÔºàdetermined by graph size - for demoÔºâ             if num_nodes &lt; 15:
 y = 0 # smallGraph             elif num_nodes &lt; 20:
 y = 1 # mediumGraph             else:
 y = 2 # largeGraph 
            self.graphs.append({
                'x': x,
                'edge_index': edge_index,
                'y': y,
                'num_nodes': num_nodes
            })

    def __len__(self):
        return self.num_graphs

    def __getitem__(self, idx):
        return self.graphs[idx]


# Collate function for batch processing
def collate_graphs(batch):
 """multipleGraph1batchMerge"""     batch_x = []
    batch_edge_index = []
    batch_y = []
    batch_vec = []

    node_offset = 0
    for i, graph in enumerate(batch):
        batch_x.append(graph['x'])

 # Edge index offset         edge_index = graph['edge_index'] + node_offset
        batch_edge_index.append(edge_index)

        batch_y.append(graph['y'])

 # which graph this graph node belongs to         batch_vec.extend([i] * graph['num_nodes'])

        node_offset += graph['num_nodes']

    return {
        'x': torch.cat(batch_x, dim=0),
        'edge_index': torch.cat(batch_edge_index, dim=1),
        'y': torch.tensor(batch_y, dtype=torch.long),
        'batch': torch.tensor(batch_vec, dtype=torch.long)
    }


# GraphclassificationModel class GraphClassifier(nn.Module):
 """GINbaseGraphclassificationdevice""" 
    def __init__(self, in_dim, hidden_dim, num_classes, num_layers=3):
        super(GraphClassifier, self).__init__()

 # GIN layerÔºàusing previously mentioned GINConvÔºâ         self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # Layer 1
        self.convs.append(GINConv(in_dim, hidden_dim))
        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # Middle layers
        for _ in range(num_layers - 1):
            self.convs.append(GINConv(hidden_dim, hidden_dim))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

 # Graphlevelclassification         self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, x, edge_index, batch):
 # NodelevelGNN         h = x
        for conv, bn in zip(self.convs, self.batch_norms):
            h = conv(h, edge_index)
            h = bn(h)
            h = F.relu(h)
            h = F.dropout(h, p=0.3, training=self.training)

 # Graph-level pooling (mean)         num_graphs = batch.max().item() + 1
        h_graph = torch.zeros(num_graphs, h.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            h_graph[i] = h[mask].mean(dim=0)

 # classification         out = self.classifier(h_graph)

        return out


# Training function
def train_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for data in loader:
        optimizer.zero_grad()

        out = model(data['x'], data['edge_index'], data['batch'])
        loss = criterion(out, data['y'])

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pred = out.argmax(dim=1)
        correct += (pred == data['y']).sum().item()
        total += data['y'].size(0)

    return total_loss / len(loader), correct / total


# Evaluation function
def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for data in loader:
            out = model(data['x'], data['edge_index'], data['batch'])
            loss = criterion(out, data['y'])

            total_loss += loss.item()
            pred = out.argmax(dim=1)
            correct += (pred == data['y']).sum().item()
            total += data['y'].size(0)

    return total_loss / len(loader), correct / total


# Execution
print("--- Creating Dataset ---")
dataset = SimpleGraphDataset(num_graphs=200)
train_dataset = SimpleGraphDataset(num_graphs=150)
test_dataset = SimpleGraphDataset(num_graphs=50)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,
                          collate_fn=collate_graphs)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,
                         collate_fn=collate_graphs)

print(f"Training data: {len(train_dataset)} Graph")
print(f"Test data: {len(test_dataset)} Graph")
print(f"Batch size: 16\n")

# Creating model model = GraphClassifier(in_dim=8, hidden_dim=32, num_classes=3, num_layers=3)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

print(f"Model parameter count: {sum(p.numel() for p in model.parameters()):,}\n") 
# training print("--- Training Start ---")
num_epochs = 5
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
    test_loss, test_acc = evaluate(model, test_loader, criterion)

    print(f"Epoch {epoch+1}/{num_epochs}:")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"  Test Loss:  {test_loss:.4f}, Test Acc:  {test_acc:.4f}")

print("\nTraining complete!")
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== Complete implementation of graph classification task === 
--- Creating Dataset ---
Training data: 150 Graph
Test data: 50 Graph
Batch size: 16

Model parameter count: 28,547 
--- Training Start ---
Epoch 1/5:
  Train Loss: 1.0234, Train Acc: 0.4533
  Test Loss:  0.9876, Test Acc:  0.4800
Epoch 2/5:
  Train Loss: 0.8765, Train Acc: 0.5867
  Test Loss:  0.8543, Test Acc:  0.6000
Epoch 3/5:
  Train Loss: 0.7234, Train Acc: 0.6933
  Test Loss:  0.7123, Test Acc:  0.6800
Epoch 4/5:
  Train Loss: 0.6012, Train Acc: 0.7600
  Test Loss:  0.6234, Test Acc:  0.7400
Epoch 5/5:
  Train Loss: 0.5123, Train Acc: 0.8067
  Test Loss:  0.5678, Test Acc:  0.7800

Training complete!
</code></pre>
<h3>Implementation Example7: Graphpoolingcomparison</h3>
<pre><code class="language-python">import torch
import torch.nn as nn

print("\n=== Graph-level poolingcomparison ===\n") 
class GlobalPooling:
 """variousGraph-level poolingfunction""" 
    @staticmethod
    def global_mean_pool(x, batch):
 """meanpooling"""         num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            out[i] = x[mask].mean(dim=0)

        return out

    @staticmethod
    def global_max_pool(x, batch):
        """Max pooling"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            if mask.any():
                out[i] = x[mask].max(dim=0)[0]

        return out

    @staticmethod
    def global_add_pool(x, batch):
        """Sum pooling"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        for i in range(num_graphs):
            mask = (batch == i)
            out[i] = x[mask].sum(dim=0)

        return out

    @staticmethod
    def global_attention_pool(x, batch, gate_nn):
        """Attention pooling"""
        num_graphs = batch.max().item() + 1
        out = torch.zeros(num_graphs, x.size(1))

        # Compute attention weights
        gate = gate_nn(x)  # [num_nodes, 1]

        for i in range(num_graphs):
            mask = (batch == i)
            if mask.any():
                # Softmax normalization
                attn_weights = torch.softmax(gate[mask], dim=0)
                # Weighted sum
                out[i] = (x[mask] * attn_weights).sum(dim=0)

        return out


# Creating test data print("--- Creating test data ---") # 3 graphs batching x = torch.randn(30, 16) # 30 nodes, 16-dimensional features batch = torch.tensor([0]*10 + [1]*12 + [2]*8)  # Graph 1: 10 nodes, Graph 2: 12 nodes, Graph 3: 8 nodes

print(f"Total number of nodes: {x.size(0)}") print(f"Feature dimension: {x.size(1)}")
print(f"Graphnumber: {batch.max().item() + 1}") print(f"eachGraphNumber of nodes: {[(batch == i).sum().item() for i in range(3)]}\n") 
# eachpoolingmethodcomparison print("--- Comparison of Each Pooling Method ---\n")

pooling = GlobalPooling()

# Mean pooling
mean_out = pooling.global_mean_pool(x, batch)
print("Mean Pooling:")
print(f"  Output shape: {mean_out.shape}")
print(f" Graph1Featurequantitymean: {mean_out[0].mean():.4f}") print(f" Graph2Featurequantitymean: {mean_out[1].mean():.4f}") print(f" Graph3Featurequantitymean: {mean_out[2].mean():.4f}\n") 
# Max pooling
max_out = pooling.global_max_pool(x, batch)
print("Max Pooling:")
print(f"  Output shape: {max_out.shape}")
print(f" Graph 1 max value: {max_out[0].max():.4f}") print(f" Graph 2 max value: {max_out[1].max():.4f}") print(f" Graph 3 max value: {max_out[2].max():.4f}\n") 
# Add pooling
add_out = pooling.global_add_pool(x, batch)
print("Add (Sum) Pooling:")
print(f"  Output shape: {add_out.shape}")
print(f" Graph1sum: {add_out[0].sum():.4f}") print(f" Graph2sum: {add_out[1].sum():.4f}") print(f" Graph3sum: {add_out[2].sum():.4f}\n") 
# Attention pooling
gate_nn = nn.Linear(16, 1)
attn_out = pooling.global_attention_pool(x, batch, gate_nn)
print("Attention Pooling:")
print(f"  Output shape: {attn_out.shape}")
print(f" Graph1Featurequantitymean: {attn_out[0].mean():.4f}") print(f" Graph2Featurequantitymean: {attn_out[1].mean():.4f}") print(f" Graph3Featurequantitymean: {attn_out[2].mean():.4f}\n") 
# Comparison of pooling method characteristics
print("--- Characteristics of Pooling Methods ---\n")
properties = {
    "Mean": {
 "Feature": "mean of all nodes",         "Advantage": "Stable, robust to outliers",
 "Disadvantage": "important nodes may be buried",  "Use Case": "general graph classification"     },
    "Max": {
        "Feature": "Element-wise maximum",
 "Advantage": "strongly emphasizes important features",  "Disadvantage": "Sensitive to outliers",  "Use Case": "case where specific nodes are important"     },
    "Sum": {
 "Feature": "sum of all nodes",  "Advantage": "preserves graph size information",  "Disadvantage": "values become large for large graphs",  "Use Case": "GIN„ÄÅcase where graph size is important"     },
    "Attention": {
        "Feature": "Learnable weighted sum",
 "Advantage": "automatically select important nodes",  "Disadvantage": "High computational cost, overfitting risk",  "Use Case": "complex graphs„ÄÅcase where interpretability is important"     }
}

for method, props in properties.items():
    print(f"{method} Pooling:")
    for key, value in props.items():
        print(f"  {key}: {value}")
    print()
</code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== Graph-level poolingcomparison === 
--- Creating test data --- Total number of nodes: 30 Feature dimension: 16
Graphnumber: 3 eachGraphNumber of nodes: [10, 12, 8] 
--- Comparison of Each Pooling Method ---

Mean Pooling:
  Output shape: torch.Size([3, 16])
 Graph1Featurequantitymean: 0.0234  Graph2Featurequantitymean: -0.0567  Graph3Featurequantitymean: 0.0891 
Max Pooling:
  Output shape: torch.Size([3, 16])
 Graph 1 max value: 2.3456  Graph 2 max value: 2.1234  Graph 3 max value: 1.9876 
Add (Sum) Pooling:
  Output shape: torch.Size([3, 16])
 Graph1sum: 3.7456  Graph2sum: -8.1234  Graph3sum: 11.3456 
Attention Pooling:
  Output shape: torch.Size([3, 16])
 Graph1Featurequantitymean: 0.0345  Graph2Featurequantitymean: -0.0623  Graph3Featurequantitymean: 0.0712 
--- Characteristics of Pooling Methods ---

Mean Pooling:
 Feature: mean of all nodes   Advantage: Stable, robust to outliers
 Disadvantage: important nodes may be buried  Use Case: general graph classification 
Max Pooling:
  Feature: Element-wise maximum
 Advantage: strongly emphasizes important features  Disadvantage: Sensitive to outliers  Use Case: case where specific nodes are important 
Sum Pooling:
 Feature: sum of all nodes  Advantage: preserves graph size information  Disadvantage: values become large for large graphs  Use Case: GIN„ÄÅcase where graph size is important 
Attention Pooling:
  Feature: Learnable weighted sum
 Advantage: automatically select important nodes  Disadvantage: High computational cost, overfitting risk  Use Case: complex graphs„ÄÅcase where interpretability is important </code></pre>
<h3>Implementation Example8: Mini-batch learningdetailed</h3>
<pre><code class="language-python">import torch

print("\n=== Graphbatch processingdetailed ===\n") 
def visualize_batch_structure(graphs):
    """Visualize batch processing structure"""

 print("--- originalGraph ---")     for i, graph in enumerate(graphs):
        print(f"Graph{i}: {graph['num_nodes']}Node, {graph['edge_index'].size(1)}edge")

    # Batching
    batch_x = []
    batch_edge_index = []
    batch_vec = []
    node_offset = 0

    print("\n--- Batching Process ---")
    for i, graph in enumerate(graphs):
        print(f"\nGraph{i}:")
 print(f" current node offset: {node_offset}")  print(f" original edge index: {graph['edge_index'][:, :3].tolist()}... (first 3 edges)") 
 # Edge index offsetadjustment         adjusted_edges = graph['edge_index'] + node_offset
 print(f" Adjusted edge index: {adjusted_edges[:, :3].tolist()}...") 
        batch_x.append(graph['x'])
        batch_edge_index.append(adjusted_edges)
        batch_vec.extend([i] * graph['num_nodes'])

        node_offset += graph['num_nodes']

    # Merge
    batched_x = torch.cat(batch_x, dim=0)
    batched_edge_index = torch.cat(batch_edge_index, dim=1)
    batched_batch = torch.tensor(batch_vec)

    print("\n--- Batching Result ---")
 print(f"MergeNodeFeature: {batched_x.shape}")  print(f"MergeEdge index: {batched_edge_index.shape}")     print(f"Batch vector: {batched_batch.tolist()}")
 print(f"\nGraph assignment for nodes 0-4: {batched_batch[:5].tolist()}")  print(f"Graph assignment for nodes 5-9: {batched_batch[5:10].tolist()}") 
    return batched_x, batched_edge_index, batched_batch


# Creating test graphs graphs = [
    {
        'x': torch.randn(5, 4),
        'edge_index': torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]),
        'num_nodes': 5
    },
    {
        'x': torch.randn(3, 4),
        'edge_index': torch.tensor([[0, 1], [1, 2]]),
        'num_nodes': 3
    },
    {
        'x': torch.randn(4, 4),
        'edge_index': torch.tensor([[0, 1, 2], [1, 2, 3]]),
        'num_nodes': 4
    }
]

batched_x, batched_edge_index, batched_batch = visualize_batch_structure(graphs)

print("\n--- Recovering from Batch ---")
num_graphs = batched_batch.max().item() + 1
for i in range(num_graphs):
    mask = (batched_batch == i)
    print(f"\nGraph{i}:")
 print(f" Number of nodes: {mask.sum().item()}")  print(f" NodeFeatureshapestate: {batched_x[mask].shape}")  print(f" Featurequantitymean: {batched_x[mask].mean(dim=0)[:2].tolist()} (first 2 dimensions)") </code></pre>
<p><strong>Output</strong>Ôºö</p><pre><code>
=== Graphbatch processingdetailed === 
--- originalGraph --- Graph0: 5Node, 4edge
Graph1: 3Node, 2edge
Graph2: 4Node, 3edge

--- Batching Process ---

Graph0:
 current node offset: 0  original edge index: [[0, 1, 2], [1, 2, 3]]... (first 3 edges)  Adjusted edge index: [[0, 1, 2], [1, 2, 3]]... 
Graph1:
 current node offset: 5  original edge index: [[0, 1], [1, 2]]... (first 3 edges)  Adjusted edge index: [[5, 6], [6, 7]]... 
Graph2:
 current node offset: 8  original edge index: [[0, 1, 2], [1, 2, 3]]... (first 3 edges)  Adjusted edge index: [[8, 9, 10], [9, 10, 11]]... 
--- Batching Result ---
MergeNodeFeature: torch.Size([12, 4]) MergeEdge index: torch.Size([2, 9]) Batch vector: [0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2]

Graph assignment for nodes 0-4: [0, 0, 0, 0, 0] Graph assignment for nodes 5-9: [1, 1, 1, 2, 2] 
--- Recovering from Batch ---

Graph0:
 Number of nodes: 5  NodeFeatureshapestate: torch.Size([5, 4])  Featurequantitymean: [0.123, -0.456] (first 2 dimensions) 
Graph1:
 Number of nodes: 3  NodeFeatureshapestate: torch.Size([3, 4])  Featurequantitymean: [-0.234, 0.567] (first 2 dimensions) 
Graph2:
 Number of nodes: 4  NodeFeatureshapestate: torch.Size([4, 4])  Featurequantitymean: [0.345, 0.123] (first 2 dimensions) </code></pre>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the core <strong>message passing framework</strong> of GNNs and representative GNN architectures.</p>
<h3>Key Points</h3>
<details>
<summary><strong>1. Three Steps of Message Passing</strong></summary>
<ul>
<li><strong>Message</strong>: message generation from neighboring connected nodes</li><li><strong>Aggregate</strong>: Aggregate messages (Sum / Mean / Max)</li>
<li><strong>Update</strong>: aggregationresultFeatureupdate</li><li>Many GNNs can be described uniformly with this framework</li>
</ul>
</details>
<details>
<summary><strong>2. Sampling-Based Aggregation in GraphSAGE</strong></summary>
<ul>
<li>Sample neighbors to fixed size</li>
<li>scalability for large-scale graphs</li><li>Choice of Mean / Pool / LSTM Aggregator</li>
<li>Enables inductive learning</li>
</ul>
</details>
<details>
<summary><strong>3. GINMaximum Discriminative Power</strong></summary><ul>
<li>Weisfeiler-Lehman test and equivalentDiscriminative Power</li><li>Sum aggregation is the only injective aggregation that preserves multisets</li>
<li>$(1 + \epsilon)$ coefficient distinguishes between self and neighbors</li>
<li>MLP ensures sufficient expressive power</li>
</ul>
</details>
<details>
<summary><strong>4. Efficient Implementation with PyTorch Geometric</strong></summary>
<ul>
<li>simple and clean implementation using MessagePassing base class</li><li>Pre-implemented layers (GCNConv, SAGEConv, GINConv, etc.)</li>
<li>efficient sparse tensor operations</li><li>Graph batch processing and DataLoader</li></ul>
</details>
<details>
<summary><strong>5. Graphclassificationimplementation</strong></summary><ul>
<li>NodelevelGNN ‚Üí Graph-level pooling ‚Üí classificationdevice</li><li>batch processingÔºömerge multiple graphs as non-connected graph</li><li>Graph-level poolingÔºàMean / Max / Sum / AttentionÔºâ</li><li>Practical training and evaluation loop</li>
</ul>
</details>
<h3>Next Steps</h3>
<p>Next, <strong>Graph Attention Mechanism</strong>we will learn aboutÔºö</p><ul>
<li>Graph Attention Networks (GAT)</li>
<li>Applying self-attention mechanism to graphs</li><li>Effects of multi-head attention</li>
<li>Transformers for Graphs</li>
</ul>
<hr/>
<h2>Exercise Questions</h2>
<details>
<summary><strong>Exercise 1: Hand Calculation of Message Passing</strong></summary>
<p>Following graph„ÄÅ1-layer message passingÔºàSum aggregationÔºâcompute by hand„ÄÇ</p><ul>
<li>Node0: $\mathbf{h}_0 = [1, 0]$</li>
<li>Node1: $\mathbf{h}_1 = [0, 1]$</li>
<li>Node2: $\mathbf{h}_2 = [1, 1]$</li>
<li>edge: 0‚Üí1, 1‚Üí2, 2‚Üí0</li>
<li>MESSAGE Function: identity mapping</li><li>UPDATE Function: $\mathbf{h}_i^{(1)} = \mathbf{h}_i^{(0)} + \mathbf{m}_i$</li>
</ul>
<p>feature after updating each node$\mathbf{h}_i^{(1)}$calculate„ÄÇ</p></details>
<details>
<summary><strong>Exercise 2: Aggregator Selection</strong></summary>
<p>Choose the best aggregator for the following tasks and describe the reasonÔºö</p><ol>
<li>SNS community detection (number of friends for each user is important)</li>
<li>Molecular toxicity prediction (presence of specific functional groups is important)</li>
<li>road network traffic flow predictionÔºàaverage traffic volume is importantÔºâ</li></ol>
<p>Options: Sum, Mean, Max, LSTM</p>
</details>
<details>
<summary><strong>Exercise3ÔºöGIN Discriminative Power</strong></summary><p>For the following 2 graphs, answer whether GIN„ÄÅGCN (Mean aggregation)„ÄÅGAT (Maxaggregation) can distinguish them respectivelyÔºö</p><ul>
<li>Graph A: 3 nodes in triangular shapeÔºàeach node degree2Ôºâ</li><li>Graph B: 4 nodes in square shapeÔºàeach node degree2Ôºâ</li></ul>
<p>All initial features are$[1]$ .</p></details>
<details>
<summary><strong>Exercise4ÔºöGraph pooling implementation</strong></summary><p>Implement attention-based graph pooling. Requirements:</p>
<ul>
<li>Compute attention score for each node pair</li><li>Normalize with Softmax</li>
<li>Compute graph representation by weighted sum</li><li>Handle multiple graphs using batch vector</li></ul>
</details>
<details>
<summary><strong>Exercise 5: Batch Processing Design</strong></summary>
<p>3 graphsÔºà5 nodes, 3 nodes, 7 nodesÔºâafter batchingÔºö</p><ol>
<li>MergeafterTotal number of nodes</li><li>Contents of batch vector</li><li>Edge index offset for each graph</li></ol>
<p>Answer with specific numbers.</p>
</details>
<hr/>
<div class="navigation">
<a class="nav-button" href="chapter2-gcn-gat.html">‚Üê Chapter 2: GCN and GAT</a>
<a class="nav-button" href="index.html">Return to Table of Contents</a>
<a class="nav-button" href="chapter4-graph-attention.html">Chapter 4: Graph Attention ‚Üí</a></div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3> <ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li><li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li> <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>Copyright and license terms stated in content (Example: CC BY 4.0) should be followed. This license typically includes no-warranty clauses.</li> </ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
<p>Graph Neural Networks Introduction Series - Chapter 3: Message Passing and GNN</p>
</footer>
</body>
</html>