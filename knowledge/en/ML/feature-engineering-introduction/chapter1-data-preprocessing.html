<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Data Preprocessing Fundamentals - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Data Preprocessing Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/feature-engineering-introduction/index.html">Feature Engineering</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/feature-engineering-introduction/chapter1-data-preprocessing.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Data Preprocessing Fundamentals</h1>
<p class="subtitle">The Foundation of Feature Engineering - Improving Data Quality</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 12</span>
<span class="meta-item">üìù Practice Problems: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Data Preprocessing Fundamentals, which the importance of data preprocessing. You will learn difference between scaling and Build preprocessing pipelines using scikit-learn.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the importance and overall picture of data preprocessing</li>
<li>‚úÖ Select appropriate handling methods for different types of missing values</li>
<li>‚úÖ Detect and appropriately handle outliers</li>
<li>‚úÖ Understand the difference between scaling and normalization, and use them appropriately</li>
<li>‚úÖ Build preprocessing pipelines using scikit-learn</li>
<li>‚úÖ Execute comprehensive preprocessing on real data</li>
</ul>
<hr/>
<h2>1.1 The Importance of Data Preprocessing</h2>
<h3>What is Data Preprocessing?</h3>
<p><strong>Data Preprocessing</strong> is the process of transforming raw data into a format suitable for machine learning models.</p>
<blockquote>
<p>"Garbage In, Garbage Out (GIGO)" - Data quality determines model performance.</p>
</blockquote>
<h3>Why Preprocessing is Necessary</h3>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Impact</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Missing Values</strong></td>
<td>Training errors, biased predictions</td>
<td>Imputation, deletion</td>
</tr>
<tr>
<td><strong>Outliers</strong></td>
<td>Model distortion, overfitting</td>
<td>Detection, transformation, deletion</td>
</tr>
<tr>
<td><strong>Scale Differences</strong></td>
<td>Training instability</td>
<td>Normalization, standardization</td>
</tr>
<tr>
<td><strong>Irrelevant Features</strong></td>
<td>Curse of dimensionality, overfitting</td>
<td>Feature selection, dimensionality reduction</td>
</tr>
</tbody>
</table>
<h3>Overall Picture of Data Preprocessing</h3>
<div class="mermaid">
graph TD
    A[Raw Data] --&gt; B[Missing Value Handling]
    B --&gt; C[Outlier Handling]
    C --&gt; D[Scaling &amp; Normalization]
    D --&gt; E[Feature Engineering]
    E --&gt; F[Feature Selection]
    F --&gt; G[Ready for Training]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#e8f5e9
    style F fill:#fce4ec
    style G fill:#c8e6c9
</div>
<h3>Example: Effects of Preprocessing</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Example: Effects of Preprocessing

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Generate sample data (features with different scales)
np.random.seed(42)
n_samples = 1000

# Feature 1: Age (20-60)
X1 = np.random.uniform(20, 60, n_samples)

# Feature 2: Annual income (3-10 million yen)
X2 = np.random.uniform(300, 1000, n_samples)

# Target: Based on age and income
y = ((X1 &gt; 40) &amp; (X2 &gt; 600)).astype(int)

# Create DataFrame
X = pd.DataFrame({'age': X1, 'income': X2})

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Training without preprocessing
model_raw = LogisticRegression(random_state=42, max_iter=1000)
model_raw.fit(X_train, y_train)
acc_raw = accuracy_score(y_test, model_raw.predict(X_test))

# Training with preprocessing
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model_scaled = LogisticRegression(random_state=42, max_iter=1000)
model_scaled.fit(X_train_scaled, y_train)
acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))

print("=== Comparison of Preprocessing Effects ===")
print(f"Without preprocessing: Accuracy = {acc_raw:.3f}")
print(f"With preprocessing: Accuracy = {acc_scaled:.3f}")
print(f"Improvement: {(acc_scaled - acc_raw) * 100:.1f}%")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Comparison of Preprocessing Effects ===
Without preprocessing: Accuracy = 0.890
With preprocessing: Accuracy = 0.920
Improvement: 3.0%
</code></pre>
<blockquote>
<p><strong>Important</strong>: Scaling improves both convergence speed and accuracy.</p>
</blockquote>
<hr/>
<h2>1.2 Missing Value Handling</h2>
<h3>Types of Missing Values</h3>
<p>Missing values are classified into three types based on their generation mechanism:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MCAR</strong><br/>(Missing Completely At Random)</td>
<td>Completely random missingness</td>
<td>Data loss due to equipment failure</td>
</tr>
<tr>
<td><strong>MAR</strong><br/>(Missing At Random)</td>
<td>Missingness dependent on other variables</td>
<td>Older people less likely to provide income</td>
</tr>
<tr>
<td><strong>MNAR</strong><br/>(Missing Not At Random)</td>
<td>Missingness itself is meaningful</td>
<td>Low-income individuals not filling income</td>
</tr>
</tbody>
</table>
<h3>Visualizing Missing Values</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - seaborn&gt;=0.12.0

"""
Example: Visualizing Missing Values

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate sample data with missing values
np.random.seed(42)
n = 100

df = pd.DataFrame({
    'age': np.random.randint(20, 70, n),
    'income': np.random.randint(300, 1200, n),
    'score': np.random.uniform(0, 100, n)
})

# Intentionally create missing values
# 10% missing in age
missing_age = np.random.choice(n, size=int(n * 0.1), replace=False)
df.loc[missing_age, 'age'] = np.nan

# 20% missing in income (age-dependent)
missing_income = df[df['age'] &gt; 50].sample(frac=0.4).index
df.loc[missing_income, 'income'] = np.nan

# 15% missing in score
missing_score = np.random.choice(n, size=int(n * 0.15), replace=False)
df.loc[missing_score, 'score'] = np.nan

# Check missing values
print("=== Missing Value Status ===")
print(df.isnull().sum())
print(f"\nMissing Rates:")
print((df.isnull().sum() / len(df) * 100).round(2))

# Visualize with heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Value Pattern Visualization (Yellow = Missing)', fontsize=14)
plt.xlabel('Features')
plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Missing Value Status ===
age        10
income     16
score      15
dtype: int64

Missing Rates:
age        10.0
income     16.0
score      15.0
dtype: float64
</code></pre>
<h3>Deletion Methods</h3>
<h4>Row Deletion (Listwise Deletion)</h4>
<pre><code class="language-python"># Delete rows containing missing values
df_droprows = df.dropna()

print(f"Original data: {len(df)} rows")
print(f"After deletion: {len(df_droprows)} rows")
print(f"Deleted rows: {len(df) - len(df_droprows)} rows ({(1 - len(df_droprows)/len(df))*100:.1f}%)")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Original data: 100 rows
After deletion: 64 rows
Deleted rows: 36 rows (36.0%)
</code></pre>
<blockquote>
<p><strong>Note</strong>: Data volume can be significantly reduced.</p>
</blockquote>
<h4>Column Deletion</h4>
<pre><code class="language-python"># Delete columns with 30% or more missing rate
threshold = 0.3
df_dropcols = df.loc[:, df.isnull().mean() &lt; threshold]

print(f"Original features: {df.shape[1]}")
print(f"After deletion: {df_dropcols.shape[1]}")
print(f"\nDeleted features: {set(df.columns) - set(df_dropcols.columns)}")
</code></pre>
<h3>Imputation Methods</h3>
<h4>Simple Imputation</h4>
<pre><code class="language-python">from sklearn.impute import SimpleImputer

# Mean imputation
imputer_mean = SimpleImputer(strategy='mean')
df_mean = pd.DataFrame(
    imputer_mean.fit_transform(df),
    columns=df.columns
)

# Median imputation
imputer_median = SimpleImputer(strategy='median')
df_median = pd.DataFrame(
    imputer_median.fit_transform(df),
    columns=df.columns
)

# Mode imputation
imputer_mode = SimpleImputer(strategy='most_frequent')
df_mode = pd.DataFrame(
    imputer_mode.fit_transform(df),
    columns=df.columns
)

# Constant imputation
imputer_constant = SimpleImputer(strategy='constant', fill_value=0)
df_constant = pd.DataFrame(
    imputer_constant.fit_transform(df),
    columns=df.columns
)

print("=== Comparison of Imputation Methods ===\n")
print(f"Original age mean: {df['age'].mean():.2f}")
print(f"After mean imputation: {df_mean['age'].mean():.2f}")
print(f"After median imputation: {df_median['age'].median():.2f}")
print(f"Mode imputation: {df_mode['age'].mode()[0]:.2f}")
</code></pre>
<h4>KNN Imputation</h4>
<pre><code class="language-python">from sklearn.impute import KNNImputer

# KNN imputation (k=5)
knn_imputer = KNNImputer(n_neighbors=5)
df_knn = pd.DataFrame(
    knn_imputer.fit_transform(df),
    columns=df.columns
)

print("\n=== KNN Imputation Details ===")
print(f"Age mean before missing: {df['age'].mean():.2f}")
print(f"Age mean after KNN imputation: {df_knn['age'].mean():.2f}")

# Visualization: Comparison of imputation methods
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
methods = [
    ('Original Data', df),
    ('Mean Imputation', df_mean),
    ('Median Imputation', df_median),
    ('Mode Imputation', df_mode),
    ('Constant Imputation', df_constant),
    ('KNN Imputation', df_knn)
]

for ax, (name, data) in zip(axes.flat, methods):
    ax.scatter(data['age'], data['income'], alpha=0.6)
    ax.set_xlabel('Age')
    ax.set_ylabel('Income')
    ax.set_title(name)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Multiple Imputation</h3>
<pre><code class="language-python">from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Multiple imputation (MICE: Multivariate Imputation by Chained Equations)
mice_imputer = IterativeImputer(random_state=42, max_iter=10)
df_mice = pd.DataFrame(
    mice_imputer.fit_transform(df),
    columns=df.columns
)

print("=== Multiple Imputation (MICE) ===")
print(f"Missing count before imputation: {df.isnull().sum().sum()}")
print(f"Missing count after imputation: {df_mice.isnull().sum().sum()}")
print(f"\nStatistics of each feature after imputation:")
print(df_mice.describe())
</code></pre>
<h3>Guidelines for Choosing Imputation Methods</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Missing rate &lt; 5%</td>
<td>Deletion</td>
<td>Minimal information loss</td>
</tr>
<tr>
<td>Numerical, normal distribution</td>
<td>Mean imputation</td>
<td>Preserves distribution</td>
</tr>
<tr>
<td>Numerical, with outliers</td>
<td>Median imputation</td>
<td>Robust</td>
</tr>
<tr>
<td>Categorical</td>
<td>Mode imputation</td>
<td>Reasonable estimate</td>
</tr>
<tr>
<td>Correlation between features</td>
<td>KNN, MICE</td>
<td>Utilizes relationships</td>
</tr>
<tr>
<td>MNAR</td>
<td>Use domain knowledge</td>
<td>Missingness itself is informative</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.3 Outlier Handling</h2>
<h3>What are Outliers?</h3>
<p><strong>Outliers</strong> are values that differ significantly from other data points, either due to measurement errors or genuine anomalies.</p>
<h3>Outlier Detection Methods</h3>
<h4>1. IQR Method (Interquartile Range)</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: 1. IQR Method (Interquartile Range)

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Sample data (with outliers)
np.random.seed(42)
data_normal = np.random.normal(50, 10, 95)
outliers = np.array([100, 105, 110, 0, -5])  # Outliers
data = np.concatenate([data_normal, outliers])

# Outlier detection using IQR method
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_iqr = (data &lt; lower_bound) | (data &gt; upper_bound)

print("=== Outlier Detection using IQR Method ===")
print(f"Q1 (25th percentile): {Q1:.2f}")
print(f"Q3 (75th percentile): {Q3:.2f}")
print(f"IQR: {IQR:.2f}")
print(f"Lower bound: {lower_bound:.2f}")
print(f"Upper bound: {upper_bound:.2f}")
print(f"Number of outliers: {outliers_iqr.sum()}")
print(f"Outliers: {data[outliers_iqr]}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Box plot
axes[0].boxplot(data, vert=True)
axes[0].axhline(y=lower_bound, color='r', linestyle='--',
                label=f'Lower bound: {lower_bound:.1f}')
axes[0].axhline(y=upper_bound, color='r', linestyle='--',
                label=f'Upper bound: {upper_bound:.1f}')
axes[0].set_ylabel('Value')
axes[0].set_title('Box Plot (IQR Method)', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Histogram
axes[1].hist(data, bins=20, alpha=0.7, edgecolor='black')
axes[1].axvline(x=lower_bound, color='r', linestyle='--', linewidth=2)
axes[1].axvline(x=upper_bound, color='r', linestyle='--', linewidth=2)
axes[1].set_xlabel('Value')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Histogram', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Outlier Detection using IQR Method ===
Q1 (25th percentile): 43.26
Q3 (75th percentile): 56.83
IQR: 13.57
Lower bound: 22.90
Upper bound: 77.19
Number of outliers: 5
Outliers: [100. 105. 110.   0.  -5.]
</code></pre>
<h4>2. Z-score Detection</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - scipy&gt;=1.11.0

"""
Example: 2. Z-score Detection

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from scipy import stats

# Calculate Z-score
z_scores = np.abs(stats.zscore(data))
threshold = 3  # Typically use 3 as threshold

outliers_zscore = z_scores &gt; threshold

print("\n=== Outlier Detection using Z-score ===")
print(f"Threshold: {threshold}")
print(f"Number of outliers: {outliers_zscore.sum()}")
print(f"Z-scores of outliers: {z_scores[outliers_zscore]}")
print(f"Outliers: {data[outliers_zscore]}")

# Visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(range(len(data)), data, c=outliers_zscore,
            cmap='coolwarm', s=50, alpha=0.7, edgecolors='black')
plt.axhline(y=data.mean() + 3*data.std(), color='r',
            linestyle='--', label='+3œÉ')
plt.axhline(y=data.mean() - 3*data.std(), color='r',
            linestyle='--', label='-3œÉ')
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Data Points (Red = Outliers)', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(range(len(z_scores)), z_scores,
            c=outliers_zscore, cmap='coolwarm', s=50,
            alpha=0.7, edgecolors='black')
plt.axhline(y=threshold, color='r', linestyle='--',
            label=f'Threshold: {threshold}')
plt.xlabel('Index')
plt.ylabel('|Z-score|')
plt.title('Z-score Distribution', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h4>3. Isolation Forest Detection</h4>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# Expand data to 2 dimensions
np.random.seed(42)
X = np.random.normal(50, 10, (95, 2))
X_outliers = np.array([[100, 100], [105, 105], [0, 0], [-5, -5], [110, 110]])
X_combined = np.vstack([X, X_outliers])

# Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
outliers_iso = iso_forest.fit_predict(X_combined)
# -1: outlier, 1: normal

print("\n=== Outlier Detection using Isolation Forest ===")
print(f"Number of outliers: {(outliers_iso == -1).sum()}")
print(f"Number of normal points: {(outliers_iso == 1).sum()}")

# Visualization
plt.figure(figsize=(10, 8))
plt.scatter(X_combined[outliers_iso == 1, 0],
            X_combined[outliers_iso == 1, 1],
            c='blue', label='Normal', alpha=0.6, s=50, edgecolors='black')
plt.scatter(X_combined[outliers_iso == -1, 0],
            X_combined[outliers_iso == -1, 1],
            c='red', label='Outliers', alpha=0.8, s=100,
            edgecolors='black', marker='X')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Outlier Detection using Isolation Forest', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>
<h3>How to Handle Outliers</h3>
<h4>1. Deletion</h4>
<pre><code class="language-python"># Delete outliers
data_cleaned = data[~outliers_iqr]

print(f"Original data: {len(data)} points")
print(f"After deletion: {len(data_cleaned)} points")
print(f"Mean (before deletion): {data.mean():.2f}")
print(f"Mean (after deletion): {data_cleaned.mean():.2f}")
</code></pre>
<h4>2. Transformation (Log Transformation)</h4>
<pre><code class="language-python"># Log transformation (positive values only)
data_positive = data[data &gt; 0]
data_log = np.log1p(data_positive)  # log(1 + x) to avoid 0

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(data_positive, bins=20, alpha=0.7, edgecolor='black')
axes[0].set_xlabel('Value')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Original Data', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].hist(data_log, bins=20, alpha=0.7, edgecolor='black', color='orange')
axes[1].set_xlabel('log(Value)')
axes[1].set_ylabel('Frequency')
axes[1].set_title('After Log Transformation', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h4>3. Capping (Winsorization)</h4>
<pre><code class="language-python">from scipy.stats import mstats

# Winsorization: Cap outliers at upper and lower bounds
data_winsorized = mstats.winsorize(data, limits=[0.05, 0.05])

print("\n=== Winsorization (Cap top and bottom 5%) ===")
print(f"Original data range: [{data.min():.2f}, {data.max():.2f}]")
print(f"Range after processing: [{data_winsorized.min():.2f}, {data_winsorized.max():.2f}]")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].boxplot(data)
axes[0].set_ylabel('Value')
axes[0].set_title('Original Data', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].boxplot(data_winsorized)
axes[1].set_ylabel('Value')
axes[1].set_title('After Winsorization', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Guidelines for Outlier Handling</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clear error</td>
<td>Deletion</td>
<td>Improves data quality</td>
</tr>
<tr>
<td>True extreme value</td>
<td>Keep or cap</td>
<td>Preserves information</td>
</tr>
<tr>
<td>Skewed distribution</td>
<td>Log transformation</td>
<td>Approximates normal distribution</td>
</tr>
<tr>
<td>Robustness needed</td>
<td>Winsorization</td>
<td>Suppresses impact</td>
</tr>
<tr>
<td>Multidimensional data</td>
<td>Isolation Forest</td>
<td>Detects complex patterns</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.4 Scaling and Normalization</h2>
<h3>Why Scaling is Necessary</h3>
<p>When features have different scales, the following problems occur:</p>
<ul>
<li>Distance-based algorithms (KNN, SVM) are dominated by large values</li>
<li>Gradient descent convergence becomes slow</li>
<li>Regularization effects become uneven</li>
</ul>
<h3>1. StandardScaler (Standardization)</h3>
<p><strong>Standardization</strong> transforms data to have mean 0 and standard deviation 1.</p>
<p>$$
z = \frac{x - \mu}{\sigma}
$$</p>
<ul>
<li>$\mu$: mean</li>
<li>$\sigma$: standard deviation</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: $$
z = \frac{x - \mu}{\sigma}
$$

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Sample data (different scales)
np.random.seed(42)
data = pd.DataFrame({
    'age': np.random.randint(20, 70, 100),
    'income': np.random.randint(300, 1500, 100),
    'score': np.random.uniform(0, 100, 100)
})

print("=== Original Data Statistics ===")
print(data.describe())

# Apply StandardScaler
scaler = StandardScaler()
data_scaled = pd.DataFrame(
    scaler.fit_transform(data),
    columns=data.columns
)

print("\n=== Statistics After Standardization ===")
print(data_scaled.describe())

# Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, col in enumerate(data.columns):
    # Original data
    axes[0, i].hist(data[col], bins=20, alpha=0.7, edgecolor='black')
    axes[0, i].set_xlabel(col)
    axes[0, i].set_ylabel('Frequency')
    axes[0, i].set_title(f'{col} (Original Data)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # After standardization
    axes[1, i].hist(data_scaled[col], bins=20, alpha=0.7,
                    edgecolor='black', color='orange')
    axes[1, i].set_xlabel(col)
    axes[1, i].set_ylabel('Frequency')
    axes[1, i].set_title(f'{col} (Standardized)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>2. MinMaxScaler (Normalization)</h3>
<p><strong>Normalization</strong> scales values to a specified range (typically [0, 1]).</p>
<p>$$
x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
$$</p>
<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler

# Apply MinMaxScaler
minmax_scaler = MinMaxScaler(feature_range=(0, 1))
data_minmax = pd.DataFrame(
    minmax_scaler.fit_transform(data),
    columns=data.columns
)

print("=== Statistics After MinMaxScaler (Normalization) ===")
print(data_minmax.describe())

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, col in enumerate(data.columns):
    axes[i].hist(data_minmax[col], bins=20, alpha=0.7,
                 edgecolor='black', color='green')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'{col} (MinMax: [0,1])', fontsize=12)
    axes[i].set_xlim(-0.1, 1.1)
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>3. RobustScaler (Robust Scaling)</h3>
<p><strong>RobustScaler</strong> uses median and IQR, making it robust to outliers.</p>
<p>$$
x_{\text{robust}} = \frac{x - \text{median}}{\text{IQR}}
$$</p>
<pre><code class="language-python">from sklearn.preprocessing import RobustScaler

# Data with outliers
data_with_outliers = data.copy()
data_with_outliers.loc[0:5, 'income'] = [5000, 5500, 6000, 100, 50, 10000]

# Apply RobustScaler
robust_scaler = RobustScaler()
data_robust = pd.DataFrame(
    robust_scaler.fit_transform(data_with_outliers),
    columns=data.columns
)

# Comparison: StandardScaler vs RobustScaler
standard_scaler = StandardScaler()
data_standard = pd.DataFrame(
    standard_scaler.fit_transform(data_with_outliers),
    columns=data.columns
)

print("=== Comparison with Data Containing Outliers ===")
print("\nStandardScaler:")
print(data_standard['income'].describe())
print("\nRobustScaler:")
print(data_robust['income'].describe())

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].boxplot(data_with_outliers['income'])
axes[0].set_ylabel('income')
axes[0].set_title('Original Data (with outliers)', fontsize=12)
axes[0].grid(True, alpha=0.3)

axes[1].boxplot(data_standard['income'])
axes[1].set_ylabel('income (scaled)')
axes[1].set_title('StandardScaler', fontsize=12)
axes[1].grid(True, alpha=0.3)

axes[2].boxplot(data_robust['income'])
axes[2].set_ylabel('income (scaled)')
axes[2].set_title('RobustScaler (Robust to outliers)', fontsize=12)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Guidelines for Scaler Selection</h3>
<table>
<thead>
<tr>
<th>Scaler</th>
<th>Use Case</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>StandardScaler</strong></td>
<td>Normal distribution, no outliers</td>
<td>Standard for many algorithms</td>
<td>Sensitive to outliers</td>
</tr>
<tr>
<td><strong>MinMaxScaler</strong></td>
<td>Range matters, neural networks</td>
<td>Interpretable [0,1]</td>
<td>Large impact from outliers</td>
</tr>
<tr>
<td><strong>RobustScaler</strong></td>
<td>With outliers</td>
<td>Robust to outliers</td>
<td>Undefined range</td>
</tr>
</tbody>
</table>
<h3>Recommendations by Algorithm</h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Scaling Needed?</th>
<th>Recommended Scaler</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Regression</td>
<td>Recommended</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>Logistic Regression</td>
<td>Required</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>SVM</td>
<td>Required</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>KNN</td>
<td>Required</td>
<td>StandardScaler, MinMaxScaler</td>
</tr>
<tr>
<td>Neural Networks</td>
<td>Required</td>
<td>MinMaxScaler, StandardScaler</td>
</tr>
<tr>
<td>Decision Trees</td>
<td>Not needed</td>
<td>-</td>
</tr>
<tr>
<td>Random Forest</td>
<td>Not needed</td>
<td>-</td>
</tr>
<tr>
<td>XGBoost</td>
<td>Not needed</td>
<td>-</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.5 Practical Example: Complete Preprocessing Pipeline</h2>
<h3>Data Preparation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Data Preparation

Purpose: Demonstrate data manipulation and preprocessing
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Generate realistic data
np.random.seed(42)
n = 1000

# Generate features
df = pd.DataFrame({
    'age': np.random.randint(18, 80, n),
    'income': np.random.normal(500, 200, n),
    'credit_score': np.random.uniform(300, 850, n),
    'loan_amount': np.random.uniform(1000, 50000, n),
    'employment_years': np.random.randint(0, 40, n)
})

# Target variable (loan approval)
df['approved'] = (
    (df['credit_score'] &gt; 600) &amp;
    (df['income'] &gt; 400) &amp;
    (df['age'] &gt; 25)
).astype(int)

# Intentionally add data quality issues
# 1. Missing values
missing_idx = np.random.choice(n, size=100, replace=False)
df.loc[missing_idx[:50], 'income'] = np.nan
df.loc[missing_idx[50:], 'credit_score'] = np.nan

# 2. Outliers
outlier_idx = np.random.choice(n, size=20, replace=False)
df.loc[outlier_idx, 'loan_amount'] = df.loc[outlier_idx, 'loan_amount'] * 10

print("=== Data Overview ===")
print(df.head(10))
print(f"\nShape: {df.shape}")
print(f"\nMissing values:")
print(df.isnull().sum())
print(f"\nBasic statistics:")
print(df.describe())
</code></pre>
<h3>Building the Preprocessing Pipeline</h3>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler

# Separate features and target
X = df.drop('approved', axis=1)
y = df['approved']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Define pipeline
# Split numerical features into two groups
sensitive_features = ['loan_amount']  # Sensitive to outliers
regular_features = ['age', 'income', 'credit_score', 'employment_years']

# Build pipeline
preprocessor = ColumnTransformer(
    transformers=[
        # Regular features: imputation ‚Üí standardization
        ('regular', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), regular_features),

        # Outlier-sensitive features: imputation ‚Üí robust scaling
        ('sensitive', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', RobustScaler())
        ]), sensitive_features)
    ]
)

# Complete pipeline (preprocessing + model)
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

print("=== Pipeline Structure ===")
print(full_pipeline)
</code></pre>
<h3>Model Training and Evaluation</h3>
<pre><code class="language-python"># Execute pipeline
full_pipeline.fit(X_train, y_train)

# Prediction
y_pred = full_pipeline.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)

print("\n=== Model Performance ===")
print(f"Accuracy: {accuracy:.3f}")
print(f"\nDetailed Report:")
print(classification_report(y_test, y_pred,
                           target_names=['Rejected', 'Approved']))

# Comparison with no preprocessing
from sklearn.ensemble import RandomForestClassifier

# Without preprocessing (simply drop missing values)
X_train_raw = X_train.dropna()
y_train_raw = y_train[X_train.dropna().index]
X_test_raw = X_test.fillna(X_test.median())

model_raw = RandomForestClassifier(n_estimators=100, random_state=42)
model_raw.fit(X_train_raw, y_train_raw)
y_pred_raw = model_raw.predict(X_test_raw)
accuracy_raw = accuracy_score(y_test, y_pred_raw)

print(f"\n=== Pipeline vs No Preprocessing ===")
print(f"With pipeline: {accuracy:.3f}")
print(f"Without preprocessing: {accuracy_raw:.3f}")
print(f"Improvement: {(accuracy - accuracy_raw) * 100:.1f}%")
print(f"\nTraining data size:")
print(f"  Pipeline: {len(X_train)} rows")
print(f"  Without preprocessing: {len(X_train_raw)} rows ({len(X_train) - len(X_train_raw)} rows deleted)")
</code></pre>
<h3>Detailed Analysis of Preprocessing</h3>
<pre><code class="language-python"># Get preprocessed data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Get feature names
feature_names = regular_features + sensitive_features

print("\n=== Data After Preprocessing ===")
print(f"Shape: {X_train_processed.shape}")
print(f"\nStatistics after preprocessing (training data):")
df_processed = pd.DataFrame(X_train_processed, columns=feature_names)
print(df_processed.describe())

# Visualization: Effects of preprocessing
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
features_to_plot = ['age', 'income', 'loan_amount']

for i, feature in enumerate(features_to_plot):
    # Before preprocessing
    axes[0, i].hist(X_train[feature].dropna(), bins=30,
                    alpha=0.7, edgecolor='black')
    axes[0, i].set_xlabel(feature)
    axes[0, i].set_ylabel('Frequency')
    axes[0, i].set_title(f'{feature} (Before Preprocessing)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # After preprocessing
    feature_idx = feature_names.index(feature)
    axes[1, i].hist(X_train_processed[:, feature_idx], bins=30,
                    alpha=0.7, edgecolor='black', color='orange')
    axes[1, i].set_xlabel(feature)
    axes[1, i].set_ylabel('Frequency')
    axes[1, i].set_title(f'{feature} (After Preprocessing)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Saving and Reusing the Pipeline</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0

"""
Example: Saving and Reusing the Pipeline

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner
Execution time: ~5 seconds
Dependencies: None
"""

import joblib

# Save the pipeline
joblib.dump(full_pipeline, 'loan_approval_pipeline.pkl')
print("Pipeline saved: loan_approval_pipeline.pkl")

# Load and use
loaded_pipeline = joblib.load('loan_approval_pipeline.pkl')

# Prediction on new data
new_data = pd.DataFrame({
    'age': [35, 22, 50],
    'income': [700, 300, np.nan],  # Contains missing values
    'credit_score': [750, 550, 800],
    'loan_amount': [25000, 5000, 100000],  # Contains outliers
    'employment_years': [10, 1, 25]
})

predictions = loaded_pipeline.predict(new_data)
probabilities = loaded_pipeline.predict_proba(new_data)

print("\n=== Predictions on New Data ===")
for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
    print(f"\nSample {i+1}:")
    print(f"  Prediction: {'Approved' if pred == 1 else 'Rejected'}")
    print(f"  Probability: Rejected={prob[0]:.2%}, Approved={prob[1]:.2%}")
</code></pre>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>The Importance of Data Preprocessing</strong></p>
<ul>
<li>Data quality determines model performance</li>
<li>Preprocessing improves accuracy and robustness</li>
</ul></li>
<li><p><strong>Missing Value Handling</strong></p>
<ul>
<li>Three types: MCAR, MAR, MNAR</li>
<li>Deletion methods, simple imputation, KNN imputation, multiple imputation</li>
<li>Selecting appropriate methods based on the situation</li>
</ul></li>
<li><p><strong>Outlier Handling</strong></p>
<ul>
<li>Detection using IQR method, Z-score, Isolation Forest</li>
<li>Handling via deletion, transformation, capping</li>
<li>Combining with domain knowledge</li>
</ul></li>
<li><p><strong>Scaling and Normalization</strong></p>
<ul>
<li>StandardScaler: mean 0, standard deviation 1</li>
<li>MinMaxScaler: scaling to specified range</li>
<li>RobustScaler: robust to outliers</li>
<li>Appropriate use by algorithm</li>
</ul></li>
<li><p><strong>Pipeline Construction</strong></p>
<ul>
<li>Reproducible preprocessing flow</li>
<li>Consistency between training and testing</li>
<li>Easy deployment to production environments</li>
</ul></li>
</ol>
<h3>Principles of Preprocessing</h3>
<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Understanding First</strong></td>
<td>Understand problems through visualization and statistics before processing</td>
</tr>
<tr>
<td><strong>Leverage Domain Knowledge</strong></td>
<td>Reflect business knowledge in preprocessing decisions</td>
</tr>
<tr>
<td><strong>Prevent Data Leakage</strong></td>
<td>Fit on training data and transform on test data</td>
</tr>
<tr>
<td><strong>Ensure Reproducibility</strong></td>
<td>Standardize processing with pipelines</td>
</tr>
<tr>
<td><strong>Incremental Approach</strong></td>
<td>Don't change too much at once, verify effects</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>Categorical Variable Encoding</strong>:</p>
<ul>
<li>One-Hot Encoding</li>
<li>Label Encoding</li>
<li>Target Encoding</li>
<li>Frequency Encoding</li>
<li>Handling high cardinality</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain the three types of missing values (MCAR, MAR, MNAR) and provide specific examples for each.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>MCAR (Missing Completely At Random)</strong></p>
<ul>
<li>Description: Missingness is completely random and unrelated to other variables</li>
<li>Example: Some data is not recorded due to sensor failure</li>
</ul></li>
<li><p><strong>MAR (Missing At Random)</strong></p>
<ul>
<li>Description: Missingness depends on other observed variables</li>
<li>Example: Elderly people are less likely to fill in health data (age is observed)</li>
</ul></li>
<li><p><strong>MNAR (Missing Not At Random)</strong></p>
<ul>
<li>Description: Missingness depends on the missing value itself</li>
<li>Example: Low-income individuals avoid filling in income (income itself causes the missingness)</li>
</ul></li>
</ol>
</details>
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>For the following data, detect outliers using the IQR method and report their count.</p>
<pre><code class="language-python">data = np.array([12, 15, 14, 10, 8, 12, 15, 14, 100, 13, 12, 14, 15, -5, 11])
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: For the following data, detect outliers using the IQR method

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np

data = np.array([12, 15, 14, 10, 8, 12, 15, 14, 100, 13, 12, 14, 15, -5, 11])

# IQR method
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (data &lt; lower_bound) | (data &gt; upper_bound)

print("=== Outlier Detection using IQR Method ===")
print(f"Q1: {Q1}")
print(f"Q3: {Q3}")
print(f"IQR: {IQR}")
print(f"Lower bound: {lower_bound}")
print(f"Upper bound: {upper_bound}")
print(f"\nNumber of outliers: {outliers.sum()}")
print(f"Outliers: {data[outliers]}")
print(f"Normal values: {data[~outliers]}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Outlier Detection using IQR Method ===
Q1: 11.5
Q3: 14.5
IQR: 3.0
Lower bound: 7.0
Upper bound: 19.0

Number of outliers: 2
Outliers: [100  -5]
Normal values: [12 15 14 10  8 12 15 14 13 12 14 15 11]
</code></pre>
</details>
<h3>Problem 3 (Difficulty: Medium)</h3>
<p>Explain the differences between StandardScaler and MinMaxScaler, and describe when each should be used.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>StandardScaler (Standardization)</strong>:</p>
<ul>
<li>Transformation formula: $z = \frac{x - \mu}{\sigma}$</li>
<li>Result: mean 0, standard deviation 1</li>
<li>Characteristics: Preserves distribution shape, range is undefined</li>
</ul>
<p><strong>MinMaxScaler (Normalization)</strong>:</p>
<ul>
<li>Transformation formula: $x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$</li>
<li>Result: specified range (typically [0, 1])</li>
<li>Characteristics: Fixed range, large impact from outliers</li>
</ul>
<p><strong>Usage Guidelines</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data close to normal distribution</td>
<td>StandardScaler</td>
</tr>
<tr>
<td>Range is important (e.g., [0, 1] required)</td>
<td>MinMaxScaler</td>
</tr>
<tr>
<td>Few outliers</td>
<td>Either is acceptable</td>
</tr>
<tr>
<td>Many outliers</td>
<td>RobustScaler (or standardization)</td>
</tr>
<tr>
<td>Neural networks</td>
<td>MinMaxScaler (match activation function range)</td>
</tr>
<tr>
<td>Linear models, SVM</td>
<td>StandardScaler</td>
</tr>
</tbody>
</table>
</details>
<h3>Problem 4 (Difficulty: Hard)</h3>
<p>For the following data, build a complete preprocessing pipeline that includes missing value handling and outlier handling.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following data, build a complete preprocessing pipel

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd

np.random.seed(42)
data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(100, 20, 100),
    'feature3': np.random.uniform(0, 1, 100)
})

# Add missing values
data.loc[0:10, 'feature1'] = np.nan
data.loc[20:25, 'feature2'] = np.nan

# Add outliers
data.loc[50, 'feature1'] = 200
data.loc[60, 'feature2'] = 500
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following data, build a complete preprocessing pipel

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import KNNImputer
from sklearn.compose import ColumnTransformer

np.random.seed(42)
data = pd.DataFrame({
    'feature1': np.random.normal(50, 10, 100),
    'feature2': np.random.normal(100, 20, 100),
    'feature3': np.random.uniform(0, 1, 100)
})

# Add missing values
data.loc[0:10, 'feature1'] = np.nan
data.loc[20:25, 'feature2'] = np.nan

# Add outliers
data.loc[50, 'feature1'] = 200
data.loc[60, 'feature2'] = 500

print("=== Data Before Preprocessing ===")
print(data.describe())
print(f"\nMissing values:\n{data.isnull().sum()}")

# Build pipeline
# feature1, feature2: with missing values and outliers ‚Üí KNN imputation + RobustScaler
# feature3: no issues ‚Üí StandardScaler

preprocessor = ColumnTransformer(
    transformers=[
        ('features_with_issues', Pipeline([
            ('imputer', KNNImputer(n_neighbors=5)),
            ('scaler', RobustScaler())
        ]), ['feature1', 'feature2']),

        ('clean_features', Pipeline([
            ('scaler', StandardScaler())
        ]), ['feature3'])
    ]
)

# Execute preprocessing
data_processed = preprocessor.fit_transform(data)

print("\n=== Data After Preprocessing ===")
df_processed = pd.DataFrame(
    data_processed,
    columns=['feature1', 'feature2', 'feature3']
)
print(df_processed.describe())
print(f"\nMissing values: {df_processed.isnull().sum().sum()}")

# Visualization
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, col in enumerate(['feature1', 'feature2', 'feature3']):
    # Before preprocessing
    axes[0, i].boxplot(data[col].dropna())
    axes[0, i].set_ylabel(col)
    axes[0, i].set_title(f'{col} (Before Preprocessing)', fontsize=12)
    axes[0, i].grid(True, alpha=0.3)

    # After preprocessing
    axes[1, i].boxplot(df_processed[col])
    axes[1, i].set_ylabel(col)
    axes[1, i].set_title(f'{col} (After Preprocessing)', fontsize=12)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n‚úì Pipeline construction completed")
print("‚úì Missing value imputation completed (KNN, k=5)")
print("‚úì Robust scaling to outliers completed (RobustScaler)")
</code></pre>
</details>
<h3>Problem 5 (Difficulty: Hard)</h3>
<p>Explain why data leakage occurs when scaling training and test data separately. Also demonstrate the correct method.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Why Data Leakage Occurs</strong>:</p>
<p>When scaling test data separately, you use the statistical information (mean, standard deviation, etc.) of the test data. This causes the following problems:</p>
<ol>
<li><strong>Using future information</strong>: In production, the statistics of new data are unknown in advance</li>
<li><strong>Biased evaluation</strong>: Using test data information leads to overestimated performance</li>
<li><strong>Lack of reproducibility</strong>: Cannot perform the same transformation during actual deployment</li>
</ol>
<p><strong>Incorrect Method (with data leakage)</strong>:</p>
<pre><code class="language-python"># ‚ùå Wrong
scaler_train = StandardScaler()
X_train_scaled = scaler_train.fit_transform(X_train)

scaler_test = StandardScaler()
X_test_scaled = scaler_test.fit_transform(X_test)  # Fitting on test data
</code></pre>
<p><strong>Correct Method</strong>:</p>
<pre><code class="language-python"># ‚úÖ Correct
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data
X_test_scaled = scaler.transform(X_test)  # Transform using training data statistics
</code></pre>
<p><strong>Verification with Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Verification with Example:

Purpose: Demonstrate machine learning model training and evaluation
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
from sklearn.preprocessing import StandardScaler

# Sample data
X_train = np.array([[1], [2], [3], [4], [5]])
X_test = np.array([[100], [200], [300]])

# Incorrect method
scaler_train = StandardScaler()
scaler_test = StandardScaler()
X_train_wrong = scaler_train.fit_transform(X_train)
X_test_wrong = scaler_test.fit_transform(X_test)

# Correct method
scaler = StandardScaler()
X_train_correct = scaler.fit_transform(X_train)
X_test_correct = scaler.transform(X_test)

print("=== Incorrect Method (with data leakage) ===")
print(f"Training data mean: {X_train_wrong.mean():.3f}")
print(f"Test data mean: {X_test_wrong.mean():.3f}")
print("‚Üí Both close to 0 (scaled independently)")

print("\n=== Correct Method ===")
print(f"Training data mean: {X_train_correct.mean():.3f}")
print(f"Test data mean: {X_test_correct.mean():.3f}")
print("‚Üí Test data transformed with training data statistics")
print(f"\nTest data values: {X_test_correct.flatten()}")
print("‚Üí Extremely large values compared to training distribution (correctly detected)")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Incorrect Method (with data leakage) ===
Training data mean: 0.000
Test data mean: 0.000
‚Üí Both close to 0 (scaled independently)

=== Correct Method ===
Training data mean: 0.000
Test data mean: 63.246
‚Üí Test data transformed with training data statistics

Test data values: [63.25 126.49 189.74]
‚Üí Extremely large values compared to training distribution (correctly detected)
</code></pre>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>G√©ron, A. (2019). <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O'Reilly Media.</li>
<li>Kuhn, M., &amp; Johnson, K. (2019). <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. CRC Press.</li>
<li>Zheng, A., &amp; Casari, A. (2018). <em>Feature Engineering for Machine Learning</em>. O'Reilly Media.</li>
<li>Little, R. J., &amp; Rubin, D. B. (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter2-categorical-encoding.html">Next Chapter: Categorical Variable Encoding ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University are not responsible for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>