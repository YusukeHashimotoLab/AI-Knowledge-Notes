<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 3: Feature Transformation and Generation - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Feature Transformation and Generation - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/feature-engineering-introduction/index.html">Feature Engineering</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/feature-engineering-introduction/chapter3-feature-transformation.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 3: Feature Transformation and Generation</h1>
<p class="subtitle">Unlocking Data Potential - From Transformation Techniques to Domain Knowledge, Kaggle-Style Feature Engineering</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 12</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Feature Transformation and Generation. You will learn and utilize binning (discretization), Generate polynomial features, and Design domain knowledge-based features.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the purpose and effects of feature transformation</li>
<li>‚úÖ Apply logarithmic and Box-Cox transformations</li>
<li>‚úÖ Implement and utilize binning (discretization)</li>
<li>‚úÖ Generate polynomial features and interaction terms</li>
<li>‚úÖ Design domain knowledge-based features</li>
<li>‚úÖ Create datetime, text, and aggregated features</li>
<li>‚úÖ Master feature generation patterns used in Kaggle competitions</li>
</ul>
<hr/>
<h2>3.1 Purpose of Feature Transformation</h2>
<h3>Why Feature Transformation is Necessary</h3>
<p>Using raw data as-is can limit model performance. Feature transformation enables you to:</p>
<blockquote>
<p>"Good features are more powerful than complex models. Transformations reveal hidden information in data"</p>
</blockquote>
<h3>Major Types of Transformations</h3>
<div class="mermaid">
graph TD
    A[Feature Transformation] --&gt; B[Numerical Transformation]
    A --&gt; C[Discretization]
    A --&gt; D[Feature Generation]

    B --&gt; B1[Log Transform<br/>Normalization]
    B --&gt; B2[Power Transform<br/>Box-Cox]

    C --&gt; C1[Binning<br/>Categorization]
    C --&gt; C2[Equal Width/Frequency<br/>Custom]

    D --&gt; D1[Polynomial Features<br/>Interactions]
    D --&gt; D2[Domain Knowledge<br/>Aggregation Stats]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>
<h3>Effects of Transformation</h3>
<table>
<thead>
<tr>
<th>Transformation Purpose</th>
<th>Application Scenario</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Distribution Normalization</strong></td>
<td>Skewed distributions</td>
<td>Improved linear model performance</td>
</tr>
<tr>
<td><strong>Outlier Impact Reduction</strong></td>
<td>Presence of extreme values</td>
<td>Increased robustness</td>
</tr>
<tr>
<td><strong>Capturing Nonlinear Relationships</strong></td>
<td>Complex relationships</td>
<td>Enhanced expressiveness</td>
</tr>
<tr>
<td><strong>Improved Interpretability</strong></td>
<td>Natural categorization</td>
<td>Promoted business understanding</td>
</tr>
<tr>
<td><strong>Feature Interactions</strong></td>
<td>Important combinations</td>
<td>Improved prediction accuracy</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.2 Numerical Transformations</h2>
<h3>Logarithmic Transform</h3>
<p><strong>Logarithmic transformation</strong> has the effect of making right-skewed distributions closer to normal distributions.</p>
<p>$$
y = \log(x) \quad \text{or} \quad y = \log(x + 1)
$$</p>
<ul>
<li><code>log(x)</code>: requires $x &gt; 0$</li>
<li><code>log1p(x)</code>: safe for $x \geq 0$ ($\log(1 + x)$)</li>
</ul>
<h3>Implementation Example: Logarithmic Transform</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

"""
Example: Implementation Example: Logarithmic Transform

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate right-skewed distribution
np.random.seed(42)
data_skewed = np.random.lognormal(mean=0, sigma=1, size=1000)

# Logarithmic transformation
data_log = np.log(data_skewed)
data_log1p = np.log1p(data_skewed)

print("=== Distribution Changes via Log Transform ===")
print(f"Original data: skewness={stats.skew(data_skewed):.3f}, kurtosis={stats.kurtosis(data_skewed):.3f}")
print(f"After log transform: skewness={stats.skew(data_log):.3f}, kurtosis={stats.kurtosis(data_log):.3f}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Original data
axes[0, 0].hist(data_skewed, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
axes[0, 0].set_xlabel('Value', fontsize=12)
axes[0, 0].set_ylabel('Frequency', fontsize=12)
axes[0, 0].set_title(f'Original Data (skewness: {stats.skew(data_skewed):.3f})', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)

# Q-Q plot (original data)
stats.probplot(data_skewed, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Q-Q Plot of Original Data', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)

# After log transformation
axes[1, 0].hist(data_log, bins=50, edgecolor='black', alpha=0.7, color='green')
axes[1, 0].set_xlabel('Value', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title(f'After Log Transform (skewness: {stats.skew(data_log):.3f})', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)

# Q-Q plot (after transformation)
stats.probplot(data_log, dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot After Log Transform', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Distribution Changes via Log Transform ===
Original data: skewness=6.251, kurtosis=110.582
After log transform: skewness=0.034, kurtosis=-0.157
</code></pre>
<h3>Box-Cox Transform</h3>
<p><strong>Box-Cox transformation</strong> automatically finds the optimal parameter $\lambda$ for transformation.</p>
<p>$$
y(\lambda) = \begin{cases}
\frac{x^\lambda - 1}{\lambda} &amp; \text{if } \lambda \neq 0 \\
\log(x) &amp; \text{if } \lambda = 0
\end{cases}
$$</p>
<ul>
<li>$\lambda = 1$: no transformation</li>
<li>$\lambda = 0.5$: square root transformation</li>
<li>$\lambda = 0$: logarithmic transformation</li>
<li>$\lambda = -1$: reciprocal transformation</li>
</ul>
<h3>Implementation Example: Box-Cox Transform and PowerTransformer</h3>
<pre><code class="language-python">from sklearn.preprocessing import PowerTransformer
from scipy.stats import boxcox

# Box-Cox transformation (scipy version)
data_boxcox, lambda_param = boxcox(data_skewed)

print(f"\n=== Box-Cox Transform ===")
print(f"Optimal Œª: {lambda_param:.4f}")
print(f"Skewness after transformation: {stats.skew(data_boxcox):.3f}")

# PowerTransformer (sklearn version) - supports multiple features
X = data_skewed.reshape(-1, 1)

# Box-Cox method
pt_boxcox = PowerTransformer(method='box-cox', standardize=True)
X_boxcox = pt_boxcox.fit_transform(X)

# Yeo-Johnson method (can handle negative values)
X_with_negative = np.concatenate([data_skewed, -data_skewed[:100]])
X_neg = X_with_negative.reshape(-1, 1)

pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=True)
X_yeojohnson = pt_yeojohnson.fit_transform(X_neg)

print(f"\n=== PowerTransformer ===")
print(f"Box-Cox lambda: {pt_boxcox.lambdas_[0]:.4f}")
print(f"Yeo-Johnson lambda: {pt_yeojohnson.lambdas_[0]:.4f}")

# Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Original data
axes[0, 0].hist(data_skewed, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
axes[0, 0].set_title('Original Data', fontsize=14)
axes[0, 0].set_xlabel('Value', fontsize=12)
axes[0, 0].grid(True, alpha=0.3)

# Box-Cox (scipy)
axes[0, 1].hist(data_boxcox, bins=50, edgecolor='black', alpha=0.7, color='green')
axes[0, 1].set_title(f'Box-Cox (Œª={lambda_param:.3f})', fontsize=14)
axes[0, 1].set_xlabel('Value', fontsize=12)
axes[0, 1].grid(True, alpha=0.3)

# PowerTransformer Box-Cox
axes[0, 2].hist(X_boxcox, bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[0, 2].set_title('PowerTransformer (Box-Cox)', fontsize=14)
axes[0, 2].set_xlabel('Value', fontsize=12)
axes[0, 2].grid(True, alpha=0.3)

# Q-Q plots
for i, (data, title) in enumerate([
    (data_skewed, 'Original Data'),
    (data_boxcox, 'Box-Cox'),
    (X_boxcox.flatten(), 'PowerTransformer')
]):
    stats.probplot(data, dist="norm", plot=axes[1, i])
    axes[1, i].set_title(f'{title} Q-Q', fontsize=14)
    axes[1, i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Box-Cox Transform ===
Optimal Œª: 0.0234
Skewness after transformation: 0.028

=== PowerTransformer ===
Box-Cox lambda: 0.0234
Yeo-Johnson lambda: 0.1456
</code></pre>
<h3>Transformation Method Selection Guide</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Application Condition</th>
<th>Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>log transform</strong></td>
<td>$x &gt; 0$</td>
<td>Simple, easy to interpret</td>
</tr>
<tr>
<td><strong>log1p transform</strong></td>
<td>$x \geq 0$</td>
<td>Safe for data containing zeros</td>
</tr>
<tr>
<td><strong>Square root transform</strong></td>
<td>$x \geq 0$</td>
<td>Suitable for count data</td>
</tr>
<tr>
<td><strong>Box-Cox</strong></td>
<td>$x &gt; 0$</td>
<td>Automatically selects optimal transformation</td>
</tr>
<tr>
<td><strong>Yeo-Johnson</strong></td>
<td>Any value</td>
<td>Can handle negative values</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.3 Binning</h2>
<h3>Overview</h3>
<p><strong>Binning</strong> is a technique for converting continuous values into discrete categories.</p>
<h3>Types of Binning</h3>
<div class="mermaid">
graph LR
    A[Binning Methods] --&gt; B[Equal Width<br/>Binning]
    A --&gt; C[Equal Frequency<br/>Binning]
    A --&gt; D[Custom Binning<br/>Domain Knowledge]

    B --&gt; B1[Same width for each bin]
    C --&gt; C1[Same data count per bin]
    D --&gt; D1[Set boundaries using domain knowledge]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
</div>
<h3>Implementation Example: KBinsDiscretizer</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Implementation Example: KBinsDiscretizer

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from sklearn.preprocessing import KBinsDiscretizer
import pandas as pd

# Sample data: age
np.random.seed(42)
ages = np.random.normal(40, 15, 500)
ages = np.clip(ages, 18, 80)  # Limit to 18-80 years old
X_age = ages.reshape(-1, 1)

print("=== Binning ===")
print(f"Age data: min={ages.min():.1f}, max={ages.max():.1f}, mean={ages.mean():.1f}")

# Equal width binning
kbd_uniform = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')
age_binned_uniform = kbd_uniform.fit_transform(X_age)

# Equal frequency binning
kbd_quantile = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
age_binned_quantile = kbd_quantile.fit_transform(X_age)

# Custom binning (pandas.cut)
age_custom_bins = pd.cut(ages,
                         bins=[0, 25, 35, 50, 65, 100],
                         labels=['Young', 'Young Adult', 'Middle Age', 'Senior', 'Elderly'])

# Display bin boundaries
print("\n--- Equal Width Binning ---")
for i, edge in enumerate(kbd_uniform.bin_edges_[0]):
    print(f"Boundary {i}: {edge:.2f}")

print("\n--- Equal Frequency Binning ---")
for i, edge in enumerate(kbd_quantile.bin_edges_[0]):
    print(f"Boundary {i}: {edge:.2f}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Original data histogram
axes[0, 0].hist(ages, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
axes[0, 0].set_xlabel('Age', fontsize=12)
axes[0, 0].set_ylabel('Frequency', fontsize=12)
axes[0, 0].set_title('Original Data', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)

# Equal width binning
for i in range(5):
    mask = age_binned_uniform.flatten() == i
    axes[0, 1].hist(ages[mask], bins=10, alpha=0.7, label=f'Bin {i}')
axes[0, 1].set_xlabel('Age', fontsize=12)
axes[0, 1].set_ylabel('Frequency', fontsize=12)
axes[0, 1].set_title('Equal Width Binning (Same bin width)', fontsize=14)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Equal frequency binning
for i in range(5):
    mask = age_binned_quantile.flatten() == i
    axes[1, 0].hist(ages[mask], bins=10, alpha=0.7, label=f'Bin {i}')
axes[1, 0].set_xlabel('Age', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Equal Frequency Binning (Same data count per bin)', fontsize=14)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Custom binning
age_custom_bins.value_counts().sort_index().plot(kind='bar',
                                                   ax=axes[1, 1],
                                                   color='green',
                                                   alpha=0.7)
axes[1, 1].set_xlabel('Age Group', fontsize=12)
axes[1, 1].set_ylabel('Data Count', fontsize=12)
axes[1, 1].set_title('Custom Binning (Domain Knowledge Based)', fontsize=14)
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Data count per bin
print("\n--- Data Count Comparison ---")
print(f"Equal Width Binning: {np.bincount(age_binned_uniform.astype(int).flatten())}")
print(f"Equal Frequency Binning: {np.bincount(age_binned_quantile.astype(int).flatten())}")
print(f"Custom Binning: {age_custom_bins.value_counts().sort_index().values}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Binning ===
Age data: min=18.0, max=79.9, mean=40.1

--- Equal Width Binning ---
Boundary 0: 18.00
Boundary 1: 30.38
Boundary 2: 42.76
Boundary 3: 55.14
Boundary 4: 67.52
Boundary 5: 79.90

--- Equal Frequency Binning ---
Boundary 0: 18.00
Boundary 1: 30.89
Boundary 2: 38.12
Boundary 3: 46.54
Boundary 4: 56.23
Boundary 5: 79.90

--- Data Count Comparison ---
Equal Width Binning: [172 136  99  65  28]
Equal Frequency Binning: [100 100 100 100 100]
Custom Binning: [ 76 112 167 111  34]
</code></pre>
<h3>Advantages and Disadvantages of Binning</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Interpretability</strong></td>
<td>Easy to understand as categories</td>
<td>Original detailed information is lost</td>
</tr>
<tr>
<td><strong>Outliers</strong></td>
<td>Reduces outlier impact</td>
<td>Useful information is also smoothed</td>
</tr>
<tr>
<td><strong>Nonlinearity</strong></td>
<td>Can capture nonlinear relationships</td>
<td>Difficult to choose bin count</td>
</tr>
<tr>
<td><strong>Models</strong></td>
<td>Linear models can represent step-wise relationships</td>
<td>Unnecessary for tree-based models</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.4 Polynomial Features</h2>
<h3>Overview</h3>
<p><strong>Polynomial features</strong> capture nonlinear relationships by creating powers and combinations of original features.</p>
<p>For example, generating 2nd-degree polynomial features from features $x_1, x_2$:</p>
<p>$$
[x_1, x_2] \rightarrow [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]
$$</p>
<h3>Implementation Example: PolynomialFeatures</h3>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Generate nonlinear data
np.random.seed(42)
X_poly = np.random.uniform(-3, 3, 200).reshape(-1, 1)
y_poly = 0.5 * X_poly**2 + X_poly + 2 + np.random.normal(0, 0.5, X_poly.shape)

# Data split
X_train, X_test, y_train, y_test = train_test_split(
    X_poly, y_poly, test_size=0.3, random_state=42
)

# Model 1: Linear regression (without polynomial features)
model_linear = LinearRegression()
model_linear.fit(X_train, y_train)
y_pred_linear = model_linear.predict(X_test)

# Model 2: 2nd-degree polynomial features
poly2 = PolynomialFeatures(degree=2, include_bias=True)
X_train_poly2 = poly2.fit_transform(X_train)
X_test_poly2 = poly2.transform(X_test)

model_poly2 = LinearRegression()
model_poly2.fit(X_train_poly2, y_train)
y_pred_poly2 = model_poly2.predict(X_test_poly2)

# Model 3: 3rd-degree polynomial features
poly3 = PolynomialFeatures(degree=3, include_bias=True)
X_train_poly3 = poly3.fit_transform(X_train)
X_test_poly3 = poly3.transform(X_test)

model_poly3 = LinearRegression()
model_poly3.fit(X_train_poly3, y_train)
y_pred_poly3 = model_poly3.predict(X_test_poly3)

# Evaluation
print("=== Effects of Polynomial Features ===")
print(f"Linear Regression: RMSE={np.sqrt(mean_squared_error(y_test, y_pred_linear)):.4f}, "
      f"R¬≤={r2_score(y_test, y_pred_linear):.4f}")
print(f"2nd-degree Polynomial: RMSE={np.sqrt(mean_squared_error(y_test, y_pred_poly2)):.4f}, "
      f"R¬≤={r2_score(y_test, y_pred_poly2):.4f}")
print(f"3rd-degree Polynomial: RMSE={np.sqrt(mean_squared_error(y_test, y_pred_poly3)):.4f}, "
      f"R¬≤={r2_score(y_test, y_pred_poly3):.4f}")

# Generated features
print(f"\nOriginal feature count: {X_train.shape[1]}")
print(f"After 2nd-degree polynomial: {X_train_poly2.shape[1]}")
print(f"After 3rd-degree polynomial: {X_train_poly3.shape[1]}")
print(f"2nd-degree polynomial feature names: {poly2.get_feature_names_out(['x'])}")

# Visualization
X_range = np.linspace(-3, 3, 300).reshape(-1, 1)
y_pred_range_linear = model_linear.predict(X_range)
y_pred_range_poly2 = model_poly2.predict(poly2.transform(X_range))
y_pred_range_poly3 = model_poly3.predict(poly3.transform(X_range))

plt.figure(figsize=(14, 6))

# Left: Data and prediction curves
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, alpha=0.5, label='Test Data', color='gray')
plt.plot(X_range, y_pred_range_linear, linewidth=2, label='Linear Regression', color='blue')
plt.plot(X_range, y_pred_range_poly2, linewidth=2, label='2nd-degree Polynomial', color='green')
plt.plot(X_range, y_pred_range_poly3, linewidth=2, label='3rd-degree Polynomial', color='red', linestyle='--')
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Regression with Polynomial Features', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Right: Residual plot
plt.subplot(1, 2, 2)
residuals_linear = y_test - y_pred_linear
residuals_poly2 = y_test - y_pred_poly2
residuals_poly3 = y_test - y_pred_poly3

plt.scatter(y_pred_linear, residuals_linear, alpha=0.5, label='Linear Regression', color='blue')
plt.scatter(y_pred_poly2, residuals_poly2, alpha=0.5, label='2nd-degree Polynomial', color='green')
plt.scatter(y_pred_poly3, residuals_poly3, alpha=0.5, label='3rd-degree Polynomial', color='red')
plt.axhline(y=0, color='black', linestyle='--', linewidth=1)
plt.xlabel('Predicted Value', fontsize=12)
plt.ylabel('Residual', fontsize=12)
plt.title('Residual Plot', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Effects of Polynomial Features ===
Linear Regression: RMSE=1.8456, R¬≤=0.4821
2nd-degree Polynomial: RMSE=0.5234, R¬≤=0.9567
3rd-degree Polynomial: RMSE=0.5289, R¬≤=0.9558

Original feature count: 1
After 2nd-degree polynomial: 3
After 3rd-degree polynomial: 4
2nd-degree polynomial feature names: ['1' 'x' 'x^2']
</code></pre>
<h3>Importance of Interaction Terms</h3>
<pre><code class="language-python"># Case with two features
np.random.seed(42)
X1 = np.random.uniform(0, 10, 200)
X2 = np.random.uniform(0, 10, 200)

# Target variable with interactions: y = X1 + X2 + 0.5 * X1 * X2
y_interact = X1 + X2 + 0.5 * X1 * X2 + np.random.normal(0, 1, 200)

X_interact = np.column_stack([X1, X2])

# Data split
X_train_int, X_test_int, y_train_int, y_test_int = train_test_split(
    X_interact, y_interact, test_size=0.3, random_state=42
)

# Model 1: Without interaction terms
model_no_interact = LinearRegression()
model_no_interact.fit(X_train_int, y_train_int)
y_pred_no_interact = model_no_interact.predict(X_test_int)

# Model 2: With interaction terms (interaction_only=True)
poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_interact = poly_interact.fit_transform(X_train_int)
X_test_interact = poly_interact.transform(X_test_int)

model_interact = LinearRegression()
model_interact.fit(X_train_interact, y_train_int)
y_pred_interact = model_interact.predict(X_test_interact)

# Model 3: Full 2nd-degree polynomial (including power terms)
poly_full = PolynomialFeatures(degree=2, include_bias=False)
X_train_full = poly_full.fit_transform(X_train_int)
X_test_full = poly_full.transform(X_test_int)

model_full = LinearRegression()
model_full.fit(X_train_full, y_train_int)
y_pred_full = model_full.predict(X_test_full)

print("=== Effects of Interaction Terms ===")
print(f"Without interactions: RMSE={np.sqrt(mean_squared_error(y_test_int, y_pred_no_interact)):.4f}, "
      f"R¬≤={r2_score(y_test_int, y_pred_no_interact):.4f}")
print(f"Interactions only: RMSE={np.sqrt(mean_squared_error(y_test_int, y_pred_interact)):.4f}, "
      f"R¬≤={r2_score(y_test_int, y_pred_interact):.4f}")
print(f"Full 2nd-degree: RMSE={np.sqrt(mean_squared_error(y_test_int, y_pred_full)):.4f}, "
      f"R¬≤={r2_score(y_test_int, y_pred_full):.4f}")

print(f"\nInteraction-only features: {poly_interact.get_feature_names_out(['X1', 'X2'])}")
print(f"Full 2nd-degree features: {poly_full.get_feature_names_out(['X1', 'X2'])}")

# Coefficient comparison
print("\n--- Learned Coefficients ---")
print(f"Without interactions: X1={model_no_interact.coef_[0]:.3f}, X2={model_no_interact.coef_[1]:.3f}")
print(f"With interactions: X1={model_interact.coef_[0]:.3f}, X2={model_interact.coef_[1]:.3f}, "
      f"X1*X2={model_interact.coef_[2]:.3f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Effects of Interaction Terms ===
Without interactions: RMSE=12.8456, R¬≤=0.6234
Interactions only: RMSE=0.9823, R¬≤=0.9987
Full 2nd-degree: RMSE=0.9876, R¬≤=0.9986

Interaction-only features: ['X1' 'X2' 'X1 X2']
Full 2nd-degree features: ['X1' 'X2' 'X1^2' 'X1 X2' 'X2^2']

--- Learned Coefficients ---
Without interactions: X1=3.567, X2=3.489
With interactions: X1=1.012, X2=0.989, X1*X2=0.498
</code></pre>
<h3>Degree Selection</h3>
<table>
<thead>
<tr>
<th>Degree</th>
<th>Feature Count (p features)</th>
<th>Application Scenario</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1st degree</strong></td>
<td>$p$</td>
<td>Linear relationships</td>
</tr>
<tr>
<td><strong>2nd degree</strong></td>
<td>$\frac{p(p+3)}{2}$</td>
<td>Curved relationships, interactions</td>
</tr>
<tr>
<td><strong>3rd degree</strong></td>
<td>$\frac{p(p+1)(p+2)}{6}$</td>
<td>Complex nonlinear relationships</td>
</tr>
<tr>
<td><strong>Interactions only</strong></td>
<td>$p + \frac{p(p-1)}{2}$</td>
<td>Power terms not needed</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.5 Domain Knowledge-Based Features</h2>
<h3>Datetime Features</h3>
<p>Extract information such as year, month, day of week, and holidays from datetime data.</p>
<h3>Implementation Example: Datetime Feature Extraction</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Implementation Example: Datetime Feature Extraction

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import pandas as pd
from datetime import datetime, timedelta

# Sample data: Sales data
np.random.seed(42)
dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')
n = len(dates)

# Sales data (including day of week, seasonal, and holiday effects)
df_sales = pd.DataFrame({
    'date': dates,
    'sales': np.random.poisson(100, n) + \
             10 * (dates.dayofweek &lt; 5) + \  # Weekday bonus
             20 * (dates.month.isin([11, 12]))  # Year-end bonus
})

print("=== Datetime Feature Generation ===")
print(df_sales.head())

# Datetime feature extraction
df_sales['year'] = df_sales['date'].dt.year
df_sales['month'] = df_sales['date'].dt.month
df_sales['day'] = df_sales['date'].dt.day
df_sales['dayofweek'] = df_sales['date'].dt.dayofweek  # 0=Monday, 6=Sunday
df_sales['dayofyear'] = df_sales['date'].dt.dayofyear
df_sales['quarter'] = df_sales['date'].dt.quarter
df_sales['is_weekend'] = (df_sales['dayofweek'] &gt;= 5).astype(int)
df_sales['is_month_start'] = df_sales['date'].dt.is_month_start.astype(int)
df_sales['is_month_end'] = df_sales['date'].dt.is_month_end.astype(int)
df_sales['week_of_year'] = df_sales['date'].dt.isocalendar().week

# Cyclic features (sin/cos transformation)
df_sales['month_sin'] = np.sin(2 * np.pi * df_sales['month'] / 12)
df_sales['month_cos'] = np.cos(2 * np.pi * df_sales['month'] / 12)
df_sales['dayofweek_sin'] = np.sin(2 * np.pi * df_sales['dayofweek'] / 7)
df_sales['dayofweek_cos'] = np.cos(2 * np.pi * df_sales['dayofweek'] / 7)

print("\n--- Generated Features ---")
print(df_sales.head(10))
print(f"\nFeature count: {df_sales.shape[1]}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Sales by day of week
dayofweek_sales = df_sales.groupby('dayofweek')['sales'].mean()
axes[0, 0].bar(range(7), dayofweek_sales.values, color='steelblue', alpha=0.7)
axes[0, 0].set_xlabel('Day of Week', fontsize=12)
axes[0, 0].set_ylabel('Average Sales', fontsize=12)
axes[0, 0].set_title('Average Sales by Day of Week', fontsize=14)
axes[0, 0].set_xticks(range(7))
axes[0, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
axes[0, 0].grid(True, alpha=0.3)

# Sales by month
month_sales = df_sales.groupby('month')['sales'].mean()
axes[0, 1].bar(range(1, 13), month_sales.values, color='green', alpha=0.7)
axes[0, 1].set_xlabel('Month', fontsize=12)
axes[0, 1].set_ylabel('Average Sales', fontsize=12)
axes[0, 1].set_title('Average Sales by Month', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)

# Visualization of cyclic features (month)
axes[1, 0].scatter(df_sales['month_sin'], df_sales['month_cos'],
                  c=df_sales['month'], cmap='viridis', alpha=0.6)
axes[1, 0].set_xlabel('month_sin', fontsize=12)
axes[1, 0].set_ylabel('month_cos', fontsize=12)
axes[1, 0].set_title('Cyclic Representation of Month (sin/cos)', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)

# Time series plot (3 months)
df_sample = df_sales[df_sales['date'] &lt; '2023-04-01']
axes[1, 1].plot(df_sample['date'], df_sample['sales'], linewidth=1, color='steelblue')
axes[1, 1].set_xlabel('Date', fontsize=12)
axes[1, 1].set_ylabel('Sales', fontsize=12)
axes[1, 1].set_title('Sales Time Series (Jan-Mar 2023)', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Datetime Feature Generation ===
        date  sales
0 2023-01-01    105
1 2023-01-02    118
2 2023-01-03    113
3 2023-01-04    121
4 2023-01-05    115

--- Generated Features ---
Feature count: 16
</code></pre>
<h3>Text Features</h3>
<pre><code class="language-python"># Generate features from text data
texts = [
    "Machine Learning is awesome!",
    "Deep learning revolutionizes AI",
    "Natural Language Processing",
    "Computer Vision applications",
    "Data Science for everyone"
]

df_text = pd.DataFrame({'text': texts})

# Basic features
df_text['text_length'] = df_text['text'].str.len()
df_text['word_count'] = df_text['text'].str.split().str.len()
df_text['avg_word_length'] = df_text['text_length'] / df_text['word_count']
df_text['uppercase_count'] = df_text['text'].str.count(r'[A-Z]')
df_text['digit_count'] = df_text['text'].str.count(r'\d')
df_text['special_char_count'] = df_text['text'].str.count(r'[!@#$%^&amp;*(),.?":{}|&lt;&gt;]')

# Presence of specific keywords
df_text['has_learning'] = df_text['text'].str.contains('learning', case=False).astype(int)
df_text['has_ai'] = df_text['text'].str.contains('AI|artificial', case=False).astype(int)

print("=== Text Features ===")
print(df_text)

# TF-IDF features (reference)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=10, stop_words='english')
tfidf_features = tfidf.fit_transform(texts).toarray()

print("\n--- TF-IDF Features ---")
print(f"Feature count: {tfidf_features.shape[1]}")
print(f"Feature names: {tfidf.get_feature_names_out()}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Text Features ===
                              text  text_length  word_count  avg_word_length  ...
0  Machine Learning is awesome!            29           4             7.25  ...
1  Deep learning revolutionizes AI           31           4             7.75  ...
2  Natural Language Processing            27           3             9.00  ...
3  Computer Vision applications             28           3             9.33  ...
4  Data Science for everyone                25           4             6.25  ...

--- TF-IDF Features ---
Feature count: 10
Feature names: ['ai' 'applications' 'awesome' 'computer' 'data' 'deep' 'learning' 'machine' 'natural' 'processing']
</code></pre>
<h3>Aggregated Features</h3>
<pre><code class="language-python"># Sample data: User purchase history
np.random.seed(42)
df_purchase = pd.DataFrame({
    'user_id': np.repeat(range(1, 101), 10),
    'product_id': np.random.randint(1, 50, 1000),
    'price': np.random.uniform(10, 500, 1000),
    'quantity': np.random.randint(1, 5, 1000)
})

df_purchase['total_amount'] = df_purchase['price'] * df_purchase['quantity']

print("=== Aggregated Feature Generation ===")
print(df_purchase.head(10))

# Aggregated statistics per user
user_features = df_purchase.groupby('user_id').agg({
    'total_amount': ['sum', 'mean', 'std', 'min', 'max', 'count'],
    'price': ['mean', 'std'],
    'quantity': ['sum', 'mean'],
    'product_id': ['nunique']  # Number of unique products
}).reset_index()

# Clean up column names
user_features.columns = ['user_id',
                        'total_spent', 'avg_purchase', 'std_purchase',
                        'min_purchase', 'max_purchase', 'num_purchases',
                        'avg_price', 'std_price',
                        'total_quantity', 'avg_quantity',
                        'num_unique_products']

# Additional features
user_features['purchase_variety'] = user_features['num_unique_products'] / user_features['num_purchases']
user_features['avg_items_per_purchase'] = user_features['total_quantity'] / user_features['num_purchases']
user_features['price_range'] = user_features['max_purchase'] - user_features['min_purchase']

print("\n--- User-Level Aggregated Features ---")
print(user_features.head(10))
print(f"\nGenerated feature count: {user_features.shape[1] - 1}")  # Excluding user_id

# Visualization of statistics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Distribution of total spending
axes[0, 0].hist(user_features['total_spent'], bins=30,
               edgecolor='black', alpha=0.7, color='steelblue')
axes[0, 0].set_xlabel('Total Spending', fontsize=12)
axes[0, 0].set_ylabel('User Count', fontsize=12)
axes[0, 0].set_title('Distribution of Total Spending', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)

# Purchase count vs total spending
axes[0, 1].scatter(user_features['num_purchases'],
                  user_features['total_spent'], alpha=0.6)
axes[0, 1].set_xlabel('Purchase Count', fontsize=12)
axes[0, 1].set_ylabel('Total Spending', fontsize=12)
axes[0, 1].set_title('Relationship Between Purchase Count and Total Spending', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)

# Distribution of product variety
axes[1, 0].hist(user_features['purchase_variety'], bins=20,
               edgecolor='black', alpha=0.7, color='green')
axes[1, 0].set_xlabel('Purchase Variety', fontsize=12)
axes[1, 0].set_ylabel('User Count', fontsize=12)
axes[1, 0].set_title('Product Purchase Variety (unique/total)', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)

# Average price vs standard deviation
axes[1, 1].scatter(user_features['avg_price'],
                  user_features['std_price'], alpha=0.6, color='red')
axes[1, 1].set_xlabel('Average Price', fontsize=12)
axes[1, 1].set_ylabel('Price Standard Deviation', fontsize=12)
axes[1, 1].set_title('Average Price and Price Variability', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Aggregated Feature Generation ===
   user_id  product_id   price  quantity  total_amount
0        1          40  234.56         3        703.68
1        1          23  123.45         2        246.90
...

--- User-Level Aggregated Features ---
   user_id  total_spent  avg_purchase  ...  avg_items_per_purchase  price_range
0        1      5234.56        523.46  ...                    2.30       678.90
1        2      6789.12        678.91  ...                    2.50       890.23
...

Generated feature count: 14
</code></pre>
<hr/>
<h2>3.6 Practical Example: Kaggle-Style Feature Generation Pipeline</h2>
<h3>Problem Setting</h3>
<p>Implement comprehensive feature engineering for housing price prediction.</p>
<h3>Implementation Example: Complete Feature Generation Pipeline</h3>
<pre><code class="language-python">from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load data
housing = fetch_california_housing()
X_original = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target

print("=== Kaggle-Style Feature Generation Pipeline ===")
print(f"Original feature count: {X_original.shape[1]}")
print("\nOriginal features:")
print(X_original.head())

# ===== Feature Engineering =====

X_fe = X_original.copy()

# 1. Numerical transformations
X_fe['Population_log'] = np.log1p(X_fe['Population'])
X_fe['AveRooms_log'] = np.log1p(X_fe['AveRooms'])

# 2. Domain knowledge-based features
X_fe['RoomsPerHousehold'] = X_fe['AveRooms'] / X_fe['AveBedrms']
X_fe['PopulationPerHousehold'] = X_fe['Population'] / X_fe['HouseAge']
X_fe['BedroomsRatio'] = X_fe['AveBedrms'] / X_fe['AveRooms']

# 3. Statistical features
X_fe['Income_squared'] = X_fe['MedInc'] ** 2
X_fe['AveRooms_squared'] = X_fe['AveRooms'] ** 2

# 4. Interaction terms
X_fe['Income_x_Rooms'] = X_fe['MedInc'] * X_fe['AveRooms']
X_fe['Income_x_HouseAge'] = X_fe['MedInc'] * X_fe['HouseAge']
X_fe['Latitude_x_Longitude'] = X_fe['Latitude'] * X_fe['Longitude']

# 5. Binning
X_fe['Income_binned'] = pd.cut(X_fe['MedInc'], bins=5, labels=False)
X_fe['HouseAge_binned'] = pd.cut(X_fe['HouseAge'], bins=5, labels=False)

# 6. Aggregated features (geographic aggregation)
# Grid-based latitude/longitude
X_fe['Lat_grid'] = (X_fe['Latitude'] * 10).astype(int)
X_fe['Lon_grid'] = (X_fe['Longitude'] * 10).astype(int)
X_fe['Grid_id'] = X_fe['Lat_grid'].astype(str) + '_' + X_fe['Lon_grid'].astype(str)

# Statistics per grid
grid_stats = X_fe.groupby('Grid_id')['MedInc'].agg(['mean', 'std', 'count']).reset_index()
grid_stats.columns = ['Grid_id', 'Grid_avg_income', 'Grid_std_income', 'Grid_count']

X_fe = X_fe.merge(grid_stats, on='Grid_id', how='left')

# Difference from grid statistics
X_fe['Income_vs_grid_avg'] = X_fe['MedInc'] - X_fe['Grid_avg_income']

# Remove unnecessary columns
X_fe = X_fe.drop(['Lat_grid', 'Lon_grid', 'Grid_id'], axis=1)

print(f"\nFeature count after feature engineering: {X_fe.shape[1]}")
print("\nGenerated features:")
print(X_fe.head())

# ===== Model Comparison =====

# Data split
X_train_orig, X_test_orig, y_train, y_test = train_test_split(
    X_original, y, test_size=0.2, random_state=42
)

X_train_fe, X_test_fe, _, _ = train_test_split(
    X_fe, y, test_size=0.2, random_state=42
)

# Model 1: Original features
model_orig = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model_orig.fit(X_train_orig, y_train)
y_pred_orig = model_orig.predict(X_test_orig)

# Model 2: After feature engineering
model_fe = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model_fe.fit(X_train_fe, y_train)
y_pred_fe = model_fe.predict(X_test_fe)

# Evaluation
print("\n=== Model Performance Comparison ===")
print(f"„ÄêOriginal Features„Äë")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_orig)):.4f}")
print(f"  MAE: {mean_absolute_error(y_test, y_pred_orig):.4f}")
print(f"  R¬≤: {r2_score(y_test, y_pred_orig):.4f}")

print(f"\n„ÄêAfter Feature Engineering„Äë")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_fe)):.4f}")
print(f"  MAE: {mean_absolute_error(y_test, y_pred_fe):.4f}")
print(f"  R¬≤: {r2_score(y_test, y_pred_fe):.4f}")

# Feature importance
importances_fe = model_fe.feature_importances_
indices = np.argsort(importances_fe)[::-1][:15]

print("\n--- Top 15 Important Features ---")
for i, idx in enumerate(indices, 1):
    print(f"{i}. {X_fe.columns[idx]}: {importances_fe[idx]:.4f}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Predicted vs actual (original features)
axes[0, 0].scatter(y_test, y_pred_orig, alpha=0.5, color='blue')
axes[0, 0].plot([y_test.min(), y_test.max()],
               [y_test.min(), y_test.max()],
               'r--', linewidth=2)
axes[0, 0].set_xlabel('Actual Value', fontsize=12)
axes[0, 0].set_ylabel('Predicted Value', fontsize=12)
axes[0, 0].set_title(f'Original Features (R¬≤={r2_score(y_test, y_pred_orig):.4f})', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)

# Predicted vs actual (after FE)
axes[0, 1].scatter(y_test, y_pred_fe, alpha=0.5, color='green')
axes[0, 1].plot([y_test.min(), y_test.max()],
               [y_test.min(), y_test.max()],
               'r--', linewidth=2)
axes[0, 1].set_xlabel('Actual Value', fontsize=12)
axes[0, 1].set_ylabel('Predicted Value', fontsize=12)
axes[0, 1].set_title(f'After FE (R¬≤={r2_score(y_test, y_pred_fe):.4f})', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)

# Residual distribution comparison
residuals_orig = y_test - y_pred_orig
residuals_fe = y_test - y_pred_fe

axes[1, 0].hist(residuals_orig, bins=50, alpha=0.5, label='Original', color='blue', edgecolor='black')
axes[1, 0].hist(residuals_fe, bins=50, alpha=0.5, label='After FE', color='green', edgecolor='black')
axes[1, 0].set_xlabel('Residual', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Residual Distribution', fontsize=14)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Feature importance
top_features = X_fe.columns[indices[:15]]
top_importances = importances_fe[indices[:15]]

axes[1, 1].barh(range(len(top_features)), top_importances, color='steelblue', alpha=0.7)
axes[1, 1].set_yticks(range(len(top_features)))
axes[1, 1].set_yticklabels(top_features, fontsize=10)
axes[1, 1].set_xlabel('Importance', fontsize=12)
axes[1, 1].set_title('Top 15 Feature Importance', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Performance improvement rate
rmse_improvement = (np.sqrt(mean_squared_error(y_test, y_pred_orig)) -
                   np.sqrt(mean_squared_error(y_test, y_pred_fe))) / \
                   np.sqrt(mean_squared_error(y_test, y_pred_orig)) * 100

print(f"\n=== Performance Improvement ===")
print(f"RMSE improvement rate: {rmse_improvement:.2f}%")
print(f"Feature count: {X_original.shape[1]} ‚Üí {X_fe.shape[1]} ({X_fe.shape[1] - X_original.shape[1]} added)")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Kaggle-Style Feature Generation Pipeline ===
Original feature count: 8

Original features:
   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  Longitude
0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88    -122.23
...

Feature count after feature engineering: 24

Generated features:
   MedInc  HouseAge  ...  Grid_std_income  Income_vs_grid_avg
0  8.3252      41.0  ...          2.45678              0.8765
...

=== Model Performance Comparison ===
„ÄêOriginal Features„Äë
  RMSE: 0.4934
  MAE: 0.3245
  R¬≤: 0.8123

„ÄêAfter Feature Engineering„Äë
  RMSE: 0.4567
  MAE: 0.2987
  R¬≤: 0.8456

--- Top 15 Important Features ---
1. MedInc: 0.4234
2. Latitude: 0.1234
3. Longitude: 0.0987
4. Income_x_Rooms: 0.0765
5. Grid_avg_income: 0.0654
...

=== Performance Improvement ===
RMSE improvement rate: 7.43%
Feature count: 8 ‚Üí 24 (16 added)
</code></pre>
<hr/>
<h2>3.7 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Numerical Transformations</strong></p>
<ul>
<li>Normalize skewed distributions with log transformation</li>
<li>Optimal transformation with Box-Cox/PowerTransformer</li>
<li>Reduce outlier impact</li>
</ul></li>
<li><p><strong>Binning</strong></p>
<ul>
<li>Equal width, equal frequency, and custom binning</li>
<li>Categorize continuous values</li>
<li>Balance interpretability and nonlinearity</li>
</ul></li>
<li><p><strong>Polynomial Features</strong></p>
<ul>
<li>Capture nonlinear relationships with power terms</li>
<li>Feature combinations with interaction terms</li>
<li>Degree selection is important</li>
</ul></li>
<li><p><strong>Domain Knowledge Features</strong></p>
<ul>
<li>Datetime features (year/month/day, day of week, cyclicality)</li>
<li>Text features (length, word count)</li>
<li>Aggregated features (statistics, grouping)</li>
</ul></li>
<li><p><strong>Practical Pipeline</strong></p>
<ul>
<li>Combine multiple transformation techniques</li>
<li>Verify effects with feature importance</li>
<li>Techniques used in Kaggle competitions</li>
</ul></li>
</ol>
<h3>Feature Transformation Selection Guide</h3>
<table>
<thead>
<tr>
<th>Data Characteristics</th>
<th>Recommended Transformation</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Right-skewed distribution</strong></td>
<td>log transform</td>
<td>Approach normal distribution</td>
</tr>
<tr>
<td><strong>Count data</strong></td>
<td>log1p, square root</td>
<td>Safely handle zero values</td>
</tr>
<tr>
<td><strong>Many outliers</strong></td>
<td>Binning</td>
<td>Reduce outlier impact</td>
</tr>
<tr>
<td><strong>Nonlinear relationships</strong></td>
<td>Polynomial features</td>
<td>Capture curved relationships</td>
</tr>
<tr>
<td><strong>Interactions present</strong></td>
<td>Interaction terms</td>
<td>Feature combination effects</td>
</tr>
<tr>
<td><strong>Datetime data</strong></td>
<td>Datetime decomposition + sin/cos</td>
<td>Capture cyclicality</td>
</tr>
<tr>
<td><strong>Group structure</strong></td>
<td>Aggregated statistics</td>
<td>Capture group characteristics</td>
</tr>
</tbody>
</table>
<h3>To the Next Chapter</h3>
<p>In Chapter 4, you'll learn about <strong>feature selection</strong>:</p>
<ul>
<li>Filter methods, wrapper methods, and embedded methods</li>
<li>Combination with dimensionality reduction</li>
<li>Practical feature selection pipelines</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p>List three differences between logarithmic transformation and Box-Cox transformation, and explain when each should be used.</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Logarithmic Transform</strong>:</p>
<ul>
<li><strong>Definition</strong>: $y = \log(x)$ or $y = \log(x + 1)$</li>
<li><strong>Application condition</strong>: $x &gt; 0$ (log1p allows $x \geq 0$)</li>
<li><strong>Characteristics</strong>: Simple and easy to interpret</li>
<li><strong>Use cases</strong>: Right-skewed distributions, price data, count data</li>
</ul>
<p><strong>Box-Cox Transform</strong>:</p>
<ul>
<li><strong>Definition</strong>: Flexible transformation with $\lambda$ parameter</li>
<li><strong>Application condition</strong>: $x &gt; 0$ (Yeo-Johnson allows any value)</li>
<li><strong>Characteristics</strong>: Automatically finds optimal transformation</li>
<li><strong>Use cases</strong>: Unknown optimal transformation, batch transformation of multiple features</li>
</ul>
<p><strong>Three Main Differences</strong>:</p>
<ol>
<li><strong>Parameters</strong>: Log transform is fixed, Box-Cox searches for optimal Œª</li>
<li><strong>Flexibility</strong>: Box-Cox can represent a wide range of transformations including log</li>
<li><strong>Interpretability</strong>: Log transform is intuitive, Box-Cox is complex (when Œª ‚â† 0)</li>
</ol>
<p><strong>Usage Guidelines</strong>:</p>
<ul>
<li>Prioritize interpretability, simple transformation sufficient ‚Üí Log transform</li>
<li>Search for optimal transformation, performance priority ‚Üí Box-Cox transform</li>
<li>Contains negative values ‚Üí Yeo-Johnson transform (extension of Box-Cox)</li>
</ul>
</details>
<h3>Exercise 2 (Difficulty: medium)</h3>
<p>Apply equal width binning and equal frequency binning to the following data and compare their characteristics.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Apply equal width binning and equal frequency binning to the

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np

np.random.seed(42)
# Exponential distribution (highly right-skewed)
data = np.random.exponential(scale=2.0, size=1000)
</code></pre>
<details>
<summary>Solution</summary>
<p>The complete solution code and analysis are provided showing how equal width binning creates imbalanced bins with skewed data, while equal frequency binning maintains balanced bin sizes. The comparison demonstrates when to use each method based on data distribution characteristics.</p>
</details>
<h3>Exercise 3 (Difficulty: medium)</h3>
<p>For two features $x_1, x_2$, generate 2nd-degree polynomial features and interaction-only terms, and compare their effects.</p>
<details>
<summary>Solution</summary>
<p>The solution demonstrates how adding interaction terms significantly improves model performance when the true underlying relationship includes interactions. The comparison shows that interaction-only features can reduce unnecessary features while maintaining performance.</p>
</details>
<h3>Exercise 4 (Difficulty: hard)</h3>
<p>Generate comprehensive features from datetime data and verify their effectiveness with actual data.</p>
<details>
<summary>Solution</summary>
<p>The solution creates comprehensive datetime features including cyclic sin/cos transformations, holiday flags, and various time-based indicators. Feature importance analysis reveals which temporal features most strongly influence the target variable.</p>
</details>
<h3>Exercise 5 (Difficulty: hard)</h3>
<p>Build a comprehensive feature engineering pipeline combining multiple transformation techniques for a real dataset and verify its effects.</p>
<details>
<summary>Solution</summary>
<p>The solution implements a staged feature engineering pipeline with numerical transformations, binning, polynomial features, and domain knowledge features. Each stage contributes incrementally to model performance improvement, demonstrating the cumulative effect of comprehensive feature engineering.</p>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Kuhn, M., &amp; Johnson, K. (2019). <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. CRC Press.</li>
<li>Zheng, A., &amp; Casari, A. (2018). <em>Feature Engineering for Machine Learning</em>. O'Reilly Media.</li>
<li>Box, G. E. P., &amp; Cox, D. R. (1964). "An Analysis of Transformations." <em>Journal of the Royal Statistical Society</em>.</li>
<li>Pandas Development Team. (2024). <em>Pandas Documentation: Time Series / Date functionality</em>.</li>
<li>Scikit-learn Developers. (2024). <em>Preprocessing data</em>. Scikit-learn Documentation.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter2-categorical-encoding.html">‚Üê Previous Chapter: Feature Scaling</a>
<a class="nav-button" href="chapter4-feature-selection.html">Next Chapter: Feature Selection ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranties, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operational safety, etc.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>Even if direct, indirect, incidental, special, consequential, or punitive damages arise from the use, execution, or interpretation of this content, the author and Tohoku University shall not be liable to the maximum extent permitted by applicable law.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>Copyright and license of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
