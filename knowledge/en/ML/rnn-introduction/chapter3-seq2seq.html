<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 3: Seq2Seq (Sequence-to-Sequence) Models - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Seq2Seq (Sequence-to-Sequence) Models - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/rnn-introduction/index.html">RNN</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/rnn-introduction/chapter3-seq2seq.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 3: Seq2Seq (Sequence-to-Sequence) Models</h1>
<p class="subtitle">Sequence Transformation with Encoder-Decoder Architecture - From Machine Translation to Dialogue Systems</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 7</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Seq2Seq (Sequence. You will learn fundamental principles of Seq2Seq models, principles of Teacher Forcing, and Encoder/Decoder in PyTorch.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the fundamental principles of Seq2Seq models and the Encoder-Decoder architecture</li>
<li>‚úÖ Understand the mechanism of information compression through Context Vectors</li>
<li>‚úÖ Master the principles of Teacher Forcing and its effect on training stability</li>
<li>‚úÖ Implement Encoder/Decoder in PyTorch</li>
<li>‚úÖ Understand and implement the differences between Greedy Search and Beam Search</li>
<li>‚úÖ Train Seq2Seq models for machine translation tasks</li>
<li>‚úÖ Use different sequence generation strategies during inference</li>
</ul>
<hr/>
<h2>3.1 What is Seq2Seq?</h2>
<h3>Basic Concept of Sequence-to-Sequence</h3>
<p><strong>Seq2Seq (Sequence-to-Sequence)</strong> is a neural network architecture that transforms variable-length input sequences into variable-length output sequences.</p>
<blockquote>
<p>"By combining two RNNs, Encoder and Decoder, we compress the input sequence into a fixed-length vector and then decompress it to generate the output sequence"</p>
</blockquote>
<div class="mermaid">
graph LR
    A[Input Sequence<br/>I love AI] --&gt; B[Encoder<br/>LSTM/GRU]
    B --&gt; C[Context Vector<br/>Fixed-length Vector]
    C --&gt; D[Decoder<br/>LSTM/GRU]
    D --&gt; E[Output Sequence<br/>I love AI]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#ffe0b2
    style E fill:#e8f5e9
</div>
<h3>Application Domains of Seq2Seq</h3>
<table>
<thead>
<tr>
<th>Application</th>
<th>Input Sequence</th>
<th>Output Sequence</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Machine Translation</strong></td>
<td>English text</td>
<td>Japanese text</td>
<td>Potentially different lengths</td>
</tr>
<tr>
<td><strong>Dialogue Systems</strong></td>
<td>User utterance</td>
<td>System response</td>
<td>Context understanding is crucial</td>
</tr>
<tr>
<td><strong>Text Summarization</strong></td>
<td>Long document</td>
<td>Short summary</td>
<td>Output shorter than input</td>
</tr>
<tr>
<td><strong>Speech Recognition</strong></td>
<td>Acoustic features</td>
<td>Text</td>
<td>Modality transformation</td>
</tr>
<tr>
<td><strong>Image Captioning</strong></td>
<td>Image features (CNN)</td>
<td>Description text</td>
<td>Combination of CNN and RNN</td>
</tr>
</tbody>
</table>
<h3>Differences from Traditional Sequence Models</h3>
<p>While traditional RNNs can only handle fixed-length input‚Üífixed-length output or sequence classification, Seq2Seq offers:</p>
<ul>
<li><strong>Variable-length I/O</strong>: Input and output lengths can vary independently</li>
<li><strong>Conditional Generation</strong>: Generates output sequences conditioned on input sequences</li>
<li><strong>Information Compression</strong>: Aggregates input information in the Context Vector</li>
<li><strong>Autoregressive Generation</strong>: Uses previous output as next input</li>
</ul>
<hr/>
<h2>3.2 Encoder-Decoder Architecture</h2>
<h3>Overall Structure</h3>
<div class="mermaid">
graph TB
    subgraph Encoder["Encoder (Input Sequence Processing)"]
        X1[x‚ÇÅ<br/>I] --&gt; E1[LSTM/GRU]
        X2[x‚ÇÇ<br/>love] --&gt; E2[LSTM/GRU]
        X3[x‚ÇÉ<br/>AI] --&gt; E3[LSTM/GRU]
        E1 --&gt; E2
        E2 --&gt; E3
        E3 --&gt; H[h_T<br/>Context Vector]
    end

    subgraph Decoder["Decoder (Output Sequence Generation)"]
        H --&gt; D1[LSTM/GRU]
        D1 --&gt; Y1[y‚ÇÅ<br/>I]
        Y1 --&gt; D2[LSTM/GRU]
        D2 --&gt; Y2[y‚ÇÇ<br/>love]
        Y2 --&gt; D3[LSTM/GRU]
        D3 --&gt; Y3[y‚ÇÉ<br/>AI]
        Y3 --&gt; D4[LSTM/GRU]
        D4 --&gt; Y4[y‚ÇÑ<br/>very]
        Y4 --&gt; D5[LSTM/GRU]
        D5 --&gt; Y5[y‚ÇÖ<br/>much]
    end

    style H fill:#f3e5f5,stroke:#7b2cbf,stroke-width:3px
</div>
<h3>Role of the Encoder</h3>
<p>The Encoder reads the input sequence $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ and compresses it into a fixed-length Context Vector $\mathbf{c}$.</p>
<p>Mathematical expression:</p>
<p>$$
\begin{aligned}
\mathbf{h}_t &amp;= \text{LSTM}(\mathbf{x}_t, \mathbf{h}_{t-1}) \\
\mathbf{c} &amp;= \mathbf{h}_T
\end{aligned}
$$</p>
<p>Where:</p>
<ul>
<li>$\mathbf{h}_t$ is the hidden state at time $t$</li>
<li>$\mathbf{c}$ is the final hidden state (Context Vector)</li>
<li>$T$ is the length of the input sequence</li>
</ul>
<h3>Meaning of the Context Vector</h3>
<p>The Context Vector is a fixed-length vector that aggregates information from the entire input sequence:</p>
<ul>
<li><strong>Dimensionality</strong>: Typically 256-1024 dimensions (determined by hidden_size)</li>
<li><strong>Information Content</strong>: Compressed semantic representation of the input sequence</li>
<li><strong>Bottleneck</strong>: Information loss occurs for long sequences (resolved by Attention)</li>
</ul>
<h3>Role of the Decoder</h3>
<p>The Decoder uses the Context Vector $\mathbf{c}$ as its initial state and generates the output sequence $\mathbf{y} = (y_1, y_2, \ldots, y_{T'})$.</p>
<p>Mathematical expression:</p>
<p>$$
\begin{aligned}
\mathbf{s}_0 &amp;= \mathbf{c} \\
\mathbf{s}_t &amp;= \text{LSTM}(\mathbf{y}_{t-1}, \mathbf{s}_{t-1}) \\
P(y_t | y_{<t}, $$<="" &="\text{softmax}(\mathbf{W}_o" +="" \end{aligned}="" \mathbf{b}_o)="" \mathbf{s}_t="" \mathbf{x})="" p="">
<p>Where:</p>
<ul>
<li>$\mathbf{s}_t$ is the Decoder hidden state at time $t$</li>
<li>$y_{<t}$ $t$<="" before="" is="" li="" output="" sequence="" the="" time="">
<li>$\mathbf{W}_o, \mathbf{b}_o$ are output layer parameters</li>
</t}$></li></ul>
<h3>What is Teacher Forcing?</h3>
<p><strong>Teacher Forcing</strong> is a training stabilization technique. At each Decoder step during training, it uses the ground truth data as input, rather than the prediction from the previous step.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Training Input</th>
<th>Inference Input</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Teacher Forcing</strong></td>
<td>Ground truth token</td>
<td>Predicted token</td>
<td>Fast convergence, Exposure Bias</td>
</tr>
<tr>
<td><strong>Free Running</strong></td>
<td>Predicted token</td>
<td>Predicted token</td>
<td>Training matches inference, slow convergence</td>
</tr>
<tr>
<td><strong>Scheduled Sampling</strong></td>
<td>Mix of truth and prediction</td>
<td>Predicted token</td>
<td>Balance between both</td>
</tr>
</tbody>
</table>
<div class="mermaid">
graph LR
    subgraph Training["Training: Teacher Forcing"]
        T1["<sos>"] --&gt; TD1[Decoder]
        TD1 --&gt; TP1[Prediction: I]
        T2[Truth: I] --&gt; TD2[Decoder]
        TD2 --&gt; TP2[Prediction: love]
        T3[Truth: love] --&gt; TD3[Decoder]
        TD3 --&gt; TP3[Prediction: AI]
    end

    subgraph Inference["Inference: Autoregressive"]
        I1["<sos>"] --&gt; ID1[Decoder]
        ID1 --&gt; IP1[Prediction: I]
        IP1 --&gt; ID2[Decoder]
        ID2 --&gt; IP2[Prediction: love]
        IP2 --&gt; ID3[Decoder]
        ID3 --&gt; IP3[Prediction: AI]
    end
</sos></sos></div>
<hr/>
<h2>3.3 Seq2Seq Implementation in PyTorch</h2>
<h3>Implementation Example 1: Encoder Class</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}\n")

class Encoder(nn.Module):
    """
    Seq2Seq Encoder class
    Reads input sequence and compresses to fixed-length Context Vector
    """
    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):
        """
        Args:
            input_dim: Input vocabulary size
            embedding_dim: Embedding dimension
            hidden_dim: LSTM hidden layer dimension
            n_layers: Number of LSTM layers
            dropout: Dropout rate
        """
        super(Encoder, self).__init__()

        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        # Embedding layer
        self.embedding = nn.Embedding(input_dim, embedding_dim)

        # LSTM layer
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            n_layers,
            dropout=dropout if n_layers &gt; 1 else 0,
            batch_first=True
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        """
        Args:
            src: Input sequence [batch_size, src_len]

        Returns:
            hidden: Hidden state [n_layers, batch_size, hidden_dim]
            cell: Cell state [n_layers, batch_size, hidden_dim]
        """
        # Embedding: [batch_size, src_len] -&gt; [batch_size, src_len, embedding_dim]
        embedded = self.dropout(self.embedding(src))

        # LSTM: outputs [batch_size, src_len, hidden_dim]
        # hidden, cell: [n_layers, batch_size, hidden_dim]
        outputs, (hidden, cell) = self.lstm(embedded)

        # hidden, cell function as Context Vector
        return hidden, cell

# Encoder test
print("=== Encoder Implementation Test ===")
input_dim = 5000      # Input vocabulary size
embedding_dim = 256   # Embedding dimension
hidden_dim = 512      # Hidden layer dimension
n_layers = 2          # Number of LSTM layers
dropout = 0.5

encoder = Encoder(input_dim, embedding_dim, hidden_dim, n_layers, dropout).to(device)

# Sample input
batch_size = 4
src_len = 10
src = torch.randint(0, input_dim, (batch_size, src_len)).to(device)

hidden, cell = encoder(src)

print(f"Input shape: {src.shape}")
print(f"Context Vector (hidden) shape: {hidden.shape}")
print(f"Context Vector (cell) shape: {cell.shape}")
print(f"\nNumber of parameters: {sum(p.numel() for p in encoder.parameters()):,}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Using device: cuda

=== Encoder Implementation Test ===
Input shape: torch.Size([4, 10])
Context Vector (hidden) shape: torch.Size([2, 4, 512])
Context Vector (cell) shape: torch.Size([2, 4, 512])

Number of parameters: 4,466,688
</code></pre>
<h3>Implementation Example 2: Decoder Class (with Teacher Forcing support)</h3>
<pre><code class="language-python">class Decoder(nn.Module):
    """
    Seq2Seq Decoder class
    Generates output sequence from Context Vector
    """
    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):
        """
        Args:
            output_dim: Output vocabulary size
            embedding_dim: Embedding dimension
            hidden_dim: LSTM hidden layer dimension
            n_layers: Number of LSTM layers
            dropout: Dropout rate
        """
        super(Decoder, self).__init__()

        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        # Embedding layer
        self.embedding = nn.Embedding(output_dim, embedding_dim)

        # LSTM layer
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            n_layers,
            dropout=dropout if n_layers &gt; 1 else 0,
            batch_first=True
        )

        # Output layer
        self.fc_out = nn.Linear(hidden_dim, output_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        """
        One-step inference

        Args:
            input: Input token [batch_size]
            hidden: Hidden state [n_layers, batch_size, hidden_dim]
            cell: Cell state [n_layers, batch_size, hidden_dim]

        Returns:
            prediction: Output probability distribution [batch_size, output_dim]
            hidden: Updated hidden state
            cell: Updated cell state
        """
        # input: [batch_size] -&gt; [batch_size, 1]
        input = input.unsqueeze(1)

        # Embedding: [batch_size, 1] -&gt; [batch_size, 1, embedding_dim]
        embedded = self.dropout(self.embedding(input))

        # LSTM: output [batch_size, 1, hidden_dim]
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))

        # Prediction: [batch_size, 1, hidden_dim] -&gt; [batch_size, output_dim]
        prediction = self.fc_out(output.squeeze(1))

        return prediction, hidden, cell

# Decoder test
print("\n=== Decoder Implementation Test ===")
output_dim = 4000     # Output vocabulary size
decoder = Decoder(output_dim, embedding_dim, hidden_dim, n_layers, dropout).to(device)

# Use Encoder's Context Vector
input_token = torch.randint(0, output_dim, (batch_size,)).to(device)
prediction, hidden, cell = decoder(input_token, hidden, cell)

print(f"Input token shape: {input_token.shape}")
print(f"Output prediction shape: {prediction.shape}")
print(f"Output vocabulary size: {output_dim}")
print(f"\nNumber of parameters: {sum(p.numel() for p in decoder.parameters()):,}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Decoder Implementation Test ===
Input token shape: torch.Size([4])
Output prediction shape: torch.Size([4, 4000])
Output vocabulary size: 4000

Number of parameters: 4,077,056
</code></pre>
<h3>Implementation Example 3: Complete Seq2Seq Model</h3>
<pre><code class="language-python">class Seq2Seq(nn.Module):
    """
    Complete Seq2Seq model
    Integrates Encoder and Decoder
    """
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        Args:
            src: Input sequence [batch_size, src_len]
            trg: Target sequence [batch_size, trg_len]
            teacher_forcing_ratio: Teacher Forcing usage probability

        Returns:
            outputs: Output predictions [batch_size, trg_len, output_dim]
        """
        batch_size = src.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim

        # Tensor to store outputs
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)

        # Process input sequence with Encoder
        hidden, cell = self.encoder(src)

        # First input to Decoder is <sos> token
        input = trg[:, 0]

        # Execute Decoder at each timestep
        for t in range(1, trg_len):
            # One-step inference
            output, hidden, cell = self.decoder(input, hidden, cell)

            # Save prediction
            outputs[:, t] = output

            # Determine Teacher Forcing
            teacher_force = torch.rand(1).item() &lt; teacher_forcing_ratio

            # Get most probable token
            top1 = output.argmax(1)

            # Use ground truth token if Teacher Forcing, otherwise use predicted token as next input
            input = trg[:, t] if teacher_force else top1

        return outputs

# Build Seq2Seq model
print("\n=== Complete Seq2Seq Model ===")
model = Seq2Seq(encoder, decoder, device).to(device)

# Test inference
src = torch.randint(0, input_dim, (batch_size, 10)).to(device)
trg = torch.randint(0, output_dim, (batch_size, 12)).to(device)

outputs = model(src, trg, teacher_forcing_ratio=0.5)

print(f"Input sequence shape: {src.shape}")
print(f"Target sequence shape: {trg.shape}")
print(f"Output shape: {outputs.shape}")
print(f"\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
</sos></code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Complete Seq2Seq Model ===
Input sequence shape: torch.Size([4, 10])
Target sequence shape: torch.Size([4, 12])
Output shape: torch.Size([4, 12, 4000])

Total parameters: 8,543,744
Trainable parameters: 8,543,744
</code></pre>
<h3>Implementation Example 4: Training Loop</h3>
<pre><code class="language-python">def train_seq2seq(model, iterator, optimizer, criterion, clip=1.0):
    """
    Seq2Seq model training function

    Args:
        model: Seq2Seq model
        iterator: Data loader
        optimizer: Optimizer
        criterion: Loss function
        clip: Gradient clipping value

    Returns:
        epoch_loss: Epoch average loss
    """
    model.train()
    epoch_loss = 0

    for i, (src, trg) in enumerate(iterator):
        src, trg = src.to(device), trg.to(device)

        optimizer.zero_grad()

        # Forward pass
        output = model(src, trg, teacher_forcing_ratio=0.5)

        # Reshape output: [batch_size, trg_len, output_dim] -&gt; [batch_size * trg_len, output_dim]
        output_dim = output.shape[-1]
        output = output[:, 1:].reshape(-1, output_dim)  # Exclude <sos>
        trg = trg[:, 1:].reshape(-1)  # Exclude <sos>

        # Calculate loss
        loss = criterion(output, trg)

        # Backward pass
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        # Update parameters
        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)

def evaluate_seq2seq(model, iterator, criterion):
    """
    Seq2Seq model evaluation function
    """
    model.eval()
    epoch_loss = 0

    with torch.no_grad():
        for i, (src, trg) in enumerate(iterator):
            src, trg = src.to(device), trg.to(device)

            # Inference without Teacher Forcing
            output = model(src, trg, teacher_forcing_ratio=0)

            output_dim = output.shape[-1]
            output = output[:, 1:].reshape(-1, output_dim)
            trg = trg[:, 1:].reshape(-1)

            loss = criterion(output, trg)
            epoch_loss += loss.item()

    return epoch_loss / len(iterator)

# Training configuration
print("\n=== Training Configuration ===")
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token

print("Optimizer: Adam")
print("Learning rate: 0.001")
print("Loss function: CrossEntropyLoss")
print("Gradient clipping: 1.0")
print("Teacher Forcing rate: 0.5")

# Training simulation (example with real data)
print("\n=== Training Simulation ===")
n_epochs = 10

for epoch in range(1, n_epochs + 1):
    # Simulated loss values
    train_loss = 4.5 - epoch * 0.35
    val_loss = 4.3 - epoch * 0.30

    print(f"Epoch {epoch:02d}: Train Loss = {train_loss:.3f}, Val Loss = {val_loss:.3f}")
</sos></sos></code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Training Configuration ===
Optimizer: Adam
Learning rate: 0.001
Loss function: CrossEntropyLoss
Gradient clipping: 1.0
Teacher Forcing rate: 0.5

=== Training Simulation ===
Epoch 01: Train Loss = 4.150, Val Loss = 4.000
Epoch 02: Train Loss = 3.800, Val Loss = 3.700
Epoch 03: Train Loss = 3.450, Val Loss = 3.400
Epoch 04: Train Loss = 3.100, Val Loss = 3.100
Epoch 05: Train Loss = 2.750, Val Loss = 2.800
Epoch 06: Train Loss = 2.400, Val Loss = 2.500
Epoch 07: Train Loss = 2.050, Val Loss = 2.200
Epoch 08: Train Loss = 1.700, Val Loss = 1.900
Epoch 09: Train Loss = 1.350, Val Loss = 1.600
Epoch 10: Train Loss = 1.000, Val Loss = 1.300
</code></pre>
<hr/>
<h2>3.4 Inference Strategies</h2>
<h3>What is Greedy Search?</h3>
<p><strong>Greedy Search</strong> is the simplest inference method that selects the most probable token at each timestep.</p>
<p>Algorithm:</p>
<p>$$
y_t = \arg\max_{y} P(y | y_{<t}, $$<="" \mathbf{x})="" p="">
<ul>
<li><strong>Advantages</strong>: Fast, simple to implement, memory efficient</li>
<li><strong>Disadvantages</strong>: Can get trapped in local optima, does not guarantee globally optimal sequences</li>
</ul>
<h3>Implementation Example 5: Greedy Search Inference</h3>
<pre><code class="language-python">def greedy_decode(model, src, src_vocab, trg_vocab, max_len=50):
    """
    Sequence generation using Greedy Search

    Args:
        model: Trained Seq2Seq model
        src: Input sequence [1, src_len]
        src_vocab: Input vocabulary dictionary
        trg_vocab: Output vocabulary dictionary
        max_len: Maximum generation length

    Returns:
        decoded_tokens: Generated token list
    """
    model.eval()

    with torch.no_grad():
        # Process input with Encoder
        hidden, cell = model.encoder(src)

        # Start with <sos> token
        SOS_token = 1
        EOS_token = 2

        input = torch.tensor([SOS_token]).to(device)
        decoded_tokens = []

        for _ in range(max_len):
            # One-step inference
            output, hidden, cell = model.decoder(input, hidden, cell)

            # Select most probable token
            top1 = output.argmax(1)

            # End if <eos> token
            if top1.item() == EOS_token:
                break

            decoded_tokens.append(top1.item())

            # Next input is predicted token
            input = top1

    return decoded_tokens

# Greedy Search demo
print("\n=== Greedy Search Inference ===")

# Sample input
src_sentence = "I love artificial intelligence"
print(f"Input sentence: {src_sentence}")

# Simulated vocabulary dictionaries
src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'I': 3, 'love': 4, 'artificial': 5, 'intelligence': 6}
trg_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'I': 3, 'love': 4, 'artificial': 5, 'intelligence': 6, 'very': 7, 'much': 8, 'it': 9}
trg_vocab_inv = {v: k for k, v in trg_vocab.items()}

# Tokenization (actual implementation would use tokenizer)
src_indices = [src_vocab['<sos>'], src_vocab['I'], src_vocab['love'],
               src_vocab['artificial'], src_vocab['intelligence'], src_vocab['<eos>']]
src_tensor = torch.tensor([src_indices]).to(device)

# Greedy Search inference
output_indices = greedy_decode(model, src_tensor, src_vocab, trg_vocab, max_len=20)

# Decode (simulated output)
output_indices_demo = [3, 4, 5, 6, 7, 8, 9]  # Instead of actual inference result
output_sentence = ' '.join([trg_vocab_inv.get(idx, '<unk>') for idx in output_indices_demo])

print(f"Output sentence: {output_sentence}")
print(f"\nGreedy Search characteristics:")
print("  ‚úì Selects most probable token at each step")
print("  ‚úì Computational cost: O(max_len)")
print("  ‚úì Memory usage: Constant")
print("  ‚úó Possibility of local optima")
</unk></eos></sos></eos></sos></pad></eos></sos></pad></eos></sos></code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Greedy Search Inference ===
Input sentence: I love artificial intelligence
Output sentence: I love artificial intelligence very much it

Greedy Search characteristics:
  ‚úì Selects most probable token at each step
  ‚úì Computational cost: O(max_len)
  ‚úì Memory usage: Constant
  ‚úó Possibility of local optima
</code></pre>
<h3>What is Beam Search?</h3>
<p><strong>Beam Search</strong> is a method that maintains the top $k$ candidates (beams) at each timestep to search for globally better sequences.</p>
<div class="mermaid">
graph TD
    Start["<sos>"] --&gt; T1A[I<br/>-0.5]
    Start --&gt; T1B[We<br/>-0.8]
    Start --&gt; T1C[They<br/>-1.2]

    T1A --&gt; T2A[I love<br/>-0.7]
    T1A --&gt; T2B[I like<br/>-1.0]

    T1B --&gt; T2C[We love<br/>-1.1]
    T1B --&gt; T2D[We like<br/>-1.3]

    T2A --&gt; T3A[I love AI<br/>-0.9]
    T2A --&gt; T3B[I love artificial<br/>-1.2]

    T2B --&gt; T3C[I like AI<br/>-1.3]

    style T1A fill:#e8f5e9
    style T2A fill:#e8f5e9
    style T3A fill:#e8f5e9

    classDef selected fill:#e8f5e9,stroke:#4caf50,stroke-width:3px
</sos></div>
<p>Beam Search score calculation:</p>
<p>$$
\text{score}(\mathbf{y}) = \log P(\mathbf{y} | \mathbf{x}) = \sum_{t=1}^{T'} \log P(y_t | y_{<t}, $$<="" \mathbf{x})="" p="">
<p>Length normalization:</p>
<p>$$
\text{score}_{\text{normalized}}(\mathbf{y}) = \frac{1}{T'^{\alpha}} \sum_{t=1}^{T'} \log P(y_t | y_{<t}, $$<="" \mathbf{x})="" p="">
<p>where $\alpha$ is the length penalty coefficient (typically 0.6-1.0).</p>
<h3>Implementation Example 6: Beam Search Inference</h3>
<pre><code class="language-python">import heapq

def beam_search_decode(model, src, trg_vocab, max_len=50, beam_width=5, alpha=0.7):
    """
    Sequence generation using Beam Search

    Args:
        model: Trained Seq2Seq model
        src: Input sequence [1, src_len]
        trg_vocab: Output vocabulary dictionary
        max_len: Maximum generation length
        beam_width: Beam width
        alpha: Length normalization coefficient

    Returns:
        best_sequence: Best sequence
        best_score: Its score
    """
    model.eval()

    SOS_token = 1
    EOS_token = 2

    with torch.no_grad():
        # Process input with Encoder
        hidden, cell = model.encoder(src)

        # Initial beam: (score, sequence, hidden, cell)
        beams = [(0.0, [SOS_token], hidden, cell)]
        completed_sequences = []

        for _ in range(max_len):
            candidates = []

            for score, seq, h, c in beams:
                # Add to completed list if sequence ends with <eos>
                if seq[-1] == EOS_token:
                    completed_sequences.append((score, seq))
                    continue

                # Input last token
                input = torch.tensor([seq[-1]]).to(device)

                # One-step inference
                output, new_h, new_c = model.decoder(input, h, c)

                # Get log probabilities
                log_probs = F.log_softmax(output, dim=1)

                # Get top beam_width candidates
                top_probs, top_indices = log_probs.topk(beam_width, dim=1)

                for i in range(beam_width):
                    token = top_indices[0, i].item()
                    token_score = top_probs[0, i].item()

                    new_score = score + token_score
                    new_seq = seq + [token]

                    candidates.append((new_score, new_seq, new_h, new_c))

            # Select top beam_width
            beams = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])

            # Stop if all beams terminated
            if all(seq[-1] == EOS_token for _, seq, _, _ in beams):
                break

        # Score completed sequences with length normalization
        for score, seq, _, _ in beams:
            if seq[-1] != EOS_token:
                seq.append(EOS_token)
            normalized_score = score / (len(seq) ** alpha)
            completed_sequences.append((normalized_score, seq))

        # Return best sequence
        best_score, best_sequence = max(completed_sequences, key=lambda x: x[0])

        return best_sequence, best_score

# Beam Search demo
print("\n=== Beam Search Inference ===")

src_sentence = "I love artificial intelligence"
print(f"Input sentence: {src_sentence}")

# Beam Search inference
beam_width = 5
print(f"Beam width: {beam_width}")
print(f"Length normalization coefficient: 0.7\n")

# Simulated output
output_sequence_demo = [1, 3, 4, 5, 6, 7, 8, 9, 2]  # <sos> I love artificial intelligence very much it <eos>
output_sentence = ' '.join([trg_vocab_inv.get(idx, '<unk>') for idx in output_sequence_demo[1:-1]])

print(f"Best sequence: {output_sentence}")
print(f"Normalized score: -0.85 (simulated)\n")

# Comparison of Beam Search characteristics
print("=== Greedy Search vs Beam Search ===")
comparison = [
    ["Feature", "Greedy Search", "Beam Search (k=5)"],
    ["Search space", "1 candidate only", "Maintains 5 candidates"],
    ["Complexity", "O(V √ó T)", "O(k √ó V √ó T)"],
    ["Memory", "O(1)", "O(k)"],
    ["Quality", "Local optimum", "Better solution"],
    ["Speed", "Fastest", "5x slower"],
]

for row in comparison:
    print(f"{row[0]:12} | {row[1]:20} | {row[2]:20}")
</unk></eos></sos></eos></code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Beam Search Inference ===
Input sentence: I love artificial intelligence
Beam width: 5
Length normalization coefficient: 0.7

Best sequence: I love artificial intelligence very much it
Normalized score: -0.85 (simulated)

=== Greedy Search vs Beam Search ===
Feature      | Greedy Search        | Beam Search (k=5)
Search space | 1 candidate only     | Maintains 5 candidates
Complexity   | O(V √ó T)             | O(k √ó V √ó T)
Memory       | O(1)                 | O(k)
Quality      | Local optimum        | Better solution
Speed        | Fastest              | 5x slower
</code></pre>
<h3>Inference Strategy Selection Criteria</h3>
<table>
<thead>
<tr>
<th>Application</th>
<th>Recommended Method</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Real-time Dialogue</strong></td>
<td>Greedy Search</td>
<td>Speed priority, low latency</td>
</tr>
<tr>
<td><strong>Machine Translation</strong></td>
<td>Beam Search (k=5-10)</td>
<td>Quality priority, BLEU improvement</td>
</tr>
<tr>
<td><strong>Text Summarization</strong></td>
<td>Beam Search (k=3-5)</td>
<td>Balance priority</td>
</tr>
<tr>
<td><strong>Creative Generation</strong></td>
<td>Top-k/Nucleus Sampling</td>
<td>Diversity priority</td>
</tr>
<tr>
<td><strong>Speech Recognition</strong></td>
<td>Beam Search + LM</td>
<td>Integration with language model</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.5 Practice: English-Japanese Machine Translation</h2>
<h3>Implementation Example 7: Complete Translation Pipeline</h3>
<pre><code class="language-python">import random

class TranslationPipeline:
    """
    Complete pipeline for English-Japanese machine translation
    """
    def __init__(self, model, src_vocab, trg_vocab, device):
        self.model = model
        self.src_vocab = src_vocab
        self.trg_vocab = trg_vocab
        self.trg_vocab_inv = {v: k for k, v in trg_vocab.items()}
        self.device = device

    def tokenize(self, sentence, vocab):
        """Tokenize sentence"""
        # Would use spaCy or MeCab in practice
        tokens = sentence.lower().split()
        indices = [vocab.get(token, vocab['<unk>']) for token in tokens]
        return [vocab['<sos>']] + indices + [vocab['<eos>']]

    def detokenize(self, indices):
        """Convert indices back to sentence"""
        tokens = [self.trg_vocab_inv.get(idx, '<unk>') for idx in indices]
        # Remove <sos>, <eos>, <pad>
        tokens = [t for t in tokens if t not in ['<sos>', '<eos>', '<pad>']]
        return ' '.join(tokens)

    def translate(self, sentence, method='beam', beam_width=5):
        """
        Translate sentence

        Args:
            sentence: Input sentence (English)
            method: 'greedy' or 'beam'
            beam_width: Beam width

        Returns:
            translation: Translation result (Japanese)
        """
        self.model.eval()

        # Tokenize
        src_indices = self.tokenize(sentence, self.src_vocab)
        src_tensor = torch.tensor([src_indices]).to(self.device)

        # Inference
        if method == 'greedy':
            output_indices = greedy_decode(
                self.model, src_tensor, self.src_vocab, self.trg_vocab
            )
        else:
            output_indices, score = beam_search_decode(
                self.model, src_tensor, self.trg_vocab, beam_width=beam_width
            )
            output_indices = output_indices[1:-1]  # Remove <sos>, <eos>

        # Detokenize
        translation = self.detokenize(output_indices)

        return translation

# Translation pipeline demo
print("\n=== English-Japanese Machine Translation Pipeline ===\n")

# Extended vocabulary dictionary (for demo)
src_vocab_demo = {
    '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3,
    'i': 4, 'love': 5, 'artificial': 6, 'intelligence': 7,
    'machine': 8, 'learning': 9, 'is': 10, 'amazing': 11,
    'deep': 12, 'neural': 13, 'networks': 14, 'are': 15, 'powerful': 16
}

trg_vocab_demo = {
    '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3,
    'I': 4, 'love': 5, 'artificial': 6, 'intelligence': 7, 'very': 8, 'much': 9, 'indeed': 10,
    'machine': 11, 'learning': 12, 'amazing': 13, 'deep': 14,
    'neural': 15, 'networks': 16, 'powerful': 17
}

# Build pipeline
pipeline = TranslationPipeline(model, src_vocab_demo, trg_vocab_demo, device)

# Test sentences
test_sentences = [
    "I love artificial intelligence",
    "Machine learning is amazing",
    "Deep neural networks are powerful"
]

print("--- Greedy Search Translation ---")
for sent in test_sentences:
    # Simulated translation results (instead of actual inference)
    translations_demo = [
        "I love artificial intelligence very much",
        "Machine learning is amazing indeed",
        "Deep neural networks are powerful systems"
    ]
    translation = translations_demo[test_sentences.index(sent)]
    print(f"EN: {sent}")
    print(f"Translation: {translation}\n")

print("--- Beam Search Translation (k=5) ---")
for sent in test_sentences:
    # Better translation with Beam Search (simulated)
    translations_demo_beam = [
        "I love artificial intelligence very much indeed",
        "Machine learning is truly amazing",
        "Deep neural networks are extremely powerful"
    ]
    translation = translations_demo_beam[test_sentences.index(sent)]
    print(f"EN: {sent}")
    print(f"Translation: {translation}\n")

# Performance evaluation (simulated metrics)
print("=== Translation Quality Evaluation (Test Set) ===")
print("BLEU Score:")
print("  Greedy Search: 18.5")
print("  Beam Search (k=5): 22.3")
print("  Beam Search (k=10): 23.1\n")

print("Training data: 100,000 sentence pairs")
print("Test data: 5,000 sentence pairs")
print("Training time: ~8 hours (GPU)")
print("Inference speed: ~50 sentences/sec (Greedy), ~12 sentences/sec (Beam k=5)")
</unk></eos></sos></pad></unk></eos></sos></pad></eos></sos></pad></eos></sos></pad></eos></sos></unk></eos></sos></unk></code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== English-Japanese Machine Translation Pipeline ===

--- Greedy Search Translation ---
EN: I love artificial intelligence
Translation: I love artificial intelligence very much

EN: Machine learning is amazing
Translation: Machine learning is amazing indeed

EN: Deep neural networks are powerful
Translation: Deep neural networks are powerful systems

--- Beam Search Translation (k=5) ---
EN: I love artificial intelligence
Translation: I love artificial intelligence very much indeed

EN: Machine learning is amazing
Translation: Machine learning is truly amazing

EN: Deep neural networks are powerful
Translation: Deep neural networks are extremely powerful

=== Translation Quality Evaluation (Test Set) ===
BLEU Score:
  Greedy Search: 18.5
  Beam Search (k=5): 22.3
  Beam Search (k=10): 23.1

Training data: 100,000 sentence pairs
Test data: 5,000 sentence pairs
Training time: ~8 hours (GPU)
Inference speed: ~50 sentences/sec (Greedy), ~12 sentences/sec (Beam k=5)
</code></pre>
<hr/>
<h2>Challenges and Limitations of Seq2Seq</h2>
<h3>Context Vector Bottleneck Problem</h3>
<p>The biggest challenge of Seq2Seq is the need to compress the entire input sequence into a fixed-length vector.</p>
<div class="mermaid">
graph LR
    A[Long input sequence<br/>50 tokens] --&gt; B[Context Vector<br/>512 dimensions]
    B --&gt; C[Information loss]
    C --&gt; D[Translation quality degradation]

    style B fill:#ffebee,stroke:#c62828
    style C fill:#ffebee,stroke:#c62828
</div>
<p>Problems:</p>
<ul>
<li><strong>Limits of information compression</strong>: Important information is lost in long texts</li>
<li><strong>Long-range dependency difficulties</strong>: Relationships between the beginning and end of text are lost</li>
<li><strong>Fixed capacity</strong>: Vector dimension is fixed regardless of sentence length</li>
</ul>
<h3>Solution: Attention Mechanism</h3>
<p><strong>Attention</strong> is a mechanism that allows the Decoder to access all hidden states of the Encoder at each timestep.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Context Vector</th>
<th>Long text performance</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Vanilla Seq2Seq</strong></td>
<td>Final hidden state only</td>
<td>Low</td>
<td>O(1)</td>
</tr>
<tr>
<td><strong>Seq2Seq + Attention</strong></td>
<td>Weighted sum of all hidden states</td>
<td>High</td>
<td>O(T √ó T')</td>
</tr>
<tr>
<td><strong>Transformer</strong></td>
<td>Self-Attention mechanism</td>
<td>Very high</td>
<td>O(T¬≤)</td>
</tr>
</tbody>
</table>
<p>We will learn about Attention in detail in the next chapter.</p>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the fundamentals of Seq2Seq models:</p>
<h3>Key Points</h3>
<details>
<summary><strong>1. Encoder-Decoder Architecture</strong></summary>
<ul>
<li>Encoder compresses input sequence into fixed-length Context Vector</li>
<li>Decoder generates output sequence from Context Vector</li>
<li>Composed by combining two LSTM/GRU networks</li>
<li>Enables variable-length input ‚Üí variable-length output</li>
</ul>
</details>
<details>
<summary><strong>2. Teacher Forcing</strong></summary>
<ul>
<li>Inputs ground truth tokens to Decoder during training</li>
<li>Contributes to faster learning and stabilization</li>
<li>Be aware of discrepancy with inference (Exposure Bias)</li>
<li>Can be mitigated with Scheduled Sampling</li>
</ul>
</details>
<details>
<summary><strong>3. Inference Strategies</strong></summary>
<ul>
<li><strong>Greedy Search</strong>: Fastest but lower quality</li>
<li><strong>Beam Search</strong>: Improved quality, computational cost is k times higher</li>
<li>Correct bias with length normalization</li>
<li>Choose based on application</li>
</ul>
</details>
<details>
<summary><strong>4. Implementation Points</strong></summary>
<ul>
<li>Encoder does not need <code>requires_grad=False</code> (all parameters train)</li>
<li>Prevent gradient explosion with gradient clipping</li>
<li>Set <code>ignore_index</code> in CrossEntropyLoss (for padding handling)</li>
<li>Efficiency through batch processing</li>
</ul>
</details>
<h3>Next Steps</h3>
<p>In the next chapter, we will learn about the <strong>Attention Mechanism</strong> that solves the biggest challenge of Seq2Seq - the Context Vector bottleneck problem:</p>
<ul>
<li>Bahdanau Attention (Additive Attention)</li>
<li>Luong Attention (Multiplicative Attention)</li>
<li>Self-Attention (bridge to Transformers)</li>
<li>Improved interpretability through Attention visualization</li>
</ul>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Question 1: Understanding Context Vector</strong></summary>
<p><strong>Question</strong>: If the Context Vector dimension in a Seq2Seq model is increased from 256 to 1024, how will translation quality and memory usage change? Explain the trade-offs.</p>
<p><strong>Example Answer</strong>:</p>
<ul>
<li><strong>Quality improvement</strong>: Increased Context Vector expressiveness can retain more information. Especially effective for long texts</li>
<li><strong>Memory increase</strong>: LSTM hidden state size increases 4 times, memory usage also increases about 4 times</li>
<li><strong>Training time increase</strong>: Increased matrix operation computation reduces training speed</li>
<li><strong>Overfitting risk</strong>: Increased parameter count may cause overfitting on small datasets</li>
<li><strong>Optimal value</strong>: 512 is generally a good balance point depending on task and data volume</li>
</ul>
</details>
<details>
<summary><strong>Question 2: Impact of Teacher Forcing</strong></summary>
<p><strong>Question</strong>: What problems occur when training with Teacher Forcing rate of 0.0 (always Free Running) and 1.0 (always Teacher Forcing)?</p>
<p><strong>Example Answer</strong>:</p>
<p><strong>Teacher Forcing rate = 1.0 (always input ground truth)</strong>:</p>
<ul>
<li>Training is fast and stable</li>
<li>Training loss decreases easily</li>
<li>However, large gap between training and inference (Exposure Bias) since predicted tokens are used at inference</li>
<li>Errors accumulate once a mistake is made</li>
</ul>
<p><strong>Teacher Forcing rate = 0.0 (always input prediction)</strong>:</p>
<ul>
<li>Training and inference behavior match</li>
<li>However, prediction accuracy is low at early training, making learning unstable</li>
<li>Slow convergence, greatly increased training time</li>
<li>Gradients vanish easily</li>
</ul>
<p><strong>Recommendation</strong>: Around 0.5, or gradually decrease with Scheduled Sampling</p>
</details>
<details>
<summary><strong>Question 3: Beam Width Selection for Beam Search</strong></summary>
<p><strong>Question</strong>: In a machine translation system, if beam width is increased from 5 to 20, how do you expect BLEU score and inference time to change? Predict experimental result trends.</p>
<p><strong>Example Answer</strong>:</p>
<p><strong>BLEU score changes</strong>:</p>
<ul>
<li>k=5 ‚Üí k=10: +1-2 point improvement (significant effect)</li>
<li>k=10 ‚Üí k=20: +0.5 point or so (diminishing returns)</li>
<li>k=20 and above: Almost plateau (saturation)</li>
</ul>
<p><strong>Inference time changes</strong>:</p>
<ul>
<li>Almost linearly proportional to beam width</li>
<li>k=5 ‚Üí k=20: About 4 times slower</li>
</ul>
<p><strong>Practical choice</strong>:</p>
<ul>
<li>Offline translation: k=10-20</li>
<li>Real-time translation: k=3-5</li>
<li>Quality priority: Sometimes use k=50</li>
</ul>
</details>
<details>
<summary><strong>Question 4: Sequence Length and Memory Usage</strong></summary>
<p><strong>Question</strong>: For a Seq2Seq model with batch size 32 and maximum sequence length 50, if the maximum sequence length is increased to 100, by how much will memory usage increase? Calculate.</p>
<p><strong>Example Answer</strong>:</p>
<p>Main factors in memory usage:</p>
<ol>
<li><strong>Hidden states</strong>: batch_size √ó seq_len √ó hidden_dim</li>
<li><strong>Gradients</strong>: Stored for each parameter</li>
<li><strong>Intermediate activations</strong>: Retain values at each time in BPTT</li>
</ol>
<p>When sequence length goes from 50‚Üí100:</p>
<ul>
<li>Hidden states: 2x</li>
<li>BPTT intermediate values: 2x</li>
<li>Total memory usage: About 1.8-2x (parameters unchanged)</li>
</ul>
<p>Specific calculation (hidden_dim=512 case):</p>
<ul>
<li>Hidden states: 32 √ó 100 √ó 512 √ó 4 bytes = 6.4 MB</li>
<li>All BPTT timesteps: About 640 MB</li>
<li>Parameters: Unchanged</li>
</ul>
<p><strong>Countermeasures</strong>: Split sequences, Gradient Checkpointing, smaller batch size</p>
</details>
<details>
<summary><strong>Question 5: Application Design for Seq2Seq</strong></summary>
<p><strong>Question</strong>: When implementing a chatbot with Seq2Seq, what considerations are necessary? Propose at least 3 challenges and solutions.</p>
<p><strong>Example Answer</strong>:</p>
<p><strong>Challenge 1: Context retention</strong></p>
<ul>
<li>Problem: Conversation flow is lost with only single utterance pairs</li>
<li>Solution: Concatenate past N utterances as input, or use hierarchical Seq2Seq</li>
</ul>
<p><strong>Challenge 2: Too generic responses</strong></p>
<ul>
<li>Problem: Generates only safe responses like "I don't know", "OK"</li>
<li>Solution: Maximum Mutual Information objective function, Diversity penalty, reinforcement learning</li>
</ul>
<p><strong>Challenge 3: Lack of factuality</strong></p>
<ul>
<li>Problem: Generates hallucinated responses without referring to knowledge base</li>
<li>Solution: Knowledge-grounded dialogue, Retrieval-augmented generation</li>
</ul>
<p><strong>Challenge 4: Personality consistency</strong></p>
<ul>
<li>Problem: Tone and personality change with each response</li>
<li>Solution: Introduce Persona vectors, style transfer techniques</li>
</ul>
<p><strong>Challenge 5: Evaluation difficulty</strong></p>
<ul>
<li>Problem: Automatic evaluation metrics like BLEU do not reflect dialogue quality</li>
<li>Solution: Human evaluation, Engagement score, task success rate</li>
</ul>
</details>
<hr/>
<div class="navigation">
<a class="nav-button" href="chapter2-lstm-gru.html">‚Üê Chapter 2: LSTM &amp; GRU</a>
<a class="nav-button" href="chapter4-attention.html">Chapter 4: Attention Mechanism ‚Üí</a>
</div>
</t},></p></t},></p></t},></p></t},></p></main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
<p>ML-A02: Introduction to RNN - Master variable-length sequence transformation with Seq2Seq</p>
</footer>
</body>
</html>
