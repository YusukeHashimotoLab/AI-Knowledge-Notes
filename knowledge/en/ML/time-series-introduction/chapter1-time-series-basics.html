<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Time Series Data Fundamentals - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Time Series Data Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/time-series-introduction/index.html">Time Series</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/time-series-introduction/chapter1-time-series-basics.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Time Series Data Fundamentals</h1>
<p class="subtitle">The Foundation of Time Series Analysis - Data Understanding and Preprocessing</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Practice Problems: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Time Series Data Fundamentals, which what is time series data. You will learn Handle time series data with pandas, Perform visualization, and concept of stationarity.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the definition and characteristics of time series data</li>
<li>‚úÖ Handle time series data with pandas</li>
<li>‚úÖ Perform visualization and exploratory analysis of time series</li>
<li>‚úÖ Understand the concept of stationarity and testing methods</li>
<li>‚úÖ Interpret autocorrelation and partial autocorrelation</li>
<li>‚úÖ Execute preprocessing of time series data</li>
</ul>
<hr/>
<h2>1.1 What is Time Series Data</h2>
<h3>Definition and Characteristics of Time Series</h3>
<p><strong>Time Series Data</strong> is a collection of observations recorded in chronological order.</p>
<blockquote>
<p>An important characteristic of time series data is the existence of <strong>temporal dependencies</strong> between data points.</p>
</blockquote>
<h3>Properties of Time Series Data</h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Temporal Order</strong></td>
<td>The order of data is important</td>
<td>Past stock prices influence the future</td>
</tr>
<tr>
<td><strong>Autocorrelation</strong></td>
<td>Past values correlate with current values</td>
<td>Continuity in temperature</td>
</tr>
<tr>
<td><strong>Trend</strong></td>
<td>Long-term upward or downward tendency</td>
<td>Sales growth</td>
</tr>
<tr>
<td><strong>Seasonality</strong></td>
<td>Periodic patterns</td>
<td>Increased power consumption in summer</td>
</tr>
<tr>
<td><strong>Non-stationarity</strong></td>
<td>Statistical properties change over time</td>
<td>Stock price volatility changes</td>
</tr>
</tbody>
</table>
<h3>Types of Time Series Data</h3>
<table>
<thead>
<tr>
<th>Classification</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Regular</strong></td>
<td>Observed at constant intervals</td>
<td>Daily stock prices, hourly temperature</td>
</tr>
<tr>
<td><strong>Irregular</strong></td>
<td>Observed at irregular intervals</td>
<td>Event logs, transaction data</td>
</tr>
<tr>
<td><strong>Univariate</strong></td>
<td>Observing one variable</td>
<td>Temperature only</td>
</tr>
<tr>
<td><strong>Multivariate</strong></td>
<td>Observing multiple variables simultaneously</td>
<td>Temperature, humidity, air pressure</td>
</tr>
</tbody>
</table>
<h3>Time Series Analysis in Business</h3>
<ul>
<li><strong>Demand Forecasting</strong>: Optimization of sales, inventory, logistics</li>
<li><strong>Financial Analysis</strong>: Stock price prediction, risk management</li>
<li><strong>Anomaly Detection</strong>: System monitoring, fraud detection</li>
<li><strong>Sensor Data</strong>: IoT, quality control in manufacturing</li>
<li><strong>Economic Analysis</strong>: GDP, unemployment rate, inflation</li>
</ul>
<h3>Time Series Data Basics with pandas</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Time Series Data Basics with pandas

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate date range
dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')

# Create time series data
np.random.seed(42)
ts_data = pd.DataFrame({
    'date': dates,
    'sales': np.random.randint(100, 500, len(dates)) + np.arange(len(dates)) * 0.5,
    'temperature': 15 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) + np.random.randn(len(dates)) * 2
})

# Set date column as index
ts_data.set_index('date', inplace=True)

print("=== Time Series Data Overview ===")
print(ts_data.head(10))
print(f"\nData types:\n{ts_data.dtypes}")
print(f"\nIndex type: {type(ts_data.index)}")
print(f"\nBasic statistics:\n{ts_data.describe()}")

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

axes[0].plot(ts_data.index, ts_data['sales'], color='blue', alpha=0.7)
axes[0].set_xlabel('Date')
axes[0].set_ylabel('Sales')
axes[0].set_title('Daily Sales Trend', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].plot(ts_data.index, ts_data['temperature'], color='red', alpha=0.7)
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Temperature (¬∞C)')
axes[1].set_title('Daily Temperature Trend', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Convenient datetime Operations</h3>
<pre><code class="language-python"># Parse dates
date_str = '2023-01-15'
parsed_date = pd.to_datetime(date_str)
print(f"Parsed date: {parsed_date}")
print(f"Type: {type(parsed_date)}")

# Create date ranges
# Daily
daily = pd.date_range('2023-01-01', '2023-01-10', freq='D')
print(f"\nDaily: {daily[:5]}")

# Weekly (Sunday start)
weekly = pd.date_range('2023-01-01', periods=10, freq='W')
print(f"Weekly: {weekly[:3]}")

# Monthly (month end)
monthly = pd.date_range('2023-01-01', periods=12, freq='M')
print(f"Monthly: {monthly[:3]}")

# Hourly
hourly = pd.date_range('2023-01-01', periods=24, freq='H')
print(f"Hourly: {hourly[:3]}")

# Extract date components
ts_data['year'] = ts_data.index.year
ts_data['month'] = ts_data.index.month
ts_data['day'] = ts_data.index.day
ts_data['dayofweek'] = ts_data.index.dayofweek  # Monday=0
ts_data['quarter'] = ts_data.index.quarter

print("\n=== Date Component Extraction ===")
print(ts_data.head())

# Slicing
print("\n=== Time Series Slicing ===")
print(f"Data for January 2023:\n{ts_data['2023-01'].head()}")
print(f"\nJanuary 1 to January 7:\n{ts_data['2023-01-01':'2023-01-07']}")
</code></pre>
<hr/>
<h2>1.2 Time Series Visualization and Exploration</h2>
<h3>Time Series Plots</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Time Series Plots

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate more complex time series data
np.random.seed(42)
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')

# Trend + Seasonality + Noise
trend = np.arange(len(dates)) * 0.5
seasonality = 100 * np.sin(2 * np.pi * np.arange(len(dates)) / 365)
noise = np.random.randn(len(dates)) * 20

sales = 1000 + trend + seasonality + noise

ts = pd.Series(sales, index=dates, name='sales')

# Basic visualization
fig, axes = plt.subplots(3, 1, figsize=(14, 10))

# Full period
axes[0].plot(ts.index, ts.values, color='steelblue', linewidth=1)
axes[0].set_ylabel('Sales')
axes[0].set_title('Time Series Plot - Full Period', fontsize=14)
axes[0].grid(True, alpha=0.3)

# 2023 only
ts_2023 = ts['2023']
axes[1].plot(ts_2023.index, ts_2023.values, color='coral', linewidth=1.5)
axes[1].set_ylabel('Sales')
axes[1].set_title('Time Series Plot - 2023', fontsize=14)
axes[1].grid(True, alpha=0.3)

# Q1 2023 only
ts_q1 = ts['2023-01':'2023-03']
axes[2].plot(ts_q1.index, ts_q1.values, color='green', linewidth=2, marker='o', markersize=3)
axes[2].set_xlabel('Date')
axes[2].set_ylabel('Sales')
axes[2].set_title('Time Series Plot - Q1 2023', fontsize=14)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Basic Statistics ===")
print(ts.describe())
</code></pre>
<h3>Rolling Statistics (Moving Average)</h3>
<pre><code class="language-python"># Moving average and moving standard deviation
rolling_mean_7 = ts.rolling(window=7).mean()
rolling_mean_30 = ts.rolling(window=30).mean()
rolling_std_30 = ts.rolling(window=30).std()

fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Moving average
axes[0].plot(ts.index, ts.values, label='Original Data', alpha=0.5, linewidth=0.8)
axes[0].plot(rolling_mean_7.index, rolling_mean_7.values,
             label='7-Day Moving Average', color='orange', linewidth=2)
axes[0].plot(rolling_mean_30.index, rolling_mean_30.values,
             label='30-Day Moving Average', color='red', linewidth=2)
axes[0].set_ylabel('Sales')
axes[0].set_title('Moving Average', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Moving standard deviation
axes[1].plot(rolling_std_30.index, rolling_std_30.values,
             color='purple', linewidth=2)
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Standard Deviation')
axes[1].set_title('30-Day Moving Standard Deviation (Volatility)', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Moving Statistics ===")
print(f"7-Day Moving Average (Latest 5):\n{rolling_mean_7.tail()}")
print(f"\n30-Day Moving Standard Deviation (Latest 5):\n{rolling_std_30.tail()}")
</code></pre>
<h3>Time Series Decomposition</h3>
<p>Time series can be decomposed into the following components:</p>
<ul>
<li><strong>Trend</strong>: Long-term tendency</li>
<li><strong>Seasonality</strong>: Periodic patterns</li>
<li><strong>Residual</strong>: Random noise</li>
</ul>
<p>Decomposition models:</p>
<ul>
<li><strong>Additive model</strong>: $y_t = T_t + S_t + R_t$</li>
<li><strong>Multiplicative model</strong>: $y_t = T_t \times S_t \times R_t$</li>
</ul>
<pre><code class="language-python">from statsmodels.tsa.seasonal import seasonal_decompose

# Seasonal decomposition (additive model)
decomposition = seasonal_decompose(ts, model='additive', period=365)

# Get decomposition results
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(14, 12))

# Original data
axes[0].plot(ts.index, ts.values, color='blue', linewidth=1)
axes[0].set_ylabel('Sales')
axes[0].set_title('Original Time Series Data', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Trend
axes[1].plot(trend.index, trend.values, color='red', linewidth=2)
axes[1].set_ylabel('Trend')
axes[1].set_title('Trend Component', fontsize=14)
axes[1].grid(True, alpha=0.3)

# Seasonality
axes[2].plot(seasonal.index, seasonal.values, color='green', linewidth=1)
axes[2].set_ylabel('Seasonality')
axes[2].set_title('Seasonal Component', fontsize=14)
axes[2].grid(True, alpha=0.3)

# Residual
axes[3].plot(residual.index, residual.values, color='purple', linewidth=1, alpha=0.7)
axes[3].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[3].set_xlabel('Date')
axes[3].set_ylabel('Residual')
axes[3].set_title('Residual Component', fontsize=14)
axes[3].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Decomposition Statistics ===")
print(f"Trend:\n{trend.describe()}")
print(f"\nSeasonality:\n{seasonal.describe()}")
print(f"\nResidual:\n{residual.describe()}")
</code></pre>
<hr/>
<h2>1.3 Stationarity</h2>
<h3>Definition of Stationarity</h3>
<p><strong>Stationarity</strong> refers to the statistical properties of a time series not changing over time.</p>
<h3>Weak Stationarity</h3>
<p>Satisfies the following three conditions:</p>
<ol>
<li><strong>Constant mean</strong>: $E[y_t] = \mu$ (constant for all $t$)</li>
<li><strong>Constant variance</strong>: $\text{Var}[y_t] = \sigma^2$ (constant for all $t$)</li>
<li><strong>Autocovariance depends only on lag</strong>: $\text{Cov}(y_t, y_{t-k})$ depends only on $k$</li>
</ol>
<h3>Strict Stationarity</h3>
<p>For any set of time points $\{t_1, t_2, \ldots, t_n\}$ and any lag $k$,</p>

$$
F(y_{t_1}, y_{t_2}, \ldots, y_{t_n}) = F(y_{t_1+k}, y_{t_2+k}, \ldots, y_{t_n+k})
$$

<p>In practice, weak stationarity is treated as "stationarity".</p>
<h3>Importance of Stationarity</h3>
<ul>
<li>Many time series models (ARIMA, etc.) assume stationarity</li>
<li>Non-stationary data leads to unstable predictions</li>
<li>Stationarization improves prediction accuracy</li>
</ul>
<h3>ADF Test (Augmented Dickey-Fuller Test)</h3>
<p><strong>Null hypothesis</strong>: The time series is non-stationary (has a unit root)</p>
<p><strong>Alternative hypothesis</strong>: The time series is stationary</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

from statsmodels.tsa.stattools import adfuller
import pandas as pd
import numpy as np

# Generate non-stationary data (random walk)
np.random.seed(42)
random_walk = np.cumsum(np.random.randn(1000))

# Generate stationary data (white noise)
white_noise = np.random.randn(1000)

def adf_test(series, name):
    """Execute ADF test and display results"""
    result = adfuller(series, autolag='AIC')

    print(f"\n=== ADF Test for {name} ===")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Number of Lags: {result[2]}")
    print(f"Number of Observations: {result[3]}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"  {key}: {value:.4f}")

    if result[1] &lt; 0.05:
        print("Conclusion: Stationary (p &lt; 0.05)")
    else:
        print("Conclusion: Non-stationary (p &gt;= 0.05)")

    return result

# Execute tests
adf_random_walk = adf_test(random_walk, "Random Walk (Non-stationary)")
adf_white_noise = adf_test(white_noise, "White Noise (Stationary)")
adf_sales = adf_test(ts.values, "Sales Data")

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(14, 10))

axes[0].plot(random_walk, color='red', linewidth=1)
axes[0].set_ylabel('Value')
axes[0].set_title(f'Random Walk (Non-stationary) - ADF p-value: {adf_random_walk[1]:.4f}',
                  fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].plot(white_noise, color='green', linewidth=1)
axes[1].set_ylabel('Value')
axes[1].set_title(f'White Noise (Stationary) - ADF p-value: {adf_white_noise[1]:.4f}',
                  fontsize=14)
axes[1].grid(True, alpha=0.3)

axes[2].plot(ts.values, color='blue', linewidth=1)
axes[2].set_xlabel('Time Point')
axes[2].set_ylabel('Sales')
axes[2].set_title(f'Sales Data - ADF p-value: {adf_sales[1]:.4f}', fontsize=14)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>KPSS Test (Kwiatkowski-Phillips-Schmidt-Shin Test)</h3>
<p><strong>Null hypothesis</strong>: The time series is stationary</p>
<p><strong>Alternative hypothesis</strong>: The time series is non-stationary</p>
<blockquote>
<p><strong>Note</strong>: KPSS has the opposite null hypothesis from ADF. Using both tests together allows for more reliable judgment.</p>
</blockquote>
<pre><code class="language-python">from statsmodels.tsa.stattools import kpss

def kpss_test(series, name):
    """Execute KPSS test and display results"""
    result = kpss(series, regression='c', nlags='auto')

    print(f"\n=== KPSS Test for {name} ===")
    print(f"KPSS Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Number of Lags: {result[2]}")
    print(f"Critical Values:")
    for key, value in result[3].items():
        print(f"  {key}: {value:.4f}")

    if result[1] &lt; 0.05:
        print("Conclusion: Non-stationary (p &lt; 0.05)")
    else:
        print("Conclusion: Stationary (p &gt;= 0.05)")

    return result

# Execute tests
kpss_random_walk = kpss_test(random_walk, "Random Walk (Non-stationary)")
kpss_white_noise = kpss_test(white_noise, "White Noise (Stationary)")

# Integrated judgment of test results
print("\n=== Integrated Judgment (ADF &amp; KPSS) ===")
results = [
    ("Random Walk", adf_random_walk[1], kpss_random_walk[1]),
    ("White Noise", adf_white_noise[1], kpss_white_noise[1])
]

for name, adf_p, kpss_p in results:
    print(f"\n{name}:")
    print(f"  ADF p-value: {adf_p:.4f} ({'Stationary' if adf_p &lt; 0.05 else 'Non-stationary'})")
    print(f"  KPSS p-value: {kpss_p:.4f} ({'Non-stationary' if kpss_p &lt; 0.05 else 'Stationary'})")

    if adf_p &lt; 0.05 and kpss_p &gt;= 0.05:
        print("  ‚Üí Conclusion: Stationary")
    elif adf_p &gt;= 0.05 and kpss_p &lt; 0.05:
        print("  ‚Üí Conclusion: Non-stationary")
    else:
        print("  ‚Üí Conclusion: Tests disagree (requires additional analysis)")
</code></pre>
<hr/>
<h2>1.4 Autocorrelation and Partial Autocorrelation</h2>
<h3>ACF (Autocorrelation Function)</h3>
<p><strong>Autocorrelation</strong> is the correlation coefficient between a time series and its lagged version.</p>

$$
\text{ACF}(k) = \frac{\text{Cov}(y_t, y_{t-k})}{\text{Var}(y_t)}
$$

<ul>
<li>$k$: Lag (time difference)</li>
<li>Value range: $[-1, 1]$</li>
</ul>
<h3>PACF (Partial Autocorrelation Function)</h3>
<p><strong>Partial autocorrelation</strong> is the correlation after removing the effects of intermediate lags.</p>
<ul>
<li>$\text{PACF}(k)$: Direct correlation at lag $k$</li>
<li>Removes the influence of intermediate lags $1, 2, \ldots, k-1$</li>
</ul>
<h3>ACF and PACF Plots</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: ACF and PACF Plots

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Generate different types of time series data
np.random.seed(42)
n = 500

# 1. AR(1) process: y_t = 0.7 * y_{t-1} + Œµ_t
ar1 = [0]
for _ in range(n):
    ar1.append(0.7 * ar1[-1] + np.random.randn())
ar1 = np.array(ar1[1:])

# 2. MA(1) process: y_t = Œµ_t + 0.7 * Œµ_{t-1}
ma1 = []
epsilon = np.random.randn(n + 1)
for i in range(n):
    ma1.append(epsilon[i] + 0.7 * epsilon[i-1])
ma1 = np.array(ma1)

# 3. White noise
white_noise = np.random.randn(n)

# ACF/PACF plots
fig, axes = plt.subplots(3, 3, figsize=(15, 12))

series_list = [
    (ar1, 'AR(1) Process'),
    (ma1, 'MA(1) Process'),
    (white_noise, 'White Noise')
]

for i, (series, name) in enumerate(series_list):
    # Time series plot
    axes[i, 0].plot(series, linewidth=1)
    axes[i, 0].set_title(name, fontsize=12)
    axes[i, 0].set_ylabel('Value')
    axes[i, 0].grid(True, alpha=0.3)

    # ACF
    plot_acf(series, lags=40, ax=axes[i, 1], alpha=0.05)
    axes[i, 1].set_title(f'{name} - ACF', fontsize=12)
    axes[i, 1].grid(True, alpha=0.3)

    # PACF
    plot_pacf(series, lags=40, ax=axes[i, 2], alpha=0.05)
    axes[i, 2].set_title(f'{name} - PACF', fontsize=12)
    axes[i, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Interpretation of ACF/PACF Patterns ===")
print("\nAR(1) Process:")
print("  - ACF: Exponentially decaying")
print("  - PACF: Cuts off after lag 1 (zero thereafter)")

print("\nMA(1) Process:")
print("  - ACF: Cuts off after lag 1 (zero thereafter)")
print("  - PACF: Exponentially decaying")

print("\nWhite Noise:")
print("  - ACF: Near zero at all lags")
print("  - PACF: Near zero at all lags")
</code></pre>
<h3>Correlogram</h3>
<p>ACF/PACF analysis on real data:</p>
<pre><code class="language-python"># ACF/PACF of sales data
fig, axes = plt.subplots(3, 1, figsize=(14, 10))

# Time series plot
axes[0].plot(ts.index, ts.values, linewidth=1)
axes[0].set_ylabel('Sales')
axes[0].set_title('Sales Data', fontsize=14)
axes[0].grid(True, alpha=0.3)

# ACF
plot_acf(ts.values, lags=100, ax=axes[1], alpha=0.05)
axes[1].set_title('ACF (Autocorrelation)', fontsize=14)
axes[1].set_xlabel('Lag')
axes[1].grid(True, alpha=0.3)

# PACF
plot_pacf(ts.values, lags=100, ax=axes[2], alpha=0.05)
axes[2].set_title('PACF (Partial Autocorrelation)', fontsize=14)
axes[2].set_xlabel('Lag')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Guidelines for Lag Selection ===")
print("Determine from ACF/PACF:")
print("  - Number of significant lags ‚Üí Determine model order")
print("  - Seasonal patterns ‚Üí Identify seasonal lags")
print("  - Decay pattern ‚Üí Determine AR or MA")
</code></pre>
<h3>Model Identification Guidelines</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>ACF</th>
<th>PACF</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AR(p)</strong></td>
<td>Exponential decay or damped oscillation</td>
<td>Cuts off after lag $p$</td>
</tr>
<tr>
<td><strong>MA(q)</strong></td>
<td>Cuts off after lag $q$</td>
<td>Exponential decay or damped oscillation</td>
</tr>
<tr>
<td><strong>ARMA(p,q)</strong></td>
<td>Decays after lag $q$</td>
<td>Decays after lag $p$</td>
</tr>
<tr>
<td><strong>White Noise</strong></td>
<td>Non-significant at all lags</td>
<td>Non-significant at all lags</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.5 Data Preprocessing</h2>
<h3>Handling Missing Values</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Handling Missing Values

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Time series data with missing values
np.random.seed(42)
dates = pd.date_range('2023-01-01', periods=365, freq='D')
values = 100 + np.cumsum(np.random.randn(365))

ts_missing = pd.Series(values, index=dates)

# Create random missing values
missing_indices = np.random.choice(365, size=30, replace=False)
ts_missing.iloc[missing_indices] = np.nan

print("=== Missing Value Status ===")
print(f"Missing count: {ts_missing.isnull().sum()}")
print(f"Missing rate: {ts_missing.isnull().sum() / len(ts_missing) * 100:.2f}%")

# Missing value handling methods

# 1. Forward Fill
ts_ffill = ts_missing.fillna(method='ffill')

# 2. Backward Fill
ts_bfill = ts_missing.fillna(method='bfill')

# 3. Linear Interpolation
ts_interpolate = ts_missing.interpolate(method='linear')

# 4. Spline Interpolation
ts_spline = ts_missing.interpolate(method='spline', order=2)

# Visualization
fig, axes = plt.subplots(3, 2, figsize=(15, 12))

methods = [
    (ts_missing, 'Original Data (with missing values)'),
    (ts_ffill, 'Forward Fill'),
    (ts_bfill, 'Backward Fill'),
    (ts_interpolate, 'Linear Interpolation'),
    (ts_spline, 'Spline Interpolation'),
    (ts_missing.dropna(), 'Drop Missing')
]

for ax, (data, title) in zip(axes.flat, methods):
    ax.plot(data.index, data.values, linewidth=1.5, marker='o' if title == 'Original Data (with missing values)' else '', markersize=2)
    ax.set_ylabel('Value')
    ax.set_title(title, fontsize=12)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n=== Statistics for Each Imputation Method ===")
print(f"Forward Fill: Mean={ts_ffill.mean():.2f}, Std={ts_ffill.std():.2f}")
print(f"Backward Fill: Mean={ts_bfill.mean():.2f}, Std={ts_bfill.std():.2f}")
print(f"Linear Interpolation: Mean={ts_interpolate.mean():.2f}, Std={ts_interpolate.std():.2f}")
print(f"Spline: Mean={ts_spline.mean():.2f}, Std={ts_spline.std():.2f}")
</code></pre>
<h3>Differencing</h3>
<p><strong>Differencing transformation</strong> is a fundamental technique to make non-stationary time series stationary.</p>
<ul>
<li><strong>First-order differencing</strong>: $\Delta y_t = y_t - y_{t-1}$ (removes trend)</li>
<li><strong>Seasonal differencing</strong>: $\Delta_s y_t = y_t - y_{t-s}$ (removes seasonality)</li>
</ul>
<pre><code class="language-python">from statsmodels.tsa.stattools import adfuller

# Non-stationary data (with trend)
trend_data = ts.copy()

# First-order differencing
diff_1 = trend_data.diff().dropna()

# Second-order differencing
diff_2 = trend_data.diff().diff().dropna()

# Seasonal differencing (7-day cycle)
diff_seasonal = trend_data.diff(7).dropna()

# Confirm stationarity with ADF test
def quick_adf(data, name):
    result = adfuller(data, autolag='AIC')
    print(f"{name}: ADF statistic={result[0]:.4f}, p-value={result[1]:.4f} ‚Üí {'Stationary' if result[1] &lt; 0.05 else 'Non-stationary'}")

print("=== Stationarization by Differencing ===")
quick_adf(trend_data.values, "Original Data")
quick_adf(diff_1.values, "First-order Differencing")
quick_adf(diff_2.values, "Second-order Differencing")
quick_adf(diff_seasonal.values, "Seasonal Differencing(7)")

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(14, 12))

axes[0].plot(trend_data.index, trend_data.values, linewidth=1)
axes[0].set_ylabel('Value')
axes[0].set_title('Original Data (Non-stationary)', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].plot(diff_1.index, diff_1.values, linewidth=1, color='orange')
axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[1].set_ylabel('Differenced Value')
axes[1].set_title('First-order Differencing', fontsize=14)
axes[1].grid(True, alpha=0.3)

axes[2].plot(diff_2.index, diff_2.values, linewidth=1, color='green')
axes[2].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[2].set_ylabel('Differenced Value')
axes[2].set_title('Second-order Differencing', fontsize=14)
axes[2].grid(True, alpha=0.3)

axes[3].plot(diff_seasonal.index, diff_seasonal.values, linewidth=1, color='purple')
axes[3].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[3].set_xlabel('Date')
axes[3].set_ylabel('Differenced Value')
axes[3].set_title('Seasonal Differencing (7 days)', fontsize=14)
axes[3].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Log Transformation</h3>
<p><strong>Log transformation</strong> stabilizes variance and converts multiplicative seasonality to additive.</p>
<pre><code class="language-python"># Data with multiplicative seasonality
np.random.seed(42)
dates = pd.date_range('2020-01-01', periods=1000, freq='D')
multiplicative = 100 * (1 + 0.001 * np.arange(1000)) * (1 + 0.3 * np.sin(2 * np.pi * np.arange(1000) / 365)) * (1 + 0.1 * np.random.randn(1000))
ts_mult = pd.Series(multiplicative, index=dates)

# Log transformation
ts_log = np.log(ts_mult)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Original data
axes[0, 0].plot(ts_mult.index, ts_mult.values, linewidth=1)
axes[0, 0].set_ylabel('Value')
axes[0, 0].set_title('Original Data (Multiplicative Seasonality)', fontsize=14)
axes[0, 0].grid(True, alpha=0.3)

# After log transformation
axes[0, 1].plot(ts_log.index, ts_log.values, linewidth=1, color='orange')
axes[0, 1].set_ylabel('log(Value)')
axes[0, 1].set_title('After Log Transformation (Additive Seasonality)', fontsize=14)
axes[0, 1].grid(True, alpha=0.3)

# Original data histogram
axes[1, 0].hist(ts_mult.values, bins=50, edgecolor='black', alpha=0.7)
axes[1, 0].set_xlabel('Value')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].set_title('Original Data Distribution', fontsize=14)
axes[1, 0].grid(True, alpha=0.3)

# Log-transformed histogram
axes[1, 1].hist(ts_log.values, bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[1, 1].set_xlabel('log(Value)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Log-transformed Distribution (Closer to Normal)', fontsize=14)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Effect of Log Transformation ===")
print(f"Original Data: Mean={ts_mult.mean():.2f}, Std={ts_mult.std():.2f}, Skewness={ts_mult.skew():.2f}")
print(f"Log Transformation: Mean={ts_log.mean():.2f}, Std={ts_log.std():.2f}, Skewness={ts_log.skew():.2f}")
</code></pre>
<h3>Train-Test Split for Time Series</h3>
<blockquote>
<p><strong>Important</strong>: For time series data, we use a future period relative to the training data as test data. Random splitting is not used because it destroys temporal dependencies.</p>
</blockquote>
<pre><code class="language-python"># Split time series data
train_size = int(len(ts) * 0.8)

train = ts[:train_size]
test = ts[train_size:]

print("=== Train-Test Split ===")
print(f"Total Data: {len(ts)} records")
print(f"Training Data: {len(train)} records ({train.index[0]} ~ {train.index[-1]})")
print(f"Test Data: {len(test)} records ({test.index[0]} ~ {test.index[-1]})")

# Visualization
plt.figure(figsize=(14, 6))
plt.plot(train.index, train.values, label='Training Data', linewidth=1.5, color='blue')
plt.plot(test.index, test.values, label='Test Data', linewidth=1.5, color='red')
plt.axvline(x=train.index[-1], color='green', linestyle='--', linewidth=2, label='Split Point')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.title('Train-Test Split for Time Series Data', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Time Series Cross-Validation
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

print("\n=== Time Series Cross-Validation Splits ===")
for i, (train_idx, test_idx) in enumerate(tscv.split(ts.values)):
    print(f"\nFold {i+1}:")
    print(f"  Train: Index {train_idx[0]} ~ {train_idx[-1]} ({len(train_idx)} records)")
    print(f"  Test: Index {test_idx[0]} ~ {test_idx[-1]} ({len(test_idx)} records)")

# Visualization
fig, axes = plt.subplots(5, 1, figsize=(14, 12))

for i, (train_idx, test_idx) in enumerate(tscv.split(ts.values)):
    axes[i].plot(ts.index[train_idx], ts.values[train_idx], color='blue', linewidth=1, label='Train')
    axes[i].plot(ts.index[test_idx], ts.values[test_idx], color='red', linewidth=1, label='Test')
    axes[i].set_ylabel('Sales')
    axes[i].set_title(f'Fold {i+1}', fontsize=12)
    axes[i].legend(loc='upper left')
    axes[i].grid(True, alpha=0.3)

axes[-1].set_xlabel('Date')
plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Time Series Data Fundamentals</strong></p>
<ul>
<li>Temporal dependencies exist</li>
<li>Components: trend, seasonality, residual</li>
<li>Date manipulation with pandas</li>
</ul></li>
<li><p><strong>Visualization and Exploration</strong></p>
<ul>
<li>Understanding patterns through time series plots</li>
<li>Understanding trends with moving statistics</li>
<li>Separating components through decomposition</li>
</ul></li>
<li><p><strong>Stationarity</strong></p>
<ul>
<li>Three conditions of weak stationarity</li>
<li>ADF test and KPSS test</li>
<li>Importance of stationarization</li>
</ul></li>
<li><p><strong>Autocorrelation</strong></p>
<ul>
<li>ACF: Correlation with all lags</li>
<li>PACF: Direct correlation</li>
<li>Application to model identification</li>
</ul></li>
<li><p><strong>Preprocessing</strong></p>
<ul>
<li>Missing value imputation methods</li>
<li>Stationarization through differencing</li>
<li>Variance stabilization through log transformation</li>
<li>Appropriate train-test split</li>
</ul></li>
</ol>
<h3>Basic Workflow for Time Series Analysis</h3>
<div class="mermaid">
graph TD
    A[Raw Data] --&gt; B[Exploratory Analysis]
    B --&gt; C[Visualization &amp; Statistical Examination]
    C --&gt; D[Stationarity Test]
    D --&gt; E{Stationary?}
    E --&gt;|No| F[Differencing &amp; Transformation]
    F --&gt; D
    E --&gt;|Yes| G[ACF/PACF Analysis]
    G --&gt; H[Model Selection]
    H --&gt; I[Prediction &amp; Evaluation]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#fff9c4
    style F fill:#ffccbc
    style G fill:#e8f5e9
    style H fill:#fce4ec
    style I fill:#c8e6c9
</div>
<h3>Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>ARIMA Models</strong>:</p>
<ul>
<li>AR (Autoregressive) models</li>
<li>MA (Moving Average) models</li>
<li>ARIMA (Autoregressive Integrated Moving Average) models</li>
<li>Seasonal ARIMA models (SARIMA)</li>
<li>Model selection and parameter estimation</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the difference between stationary and non-stationary time series, and describe why stationarity is important.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Stationary Time Series</strong>:</p>
<ul>
<li>Constant mean</li>
<li>Constant variance</li>
<li>Autocovariance depends only on time lag</li>
<li>Examples: White noise, AR(1) (|œÜ| &lt; 1)</li>
</ul>
<p><strong>Non-stationary Time Series</strong>:</p>
<ul>
<li>Mean or variance changes over time</li>
<li>Contains trend or seasonality</li>
<li>Examples: Random walk, growing sales data</li>
</ul>
<p><strong>Why Stationarity is Important</strong>:</p>
<ol>
<li><strong>Prediction Stability</strong>: Since statistical properties are constant, future predictions are reliable</li>
<li><strong>Model Application</strong>: Many time series models (ARIMA, etc.) assume stationarity</li>
<li><strong>Statistical Inference</strong>: Enables parameter estimation and hypothesis testing</li>
<li><strong>Generalizability</strong>: Past patterns can be applied to the future</li>
</ol>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Execute an ADF test on the following data and determine its stationarity.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Execute an ADF test on the following data and determine its 

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np
np.random.seed(123)
data = np.cumsum(np.random.randn(200)) + 10
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Execute an ADF test on the following data and determine its 

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from statsmodels.tsa.stattools import adfuller
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(123)
data = np.cumsum(np.random.randn(200)) + 10

# ADF test
result = adfuller(data, autolag='AIC')

print("=== ADF Test Results ===")
print(f"ADF Statistic: {result[0]:.4f}")
print(f"p-value: {result[1]:.4f}")
print(f"Lags Used: {result[2]}")
print(f"Number of Observations: {result[3]}")
print(f"\nCritical Values:")
for key, value in result[4].items():
    print(f"  {key}: {value:.4f}")

print(f"\nJudgment:")
if result[1] &lt; 0.05:
    print("  ‚Üí Stationary (p &lt; 0.05, reject null hypothesis)")
else:
    print("  ‚Üí Non-stationary (p &gt;= 0.05, cannot reject null hypothesis)")
    print("  ‚Üí Data is random walk (cumulative sum), so non-stationary is reasonable")

# Visualization
plt.figure(figsize=(12, 5))
plt.plot(data, linewidth=1.5)
plt.axhline(y=data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.2f}')
plt.xlabel('Time Point')
plt.ylabel('Value')
plt.title(f'Time Series Data (Random Walk) - ADF p-value: {result[1]:.4f}', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Stationarize with first-order differencing
diff_data = np.diff(data)
result_diff = adfuller(diff_data, autolag='AIC')

print(f"\n=== ADF Test After First-order Differencing ===")
print(f"ADF Statistic: {result_diff[0]:.4f}")
print(f"p-value: {result_diff[1]:.4f}")
print(f"Judgment: {'Stationary' if result_diff[1] &lt; 0.05 else 'Non-stationary'}")
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>=== ADF Test Results ===
ADF Statistic: -1.2345
p-value: 0.6543
‚Üí Non-stationary (p &gt;= 0.05)

=== ADF Test After First-order Differencing ===
p-value: 0.0001
Judgment: Stationary
</code></pre>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Explain the difference between ACF and PACF, and describe the ACF/PACF patterns for AR(2) and MA(2) processes.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Difference Between ACF and PACF</strong>:</p>
<ul>
<li><strong>ACF (Autocorrelation Function)</strong>:
<ul>
<li>Correlation between the time series and its lagged version</li>
<li>Includes effects of all intermediate lags</li>
<li>$\text{Corr}(y_t, y_{t-k})$</li>
</ul>
</li>
<li><strong>PACF (Partial Autocorrelation Function)</strong>:
<ul>
<li>Correlation after removing effects of intermediate lags</li>
<li>Only the direct effect of lag $k$</li>
<li>$\text{Corr}(y_t, y_{t-k} \mid y_{t-1}, \ldots, y_{t-k+1})$</li>
</ul>
</li>
</ul>
<p><strong>Patterns</strong>:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ACF</th>
<th>PACF</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AR(2)</strong></td>
<td>Exponential decay or damped oscillation (continues indefinitely)</td>
<td>Cuts off after lag 2 (zero thereafter)</td>
</tr>
<tr>
<td><strong>MA(2)</strong></td>
<td>Cuts off after lag 2 (zero thereafter)</td>
<td>Exponential decay or damped oscillation (continues indefinitely)</td>
</tr>
</tbody>
</table>
<p><strong>AR(2) Example</strong>: $y_t = 0.5y_{t-1} + 0.3y_{t-2} + \epsilon_t$</p>
<ul>
<li>ACF: Gradually decaying pattern</li>
<li>PACF: Significant at lags 1 and 2, zero after lag 3</li>
</ul>
<p><strong>MA(2) Example</strong>: $y_t = \epsilon_t + 0.5\epsilon_{t-1} + 0.3\epsilon_{t-2}$</p>
<ul>
<li>ACF: Significant at lags 1 and 2, zero after lag 3</li>
<li>PACF: Gradually decaying pattern</li>
</ul>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>For the following time series data, perform appropriate preprocessing (missing value handling, stationarization) and execute ADF tests before and after processing.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following time series data, perform appropriate prep

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np

np.random.seed(42)
dates = pd.date_range('2023-01-01', periods=365, freq='D')
trend = np.arange(365) * 0.5
seasonal = 50 * np.sin(2 * np.pi * np.arange(365) / 365)
noise = np.random.randn(365) * 10
data = 100 + trend + seasonal + noise

ts = pd.Series(data, index=dates)

# Add missing values
ts.iloc[50:60] = np.nan
ts.iloc[200:205] = np.nan
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following time series data, perform appropriate prep

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

np.random.seed(42)
dates = pd.date_range('2023-01-01', periods=365, freq='D')
trend = np.arange(365) * 0.5
seasonal = 50 * np.sin(2 * np.pi * np.arange(365) / 365)
noise = np.random.randn(365) * 10
data = 100 + trend + seasonal + noise

ts = pd.Series(data, index=dates)
ts.iloc[50:60] = np.nan
ts.iloc[200:205] = np.nan

print("=== Preprocessing Steps ===\n")

# Step 1: Check missing values
print(f"1. Missing Value Check")
print(f"   Missing count: {ts.isnull().sum()} ({ts.isnull().sum() / len(ts) * 100:.2f}%)")

# Step 2: Impute missing values (linear interpolation)
ts_filled = ts.interpolate(method='linear')
print(f"\n2. Missing Value Imputation (Linear Interpolation)")
print(f"   Missing count after imputation: {ts_filled.isnull().sum()}")

# Step 3: Stationarity test (original data)
result_original = adfuller(ts_filled.values, autolag='AIC')
print(f"\n3. ADF Test on Original Data")
print(f"   ADF Statistic: {result_original[0]:.4f}")
print(f"   p-value: {result_original[1]:.4f}")
print(f"   Judgment: {'Stationary' if result_original[1] &lt; 0.05 else 'Non-stationary'}")

# Step 4: Stationarization through first-order differencing
ts_diff = ts_filled.diff().dropna()
result_diff = adfuller(ts_diff.values, autolag='AIC')
print(f"\n4. ADF Test After First-order Differencing")
print(f"   ADF Statistic: {result_diff[0]:.4f}")
print(f"   p-value: {result_diff[1]:.4f}")
print(f"   Judgment: {'Stationary' if result_diff[1] &lt; 0.05 else 'Non-stationary'}")

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(14, 12))

# Original data (with missing values)
axes[0].plot(ts.index, ts.values, linewidth=1, marker='o', markersize=2)
axes[0].set_ylabel('Value')
axes[0].set_title(f'Original Data (with missing values) - Missing count: {ts.isnull().sum()}', fontsize=14)
axes[0].grid(True, alpha=0.3)

# After missing value imputation
axes[1].plot(ts_filled.index, ts_filled.values, linewidth=1, color='orange')
axes[1].set_ylabel('Value')
axes[1].set_title('After Linear Interpolation', fontsize=14)
axes[1].grid(True, alpha=0.3)

# First-order differencing
axes[2].plot(ts_diff.index, ts_diff.values, linewidth=1, color='green')
axes[2].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[2].set_ylabel('Differenced Value')
axes[2].set_title(f'First-order Differencing (Stationarized) - ADF p-value: {result_diff[1]:.4f}', fontsize=14)
axes[2].grid(True, alpha=0.3)

# Histogram comparison
axes[3].hist(ts_filled.values, bins=30, alpha=0.5, label='Original Data', edgecolor='black')
axes[3].hist(ts_diff.values, bins=30, alpha=0.5, label='First-order Differencing', edgecolor='black')
axes[3].set_xlabel('Value')
axes[3].set_ylabel('Frequency')
axes[3].set_title('Distribution Comparison', fontsize=14)
axes[3].legend()
axes[3].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n=== Preprocessing Complete ===")
print("‚úì Missing value imputation complete")
print("‚úì Stationarization complete (first-order differencing)")
print(f"‚úì Processed data count: {len(ts_diff)}")
</code></pre>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Explain why random splitting (like in regular machine learning) should not be used for train-test split in time series data. Also, demonstrate appropriate splitting methods.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Reasons Why Random Splitting Should Not Be Used</strong>:</p>
<ol>
<li><p><strong>Destruction of Temporal Dependencies</strong></p>
<ul>
<li>Time series data has important temporal order</li>
<li>Random splitting mixes past and future</li>
<li>Autocorrelation structure is destroyed</li>
</ul></li>
<li><p><strong>Data Leakage</strong></p>
<ul>
<li>Future data is used for training</li>
<li>Test data information leaks into training</li>
<li>Performance is overestimated</li>
</ul></li>
<li><p><strong>Divergence from Reality</strong></p>
<ul>
<li>In practice, we predict the future</li>
<li>Correct approach is to train on past data only and predict the future</li>
<li>Random splitting does not reflect real operation</li>
</ul></li>
</ol>
<p><strong>Appropriate Splitting Methods</strong>:</p>
<h4>1. Simple Time Series Split</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: 1. Simple Time Series Split

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Sample data
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
ts = pd.Series(np.random.randn(len(dates)).cumsum(), index=dates)

# 80% training, 20% test
split_point = int(len(ts) * 0.8)
train = ts[:split_point]
test = ts[split_point:]

print("=== Simple Time Series Split ===")
print(f"Train: {train.index[0]} ~ {train.index[-1]} ({len(train)} records)")
print(f"Test: {test.index[0]} ~ {test.index[-1]} ({len(test)} records)")
</code></pre>
<h4>2. Time Series Cross-Validation (TimeSeriesSplit)</h4>
<pre><code class="language-python">from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

print("\n=== Time Series Cross-Validation ===")
for i, (train_idx, test_idx) in enumerate(tscv.split(ts)):
    print(f"Fold {i+1}:")
    print(f"  Train: {len(train_idx)} records")
    print(f"  Test: {len(test_idx)} records")
</code></pre>
<h4>3. Walk-Forward Validation</h4>
<pre><code class="language-python"># Fixed window size with rolling validation
window_size = 365  # 1 year
test_size = 30     # 30 days

print("\n=== Walk-Forward Validation ===")
for i in range(0, len(ts) - window_size - test_size, test_size):
    train_start = i
    train_end = i + window_size
    test_end = train_end + test_size

    train_fold = ts[train_start:train_end]
    test_fold = ts[train_end:test_end]

    print(f"\nFold {i//test_size + 1}:")
    print(f"  Train: {train_fold.index[0]} ~ {train_fold.index[-1]}")
    print(f"  Test: {test_fold.index[0]} ~ {test_fold.index[-1]}")
</code></pre>
<p><strong>Example of Incorrect Method</strong>:</p>
<pre><code class="language-python"># ‚ùå Should NEVER be done
from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(ts, test_size=0.2, shuffle=True)  # NG!
</code></pre>
<p><strong>Principles of Correct Methods</strong>:</p>
<ul>
<li>Training data is always in the past relative to test data</li>
<li>Preserve temporal order</li>
<li>Mimic real operation (predict past ‚Üí future)</li>
<li>Prevent data leakage</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). <em>Time Series Analysis: Forecasting and Control</em> (5th ed.). Wiley.</li>
<li>Hyndman, R. J., &amp; Athanasopoulos, G. (2021). <em>Forecasting: Principles and Practice</em> (3rd ed.). OTexts. <a href="https://otexts.com/fpp3/">https://otexts.com/fpp3/</a></li>
<li>Tsay, R. S. (2010). <em>Analysis of Financial Time Series</em> (3rd ed.). Wiley.</li>
<li>Hamilton, J. D. (1994). <em>Time Series Analysis</em>. Princeton University Press.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter2-arima-models.html">Next Chapter: ARIMA Models ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operability, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<p><strong>Creator</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
