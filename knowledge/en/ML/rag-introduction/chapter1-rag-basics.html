<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="RAG Fundamentals - Architecture, Document Processing, and Chunking Strategies">
    <title>Chapter 1: RAG Fundamentals - RAG Introduction Series</title>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem 1.5rem; }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        header .container { padding: 0 1.5rem; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; font-weight: 700; }
        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        .meta span { display: inline-flex; align-items: center; gap: 0.3rem; }
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }
        h3 { font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.8rem; color: var(--primary-color); }
        h4 { font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.6rem; color: var(--primary-color); }
        p { margin-bottom: 1.2rem; }
        a { color: var(--link-color); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--link-hover); text-decoration: underline; }
        ul, ol { margin-left: 2rem; margin-bottom: 1.2rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: var(--code-bg);
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--secondary-color);
        }
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre code { background: none; padding: 0; }
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }
        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }
        .example-box {
            background: #f8f9fa;
            border-left: 4px solid var(--accent-color);
            padding: 1.2rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        .note-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .container { padding: 1rem; }
            h1 { font-size: 1.6rem; }
            h2 { font-size: 1.4rem; }
            .meta { font-size: 0.85rem; }
        }
    
        .feedback-notice {
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 2rem;
            margin: 3rem auto;
            max-width: 900px;
        }

        .feedback-notice h3 {
            color: #856404;
            font-size: 1.3rem;
            margin-bottom: 1rem;
            text-align: center;
        }

        .feedback-notice p {
            color: #856404;
            font-size: 1rem;
            margin-bottom: 1.5rem;
            text-align: center;
        }

        .feedback-options {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .feedback-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s;
        }

        .feedback-button:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/rag-introduction/index.html">RAG</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>Chapter 1: RAG Fundamentals</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">Architecture and Document Processing</p>
            <div class="meta">
                <span>üìñ Study Time: 30-35 minutes</span>
                <span>üìä Difficulty: Intermediate</span>
                <span>üíª Code Examples: 6</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2>1. What is RAG</h2>

        <h3>1.1 RAG Overview</h3>
        <p>RAG (Retrieval-Augmented Generation) is a technique for incorporating external knowledge into large language models (LLMs). By combining the generative capabilities of LLMs with retrieval systems, it enables responses based on up-to-date information and specialized knowledge.</p>

        <p><strong>Key Benefits:</strong></p>
        <ul>
            <li><strong>Utilizing Current Information</strong>: Access to information not included in the model's training data</li>
            <li><strong>Reducing Hallucinations</strong>: Improved accuracy through responses based on retrieval results</li>
            <li><strong>Cost Efficiency</strong>: Adding knowledge without the need for fine-tuning</li>
            <li><strong>Transparency</strong>: Clarifying information sources and providing verifiable responses</li>
        </ul>

        <h3>1.2 RAG Architecture</h3>
        <p>A RAG system consists of three main components:</p>

        <div class="example-box">
            <strong>RAG Pipeline:</strong>
            <ol>
                <li><strong>Index Building</strong>: Document loading ‚Üí Chunking ‚Üí Embedding ‚Üí Vector DB storage</li>
                <li><strong>Retrieval</strong>: Query ‚Üí Embedding ‚Üí Similar document retrieval</li>
                <li><strong>Generation</strong>: Retrieval results + Query ‚Üí LLM prompt ‚Üí Response generation</li>
            </ol>
        </div>

        <h4>Implementation Example 1: Basic RAG Architecture</h4>
        <pre><code>from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

class SimpleRAG:
    def __init__(self, api_key):
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.llm = ChatOpenAI(temperature=0, openai_api_key=api_key)
        self.vectorstore = None

    def index_documents(self, file_paths):
        """Index documents"""
        documents = []
        for path in file_paths:
            loader = TextLoader(path, encoding='utf-8')
            documents.extend(loader.load())

        # Chunking
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        splits = text_splitter.split_documents(documents)

        # Create vector store
        self.vectorstore = FAISS.from_documents(splits, self.embeddings)
        print(f"Indexing complete: {len(splits)} chunks")

    def query(self, question):
        """Question answering"""
        if not self.vectorstore:
            raise ValueError("Documents not indexed")

        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3})
        )

        result = qa_chain({"query": question})
        return result["result"]

# Usage example
rag = SimpleRAG(api_key="your-api-key")
rag.index_documents(["docs/manual.txt", "docs/faq.txt"])
answer = rag.query("What is the product warranty period?")
print(answer)</code></pre>

        <h2>2. Document Processing</h2>

        <h3>2.1 Document Loaders</h3>
        <p>Various loaders are available for loading documents in different formats:</p>

        <h4>Implementation Example 2: Multi-Format Loader</h4>
        <pre><code>from langchain.document_loaders import (
    TextLoader, PDFLoader, CSVLoader,
    UnstructuredMarkdownLoader, UnstructuredHTMLLoader
)
import os

class UniversalDocumentLoader:
    """Multi-format document loader"""

    LOADERS = {
        '.txt': TextLoader,
        '.pdf': PDFLoader,
        '.csv': CSVLoader,
        '.md': UnstructuredMarkdownLoader,
        '.html': UnstructuredHTMLLoader,
    }

    def load_documents(self, directory):
        """Load all documents in directory"""
        documents = []

        for root, _, files in os.walk(directory):
            for file in files:
                file_path = os.path.join(root, file)
                ext = os.path.splitext(file)[1].lower()

                if ext in self.LOADERS:
                    loader_class = self.LOADERS[ext]
                    try:
                        loader = loader_class(file_path)
                        docs = loader.load()

                        # Add metadata
                        for doc in docs:
                            doc.metadata['source_file'] = file
                            doc.metadata['file_type'] = ext

                        documents.extend(docs)
                        print(f"Loaded: {file} ({len(docs)} documents)")
                    except Exception as e:
                        print(f"Error ({file}): {e}")

        return documents

# Usage example
loader = UniversalDocumentLoader()
documents = loader.load_documents("./knowledge_base")
print(f"Total documents: {len(documents)}")</code></pre>

        <h3>2.2 Metadata Management</h3>
        <p>Proper metadata management improves retrieval accuracy and enables filtering.</p>

        <h4>Implementation Example 3: Metadata Enrichment</h4>
        <pre><code>from datetime import datetime
from langchain.schema import Document
import hashlib

class MetadataEnricher:
    """Document metadata enrichment"""

    def enrich_documents(self, documents):
        """Add and enhance metadata"""
        enriched = []

        for doc in documents:
            # Basic metadata
            metadata = doc.metadata.copy()

            # Timestamp
            metadata['indexed_at'] = datetime.now().isoformat()

            # Document length
            metadata['char_count'] = len(doc.page_content)
            metadata['word_count'] = len(doc.page_content.split())

            # Hash value (for duplicate detection)
            content_hash = hashlib.md5(
                doc.page_content.encode()
            ).hexdigest()
            metadata['content_hash'] = content_hash

            # Category estimation (simplified)
            metadata['category'] = self._estimate_category(doc.page_content)

            enriched.append(Document(
                page_content=doc.page_content,
                metadata=metadata
            ))

        return enriched

    def _estimate_category(self, text):
        """Estimate category from content"""
        keywords = {
            'technical': ['API', 'code', 'implementation', 'function'],
            'business': ['contract', 'pricing', 'sales', 'business'],
            'support': ['issue', 'error', 'trouble', 'support']
        }

        text_lower = text.lower()
        scores = {}

        for category, terms in keywords.items():
            score = sum(1 for term in terms if term.lower() in text_lower)
            scores[category] = score

        return max(scores, key=scores.get) if max(scores.values()) > 0 else 'general'

# Usage example
enricher = MetadataEnricher()
enriched_docs = enricher.enrich_documents(documents)

# Filtering by metadata
technical_docs = [
    doc for doc in enriched_docs
    if doc.metadata.get('category') == 'technical'
]
print(f"Technical documents: {len(technical_docs)}")</code></pre>

        <h2>3. Chunking Strategies</h2>

        <h3>3.1 Fixed-Size Chunking</h3>
        <p>The simplest method that divides documents by a specified number of characters or tokens.</p>

        <h4>Implementation Example 4: Fixed-Size Chunking</h4>
        <pre><code>from langchain.text_splitter import CharacterTextSplitter
import tiktoken

class FixedSizeChunker:
    """Fixed-size chunking"""

    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def chunk_by_characters(self, text):
        """Character-based splitting"""
        splitter = CharacterTextSplitter(
            separator="\n\n",
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len
        )
        return splitter.split_text(text)

    def chunk_by_tokens(self, text):
        """Token-based splitting"""
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            encoding_name="cl100k_base",
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        return splitter.split_text(text)

    def analyze_chunks(self, chunks):
        """Chunk statistics"""
        stats = {
            'total_chunks': len(chunks),
            'avg_length': sum(len(c) for c in chunks) / len(chunks),
            'min_length': min(len(c) for c in chunks),
            'max_length': max(len(c) for c in chunks),
        }
        return stats

# Usage example
chunker = FixedSizeChunker(chunk_size=500, chunk_overlap=50)

text = """Long document text..."""
chunks = chunker.chunk_by_tokens(text)
stats = chunker.analyze_chunks(chunks)

print(f"Number of chunks: {stats['total_chunks']}")
print(f"Average length: {stats['avg_length']:.1f} characters")</code></pre>

        <h3>3.2 Semantic Chunking</h3>
        <p>An advanced technique that divides documents considering semantic coherence.</p>

        <h4>Implementation Example 5: Semantic Chunking</h4>
        <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SemanticChunker:
    """Semantic chunking"""

    def __init__(self, embeddings, similarity_threshold=0.7):
        self.embeddings = embeddings
        self.threshold = similarity_threshold

    def chunk_by_similarity(self, text, min_chunk_size=100):
        """Similarity-based splitting"""
        # First split into sentences
        sentences = self._split_sentences(text)

        if len(sentences) <= 1:
            return [text]

        # Get embeddings for each sentence
        sentence_embeddings = self.embeddings.embed_documents(sentences)

        # Group based on similarity
        chunks = []
        current_chunk = [sentences[0]]

        for i in range(1, len(sentences)):
            # Calculate similarity with previous sentence
            sim = cosine_similarity(
                [sentence_embeddings[i-1]],
                [sentence_embeddings[i]]
            )[0][0]

            if sim >= self.threshold:
                current_chunk.append(sentences[i])
            else:
                # Start new chunk
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text) >= min_chunk_size:
                    chunks.append(chunk_text)
                current_chunk = [sentences[i]]

        # Add final chunk
        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _split_sentences(self, text):
        """Sentence splitting (simplified)"""
        import re
        sentences = re.split(r'[„ÄÇÔºÅÔºü\n]+', text)
        return [s.strip() for s in sentences if s.strip()]

# Usage example
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key="your-api-key")
semantic_chunker = SemanticChunker(embeddings, similarity_threshold=0.75)

text = """Machine learning is a branch of artificial intelligence. It learns from data.
Deep learning uses neural networks. It excels at image recognition.
Natural language processing deals with text. It enables translation and summarization."""

chunks = semantic_chunker.chunk_by_similarity(text)
for i, chunk in enumerate(chunks, 1):
    print(f"Chunk {i}: {chunk}")</code></pre>

        <h3>3.3 Hierarchical Chunking</h3>
        <p>Performs hierarchical splitting that considers document structure (headings, paragraphs, etc.).</p>

        <h4>Implementation Example 6: Hierarchical Chunking</h4>
        <pre><code>from langchain.text_splitter import MarkdownHeaderTextSplitter
from typing import List, Dict

class HierarchicalChunker:
    """Hierarchical chunking"""

    def chunk_markdown(self, markdown_text):
        """Structure-based splitting for Markdown"""
        headers_to_split_on = [
            ("#", "H1"),
            ("##", "H2"),
            ("###", "H3"),
        ]

        splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        splits = splitter.split_text(markdown_text)

        # Create chunks with hierarchy information
        hierarchical_chunks = []
        for split in splits:
            chunk = {
                'content': split.page_content,
                'metadata': split.metadata,
                'hierarchy': self._build_hierarchy(split.metadata)
            }
            hierarchical_chunks.append(chunk)

        return hierarchical_chunks

    def _build_hierarchy(self, metadata: Dict) -> str:
        """Build hierarchy path"""
        parts = []
        for level in ['H1', 'H2', 'H3']:
            if level in metadata:
                parts.append(metadata[level])
        return ' > '.join(parts)

    def chunk_with_context(self, text, chunk_size=500):
        """Retain parent chunk context"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        # Create parent chunks
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size * 3,
            chunk_overlap=0
        )
        parent_chunks = parent_splitter.split_text(text)

        # Create child chunks (preserving parent information)
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=50
        )

        chunks_with_context = []
        for parent_idx, parent in enumerate(parent_chunks):
            child_chunks = child_splitter.split_text(parent)

            for child_idx, child in enumerate(child_chunks):
                chunks_with_context.append({
                    'content': child,
                    'parent_id': parent_idx,
                    'child_id': child_idx,
                    'parent_summary': parent[:200] + '...'  # Parent summary
                })

        return chunks_with_context

# Usage example
hierarchical_chunker = HierarchicalChunker()

markdown_text = """
# Data Science
Learn the fundamentals of data analysis.

## Statistics
### Descriptive Statistics
Study mean, variance, and standard deviation.

### Inferential Statistics
Explain hypothesis testing and confidence intervals.

## Machine Learning
### Supervised Learning
Cover regression and classification algorithms.
"""

chunks = hierarchical_chunker.chunk_markdown(markdown_text)
for chunk in chunks:
    print(f"Hierarchy: {chunk['hierarchy']}")
    print(f"Content: {chunk['content'][:50]}...")
    print()</code></pre>

        <div class="note-box">
            <strong>Choosing Chunking Strategies:</strong>
            <ul>
                <li><strong>Fixed-Size</strong>: Simple and fast, applicable to general documents</li>
                <li><strong>Semantic</strong>: Use when semantic consistency is important</li>
                <li><strong>Hierarchical</strong>: Optimal for structured documents (technical documentation, manuals)</li>
            </ul>
        </div>

        <h2>Summary</h2>
        <ul>
            <li>RAG is a powerful LLM augmentation technique combining retrieval and generation</li>
            <li>Proper loaders and metadata management are crucial in document processing</li>
            <li>Choose chunking strategies based on use cases</li>
            <li>Understand three approaches: fixed-size, semantic, and hierarchical</li>
        </ul>

        <div class="nav-buttons">
            <a href="./index.html" class="nav-button">‚Üê Series Index</a>
            <a href="./chapter2-embeddings.html" class="nav-button">Chapter 2 ‚Üí</a>
        </div>
    </main>


        <div class="feedback-notice">
            <h3>‚ö†Ô∏è Help Us Improve Content Quality</h3>
            <p>This content was created with AI assistance. If you find errors or areas for improvement, please report them through one of the following methods:</p>
            <div class="feedback-options">
                <a href="https://forms.gle/9GfVBa2Qa7Uy9taQA" target="_blank" class="feedback-button">
                    üìù Correction Request Form
                </a>
                <a href="mailto:yusuke.hashimoto.d8@tohoku.ac.jp" class="feedback-button">
                    ‚úâÔ∏è Contact by Email
                </a>
            </div>
        </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
