<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Computer Vision Introduction Series - Complete Practical Guide from Image Processing to Object Detection and Segmentation">
    <title>Computer Vision Introduction Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Computer Vision</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>üì∑ Computer Vision Introduction Series v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">From Image Processing to Object Detection and Segmentation</p>
            <div class="meta">
                <span>üìñ Total Learning Time: 6-7 hours</span>
                <span>üìä Level: Beginner to Intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Learn comprehensively from basic image processing with OpenCV to object detection using deep learning, semantic segmentation, and image generation</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is a practical educational content consisting of five chapters that allows you to learn the theory and implementation of Computer Vision progressively from the basics.</p>

        <p><strong>Computer Vision</strong> is a technology that enables computers to extract and understand meaningful information from images and videos. Computer vision techniques are diverse, ranging from classical image processing techniques such as image filtering and edge detection, to image classification using CNNs, object detection with YOLO and Faster R-CNN, semantic segmentation using U-Net and Mask R-CNN, and even image generation with GANs and Diffusion Models. They are utilized across all industries, including autonomous driving, medical image diagnosis, manufacturing quality inspection, facial recognition systems, and AR/VR applications. You will understand and be able to implement image recognition technologies being commercialized by companies like Google, Tesla, Amazon, and Meta. We provide practical knowledge using major libraries such as OpenCV, PyTorch, and TensorFlow.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>‚úÖ <strong>From Theory to Practice</strong>: Systematic learning from image processing fundamentals to the latest deep learning techniques</li>
            <li>‚úÖ <strong>Implementation-Focused</strong>: Over 50 executable Python/OpenCV/PyTorch code examples</li>
            <li>‚úÖ <strong>Industry-Oriented</strong>: Practical projects designed for real-world applications</li>
            <li>‚úÖ <strong>Latest Technology Standards</strong>: Implementation using YOLO, U-Net, Mask R-CNN, and Transformers</li>
            <li>‚úÖ <strong>Practical Applications</strong>: Hands-on practice in object detection, segmentation, pose estimation, and image generation</li>
        </ul>

        <p><strong>Total Learning Time</strong>: 6-7 hours (including code execution and exercises)</p>

        <h2 id="learning">How to Study</h2>

        <h3>Recommended Learning Sequence</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: Image Processing Basics] --> B[Chapter 2: Image Classification]
    B --> C[Chapter 3: Object Detection]
    C --> D[Chapter 4: Segmentation]
    D --> E[Chapter 5: Advanced Applications]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>For Beginners (completely new to computer vision):</strong><br>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (all chapters recommended)<br>
        - Duration: 6-7 hours</p>

        <p><strong>For Intermediate Learners (with machine learning experience):</strong><br>
        - Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br>
        - Duration: 5-6 hours</p>

        <p><strong>For Specific Topic Reinforcement:</strong><br>
        - Image Processing Basics & OpenCV: Chapter 1 (intensive study)<br>
        - CNN & Image Classification: Chapter 2 (intensive study)<br>
        - Object Detection: Chapter 3 (intensive study)<br>
        - Segmentation: Chapter 4 (intensive study)<br>
        - Advanced Applications: Chapter 5 (intensive study)<br>
        - Duration: 70-90 minutes/chapter</p>

        <h2 id="chapters">Chapter Details</h2>

        <h3><a href="./chapter1-image-processing-basics.html">Chapter 1: Image Processing Basics</a></h3>
        <p><strong>Difficulty</strong>: Beginner<br>
        <strong>Reading Time</strong>: 70-80 minutes<br>
        <strong>Code Examples</strong>: 12</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Image Fundamentals</strong> - Pixels, color spaces (RGB, HSV, grayscale), image formats</li>
            <li><strong>OpenCV Introduction</strong> - Image reading, saving, displaying, basic operations</li>
            <li><strong>Filtering</strong> - Blurring, sharpening, noise reduction</li>
            <li><strong>Edge Detection</strong> - Sobel, Canny, Laplacian</li>
            <li><strong>Feature Extraction</strong> - SIFT, SURF, ORB, Harris Corner</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand basic image structure and color spaces</li>
            <li>‚úÖ Manipulate images with OpenCV</li>
            <li>‚úÖ Apply various filters</li>
            <li>‚úÖ Use edge detection algorithms</li>
            <li>‚úÖ Extract features from images</li>
        </ul>

        <p><strong><a href="./chapter1-image-processing-basics.html">Read Chapter 1 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter2-image-classification.html">Chapter 2: Image Classification</a></h3>
        <p><strong>Difficulty</strong>: Beginner to Intermediate<br>
        <strong>Reading Time</strong>: 80-90 minutes<br>
        <strong>Code Examples</strong>: 11</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>CNN (Convolutional Neural Networks)</strong> - Convolutional layers, pooling layers, fully connected layers</li>
            <li><strong>Representative CNN Architectures</strong> - LeNet, AlexNet, VGG, ResNet, EfficientNet</li>
            <li><strong>Transfer Learning</strong> - Leveraging pre-trained models, Fine-tuning</li>
            <li><strong>Data Augmentation</strong> - Rotation, flipping, cropping, color adjustment</li>
            <li><strong>Practical Projects</strong> - Image classification on CIFAR-10 and ImageNet</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand CNN mechanisms</li>
            <li>‚úÖ Explain representative CNN architectures</li>
            <li>‚úÖ Implement Transfer Learning</li>
            <li>‚úÖ Apply Data Augmentation</li>
            <li>‚úÖ Build and evaluate image classification models</li>
        </ul>

        <p><strong><a href="./chapter2-image-classification.html">Read Chapter 2 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter3-object-detection.html">Chapter 3: Object Detection</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 80-90 minutes<br>
        <strong>Code Examples</strong>: 10</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Object Detection Fundamentals</strong> - Bounding Box, IoU, NMS, mAP evaluation metrics</li>
            <li><strong>Two-Stage Detectors</strong> - R-CNN, Fast R-CNN, Faster R-CNN</li>
            <li><strong>One-Stage Detectors</strong> - YOLO (v3, v5, v8), SSD, RetinaNet</li>
            <li><strong>Anchor-Free Detectors</strong> - FCOS, CenterNet, EfficientDet</li>
            <li><strong>Practical Projects</strong> - Object detection on COCO and Pascal VOC</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand basic object detection concepts</li>
            <li>‚úÖ Explain differences between Two-Stage and One-Stage detectors</li>
            <li>‚úÖ Implement object detection with YOLO</li>
            <li>‚úÖ Evaluate detection results (mAP calculation)</li>
            <li>‚úÖ Train detectors on custom datasets</li>
        </ul>

        <p><strong><a href="./chapter3-object-detection.html">Read Chapter 3 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter4-segmentation.html">Chapter 4: Segmentation</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 70-80 minutes<br>
        <strong>Code Examples</strong>: 9</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Types of Segmentation</strong> - Semantic, Instance, Panoptic Segmentation</li>
            <li><strong>U-Net</strong> - Encoder-decoder structure, Skip Connections</li>
            <li><strong>Mask R-CNN</strong> - Instance Segmentation implementation</li>
            <li><strong>DeepLab</strong> - Atrous Convolution, ASPP, semantic segmentation</li>
            <li><strong>Practical Projects</strong> - Medical image segmentation, autonomous driving scene understanding</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand types of segmentation</li>
            <li>‚úÖ Explain U-Net mechanisms</li>
            <li>‚úÖ Implement Mask R-CNN</li>
            <li>‚úÖ Evaluate segmentation results (IoU, Dice coefficient)</li>
            <li>‚úÖ Train segmentation models on custom datasets</li>
        </ul>

        <p><strong><a href="./chapter4-segmentation.html">Read Chapter 4 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter5-advanced-applications.html">Chapter 5: Advanced Applications</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 80-90 minutes<br>
        <strong>Code Examples</strong>: 10</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Pose Estimation</strong> - OpenPose, MediaPipe, keypoint detection</li>
            <li><strong>Face Recognition</strong> - Face detection, facial landmarks, face authentication (FaceNet, ArcFace)</li>
            <li><strong>Image Generation</strong> - GAN, VAE, Diffusion Models, StyleGAN</li>
            <li><strong>OCR (Optical Character Recognition)</strong> - CRNN, Tesseract, EasyOCR, TrOCR</li>
            <li><strong>Vision Transformer</strong> - ViT, DINO, CLIP, multimodal learning</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Implement pose estimation</li>
            <li>‚úÖ Build face recognition systems</li>
            <li>‚úÖ Use image generation models</li>
            <li>‚úÖ Implement OCR systems</li>
            <li>‚úÖ Understand Vision Transformer mechanisms</li>
        </ul>

        <p><strong><a href="./chapter5-advanced-applications.html">Read Chapter 5 ‚Üí</a></strong></p>

        <hr>

        <h2 id="outcomes">Overall Learning Outcomes</h2>

        <p>Upon completing this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>‚úÖ Explain image processing fundamentals and OpenCV usage</li>
            <li>‚úÖ Understand mechanisms of CNN, object detection, and segmentation</li>
            <li>‚úÖ Explain roles and use cases of YOLO, U-Net, and Mask R-CNN</li>
            <li>‚úÖ Understand technologies for pose estimation, face recognition, and image generation</li>
            <li>‚úÖ Explain current trends in computer vision technologies</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>‚úÖ Implement image processing with OpenCV</li>
            <li>‚úÖ Build image classification models with CNN</li>
            <li>‚úÖ Create object detection systems with YOLO</li>
            <li>‚úÖ Implement segmentation with U-Net and Mask R-CNN</li>
            <li>‚úÖ Develop pose estimation, face recognition, and OCR systems</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>‚úÖ Select appropriate computer vision techniques for projects</li>
            <li>‚úÖ Train custom models on custom datasets</li>
            <li>‚úÖ Properly evaluate model performance</li>
            <li>‚úÖ Design and implement computer vision systems</li>
            <li>‚úÖ Apply computer vision to real-world problems</li>
        </ul>

        <hr>

        <h2 id="prerequisites">Prerequisites</h2>

        <p>To effectively learn this series, it is desirable to have the following knowledge:</p>

        <h3>Required (Must Have)</h3>
        <ul>
            <li>‚úÖ <strong>Python Basics</strong>: Variables, functions, classes, modules</li>
            <li>‚úÖ <strong>NumPy Basics</strong>: Array manipulation, vector and matrix operations</li>
            <li>‚úÖ <strong>Machine Learning Fundamentals</strong>: Concepts of training, validation, and testing</li>
            <li>‚úÖ <strong>Linear Algebra Basics</strong>: Vectors, matrices, matrix multiplication</li>
            <li>‚úÖ <strong>PyTorch/TensorFlow Basics</strong>: Tensor operations, model building (recommended)</li>
        </ul>

        <h3>Recommended (Nice to Have)</h3>
        <ul>
            <li>üí° <strong>Deep Learning Fundamentals</strong>: Neural networks, gradient descent</li>
            <li>üí° <strong>Image Processing Experience</strong>: Experience using PIL and OpenCV</li>
            <li>üí° <strong>Calculus Basics</strong>: Partial derivatives, gradients (for deep learning)</li>
            <li>üí° <strong>Statistics Basics</strong>: Probability distributions, expected values (for evaluation metrics)</li>
            <li>üí° <strong>GPU Environment</strong>: CUDA, experience with GPU training</li>
        </ul>

        <p><strong>Recommended Prior Learning</strong>:</p>
        <ul>
            <li>üìö <a href="../machine-learning-basics/">Machine Learning Introduction Series</a> - ML fundamentals</li>
            <li>üìö <a href="../deep-learning-fundamentals/">Deep Learning Fundamentals</a> - Neural networks, PyTorch</li>
            <li>üìö <a href="../python-for-data-science/">Python for Data Science</a> - NumPy, pandas, matplotlib</li>
            <li>üìö <a href="../linear-algebra-for-ml/">Linear Algebra for Machine Learning</a> - Vector and matrix operations</li>
        </ul>

        <hr>

        <h2 id="tech">Technologies and Tools Used</h2>

        <h3>Main Libraries</h3>
        <ul>
            <li><strong>OpenCV 4.8+</strong> - Image processing, computer vision</li>
            <li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
            <li><strong>torchvision 0.15+</strong> - Image datasets, models, transformations</li>
            <li><strong>NumPy 1.24+</strong> - Numerical computation</li>
            <li><strong>Matplotlib 3.7+</strong> - Visualization</li>
            <li><strong>Pillow 10.0+</strong> - Image processing</li>
            <li><strong>albumentations 1.3+</strong> - Data Augmentation</li>
        </ul>

        <h3>Specialized Libraries</h3>
        <ul>
            <li><strong>Ultralytics YOLOv8</strong> - Object detection</li>
            <li><strong>MMDetection</strong> - Object detection framework</li>
            <li><strong>Detectron2</strong> - Facebook AI's detection and segmentation library</li>
            <li><strong>MediaPipe</strong> - Pose estimation, face recognition</li>
            <li><strong>EasyOCR</strong> - Optical character recognition</li>
            <li><strong>timm</strong> - PyTorch Image Models</li>
        </ul>

        <h3>Development Environment</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - Programming language</li>
            <li><strong>Jupyter Notebook</strong> - Interactive development environment</li>
            <li><strong>Google Colab</strong> - Cloud GPU environment (recommended)</li>
            <li><strong>CUDA 11.8+</strong> - GPU acceleration (recommended)</li>
            <li><strong>cuDNN 8.6+</strong> - Deep learning GPU optimization</li>
        </ul>

        <h3>Datasets</h3>
        <ul>
            <li><strong>ImageNet</strong> - Large-scale image classification dataset</li>
            <li><strong>COCO</strong> - Object detection and segmentation dataset</li>
            <li><strong>CIFAR-10/100</strong> - Small-scale image classification dataset</li>
            <li><strong>Pascal VOC</strong> - Object detection dataset</li>
            <li><strong>Cityscapes</strong> - Autonomous driving segmentation dataset</li>
        </ul>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Are you ready? Start with Chapter 1 and master computer vision technologies!</p>

        <p><strong><a href="./chapter1-image-processing-basics.html">Chapter 1: Image Processing Basics ‚Üí</a></strong></p>

        <hr>

        <h2 id="next">Next Steps</h2>

        <p>After completing this series, we recommend proceeding to the following topics:</p>

        <h3>Advanced Study</h3>
        <ul>
            <li>üìö <strong>3D Computer Vision</strong>: Stereo vision, 3D reconstruction, SLAM</li>
            <li>üìö <strong>Video Analysis</strong>: Action recognition, object tracking, temporal analysis</li>
            <li>üìö <strong>Multimodal Learning</strong>: CLIP, ALIGN, integration of images and text</li>
            <li>üìö <strong>Edge Device Deployment</strong>: TensorRT, ONNX, mobile optimization</li>
        </ul>

        <h3>Related Series</h3>
        <ul>
            <li>üéØ <a href="../deep-learning-advanced/">Advanced Deep Learning</a> - Transformer, Attention, latest architectures</li>
            <li>üéØ <a href="../autonomous-driving-cv/">Computer Vision for Autonomous Driving</a> - Sensor fusion, scene understanding</li>
            <li>üéØ <a href="../medical-image-analysis/">Medical Image Analysis</a> - CT, MRI, lesion detection</li>
        </ul>

        <h3>Practical Projects</h3>
        <ul>
            <li>üöÄ Real-time Object Detection System - Detection application using webcam</li>
            <li>üöÄ Face Recognition System Development - Implementation from detection to authentication</li>
            <li>üöÄ Medical Image Segmentation - Lung and tumor segmentation</li>
            <li>üöÄ Autonomous Driving Simulator - Lane detection, vehicle detection, scene understanding</li>
        </ul>

        <hr>

        <p><strong>Version History</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 First edition released</li>
        </ul>

        <hr>

        <p><strong>Your computer vision journey begins here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
