<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 1: Fundamentals of Image Processing - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Fundamentals of Image Processing - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/computer-vision-introduction/index.html">Computer Vision</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/computer-vision-introduction/chapter1-image-processing-basics.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Fundamentals of Image Processing</h1>
<p class="subtitle">First Steps in Computer Vision - Understanding Digital Image Representation and Basic Operations</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 10</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Fundamentals of Image Processing, which image representation. You will learn digital image representation methods (pixels, basic image processing operations such as resizing, and filtering techniques including smoothing.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand digital image representation methods (pixels, channels)</li>
<li>‚úÖ Explain differences and applications of color spaces such as RGB, HSV, and LAB</li>
<li>‚úÖ Read, save, and display images using OpenCV and PIL</li>
<li>‚úÖ Implement basic image processing operations such as resizing, rotation, and color conversion</li>
<li>‚úÖ Apply filtering techniques including smoothing and edge detection</li>
<li>‚úÖ Build image data preprocessing pipelines for machine learning</li>
</ul>
<hr/>
<h2>1.1 Image Representation</h2>
<h3>Digital Images and Pixels</h3>
<p><strong>Digital images</strong> are represented as a collection of discrete points (pixels). Each pixel holds color information (intensity values).</p>
<blockquote>
<p>"Images are treated as two-dimensional arrays of pixels, forming a data structure that enables numerical computation."</p>
</blockquote>
<h4>Basic Image Structure</h4>
<ul>
<li><strong>Height</strong>: Number of pixels in the vertical direction</li>
<li><strong>Width</strong>: Number of pixels in the horizontal direction</li>
<li><strong>Channels</strong>: Number of color components (Grayscale=1, RGB=3)</li>
</ul>
<p>Image shape is typically represented in one of the following formats:</p>
<table>
<thead>
<tr>
<th>Format</th>
<th>Dimension Order</th>
<th>Used by Libraries</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>HWC</strong></td>
<td>(Height, Width, Channels)</td>
<td>OpenCV, PIL, matplotlib</td>
</tr>
<tr>
<td><strong>CHW</strong></td>
<td>(Channels, Height, Width)</td>
<td>PyTorch, Caffe</td>
</tr>
<tr>
<td><strong>NHWC</strong></td>
<td>(Batch, Height, Width, Channels)</td>
<td>TensorFlow, Keras</td>
</tr>
<tr>
<td><strong>NCHW</strong></td>
<td>(Batch, Channels, Height, Width)</td>
<td>PyTorch</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Image shape is typically represented in one of the following

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# Create a grayscale image (height 5, width 5)
gray_image = np.array([
    [0, 50, 100, 150, 200],
    [50, 100, 150, 200, 250],
    [100, 150, 200, 250, 255],
    [150, 200, 250, 255, 200],
    [200, 250, 255, 200, 150]
], dtype=np.uint8)

# Create an RGB image (height 3, width 3, channels 3)
rgb_image = np.zeros((3, 3, 3), dtype=np.uint8)
rgb_image[0, 0] = [255, 0, 0]      # Red
rgb_image[0, 1] = [0, 255, 0]      # Green
rgb_image[0, 2] = [0, 0, 255]      # Blue
rgb_image[1, 1] = [255, 255, 0]    # Yellow
rgb_image[2, 2] = [255, 255, 255]  # White

print("=== Grayscale Image ===")
print(f"Shape: {gray_image.shape}")
print(f"Data type: {gray_image.dtype}")
print(f"Min value: {gray_image.min()}, Max value: {gray_image.max()}")
print(f"\nImage data:\n{gray_image}")

print("\n=== RGB Image ===")
print(f"Shape: {rgb_image.shape} (Height, Width, Channels)")
print(f"Top-left pixel value (R,G,B): {rgb_image[0, 0]}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(10, 4))
axes[0].imshow(gray_image, cmap='gray')
axes[0].set_title('Grayscale Image (5√ó5)')
axes[0].axis('off')

axes[1].imshow(rgb_image)
axes[1].set_title('RGB Image (3√ó3)')
axes[1].axis('off')

plt.tight_layout()
print("\nImages visualized")
</code></pre>
<h3>Color Spaces</h3>
<p>Color spaces are methods for representing colors numerically. Selecting an appropriate color space based on the application is important.</p>
<h4>RGB (Red, Green, Blue)</h4>
<p>The most common color space, based on additive color mixing (primary colors of light).</p>
<ul>
<li>Value range per channel: 0-255 (8-bit integer) or 0.0-1.0 (floating-point)</li>
<li>Applications: Digital cameras, displays, image storage</li>
<li>Characteristics: Intuitive but does not align with human color perception</li>
</ul>
<h4>HSV (Hue, Saturation, Value)</h4>
<p>Represents colors using hue, saturation, and value (brightness).</p>
<ul>
<li><strong>Hue</strong>: 0-179 degrees (0-180 in OpenCV), type of color</li>
<li><strong>Saturation</strong>: 0-255, vividness of color</li>
<li><strong>Value</strong>: 0-255, brightness of color</li>
<li>Applications: Color-based object detection, image segmentation</li>
</ul>
<h4>LAB (L*a*b*)</h4>
<p>A color space closer to human vision.</p>
<ul>
<li><strong>L</strong>: Lightness (0-100)</li>
<li><strong>a</strong>: Green-red axis (-128 to 127)</li>
<li><strong>b</strong>: Blue-yellow axis (-128 to 127)</li>
<li>Applications: Color difference calculation, image correction</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: A color space closer to human vision.

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create sample image (red, green, blue blocks)
sample = np.zeros((100, 300, 3), dtype=np.uint8)
sample[:, 0:100] = [255, 0, 0]     # Red
sample[:, 100:200] = [0, 255, 0]   # Green
sample[:, 200:300] = [0, 0, 255]   # Blue

# Convert from BGR to RGB (OpenCV uses BGR order)
sample_rgb = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)

# Convert to various color spaces
sample_hsv = cv2.cvtColor(sample, cv2.COLOR_BGR2HSV)
sample_lab = cv2.cvtColor(sample, cv2.COLOR_BGR2LAB)
sample_gray = cv2.cvtColor(sample, cv2.COLOR_BGR2GRAY)

# Visualization
fig, axes = plt.subplots(2, 4, figsize=(16, 8))

# Original image (RGB)
axes[0, 0].imshow(sample_rgb)
axes[0, 0].set_title('Original (RGB)')
axes[0, 0].axis('off')

# HSV (display each channel separately)
axes[0, 1].imshow(sample_hsv[:, :, 0], cmap='hsv')
axes[0, 1].set_title('HSV - Hue')
axes[0, 1].axis('off')

axes[0, 2].imshow(sample_hsv[:, :, 1], cmap='gray')
axes[0, 2].set_title('HSV - Saturation')
axes[0, 2].axis('off')

axes[0, 3].imshow(sample_hsv[:, :, 2], cmap='gray')
axes[0, 3].set_title('HSV - Value')
axes[0, 3].axis('off')

# LAB (display each channel separately)
axes[1, 0].imshow(sample_lab[:, :, 0], cmap='gray')
axes[1, 0].set_title('LAB - L (Lightness)')
axes[1, 0].axis('off')

axes[1, 1].imshow(sample_lab[:, :, 1], cmap='RdYlGn_r')
axes[1, 1].set_title('LAB - a (Green-Red)')
axes[1, 1].axis('off')

axes[1, 2].imshow(sample_lab[:, :, 2], cmap='RdYlBu_r')
axes[1, 2].set_title('LAB - b (Blue-Yellow)')
axes[1, 2].axis('off')

# Grayscale
axes[1, 3].imshow(sample_gray, cmap='gray')
axes[1, 3].set_title('Grayscale')
axes[1, 3].axis('off')

plt.tight_layout()
print("Converted to various color spaces and visualized")

# Check numerical values
print(f"\nPixel values at center of red block (position [50, 50]):")
print(f"  RGB: {sample_rgb[50, 50]}")
print(f"  HSV: {sample_hsv[50, 50]}")
print(f"  LAB: {sample_lab[50, 50]}")
print(f"  Gray: {sample_gray[50, 50]}")
</code></pre>
<h3>Loading and Saving Images</h3>
<h4>Using OpenCV</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Using OpenCV

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 5-10 seconds
Dependencies: None
"""

import cv2
import numpy as np

# Load image (create dummy image if not exists)
try:
    image = cv2.imread('sample.jpg')
    if image is None:
        raise FileNotFoundError
except:
    # Create dummy image (gradient)
    image = np.zeros((480, 640, 3), dtype=np.uint8)
    for i in range(480):
        for j in range(640):
            image[i, j] = [i * 255 // 480, j * 255 // 640, 128]
    print("Created dummy image")

print(f"Image shape: {image.shape}")
print(f"Data type: {image.dtype}")

# Load as grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Save images
cv2.imwrite('output_color.jpg', image)
cv2.imwrite('output_gray.jpg', gray)

print("\nImages saved:")
print("  - output_color.jpg (color)")
print("  - output_gray.jpg (grayscale)")

# Display image information
print(f"\nColor image:")
print(f"  Shape: {image.shape}")
print(f"  Memory size: {image.nbytes / 1024:.2f} KB")

print(f"\nGrayscale image:")
print(f"  Shape: {gray.shape}")
print(f"  Memory size: {gray.nbytes / 1024:.2f} KB")
</code></pre>
<h4>Using PIL</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pillow&gt;=10.0.0

"""
Example: Using PIL

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 5-10 seconds
Dependencies: None
"""

from PIL import Image
import numpy as np

# Create PIL Image (gradient)
width, height = 640, 480
array = np.zeros((height, width, 3), dtype=np.uint8)
for i in range(height):
    for j in range(width):
        array[i, j] = [i * 255 // height, j * 255 // width, 128]

pil_image = Image.fromarray(array)

print(f"PIL image size: {pil_image.size}")  # (width, height)
print(f"PIL image mode: {pil_image.mode}")

# Convert to grayscale
pil_gray = pil_image.convert('L')

# Save
pil_image.save('output_pil_color.png')
pil_gray.save('output_pil_gray.png')

# PIL ‚Üí NumPy array
np_array = np.array(pil_image)
print(f"\nConverted to NumPy array:")
print(f"  Shape: {np_array.shape}")

# NumPy array ‚Üí PIL
pil_from_numpy = Image.fromarray(np_array)
print(f"\nConverted to PIL image:")
print(f"  Size: {pil_from_numpy.size}")

print("\nSaved images using PIL:")
print("  - output_pil_color.png")
print("  - output_pil_gray.png")
</code></pre>
<hr/>
<h2>1.2 Basic Image Processing</h2>
<h3>Resizing and Cropping</h3>
<p><strong>Resizing</strong> changes the size of an image. Quality varies depending on the interpolation method.</p>
<table>
<thead>
<tr>
<th>Interpolation Method</th>
<th>Characteristics</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>NEAREST</strong></td>
<td>Nearest neighbor, fast but low quality</td>
<td>Integer scaling</td>
</tr>
<tr>
<td><strong>LINEAR</strong></td>
<td>Linear interpolation, good balance</td>
<td>General downscaling</td>
</tr>
<tr>
<td><strong>CUBIC</strong></td>
<td>Cubic interpolation, high quality but slow</td>
<td>Upscaling</td>
</tr>
<tr>
<td><strong>LANCZOS</strong></td>
<td>Highest quality, slowest</td>
<td>When high quality is required</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Resizingchanges the size of an image. Quality varies dependi

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create sample image (checkerboard)
def create_checkerboard(size=200, square_size=20):
    image = np.zeros((size, size, 3), dtype=np.uint8)
    for i in range(0, size, square_size):
        for j in range(0, size, square_size):
            if (i // square_size + j // square_size) % 2 == 0:
                image[i:i+square_size, j:j+square_size] = [255, 255, 255]
    return image

original = create_checkerboard(200, 20)

# Resize with various interpolation methods
resized_nearest = cv2.resize(original, (400, 400), interpolation=cv2.INTER_NEAREST)
resized_linear = cv2.resize(original, (400, 400), interpolation=cv2.INTER_LINEAR)
resized_cubic = cv2.resize(original, (400, 400), interpolation=cv2.INTER_CUBIC)
resized_lanczos = cv2.resize(original, (400, 400), interpolation=cv2.INTER_LANCZOS4)

# Downscale
resized_small = cv2.resize(original, (100, 100), interpolation=cv2.INTER_AREA)

# Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

images = [
    (cv2.cvtColor(original, cv2.COLOR_BGR2RGB), "Original (200√ó200)"),
    (cv2.cvtColor(resized_nearest, cv2.COLOR_BGR2RGB), "NEAREST (400√ó400)"),
    (cv2.cvtColor(resized_linear, cv2.COLOR_BGR2RGB), "LINEAR (400√ó400)"),
    (cv2.cvtColor(resized_cubic, cv2.COLOR_BGR2RGB), "CUBIC (400√ó400)"),
    (cv2.cvtColor(resized_lanczos, cv2.COLOR_BGR2RGB), "LANCZOS (400√ó400)"),
    (cv2.cvtColor(resized_small, cv2.COLOR_BGR2RGB), "AREA (100√ó100 downscale)"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img)
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Compared resizing with various interpolation methods")

# Crop (trim)
print("\n=== Cropping Example ===")
height, width = original.shape[:2]
x, y, w, h = 50, 50, 100, 100  # (x, y, width, height)

cropped = original[y:y+h, x:x+w]
print(f"Original image: {original.shape}")
print(f"After cropping: {cropped.shape}")
print(f"Crop region: x={x}, y={y}, width={w}, height={h}")
</code></pre>
<h3>Rotation and Flipping</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Rotation and Flipping

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create test image (arrow pattern)
def create_arrow(size=200):
    image = np.ones((size, size, 3), dtype=np.uint8) * 255
    # Draw arrow
    pts = np.array([[100, 50], [150, 100], [125, 100], [125, 150],
                    [75, 150], [75, 100], [50, 100]], np.int32)
    cv2.fillPoly(image, [pts], (0, 0, 255))
    return image

original = create_arrow()

# Rotation (90, 180, 270 degrees)
rotated_90 = cv2.rotate(original, cv2.ROTATE_90_CLOCKWISE)
rotated_180 = cv2.rotate(original, cv2.ROTATE_180)
rotated_270 = cv2.rotate(original, cv2.ROTATE_90_COUNTERCLOCKWISE)

# Arbitrary angle rotation (45 degrees)
height, width = original.shape[:2]
center = (width // 2, height // 2)
rotation_matrix = cv2.getRotationMatrix2D(center, 45, 1.0)
rotated_45 = cv2.warpAffine(original, rotation_matrix, (width, height))

# Flipping
flipped_horizontal = cv2.flip(original, 1)  # Horizontal flip
flipped_vertical = cv2.flip(original, 0)    # Vertical flip
flipped_both = cv2.flip(original, -1)       # Both directions

# Visualization
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

images = [
    (cv2.cvtColor(original, cv2.COLOR_BGR2RGB), "Original"),
    (cv2.cvtColor(rotated_90, cv2.COLOR_BGR2RGB), "Rotated 90¬∞"),
    (cv2.cvtColor(rotated_180, cv2.COLOR_BGR2RGB), "Rotated 180¬∞"),
    (cv2.cvtColor(rotated_270, cv2.COLOR_BGR2RGB), "Rotated 270¬∞"),
    (cv2.cvtColor(rotated_45, cv2.COLOR_BGR2RGB), "Rotated 45¬∞"),
    (cv2.cvtColor(flipped_horizontal, cv2.COLOR_BGR2RGB), "Horizontal Flip"),
    (cv2.cvtColor(flipped_vertical, cv2.COLOR_BGR2RGB), "Vertical Flip"),
    (cv2.cvtColor(flipped_both, cv2.COLOR_BGR2RGB), "Both Directions Flip"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img)
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Visualized rotation and flipping operations")

# Rotation matrix details
print("\n=== 45-Degree Rotation Transform Matrix ===")
print(rotation_matrix)
</code></pre>
<h3>Color Conversion and Histograms</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Color Conversion and Histograms

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create sample image (gradient + noise)
def create_sample_image():
    image = np.zeros((300, 400, 3), dtype=np.uint8)
    for i in range(300):
        for j in range(400):
            image[i, j] = [
                int(i * 255 / 300),
                int(j * 255 / 400),
                128
            ]
    # Add noise
    noise = np.random.randint(-30, 30, image.shape, dtype=np.int16)
    image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)
    return image

image = create_sample_image()
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Calculate histograms
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Original image
axes[0, 0].imshow(image_rgb)
axes[0, 0].set_title('Original Image')
axes[0, 0].axis('off')

# RGB histogram
colors = ('r', 'g', 'b')
for i, color in enumerate(colors):
    hist = cv2.calcHist([image_rgb], [i], None, [256], [0, 256])
    axes[0, 1].plot(hist, color=color, label=f'{color.upper()} channel')
axes[0, 1].set_title('RGB Histogram')
axes[0, 1].set_xlabel('Pixel Value')
axes[0, 1].set_ylabel('Frequency')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Histogram equalization
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
equalized = cv2.equalizeHist(gray)

axes[1, 0].imshow(equalized, cmap='gray')
axes[1, 0].set_title('After Histogram Equalization')
axes[1, 0].axis('off')

# Compare histograms before and after equalization
hist_before = cv2.calcHist([gray], [0], None, [256], [0, 256])
hist_after = cv2.calcHist([equalized], [0], None, [256], [0, 256])
axes[1, 1].plot(hist_before, 'b-', label='Before Equalization', alpha=0.7)
axes[1, 1].plot(hist_after, 'r-', label='After Equalization', alpha=0.7)
axes[1, 1].set_title('Grayscale Histogram')
axes[1, 1].set_xlabel('Pixel Value')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
print("Visualized histograms and equalization")

# Statistical information
print("\n=== Image Statistics ===")
print(f"Mean value (before equalization): {gray.mean():.2f}")
print(f"Mean value (after equalization): {equalized.mean():.2f}")
print(f"Standard deviation (before equalization): {gray.std():.2f}")
print(f"Standard deviation (after equalization): {equalized.std():.2f}")
</code></pre>
<hr/>
<h2>1.3 Filtering</h2>
<h3>Smoothing Filters</h3>
<p><strong>Smoothing</strong> removes noise from images or creates blurring effects.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Smoothingremoves noise from images or creates blurring effec

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create image with noise
def create_noisy_image(size=200):
    # Clean image (circle)
    image = np.zeros((size, size), dtype=np.uint8)
    cv2.circle(image, (size//2, size//2), size//3, 255, -1)

    # Add Gaussian noise
    noise = np.random.normal(0, 25, image.shape)
    noisy = np.clip(image + noise, 0, 255).astype(np.uint8)

    # Add salt &amp; pepper noise
    salt_pepper = noisy.copy()
    salt = np.random.random(image.shape) &lt; 0.02
    pepper = np.random.random(image.shape) &lt; 0.02
    salt_pepper[salt] = 255
    salt_pepper[pepper] = 0

    return image, noisy, salt_pepper

clean, gaussian_noisy, sp_noisy = create_noisy_image()

# Apply various smoothing filters
# Mean Filter
mean_blur = cv2.blur(gaussian_noisy, (5, 5))

# Gaussian Filter
gaussian_blur = cv2.GaussianBlur(gaussian_noisy, (5, 5), 0)

# Median Filter - effective for salt &amp; pepper noise
median_blur = cv2.medianBlur(sp_noisy, 5)

# Bilateral Filter - smooths while preserving edges
bilateral = cv2.bilateralFilter(gaussian_noisy, 9, 75, 75)

# Visualization
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

images = [
    (clean, "Clean Image"),
    (gaussian_noisy, "Gaussian Noise"),
    (mean_blur, "Mean Filter"),
    (gaussian_blur, "Gaussian Filter"),
    (sp_noisy, "Salt &amp; Pepper Noise"),
    (median_blur, "Median Filter"),
    (bilateral, "Bilateral Filter"),
    (gaussian_noisy - gaussian_blur, "Noise Component (Difference)"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img, cmap='gray')
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Compared effects of various smoothing filters")

# Calculate PSNR (Peak Signal-to-Noise Ratio)
def calculate_psnr(original, filtered):
    mse = np.mean((original - filtered) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 255.0
    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
    return psnr

print("\n=== Noise Removal Performance (PSNR, higher is better) ===")
print(f"Noisy image: {calculate_psnr(clean, gaussian_noisy):.2f} dB")
print(f"Mean filter: {calculate_psnr(clean, mean_blur):.2f} dB")
print(f"Gaussian filter: {calculate_psnr(clean, gaussian_blur):.2f} dB")
print(f"Bilateral filter: {calculate_psnr(clean, bilateral):.2f} dB")
</code></pre>
<h3>Edge Detection</h3>
<p><strong>Edge detection</strong> identifies rapid changes in intensity within an image.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Edge detectionidentifies rapid changes in intensity within a

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create test image (multiple shapes)
def create_shapes_image(size=300):
    image = np.ones((size, size), dtype=np.uint8) * 200
    # Rectangle
    cv2.rectangle(image, (50, 50), (120, 120), 50, -1)
    # Circle
    cv2.circle(image, (220, 80), 40, 100, -1)
    # Triangle
    pts = np.array([[150, 200], [100, 280], [200, 280]], np.int32)
    cv2.fillPoly(image, [pts], 150)
    return image

image = create_shapes_image()

# Sobel filter (X direction, Y direction)
sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)
sobel_combined = np.uint8(sobel_combined / sobel_combined.max() * 255)

# Laplacian filter
laplacian = cv2.Laplacian(image, cv2.CV_64F)
laplacian = np.uint8(np.absolute(laplacian))

# Canny edge detection
canny = cv2.Canny(image, 50, 150)

# Scharr filter (more accurate than Sobel)
scharr_x = cv2.Scharr(image, cv2.CV_64F, 1, 0)
scharr_y = cv2.Scharr(image, cv2.CV_64F, 0, 1)
scharr_combined = np.sqrt(scharr_x**2 + scharr_y**2)
scharr_combined = np.uint8(scharr_combined / scharr_combined.max() * 255)

# Visualization
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

images = [
    (image, "Original Image"),
    (sobel_x, "Sobel X"),
    (sobel_y, "Sobel Y"),
    (sobel_combined, "Sobel Combined"),
    (laplacian, "Laplacian"),
    (canny, "Canny"),
    (scharr_combined, "Scharr Combined"),
    (image, "Original Image (Reference)"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img, cmap='gray')
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Compared various edge detection filters")

# Analyze edge strength
print("\n=== Edge Detection Result Statistics ===")
print(f"Sobel: Mean intensity = {sobel_combined.mean():.2f}")
print(f"Laplacian: Mean intensity = {laplacian.mean():.2f}")
print(f"Canny: Edge pixel count = {np.sum(canny &gt; 0)}")
print(f"Scharr: Mean intensity = {scharr_combined.mean():.2f}")
</code></pre>
<h3>Morphological Operations</h3>
<p><strong>Morphological operations</strong> are shape processing techniques for binary images.</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Effect</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dilation</strong></td>
<td>Expands white regions</td>
<td>Fill holes, connect broken parts</td>
</tr>
<tr>
<td><strong>Erosion</strong></td>
<td>Shrinks white regions</td>
<td>Remove noise, eliminate thin lines</td>
</tr>
<tr>
<td><strong>Opening</strong></td>
<td>Erosion ‚Üí Dilation</td>
<td>Remove small noise</td>
</tr>
<tr>
<td><strong>Closing</strong></td>
<td>Dilation ‚Üí Erosion</td>
<td>Fill holes and gaps</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Morphological operationsare shape processing techniques for 

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create noisy binary image
def create_noisy_binary_image(size=200):
    image = np.zeros((size, size), dtype=np.uint8)
    # Main shape (rectangle)
    cv2.rectangle(image, (50, 50), (150, 150), 255, -1)
    # Add small holes
    for _ in range(10):
        x, y = np.random.randint(60, 140, 2)
        cv2.circle(image, (x, y), 3, 0, -1)
    # Add small noise
    for _ in range(20):
        x, y = np.random.randint(0, size, 2)
        cv2.circle(image, (x, y), 2, 255, -1)
    return image

binary = create_noisy_binary_image()

# Create kernel (structuring element)
kernel = np.ones((5, 5), np.uint8)

# Morphological operations
erosion = cv2.erode(binary, kernel, iterations=1)
dilation = cv2.dilate(binary, kernel, iterations=1)
opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
gradient = cv2.morphologyEx(binary, cv2.MORPH_GRADIENT, kernel)
tophat = cv2.morphologyEx(binary, cv2.MORPH_TOPHAT, kernel)
blackhat = cv2.morphologyEx(binary, cv2.MORPH_BLACKHAT, kernel)

# Visualization
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

images = [
    (binary, "Original Image (with noise)"),
    (erosion, "Erosion"),
    (dilation, "Dilation"),
    (opening, "Opening"),
    (closing, "Closing"),
    (gradient, "Gradient"),
    (tophat, "Top Hat"),
    (blackhat, "Black Hat"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img, cmap='gray')
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Visualized morphological operations")

# Compare pixel counts
print("\n=== Changes in White Pixel Count ===")
print(f"Original image: {np.sum(binary == 255):,} pixels")
print(f"After erosion: {np.sum(erosion == 255):,} pixels ({np.sum(erosion == 255) / np.sum(binary == 255) * 100:.1f}%)")
print(f"After dilation: {np.sum(dilation == 255):,} pixels ({np.sum(dilation == 255) / np.sum(binary == 255) * 100:.1f}%)")
print(f"After opening: {np.sum(opening == 255):,} pixels")
print(f"After closing: {np.sum(closing == 255):,} pixels")
</code></pre>
<hr/>
<h2>1.4 Feature Extraction</h2>
<h3>Corner Detection</h3>
<p><strong>Corners</strong> are important feature points in images, used for object recognition and tracking.</p>
<h4>Harris Corner Detection</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Harris Corner Detection

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create checkerboard pattern
def create_checkerboard_complex(size=400):
    image = np.zeros((size, size), dtype=np.uint8)
    square_size = 40
    for i in range(0, size, square_size):
        for j in range(0, size, square_size):
            if (i // square_size + j // square_size) % 2 == 0:
                image[i:i+square_size, j:j+square_size] = 255
    # Additional shape
    cv2.circle(image, (300, 300), 50, 128, -1)
    return image

image = create_checkerboard_complex()

# Harris corner detection
harris = cv2.cornerHarris(image, blockSize=2, ksize=3, k=0.04)
harris = cv2.dilate(harris, None)  # Enhance results

# Apply threshold to detect corners
image_harris = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
image_harris[harris &gt; 0.01 * harris.max()] = [0, 0, 255]

# Shi-Tomasi corner detection (Good Features to Track)
corners = cv2.goodFeaturesToTrack(image, maxCorners=100, qualityLevel=0.01, minDistance=10)
image_shi = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
if corners is not None:
    for corner in corners:
        x, y = corner.ravel()
        cv2.circle(image_shi, (int(x), int(y)), 5, (0, 255, 0), -1)

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(image, cmap='gray')
axes[0].set_title('Original Image')
axes[0].axis('off')

axes[1].imshow(cv2.cvtColor(image_harris, cv2.COLOR_BGR2RGB))
axes[1].set_title(f'Harris Corner Detection')
axes[1].axis('off')

axes[2].imshow(cv2.cvtColor(image_shi, cv2.COLOR_BGR2RGB))
axes[2].set_title(f'Shi-Tomasi ({len(corners) if corners is not None else 0} corners)')
axes[2].axis('off')

plt.tight_layout()
print("Compared corner detection algorithms")

if corners is not None:
    print(f"\nNumber of corners detected: {len(corners)}")
    print(f"First 5 corner coordinates:")
    for i, corner in enumerate(corners[:5]):
        x, y = corner.ravel()
        print(f"  Corner{i+1}: ({x:.1f}, {y:.1f})")
</code></pre>
<h3>SIFT / ORB Features</h3>
<p><strong>SIFT (Scale-Invariant Feature Transform)</strong> is a feature descriptor invariant to scale and rotation.</p>
<blockquote>
<p>Note: In some versions of OpenCV, SIFT is included in opencv-contrib due to patent issues. Here we mainly use ORB (Oriented FAST and Rotated BRIEF).</p>
</blockquote>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Note: In some versions of OpenCV, SIFT is included in opencv

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Create distinctive image
def create_feature_image(size=400):
    image = np.ones((size, size), dtype=np.uint8) * 200
    # Multiple shapes
    cv2.rectangle(image, (50, 50), (150, 150), 50, -1)
    cv2.circle(image, (300, 100), 50, 100, -1)
    cv2.rectangle(image, (100, 250), (200, 350), 150, 3)
    pts = np.array([[250, 250], [350, 280], [320, 350]], np.int32)
    cv2.fillPoly(image, [pts], 80)
    return image

image = create_feature_image()

# Create ORB feature detector
orb = cv2.ORB_create(nfeatures=100)

# Detect keypoints and descriptors
keypoints, descriptors = orb.detectAndCompute(image, None)

# Draw keypoints
image_keypoints = cv2.drawKeypoints(
    image, keypoints, None,
    color=(0, 255, 0),
    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].imshow(image, cmap='gray')
axes[0].set_title('Original Image')
axes[0].axis('off')

axes[1].imshow(image_keypoints)
axes[1].set_title(f'ORB Feature Points ({len(keypoints)} points)')
axes[1].axis('off')

plt.tight_layout()
print(f"Detected ORB features: {len(keypoints)} keypoints")

# Feature details
print("\n=== ORB Feature Details ===")
print(f"Number of keypoints: {len(keypoints)}")
if descriptors is not None:
    print(f"Descriptor shape: {descriptors.shape}")
    print(f"  Each keypoint is described by a {descriptors.shape[1]}-dimensional vector")

# First 5 keypoint information
print("\nFirst 5 keypoints:")
for i, kp in enumerate(keypoints[:5]):
    print(f"  Point {i+1}: Position=({kp.pt[0]:.1f}, {kp.pt[1]:.1f}), "
          f"Size={kp.size:.1f}, Angle={kp.angle:.1f}¬∞")
</code></pre>
<h3>HOG (Histogram of Oriented Gradients)</h3>
<p><strong>HOG</strong> uses histograms of gradient orientations as features. Widely used for pedestrian detection and other applications.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: HOGuses histograms of gradient orientations as features. Wid

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

from skimage.feature import hog
from skimage import exposure
import numpy as np
import matplotlib.pyplot as plt

# Create simple person silhouette
def create_person_silhouette(size=128):
    image = np.zeros((size, size), dtype=np.uint8)
    # Head
    cv2.circle(image, (size//2, size//4), size//8, 255, -1)
    # Body
    cv2.rectangle(image, (size//2 - size//10, size//4 + size//10),
                  (size//2 + size//10, size//2 + size//6), 255, -1)
    # Arms
    cv2.line(image, (size//2 - size//10, size//4 + size//6),
             (size//2 - size//4, size//2), 255, size//20)
    cv2.line(image, (size//2 + size//10, size//4 + size//6),
             (size//2 + size//4, size//2), 255, size//20)
    # Legs
    cv2.line(image, (size//2 - size//20, size//2 + size//6),
             (size//2 - size//10, size - size//8), 255, size//20)
    cv2.line(image, (size//2 + size//20, size//2 + size//6),
             (size//2 + size//10, size - size//8), 255, size//20)
    return image

image = create_person_silhouette()

# Calculate HOG features
fd, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),
                    cells_per_block=(2, 2), visualize=True)

# Enhance HOG image contrast
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(image, cmap='gray')
axes[0].set_title('Original Image (Person Silhouette)')
axes[0].axis('off')

axes[1].imshow(hog_image_rescaled, cmap='gray')
axes[1].set_title('HOG Feature Visualization')
axes[1].axis('off')

# Display part of HOG feature vector
axes[2].bar(range(min(100, len(fd))), fd[:100])
axes[2].set_title('HOG Feature Vector (First 100 Dimensions)')
axes[2].set_xlabel('Dimension')
axes[2].set_ylabel('Value')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
print("Calculated HOG features")

print(f"\n=== HOG Feature Details ===")
print(f"Feature vector dimension: {len(fd)}")
print(f"Mean value: {fd.mean():.4f}")
print(f"Standard deviation: {fd.std():.4f}")
print(f"Max value: {fd.max():.4f}")
print(f"Min value: {fd.min():.4f}")
</code></pre>
<hr/>
<h2>1.5 Image Data Preprocessing</h2>
<h3>Normalization and Standardization</h3>
<p>In machine learning, proper scaling of image data is important.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Formula</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Min-Max Normalization</strong></td>
<td>$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$</td>
<td>Convert range to [0, 1]</td>
</tr>
<tr>
<td><strong>Standardization (Z-score)</strong></td>
<td>$x' = \frac{x - \mu}{\sigma}$</td>
<td>Convert to mean 0, variance 1</td>
</tr>
<tr>
<td><strong>ImageNet Normalization</strong></td>
<td>Standardize per channel</td>
<td>Using pretrained models</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: In machine learning, proper scaling of image data is importa

Purpose: Demonstrate data visualization techniques
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt

# Create sample image
np.random.seed(42)
image = np.random.randint(50, 200, (100, 100, 3), dtype=np.uint8)

print("=== Original Image Statistics ===")
print(f"Shape: {image.shape}")
print(f"Data type: {image.dtype}")
print(f"Value range: [{image.min()}, {image.max()}]")
print(f"Mean value: {image.mean():.2f}")
print(f"Standard deviation: {image.std():.2f}")

# Min-Max normalization [0, 1]
normalized = image.astype(np.float32) / 255.0

print("\n=== After Min-Max Normalization ===")
print(f"Data type: {normalized.dtype}")
print(f"Value range: [{normalized.min():.3f}, {normalized.max():.3f}]")
print(f"Mean value: {normalized.mean():.3f}")

# Standardization (Z-score)
mean = image.mean(axis=(0, 1), keepdims=True)
std = image.std(axis=(0, 1), keepdims=True)
standardized = (image.astype(np.float32) - mean) / (std + 1e-7)

print("\n=== After Standardization (Z-score) ===")
print(f"Mean value: {standardized.mean():.6f} (‚âà 0)")
print(f"Standard deviation: {standardized.std():.6f} (‚âà 1)")
print(f"Value range: [{standardized.min():.3f}, {standardized.max():.3f}]")

# ImageNet standardization (for pretrained models)
imagenet_mean = np.array([0.485, 0.456, 0.406])
imagenet_std = np.array([0.229, 0.224, 0.225])
imagenet_normalized = (normalized - imagenet_mean) / imagenet_std

print("\n=== After ImageNet Standardization ===")
print(f"R channel mean: {imagenet_normalized[:,:,0].mean():.3f}")
print(f"G channel mean: {imagenet_normalized[:,:,1].mean():.3f}")
print(f"B channel mean: {imagenet_normalized[:,:,2].mean():.3f}")

# Visualization
fig, axes = plt.subplots(1, 4, figsize=(16, 4))

axes[0].imshow(image)
axes[0].set_title('Original Image\n[50, 200]')
axes[0].axis('off')

axes[1].imshow(normalized)
axes[1].set_title('Min-Max Normalization\n[0, 1]')
axes[1].axis('off')

# Adjust standardized image for visualization
standardized_vis = (standardized - standardized.min()) / (standardized.max() - standardized.min())
axes[2].imshow(standardized_vis)
axes[2].set_title('Standardization\nMean‚âà0, Variance‚âà1')
axes[2].axis('off')

# Adjust ImageNet normalization
imagenet_vis = (imagenet_normalized - imagenet_normalized.min()) / \
               (imagenet_normalized.max() - imagenet_normalized.min())
axes[3].imshow(imagenet_vis)
axes[3].set_title('ImageNet Standardization')
axes[3].axis('off')

plt.tight_layout()
print("\nVisualized effects of normalization and standardization")
</code></pre>
<h3>Data Augmentation</h3>
<p><strong>Data augmentation</strong> generates diverse variations from limited training data to improve model generalization performance.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0
# - pillow&gt;=10.0.0

"""
Example: Data augmentationgenerates diverse variations from limited t

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageEnhance

# Create sample object image
def create_sample_object(size=128):
    image = np.ones((size, size, 3), dtype=np.uint8) * 255
    # Arrow-shaped object
    pts = np.array([[size//2, size//4], [3*size//4, size//2],
                    [5*size//8, size//2], [5*size//8, 3*size//4],
                    [3*size//8, 3*size//4], [3*size//8, size//2],
                    [size//4, size//2]], np.int32)
    cv2.fillPoly(image, [pts], (30, 144, 255))
    return image

original = create_sample_object()

# Apply various data augmentation techniques
# 1. Rotation
rotated = cv2.rotate(original, cv2.ROTATE_90_CLOCKWISE)

# 2. Horizontal flip
flipped = cv2.flip(original, 1)

# 3. Random crop &amp; resize
h, w = original.shape[:2]
crop_size = 96
x, y = np.random.randint(0, w - crop_size), np.random.randint(0, h - crop_size)
cropped = original[y:y+crop_size, x:x+crop_size]
cropped_resized = cv2.resize(cropped, (128, 128))

# 4. Brightness adjustment
pil_img = Image.fromarray(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
brightness = ImageEnhance.Brightness(pil_img).enhance(1.5)
brightness = cv2.cvtColor(np.array(brightness), cv2.COLOR_RGB2BGR)

# 5. Contrast adjustment
contrast = ImageEnhance.Contrast(pil_img).enhance(1.5)
contrast = cv2.cvtColor(np.array(contrast), cv2.COLOR_RGB2BGR)

# 6. Gaussian noise
noisy = original.copy().astype(np.float32)
noise = np.random.normal(0, 10, original.shape)
noisy = np.clip(noisy + noise, 0, 255).astype(np.uint8)

# 7. Blur
blurred = cv2.GaussianBlur(original, (5, 5), 0)

# 8. Hue shift (operate in HSV space)
hsv = cv2.cvtColor(original, cv2.COLOR_BGR2HSV).astype(np.float32)
hsv[:, :, 0] = (hsv[:, :, 0] + 30) % 180  # Shift hue
hue_shifted = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

# Visualization
fig, axes = plt.subplots(3, 3, figsize=(12, 12))
axes = axes.flatten()

images = [
    (cv2.cvtColor(original, cv2.COLOR_BGR2RGB), "Original Image"),
    (cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB), "Rotation (90¬∞)"),
    (cv2.cvtColor(flipped, cv2.COLOR_BGR2RGB), "Horizontal Flip"),
    (cv2.cvtColor(cropped_resized, cv2.COLOR_BGR2RGB), "Random Crop"),
    (cv2.cvtColor(brightness, cv2.COLOR_BGR2RGB), "Brightness Adjustment (1.5x)"),
    (cv2.cvtColor(contrast, cv2.COLOR_BGR2RGB), "Contrast Adjustment"),
    (cv2.cvtColor(noisy, cv2.COLOR_BGR2RGB), "Gaussian Noise"),
    (cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB), "Blur"),
    (cv2.cvtColor(hue_shifted, cv2.COLOR_BGR2RGB), "Hue Shift"),
]

for ax, (img, title) in zip(axes, images):
    ax.imshow(img)
    ax.set_title(title)
    ax.axis('off')

plt.tight_layout()
print("Visualized various data augmentation techniques")
print("\nBy combining these techniques, hundreds to thousands of variations can be generated from a single image")
</code></pre>
<h3>Preprocessing Pipeline with PyTorch Transforms</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pillow&gt;=10.0.0
# - torch&gt;=2.0.0, &lt;2.3.0
# - torchvision&gt;=0.15.0

"""
Example: Preprocessing Pipeline with PyTorch Transforms

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
from torchvision import transforms
from PIL import Image
import numpy as np

# Create sample image
sample_np = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
sample_pil = Image.fromarray(sample_np)

print("=== Preprocessing Pipeline with PyTorch Transforms ===\n")

# Training transformation pipeline
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Validation/test transformation pipeline
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Apply transformations
train_tensor = train_transform(sample_pil)
val_tensor = val_transform(sample_pil)

print("Training transformation:")
print(f"  Input: PIL Image {sample_pil.size}")
print(f"  Output: Tensor {train_tensor.shape}")
print(f"  Data type: {train_tensor.dtype}")
print(f"  Value range: [{train_tensor.min():.3f}, {train_tensor.max():.3f}]")

print("\nValidation transformation:")
print(f"  Input: PIL Image {sample_pil.size}")
print(f"  Output: Tensor {val_tensor.shape}")
print(f"  Value range: [{val_tensor.min():.3f}, {val_tensor.max():.3f}]")

# Simulate batch processing
batch_size = 4
batch_tensors = [train_transform(sample_pil) for _ in range(batch_size)]
batch = torch.stack(batch_tensors)

print(f"\nBatch processing:")
print(f"  Batch size: {batch_size}")
print(f"  Batch tensor shape: {batch.shape}")
print(f"  ‚Üí [Batch, Channels, Height, Width]")

# Individual transformation examples
print("\n=== Individual Transformation Details ===")

# ToTensor only
to_tensor = transforms.ToTensor()
tensor_only = to_tensor(sample_pil)
print(f"\n1. ToTensor:")
print(f"   PIL (H, W, C) ‚Üí Tensor (C, H, W)")
print(f"   Value range: [0, 255] ‚Üí [0.0, 1.0]")
print(f"   Shape change: {sample_pil.size} ‚Üí {tensor_only.shape}")

# Effect of Normalize
normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
normalized = normalize(tensor_only)
print(f"\n2. Normalize (mean=0.5, std=0.5):")
print(f"   Mean before transformation: {tensor_only.mean():.3f}")
print(f"   Mean after transformation: {normalized.mean():.3f}")
print(f"   Value range: [0.0, 1.0] ‚Üí [{normalized.min():.3f}, {normalized.max():.3f}]")

print("\nPreprocessing pipeline construction complete")
</code></pre>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the fundamentals of image processing.</p>
<h3>Key Points</h3>
<ul>
<li><strong>Digital images</strong> are represented as arrays of pixels, handled in HWC or CHW format</li>
<li><strong>Color spaces</strong> (RGB, HSV, LAB) should be selected based on application</li>
<li><strong>Basic operations</strong> (resizing, rotation, color conversion) are important for image analysis preprocessing</li>
<li><strong>Filtering</strong> (smoothing, edge detection) extracts image features</li>
<li><strong>Features</strong> (corners, SIFT, HOG) form the foundation of object recognition</li>
<li><strong>Preprocessing and data augmentation</strong> are essential for machine learning model performance improvement</li>
</ul>
<h3>Preview of Next Chapter</h3>
<p>Chapter 2 will cover the following topics:</p>
<ul>
<li>Fundamentals of Convolutional Neural Networks (CNN)</li>
<li>Mechanisms of convolutional and pooling layers</li>
<li>Representative CNN architectures (LeNet, AlexNet)</li>
<li>CNN implementation and image classification with PyTorch</li>
</ul>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Exercise 1: Understanding Color Spaces</strong></summary>
<p><strong>Problem</strong>: Convert an RGB image to HSV space and extract regions of a specific color (e.g., red).</p>
<p><strong>Hint</strong>:</p>
<ul>
<li>In HSV space, colors can be specified more easily by hue</li>
<li>Red hue range: 0-10 and 170-180 (OpenCV)</li>
</ul>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import cv2
import numpy as np

# Create RGB image (red, green, blue regions)
image = np.zeros((300, 300, 3), dtype=np.uint8)
image[:, 0:100] = [0, 0, 255]    # Red (BGR)
image[:, 100:200] = [0, 255, 0]  # Green
image[:, 200:300] = [255, 0, 0]  # Blue

# Convert to HSV
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# Define red color range
lower_red1 = np.array([0, 100, 100])
upper_red1 = np.array([10, 255, 255])
lower_red2 = np.array([170, 100, 100])
upper_red2 = np.array([180, 255, 255])

# Create mask
mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
red_mask = cv2.bitwise_or(mask1, mask2)

# Apply mask
red_only = cv2.bitwise_and(image, image, mask=red_mask)

print(f"Number of red region pixels: {np.sum(red_mask &gt; 0)}")
</code></pre>
</details>
<details>
<summary><strong>Exercise 2: Comparing Interpolation Methods</strong></summary>
<p><strong>Problem</strong>: When upscaling a 100√ó100 image to 500√ó500, compare the processing time and visual quality of four interpolation methods: NEAREST, LINEAR, CUBIC, and LANCZOS.</p>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: 5-10 seconds
Dependencies: None
"""

import cv2
import numpy as np
import time

# Test image
image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)

methods = {
    'NEAREST': cv2.INTER_NEAREST,
    'LINEAR': cv2.INTER_LINEAR,
    'CUBIC': cv2.INTER_CUBIC,
    'LANCZOS': cv2.INTER_LANCZOS4
}

print("Processing time comparison for interpolation methods:")
for name, method in methods.items():
    start = time.time()
    resized = cv2.resize(image, (500, 500), interpolation=method)
    elapsed = time.time() - start
    print(f"  {name:10s}: {elapsed*1000:.2f} ms")
</code></pre>
</details>
<details>
<summary><strong>Exercise 3: Implementing a Custom Filter</strong></summary>
<p><strong>Problem</strong>: Implement convolution operation manually using the following kernel.</p>
<pre><code>Sharpening kernel:
 0  -1   0
-1   5  -1
 0  -1   0
</code></pre>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

import numpy as np
import cv2

def custom_convolution(image, kernel):
    """Custom convolution function"""
    img_h, img_w = image.shape
    ker_h, ker_w = kernel.shape
    pad_h, pad_w = ker_h // 2, ker_w // 2

    # Padding
    padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')

    # Output image
    output = np.zeros_like(image)

    # Convolution
    for i in range(img_h):
        for j in range(img_w):
            region = padded[i:i+ker_h, j:j+ker_w]
            output[i, j] = np.sum(region * kernel)

    return np.clip(output, 0, 255).astype(np.uint8)

# Sharpening kernel
sharpen_kernel = np.array([[0, -1, 0],
                           [-1, 5, -1],
                           [0, -1, 0]])

# Test
test_image = np.random.randint(50, 200, (100, 100), dtype=np.uint8)
sharpened = custom_convolution(test_image, sharpen_kernel)

print("Implemented custom convolution")
print(f"Input: {test_image.shape}")
print(f"Output: {sharpened.shape}")
</code></pre>
</details>
<details>
<summary><strong>Exercise 4: Applying PyTorch Transforms</strong></summary>
<p><strong>Problem</strong>: Create a data augmentation pipeline that meets the following requirements:</p>
<ul>
<li>80% probability of horizontal flip</li>
<li>Randomly adjust brightness and contrast by ¬±20%</li>
<li>Random rotation of ¬±10 degrees</li>
<li>Finally resize to 224√ó224</li>
<li>Apply ImageNet standardization</li>
</ul>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torchvision&gt;=0.15.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

from torchvision import transforms

augmentation_pipeline = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.8),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomRotation(10),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

print("Created data augmentation pipeline")
print(f"Number of transformations: {len(augmentation_pipeline.transforms)}")
</code></pre>
</details>
<details>
<summary><strong>Exercise 5: Applying Edge Detection</strong></summary>
<p><strong>Problem</strong>: Use Canny edge detection to identify rectangular regions in an image and calculate their area.</p>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - opencv-python&gt;=4.8.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import cv2
import numpy as np

# Create image with rectangles
image = np.zeros((300, 400), dtype=np.uint8)
cv2.rectangle(image, (50, 50), (200, 150), 255, -1)
cv2.rectangle(image, (250, 100), (350, 250), 255, -1)

# Canny edge detection
edges = cv2.Canny(image, 50, 150)

# Find contours
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL,
                                cv2.CHAIN_APPROX_SIMPLE)

print(f"Number of contours detected: {len(contours)}")

for i, contour in enumerate(contours):
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)
    print(f"Contour{i+1}: Area={area:.0f}, Perimeter={perimeter:.2f}")
</code></pre>
</details>
<hr/>
<h2>References</h2>
<ul>
<li><a href="https://docs.opencv.org/">OpenCV Documentation</a> - Official documentation</li>
<li><a href="https://pillow.readthedocs.io/">Pillow (PIL) Documentation</a> - Python Imaging Library</li>
<li><a href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms</a> - PyTorch image transformations</li>
<li>Szeliski, R. (2010). <em>Computer Vision: Algorithms and Applications</em>. Springer.</li>
<li>Gonzalez, R. C., &amp; Woods, R. E. (2018). <em>Digital Image Processing</em> (4th ed.). Pearson.</li>
<li>Bradski, G., &amp; Kaehler, A. (2008). <em>Learning OpenCV</em>. O'Reilly Media.</li>
</ul>
<hr/>
<div class="navigation">
<a class="nav-button" href="index.html">üìö Return to Course Index</a>
<a class="nav-button" href="chapter2-image-classification.html">Next Chapter: Image Classification ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links or third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
