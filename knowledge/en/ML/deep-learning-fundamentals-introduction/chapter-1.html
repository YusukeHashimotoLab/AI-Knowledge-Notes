<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Basic Concepts of Deep Learning - Deep Learning Fundamentals Introduction - AI Terakoya</title>
    <meta name="description" content="Learn the definition, history, basic structure of neural networks, and activation functions of deep learning. Connect theory and practice through implementation examples in NumPy and PyTorch.">

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .chapter-description {
            margin: 1.5rem 0;
            padding: 1rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-left: 4px solid #7b2cbf;
            border-radius: 8px;
            font-size: 1.05rem;
            line-height: 1.8;
            color: #2d3748;
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        .nav-button.disabled {
            opacity: 0.5;
            cursor: not-allowed;
            background: #6c757d;
        }

        .nav-button.disabled:hover {
            transform: none;
            box-shadow: var(--box-shadow);
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }

        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        .success-box {
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: var(--spacing-md);
            margin: var(--spacing-md) 0;
            border-radius: var(--border-radius);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .container {
                padding: 0 var(--spacing-sm) var(--spacing-lg);
            }

            .navigation {
                flex-direction: column;
            }
        }
    </style>

    <!-- MathJax for mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
            }
        });
    </script>
</head>
<body>
    <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="./index.html">Deep Learning Fundamentals Introduction</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

    <header>
        <div class="header-content">
            <h1>Chapter 1: Basic Concepts of Deep Learning</h1>
            <p class="subtitle">Understanding Definition, History, and Basic Structure of Neural Networks</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 30-35 minutes</span>
                <span class="meta-item">üíª Code Examples: 8</span>
                <span class="meta-item">üìù Exercises: 5</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
            </div>
        </div>
    </header>

    <main class="container">
        <div class="chapter-description">
            In this chapter, we will learn about the definition and history of deep learning, the basic structure of neural networks, and activation functions. We will trace the development from perceptrons to modern deep learning and verify theory with implementation code.
        </div>

        <div class="learning-objectives">
            <h2>Learning Objectives</h2>
            <ul>
                <li>Understand the definition of deep learning and its relationship with machine learning</li>
                <li>Grasp the historical development of deep learning</li>
                <li>Understand the basic structure of neural networks (layers, neurons, weights)</li>
                <li>Learn the characteristics of major activation functions (Sigmoid, ReLU, Softmax)</li>
                <li>Be able to implement simple neural networks in NumPy and PyTorch</li>
            </ul>
        </div>

        <h2 id="definition">1. Definition of Deep Learning</h2>

        <h3>1.1 Relationship with Machine Learning</h3>

        <p><strong>Deep Learning</strong> is a branch of <strong>Machine Learning</strong> that uses multi-layered neural networks to automatically learn features from data.</p>

        <div class="mermaid">
graph TB
    A[Artificial Intelligence AI] --> B[Machine Learning ML]
    B --> C[Deep Learning DL]

    A --> A1[Rule-based Systems<br/>if-then rules]
    B --> B1[Traditional Machine Learning<br/>Decision Tree, SVM]
    C --> C1[Deep Neural Networks<br/>CNN, RNN, Transformer]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
        </div>

        <table>
            <thead>
                <tr>
                    <th>Item</th>
                    <th>Traditional Machine Learning</th>
                    <th>Deep Learning</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Feature Extraction</strong></td>
                    <td>Manually designed by humans</td>
                    <td>Automatically learned</td>
                </tr>
                <tr>
                    <td><strong>Model Structure</strong></td>
                    <td>Shallow (1-2 layers)</td>
                    <td>Deep (3+ layers to hundreds)</td>
                </tr>
                <tr>
                    <td><strong>Data Amount</strong></td>
                    <td>Effective with small to medium data</td>
                    <td>Shows true potential with large data</td>
                </tr>
                <tr>
                    <td><strong>Computing Resources</strong></td>
                    <td>CPU sufficient</td>
                    <td>GPU recommended (parallel computing)</td>
                </tr>
                <tr>
                    <td><strong>Accuracy</strong></td>
                    <td>Task dependent</td>
                    <td>High accuracy in images, audio, text</td>
                </tr>
            </tbody>
        </table>

        <h3>1.2 What Does "Deep" Mean</h3>

        <p>"Deep" refers to the <strong>number of layers</strong> in neural networks:</p>

        <ul>
            <li><strong>Shallow Network</strong>: 1-2 hidden layers</li>
            <li><strong>Deep Network</strong>: 3 or more hidden layers</li>
            <li><strong>Very Deep Network</strong>: Dozens to hundreds of layers (ResNet-152, GPT-3, etc.)</li>
        </ul>

        <div class="info-box">
            <p><strong>Important Concept</strong>: As networks become deeper, <strong>hierarchical feature learning</strong> becomes possible.</p>
            <ul>
                <li><strong>Lower Layers (initial layers)</strong>: Simple features (edges, colors, textures)</li>
                <li><strong>Middle Layers</strong>: Complex patterns (parts like eyes, nose, ears)</li>
                <li><strong>Higher Layers (deep layers)</strong>: Abstract concepts (entire face, entire object)</li>
            </ul>
        </div>

        <h3>1.3 Representation Learning and Feature Extraction</h3>

        <p>The essence of deep learning is <strong>representation learning</strong>:</p>

        <ul>
            <li><strong>Traditional Approach</strong>: Engineers manually design features (SIFT, HOG, etc.)</li>
            <li><strong>Deep Learning</strong>: Networks automatically learn optimal features from data</li>
        </ul>

        <p>For example, in image classification tasks:</p>
        <ul>
            <li>Traditional: Engineers design features like "edge detection", "color histogram"</li>
            <li>DL: Networks automatically discover useful features from training data</li>
        </ul>

        <h2 id="history">2. History of Deep Learning</h2>

        <h3>2.1 Period 1: Birth (1943-1969)</h3>

        <h4>Perceptron (1958)</h4>
        <p>The first neural network with a learning algorithm, invented by Frank Rosenblatt.</p>

        <ul>
            <li><strong>Single-layer structure</strong>: Input and output layers only</li>
            <li><strong>Linearly separable</strong>: Can only solve problems separable by a straight line</li>
            <li><strong>Limitation</strong>: Cannot solve XOR problem (Minsky and Papert's criticism, 1969)</li>
        </ul>

        <h3>2.2 Period 2: AI Winter (1969-1986)</h3>

        <p>Limitations of perceptrons became clear, reducing funding for AI research. However, important theoretical developments occurred:</p>

        <ul>
            <li><strong>Multi-Layer Perceptron (MLP)</strong>: Introduced hidden layers to handle non-linear problems</li>
            <li><strong>Problem</strong>: Effective learning algorithm not yet discovered</li>
        </ul>

        <h3>2.3 Period 3: Revival (1986-2006)</h3>

        <h4>Backpropagation (1986)</h4>
        <p>Rumelhart, Hinton, and Williams proposed an efficient learning method for multi-layer neural networks.</p>

        <ul>
            <li><strong>Chain Rule</strong> for gradient computation</li>
            <li><strong>Multi-layer networks</strong> became trainable</li>
            <li><strong>Limitation</strong>: Vanishing gradient problem in deep networks</li>
        </ul>

        <h3>2.4 Period 4: Dawn of Deep Learning (2006-2012)</h3>

        <p>Geoffrey Hinton and others proposed Deep Belief Networks (DBN):</p>

        <ul>
            <li><strong>Pre-training</strong>: Initialize weights with unsupervised learning</li>
            <li><strong>Fine-tuning</strong>: Adjust with supervised learning</li>
            <li><strong>Achievement</strong>: Training deep networks became practical</li>
        </ul>

        <h3>2.5 Period 5: Deep Learning Revolution (2012-Present)</h3>

        <h4>ImageNet 2012: AlexNet</h4>
        <p>AlexNet by Alex Krizhevsky et al. dominated image recognition competition:</p>

        <ul>
            <li><strong>5-layer Convolutional Neural Network (CNN)</strong></li>
            <li><strong>ReLU activation function</strong> adoption</li>
            <li><strong>GPU parallel computing</strong> utilization</li>
            <li><strong>Dropout</strong> for regularization</li>
            <li><strong>Achievement</strong>: Reduced error rate from 26% to 15%</li>
        </ul>

        <h4>Major Subsequent Milestones</h4>
        <ul>
            <li><strong>2014</strong>: Proposal of GAN (Generative Adversarial Networks)</li>
            <li><strong>2015</strong>: ResNet (152 layers) achieves human-level accuracy on ImageNet</li>
            <li><strong>2017</strong>: Proposal of Transformer (attention mechanism)</li>
            <li><strong>2018</strong>: BERT (revolution in natural language processing)</li>
            <li><strong>2020</strong>: GPT-3 (large language model with 175 billion parameters)</li>
            <li><strong>2022</strong>: ChatGPT, Stable Diffusion (practical generative AI)</li>
        </ul>

        <div class="mermaid">
timeline
    title History of Deep Learning
    1958 : Perceptron
    1969 : AI Winter Begins
    1986 : Backpropagation
    2006 : Dawn of Deep Learning
    2012 : AlexNet Revolution
    2017 : Transformer
    2022 : ChatGPT
        </div>

        <h2 id="structure">3. Basic Structure of Neural Networks</h2>

        <h3>3.1 Types of Layers</h3>

        <p>Neural networks consist of multiple <strong>layers</strong>:</p>

        <ul>
            <li><strong>Input Layer</strong>: Layer that receives data</li>
            <li><strong>Hidden Layer(s)</strong>: Layer(s) that transform and process data (1 or more)</li>
            <li><strong>Output Layer</strong>: Layer that outputs final prediction results</li>
        </ul>

        <div class="mermaid">
graph LR
    I1((Input 1)) --> H1((Hidden 1-1))
    I2((Input 2)) --> H1
    I3((Input 3)) --> H1

    I1 --> H2((Hidden 1-2))
    I2 --> H2
    I3 --> H2

    I1 --> H3((Hidden 1-3))
    I2 --> H3
    I3 --> H3

    H1 --> H4((Hidden 2-1))
    H2 --> H4
    H3 --> H4

    H1 --> H5((Hidden 2-2))
    H2 --> H5
    H3 --> H5

    H4 --> O1((Output 1))
    H5 --> O1
    H4 --> O2((Output 2))
    H5 --> O2

    style I1 fill:#e3f2fd
    style I2 fill:#e3f2fd
    style I3 fill:#e3f2fd
    style H1 fill:#fff3e0
    style H2 fill:#fff3e0
    style H3 fill:#fff3e0
    style H4 fill:#f3e5f5
    style H5 fill:#f3e5f5
    style O1 fill:#e8f5e9
    style O2 fill:#e8f5e9
        </div>

        <h3>3.2 Neurons and Activation</h3>

        <p><strong>Neurons</strong> in each layer perform the following processing:</p>

        <ol>
            <li><strong>Weighted sum calculation</strong>: input √ó weight + bias</li>
            <li><strong>Apply activation function</strong>: non-linear transformation</li>
        </ol>

        <p>Expressed as equations:</p>

        <p>$$z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \mathbf{w}^T \mathbf{x} + b$$</p>
        <p>$$a = f(z)$$</p>

        <p>Where:</p>
        <ul>
            <li>$\mathbf{x}$: input vector</li>
            <li>$\mathbf{w}$: weight vector</li>
            <li>$b$: bias (intercept)</li>
            <li>$z$: linear combination (weighted sum)</li>
            <li>$f$: activation function</li>
            <li>$a$: activation output</li>
        </ul>

        <h3>3.3 Weights and Biases</h3>

        <p><strong>Weights</strong> and <strong>biases</strong> are parameters that neural networks learn:</p>

        <ul>
            <li><strong>Weight $w$</strong>: Coefficient representing the importance of each input (optimized by learning)</li>
            <li><strong>Bias $b$</strong>: Constant term that adjusts the threshold of activation function</li>
        </ul>

        <div class="info-box">
            <p><strong>Intuitive Understanding</strong>: Analogy to linear regression $y = wx + b$</p>
            <ul>
                <li>$w$: slope (magnitude of influence input has on output)</li>
                <li>$b$: intercept (shift of output baseline)</li>
            </ul>
        </div>

        <h2 id="activation">4. Activation Functions</h2>

        <p>An <strong>activation function</strong> is a function that introduces <strong>non-linearity</strong> into neural networks. Without activation functions, no matter how deep the layers, it would only be a combination of linear transformations, unable to learn complex patterns.</p>

        <h3>4.1 Sigmoid Function</h3>

        <p>One of the most classical activation functions, compresses output to range from 0 to 1.</p>

        <p><strong>Equation</strong>:</p>
        <p>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</p>

        <p><strong>Characteristics</strong>:</p>
        <ul>
            <li>Output range: $(0, 1)$</li>
            <li>Differentiable and smooth</li>
            <li>Can be interpreted as probability (used in output layer for binary classification)</li>
        </ul>

        <p><strong>Problems</strong>:</p>
        <ul>
            <li><strong>Vanishing gradient problem</strong>: When $x$ is large or small, gradient becomes nearly 0</li>
            <li><strong>Output not centered</strong>: Always positive values (reduces learning efficiency)</li>
            <li><strong>Computational cost</strong>: Expensive exponential function calculation</li>
        </ul>

        <p><strong>NumPy Implementation</strong>:</p>

        <pre><code class="language-python">import numpy as np

def sigmoid(x):
    """
    Sigmoid activation function

    Parameters:
    -----------
    x : array-like
        Input

    Returns:
    --------
    array-like
        Output in range 0 to 1
    """
    return 1 / (1 + np.exp(-x))

# Usage example
x = np.array([-2, -1, 0, 1, 2])
print("Input:", x)
print("Sigmoid output:", sigmoid(x))
# Example output: [0.119, 0.269, 0.5, 0.731, 0.881]
</code></pre>

        <h3>4.2 ReLU (Rectified Linear Unit)</h3>

        <p>The most widely used activation function in modern deep learning.</p>

        <p><strong>Equation</strong>:</p>
        <p>$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$</p>

        <p><strong>Characteristics</strong>:</p>
        <ul>
            <li>Very fast computation (max operation only)</li>
            <li>Mitigates vanishing gradient problem (gradient is always 1 when $x > 0$)</li>
            <li>Sparse activation (about 50% of neurons become 0)</li>
        </ul>

        <p><strong>Problems</strong>:</p>
        <ul>
            <li><strong>Dying ReLU problem</strong>: Gradient becomes 0 for negative inputs, causing neurons to "die"</li>
        </ul>

        <p><strong>NumPy Implementation</strong>:</p>

        <pre><code class="language-python">def relu(x):
    """
    ReLU activation function

    Parameters:
    -----------
    x : array-like
        Input

    Returns:
    --------
    array-like
        Negative values become 0, positive values unchanged
    """
    return np.maximum(0, x)

# Usage example
x = np.array([-2, -1, 0, 1, 2])
print("Input:", x)
print("ReLU output:", relu(x))
# Example output: [0, 0, 0, 1, 2]
</code></pre>

        <h3>4.3 Softmax Function</h3>

        <p>Activation function used in output layer for multi-class classification, converts output to probability distribution.</p>

        <p><strong>Equation</strong>:</p>
        <p>$$\text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$</p>

        <p><strong>Characteristics</strong>:</p>
        <ul>
            <li>Sum of outputs equals 1 (probability distribution)</li>
            <li>All outputs in range 0 to 1</li>
            <li>Largest input value has largest probability</li>
        </ul>

        <p><strong>NumPy Implementation</strong>:</p>

        <pre><code class="language-python">def softmax(x):
    """
    Softmax activation function

    Parameters:
    -----------
    x : array-like
        Input (logits)

    Returns:
    --------
    array-like
        Probability distribution (sum = 1)
    """
    # Subtract max for numerical stability
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

# Usage example
x = np.array([2.0, 1.0, 0.1])
print("Input:", x)
print("Softmax output:", softmax(x))
print("Sum:", softmax(x).sum())
# Example output: [0.659, 0.242, 0.099]
# Sum: 1.0
</code></pre>

        <h3>4.4 tanh (Hyperbolic Tangent)</h3>

        <p>Improved version of Sigmoid, compresses output to range from -1 to 1.</p>

        <p><strong>Equation</strong>:</p>
        <p>$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}$$</p>

        <p><strong>Characteristics</strong>:</p>
        <ul>
            <li>Output range: $(-1, 1)$</li>
            <li>Output is centered (better learning efficiency than Sigmoid)</li>
            <li>Often used in RNN (Recurrent Neural Networks)</li>
        </ul>

        <p><strong>NumPy Implementation</strong>:</p>

        <pre><code class="language-python">def tanh(x):
    """
    tanh activation function

    Parameters:
    -----------
    x : array-like
        Input

    Returns:
    --------
    array-like
        Output in range -1 to 1
    """
    return np.tanh(x)

# Usage example
x = np.array([-2, -1, 0, 1, 2])
print("Input:", x)
print("tanh output:", tanh(x))
# Example output: [-0.964, -0.762, 0.0, 0.762, 0.964]
</code></pre>

        <h3>4.5 Comparison of Activation Functions</h3>

        <table>
            <thead>
                <tr>
                    <th>Function</th>
                    <th>Output Range</th>
                    <th>Computation Speed</th>
                    <th>Vanishing Gradient</th>
                    <th>Main Use</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Sigmoid</strong></td>
                    <td>(0, 1)</td>
                    <td>Slow</td>
                    <td>Yes</td>
                    <td>Binary classification output layer</td>
                </tr>
                <tr>
                    <td><strong>tanh</strong></td>
                    <td>(-1, 1)</td>
                    <td>Slow</td>
                    <td>Yes</td>
                    <td>RNN hidden layers</td>
                </tr>
                <tr>
                    <td><strong>ReLU</strong></td>
                    <td>[0, ‚àû)</td>
                    <td>Fast</td>
                    <td>No (in positive region)</td>
                    <td>CNN hidden layers (most common)</td>
                </tr>
                <tr>
                    <td><strong>Softmax</strong></td>
                    <td>(0, 1), sum=1</td>
                    <td>Medium</td>
                    <td>N/A</td>
                    <td>Multi-class classification output layer</td>
                </tr>
            </tbody>
        </table>

        <h2 id="implementation">5. Implementing Simple Neural Networks</h2>

        <h3>5.1 NumPy Implementation</h3>

        <p>First, we'll implement from basics using NumPy. This allows us to understand the internal workings of neural networks.</p>

        <pre><code class="language-python">import numpy as np

class SimpleNN:
    """
    Simple 3-layer neural network

    Structure: Input layer ‚Üí Hidden layer ‚Üí Output layer
    """

    def __init__(self, input_size, hidden_size, output_size):
        """
        Parameters:
        -----------
        input_size : int
            Number of neurons in input layer
        hidden_size : int
            Number of neurons in hidden layer
        output_size : int
            Number of neurons in output layer
        """
        # Initialize weights (small random values)
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))

        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

    def forward(self, X):
        """
        Forward Propagation

        Parameters:
        -----------
        X : array-like, shape (n_samples, input_size)
            Input data

        Returns:
        --------
        array-like, shape (n_samples, output_size)
            Output (probability distribution)
        """
        # Layer 1: Input ‚Üí Hidden layer
        self.z1 = np.dot(X, self.W1) + self.b1  # Linear transformation
        self.a1 = relu(self.z1)                 # ReLU activation

        # Layer 2: Hidden layer ‚Üí Output layer
        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Linear transformation
        self.a2 = softmax(self.z2)                     # Softmax activation

        return self.a2

# Usage example
# Input: 4 dimensions, Hidden layer: 10 neurons, Output: 3 classes
model = SimpleNN(input_size=4, hidden_size=10, output_size=3)

# Dummy data (5 samples, 4 dimensions)
X = np.array([
    [5.1, 3.5, 1.4, 0.2],
    [6.3, 2.9, 5.6, 1.8],
    [5.8, 2.7, 5.1, 1.9],
    [5.0, 3.4, 1.5, 0.2],
    [6.7, 3.1, 5.6, 2.4]
])

# Prediction
predictions = model.forward(X)
print("Prediction probabilities:\n", predictions)
print("\nPredicted classes:", np.argmax(predictions, axis=1))
</code></pre>

        <h3>5.2 PyTorch Implementation</h3>

        <p>Next, we'll implement the same network in PyTorch. PyTorch has built-in automatic differentiation and GPU support, allowing more concise code.</p>

        <pre><code class="language-python">import torch
import torch.nn as nn

class SimpleNNPyTorch(nn.Module):
    """
    3-layer neural network in PyTorch
    """

    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNNPyTorch, self).__init__()

        # Define layers
        self.fc1 = nn.Linear(input_size, hidden_size)  # Input ‚Üí Hidden layer
        self.relu = nn.ReLU()                          # ReLU activation
        self.fc2 = nn.Linear(hidden_size, output_size) # Hidden layer ‚Üí Output
        self.softmax = nn.Softmax(dim=1)               # Softmax activation

    def forward(self, x):
        """
        Forward propagation

        Parameters:
        -----------
        x : torch.Tensor, shape (n_samples, input_size)
            Input data

        Returns:
        --------
        torch.Tensor, shape (n_samples, output_size)
            Output (probability distribution)
        """
        x = self.fc1(x)      # Linear transformation
        x = self.relu(x)     # ReLU activation
        x = self.fc2(x)      # Linear transformation
        x = self.softmax(x)  # Softmax activation
        return x

# Usage example
model_pytorch = SimpleNNPyTorch(input_size=4, hidden_size=10, output_size=3)

# Convert dummy data to Tensor
X_torch = torch.tensor(X, dtype=torch.float32)

# Prediction
with torch.no_grad():  # Disable gradient calculation (for inference)
    predictions_pytorch = model_pytorch(X_torch)

print("PyTorch prediction probabilities:\n", predictions_pytorch.numpy())
print("\nPredicted classes:", torch.argmax(predictions_pytorch, dim=1).numpy())
</code></pre>

        <h3>5.3 NumPy Implementation vs PyTorch Implementation</h3>

        <table>
            <thead>
                <tr>
                    <th>Item</th>
                    <th>NumPy Implementation</th>
                    <th>PyTorch Implementation</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Code Amount</strong></td>
                    <td>More (manual implementation)</td>
                    <td>Less (high-level API)</td>
                </tr>
                <tr>
                    <td><strong>Learning Purpose</strong></td>
                    <td>Ideal for understanding internals</td>
                    <td>Ideal for practical development</td>
                </tr>
                <tr>
                    <td><strong>Automatic Differentiation</strong></td>
                    <td>Manual implementation required</td>
                    <td>Automatic computation</td>
                </tr>
                <tr>
                    <td><strong>GPU Support</strong></td>
                    <td>Difficult</td>
                    <td>Easy (just .cuda())</td>
                </tr>
                <tr>
                    <td><strong>Performance</strong></td>
                    <td>Slow</td>
                    <td>Fast (optimized)</td>
                </tr>
            </tbody>
        </table>

        <h2 id="exercises">Exercises</h2>

        <details>
            <summary>Exercise 1: Implementing and Visualizing Activation Functions</summary>
            <p><strong>Problem</strong>: Implement Sigmoid, ReLU, and tanh activation functions, and draw graphs in the range x = -5 to 5. Compare the characteristics of each function.</p>

            <p><strong>Hints</strong>:</p>
            <ul>
                <li>Use matplotlib.pyplot</li>
                <li>Generate equally spaced points with np.linspace(-5, 5, 100)</li>
                <li>Plot each function with plt.plot()</li>
            </ul>
        </details>

        <details>
            <summary>Exercise 2: Softmax Temperature Parameter</summary>
            <p><strong>Problem</strong>: Implement the following formula with temperature parameter T introduced to the Softmax function:</p>
            <p>$$\text{softmax}(\mathbf{x}, T)_i = \frac{e^{x_i/T}}{\sum_{j=1}^{n} e^{x_j/T}}$$</p>
            <p>For input x = [2.0, 1.0, 0.1], compare outputs for T = 0.5, 1.0, 2.0. How does the output change as temperature increases?</p>
        </details>

        <details>
            <summary>Exercise 3: XOR Problem</summary>
            <p><strong>Problem</strong>: Design a neural network to solve the XOR problem (exclusive OR).</p>
            <ul>
                <li>Input: 2-dimensional (x1, x2), each element is 0 or 1</li>
                <li>Output: x1 XOR x2 (1 if x1 and x2 differ, 0 if same)</li>
                <li>Network structure: Input layer(2) ‚Üí Hidden layer(2) ‚Üí Output layer(1)</li>
            </ul>
            <p>Modify the SimpleNN class to handle the XOR problem (learning will be covered in the next chapter).</p>
        </details>

        <details>
            <summary>Exercise 4: Weight Initialization</summary>
            <p><strong>Problem</strong>: Implement weight initialization in the SimpleNN class using the following 3 methods and compare the weight distributions after initialization:</p>
            <ol>
                <li><strong>All zeros</strong>: W = np.zeros(...)</li>
                <li><strong>Normal distribution</strong>: W = np.random.randn(...) * 0.01</li>
                <li><strong>Xavier initialization</strong>: W = np.random.randn(...) * np.sqrt(1/n_in)</li>
            </ol>
            <p>Calculate the mean and standard deviation of weights for each initialization, and consider which method is most appropriate.</p>
        </details>

        <details>
            <summary>Exercise 5: Multi-layer Network</summary>
            <p><strong>Problem</strong>: Extend the SimpleNN class to have 2 hidden layers:</p>
            <ul>
                <li>Input layer ‚Üí Hidden layer 1 (10 neurons, ReLU) ‚Üí Hidden layer 2 (5 neurons, ReLU) ‚Üí Output layer (3 classes, Softmax)</li>
            </ul>
            <p>Verify the output shape of each layer for 4-dimensional input data.</p>
        </details>

        <h2 id="summary">Summary</h2>

        <p>In this chapter, we learned the basic concepts of deep learning:</p>

        <ul>
            <li><strong>Definition</strong>: Deep learning is representation learning using multi-layered neural networks</li>
            <li><strong>History</strong>: Development trajectory of about 70 years from perceptrons to the modern era</li>
            <li><strong>Structure</strong>: Composed of input layer, hidden layers, and output layer, each layer is a collection of neurons</li>
            <li><strong>Activation Functions</strong>: Introduce non-linearity with Sigmoid, ReLU, Softmax, tanh, etc.</li>
            <li><strong>Implementation</strong>: Understand principles with NumPy, implement efficiently with PyTorch</li>
        </ul>

        <div class="success-box">
            <p><strong>Next Chapter Preview</strong>: In Chapter 2, we will learn about detailed forward propagation calculations, the role of weight matrices, and loss functions, and deeply understand how neural networks make predictions.</p>
        </div>

        <div class="navigation">
            <a href="./index.html" class="nav-button">‚Üê Back to Series Top</a>
            <a href="#" class="nav-button disabled">Chapter 2 ‚Üí (In Preparation)</a>
        </div>
    </main>

    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranties, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operability, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0), which typically include no-warranty clauses.</li>
        </ul>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
