<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Recommendation Systems Fundamentals - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/recommendation-systems-introduction/index.html">Recommendation Systems</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: Recommendation Systems Fundamentals</h1>
<p class="subtitle">Basic Concepts of Recommendation Systems and Data Processing Foundations</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 9</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the role and business value of recommendation systems</li>
<li>‚úÖ Learn the classification of recommendation tasks and evaluation metrics</li>
<li>‚úÖ Grasp key challenges such as the Cold Start problem</li>
<li>‚úÖ Perform preprocessing on the MovieLens dataset</li>
<li>‚úÖ Build User-Item matrices and implement Train-Test splitting</li>
<li>‚úÖ Implement recommendation system fundamentals in Python</li>
</ul>
<hr/>
<h2>1.1 What are Recommendation Systems</h2>
<h3>Importance of Personalization</h3>
<p><strong>Recommendation Systems</strong> are technologies that suggest optimal items (products, content, services, etc.) based on user preferences and behavior.</p>
<blockquote>
<p>"In an age of information overload, recommendation systems play a crucial role in connecting users with valuable content."</p>
</blockquote>
<h3>Applications of Recommendation Systems</h3>
<table>
<thead>
<tr>
<th>Industry</th>
<th>Application Examples</th>
<th>Recommendation Target</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>E-commerce</strong></td>
<td>Amazon, Rakuten</td>
<td>Products</td>
</tr>
<tr>
<td><strong>Video Streaming</strong></td>
<td>Netflix, YouTube</td>
<td>Movies, Videos</td>
</tr>
<tr>
<td><strong>Music Streaming</strong></td>
<td>Spotify, Apple Music</td>
<td>Songs, Playlists</td>
</tr>
<tr>
<td><strong>Social Networks</strong></td>
<td>Facebook, Twitter</td>
<td>Friends, Posts</td>
</tr>
<tr>
<td><strong>News</strong></td>
<td>Google News</td>
<td>Articles</td>
</tr>
<tr>
<td><strong>Job Search</strong></td>
<td>LinkedIn</td>
<td>Jobs, Candidates</td>
</tr>
</tbody>
</table>
<h3>Business Value</h3>
<ul>
<li><strong>Revenue Growth</strong>: Increased revenue through cross-selling and up-selling (35% of Amazon's sales come from recommendations)</li>
<li><strong>Enhanced Engagement</strong>: Increased user dwell time and content consumption</li>
<li><strong>Improved Customer Satisfaction</strong>: Higher satisfaction through personalized experiences</li>
<li><strong>Reduced Churn Rate</strong>: Prevention of user abandonment through relevant content</li>
<li><strong>Inventory Optimization</strong>: Discovery and promotion of long-tail products</li>
</ul>
<h3>Types of Recommendations</h3>
<div class="mermaid">
graph TD
    A[Recommendation Systems] --&gt; B[Collaborative Filtering]
    A --&gt; C[Content-Based]
    A --&gt; D[Hybrid]

    B --&gt; B1[User-based]
    B --&gt; B2[Item-based]
    B --&gt; B3[Matrix Factorization]

    C --&gt; C1[Feature Extraction]
    C --&gt; C2[Similarity Computation]

    D --&gt; D1[Weighted Hybrid]
    D --&gt; D2[Switching Hybrid]
    D --&gt; D3[Feature Combination]

    style A fill:#e8f5e9
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
</div>
<hr/>
<h2>1.2 Classification of Recommendation Tasks</h2>
<h3>Explicit vs Implicit Feedback</h3>
<p>User feedback can be explicit or implicit.</p>
<table>
<thead>
<tr>
<th>Feedback Type</th>
<th>Description</th>
<th>Examples</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Explicit</strong></td>
<td>Direct user ratings</td>
<td>Star ratings, likes, reviews</td>
<td>Clear preference information</td>
<td>Limited data</td>
</tr>
<tr>
<td><strong>Implicit</strong></td>
<td>Inferred from behavior</td>
<td>Clicks, watch time, purchases</td>
<td>Abundant data</td>
<td>Ambiguous interpretation</td>
</tr>
</tbody>
</table>
<h4>Example of Explicit Feedback</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Example of Explicit Feedback

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np

# Explicit Feedback: Movie rating data
np.random.seed(42)
n_ratings = 100

explicit_data = pd.DataFrame({
    'user_id': np.random.randint(1, 21, n_ratings),
    'item_id': np.random.randint(1, 51, n_ratings),
    'rating': np.random.randint(1, 6, n_ratings),  # Ratings from 1-5
    'timestamp': pd.date_range('2024-01-01', periods=n_ratings, freq='H')
})

print("=== Explicit Feedback (Rating Data) ===")
print(explicit_data.head(10))
print(f"\nRating Distribution:")
print(explicit_data['rating'].value_counts().sort_index())
print(f"\nAverage Rating: {explicit_data['rating'].mean():.2f}")
print(f"Rating Standard Deviation: {explicit_data['rating'].std():.2f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Explicit Feedback (Rating Data) ===
   user_id  item_id  rating           timestamp
0        7       40       4 2024-01-01 00:00:00
1       20       34       3 2024-01-01 01:00:00
2       18       48       1 2024-01-01 02:00:00
3       11       14       5 2024-01-01 03:00:00
4        6       21       1 2024-01-01 04:00:00
5       17       28       4 2024-01-01 05:00:00
6        3        9       1 2024-01-01 06:00:00
7        9       37       4 2024-01-01 07:00:00
8       20       17       5 2024-01-01 08:00:00
9        8       46       2 2024-01-01 09:00:00

Rating Distribution:
1    23
2    19
3    18
4    21
5    19
Name: rating, dtype: int64

Average Rating: 2.98
Rating Standard Deviation: 1.47
</code></pre>
<h4>Example of Implicit Feedback</h4>
<pre><code class="language-python"># Implicit Feedback: Viewing data
implicit_data = pd.DataFrame({
    'user_id': np.random.randint(1, 21, n_ratings),
    'item_id': np.random.randint(1, 51, n_ratings),
    'watch_time': np.random.randint(1, 120, n_ratings),  # minutes
    'completed': np.random.choice([0, 1], n_ratings, p=[0.3, 0.7]),
    'timestamp': pd.date_range('2024-01-01', periods=n_ratings, freq='H')
})

# Infer preferences from implicit feedback
# Assume "liked" if watch time is long or video was completed
implicit_data['preference'] = (
    (implicit_data['watch_time'] &gt; 60) |
    (implicit_data['completed'] == 1)
).astype(int)

print("\n=== Implicit Feedback (Viewing Data) ===")
print(implicit_data.head(10))
print(f"\nCompletion Rate: {implicit_data['completed'].mean():.1%}")
print(f"Inferred Preference Rate: {implicit_data['preference'].mean():.1%}")
</code></pre>
<h3>Rating Prediction</h3>
<p><strong>Rating Prediction</strong> is the task of predicting the rating a user would give to an item they have not yet rated.</p>
<p>$$
\hat{r}_{ui} = f(\text{user}_u, \text{item}_i)
$$</p>
<ul>
<li>$\hat{r}_{ui}$: Predicted rating of user $u$ for item $i$</li>
<li>$f$: Recommendation model</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: $$
\hat{r}_{ui} = f(\text{user}_u, \text{item}_i)
$$

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Build User-Item matrix (simplified version)
ratings_matrix = explicit_data.pivot_table(
    index='user_id',
    columns='item_id',
    values='rating',
    aggfunc='mean'
)

print("=== User-Item Rating Matrix ===")
print(f"Shape: {ratings_matrix.shape}")
print(f"Missing Rate: {ratings_matrix.isnull().sum().sum() / (ratings_matrix.shape[0] * ratings_matrix.shape[1]):.1%}")
print(f"\nMatrix (sample):")
print(ratings_matrix.iloc[:5, :5])
</code></pre>
<h3>Top-N Recommendation</h3>
<p><strong>Top-N Recommendation</strong> is the task of recommending the top N items to each user.</p>
<pre><code class="language-python"># Simple Top-N recommendation (popularity-based)
def popularity_based_recommendation(data, n=5):
    """Popularity-based Top-N recommendation"""
    item_popularity = data.groupby('item_id')['rating'].agg(['count', 'mean'])
    item_popularity['score'] = (
        item_popularity['count'] * 0.3 +
        item_popularity['mean'] * 0.7
    )
    top_n = item_popularity.nlargest(n, 'score')
    return top_n

top_items = popularity_based_recommendation(explicit_data, n=5)

print("\n=== Top-5 Recommended Items (Popularity-based) ===")
print(top_items)
print(f"\nRationale:")
print("- Score = Rating Count √ó 0.3 + Average Rating √ó 0.7")
</code></pre>
<h3>Ranking Problems</h3>
<p><strong>Ranking problems</strong> involve ordering candidate items. Items are sorted by relevance.</p>
<pre><code class="language-python"># Ranking example: Order items by score for each user
def rank_items_for_user(user_id, data):
    """Item ranking for a specific user"""
    # Consider user's past rating tendencies
    user_ratings = data[data['user_id'] == user_id]
    user_avg_rating = user_ratings['rating'].mean()

    # Information about all items
    all_items = data.groupby('item_id')['rating'].agg(['mean', 'count'])

    # Scoring (simplified version)
    all_items['score'] = (
        all_items['mean'] * 0.5 +
        user_avg_rating * 0.3 +
        np.log1p(all_items['count']) * 0.2
    )

    ranked_items = all_items.sort_values('score', ascending=False)
    return ranked_items

# Ranking for User 7
user_ranking = rank_items_for_user(7, explicit_data)
print("\n=== Item Ranking for User 7 (Top 10) ===")
print(user_ranking.head(10))
</code></pre>
<hr/>
<h2>1.3 Evaluation Metrics</h2>
<h3>Precision, Recall, F1</h3>
<p>These are basic metrics for measuring the accuracy of recommendation systems.</p>
<p>$$
\text{Precision@K} = \frac{\text{Number of Relevant Recommended Items}}{K}
$$</p>
<p>$$
\text{Recall@K} = \frac{\text{Number of Relevant Recommended Items}}{\text{Total Number of Relevant Items}}
$$</p>
<p>$$
\text{F1@K} = 2 \cdot \frac{\text{Precision@K} \cdot \text{Recall@K}}{\text{Precision@K} + \text{Recall@K}}
$$</p>
<pre><code class="language-python">def precision_recall_at_k(recommended, relevant, k):
    """Calculate Precision@K and Recall@K"""
    recommended_k = recommended[:k]

    # Items that are both recommended and relevant
    hits = len(set(recommended_k) &amp; set(relevant))

    precision = hits / k if k &gt; 0 else 0
    recall = hits / len(relevant) if len(relevant) &gt; 0 else 0

    if precision + recall &gt; 0:
        f1 = 2 * precision * recall / (precision + recall)
    else:
        f1 = 0

    return precision, recall, f1

# Example: Recommendations to a user
recommended_items = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]  # Recommended items
relevant_items = [2, 3, 5, 8, 11, 15]  # Actually relevant items

for k in [5, 10]:
    p, r, f = precision_recall_at_k(recommended_items, relevant_items, k)
    print(f"\n=== Evaluation at K={k} ===")
    print(f"Precision@{k}: {p:.3f}")
    print(f"Recall@{k}: {r:.3f}")
    print(f"F1@{k}: {f:.3f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Evaluation at K=5 ===
Precision@5: 0.400
Recall@5: 0.333
F1@5: 0.364

=== Evaluation at K=10 ===
Precision@10: 0.400
Recall@10: 0.667
F1@10: 0.500
</code></pre>
<h3>NDCG (Normalized Discounted Cumulative Gain)</h3>
<p><strong>NDCG</strong> is a metric that evaluates ranking quality. It emphasizes placing highly relevant items at the top.</p>
<p>$$
\text{DCG@K} = \sum_{i=1}^{K} \frac{2^{rel_i} - 1}{\log_2(i + 1)}
$$</p>
<p>$$
\text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}
$$</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np

def dcg_at_k(relevances, k):
    """Calculate DCG@K"""
    relevances = np.array(relevances[:k])
    if relevances.size:
        discounts = np.log2(np.arange(2, relevances.size + 2))
        return np.sum((2**relevances - 1) / discounts)
    return 0.0

def ndcg_at_k(relevances, k):
    """Calculate NDCG@K"""
    dcg = dcg_at_k(relevances, k)
    ideal_relevances = sorted(relevances, reverse=True)
    idcg = dcg_at_k(ideal_relevances, k)

    if idcg == 0:
        return 0.0
    return dcg / idcg

# Example: Relevance scores of recommendation results (5-level scale)
relevances = [3, 2, 5, 0, 1, 4, 2, 0, 3, 1]  # Relevance in recommendation order

print("=== NDCG Evaluation ===")
for k in [3, 5, 10]:
    ndcg = ndcg_at_k(relevances, k)
    print(f"NDCG@{k}: {ndcg:.3f}")

print(f"\nRelevance List (recommendation order): {relevances}")
print(f"Ideal Order: {sorted(relevances, reverse=True)}")
</code></pre>
<h3>MAP (Mean Average Precision)</h3>
<p><strong>MAP</strong> is the mean of Average Precision across all users.</p>
<p>$$
\text{AP@K} = \frac{1}{\min(m, K)} \sum_{k=1}^{K} \text{Precision@k} \cdot \text{rel}(k)
$$</p>
<p>$$
\text{MAP@K} = \frac{1}{|U|} \sum_{u \in U} \text{AP@K}_u
$$</p>
<pre><code class="language-python">def average_precision_at_k(recommended, relevant, k):
    """Calculate Average Precision@K"""
    recommended_k = recommended[:k]

    score = 0.0
    num_hits = 0.0

    for i, item in enumerate(recommended_k):
        if item in relevant:
            num_hits += 1.0
            score += num_hits / (i + 1.0)

    if len(relevant) == 0:
        return 0.0

    return score / min(len(relevant), k)

# Example: MAP calculation for multiple users
users_recommendations = [
    ([1, 3, 5, 7, 9], [3, 5, 9]),      # User 1
    ([2, 4, 6, 8, 10], [4, 8]),        # User 2
    ([1, 2, 3, 4, 5], [1, 2, 5]),      # User 3
]

aps = []
for recommended, relevant in users_recommendations:
    ap = average_precision_at_k(recommended, relevant, k=5)
    aps.append(ap)
    print(f"Recommended: {recommended}, Relevant: {relevant} -&gt; AP@5: {ap:.3f}")

map_score = np.mean(aps)
print(f"\n=== MAP@5: {map_score:.3f} ===")
</code></pre>
<h3>Coverage, Diversity, Serendipity</h3>
<p>These metrics evaluate the quality of recommendations from multiple perspectives.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Description</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Coverage</strong></td>
<td>Proportion of items recommended</td>
<td>Long-tail item discovery</td>
</tr>
<tr>
<td><strong>Diversity</strong></td>
<td>Variety within recommendation list</td>
<td>Filter bubble avoidance</td>
</tr>
<tr>
<td><strong>Serendipity</strong></td>
<td>Unexpected yet relevant recommendations</td>
<td>Promoting new discoveries</td>
</tr>
</tbody>
</table>
<pre><code class="language-python">def calculate_coverage(all_recommendations, total_items):
    """Calculate coverage"""
    unique_recommended = set()
    for recs in all_recommendations:
        unique_recommended.update(recs)

    coverage = len(unique_recommended) / total_items
    return coverage

def calculate_diversity(recommendations):
    """Calculate diversity of recommendation list (uniqueness rate)"""
    unique_items = len(set(recommendations))
    diversity = unique_items / len(recommendations)
    return diversity

# Example: Calculate coverage and diversity
all_recs = [
    [1, 2, 3, 4, 5],
    [1, 3, 6, 7, 8],
    [2, 4, 9, 10, 11],
    [1, 5, 12, 13, 14]
]

total_items = 50  # Total number of items

coverage = calculate_coverage(all_recs, total_items)
print(f"=== Coverage and Diversity ===")
print(f"Coverage: {coverage:.1%}")
print(f"Unique Recommended Items: {len(set([item for recs in all_recs for item in recs]))}")

for i, recs in enumerate(all_recs):
    diversity = calculate_diversity(recs)
    print(f"User {i+1} Recommendation Diversity: {diversity:.1%}")
</code></pre>
<hr/>
<h2>1.4 Challenges in Recommendation Systems</h2>
<h3>Cold Start Problem</h3>
<p><strong>The Cold Start Problem</strong> refers to insufficient data for new users or new items.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Solutions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>User Cold Start</strong></td>
<td>Unknown preferences for new users</td>
<td>Recommend popular items, utilize demographic information</td>
</tr>
<tr>
<td><strong>Item Cold Start</strong></td>
<td>No ratings for new items</td>
<td>Content-based recommendation, utilize metadata</td>
</tr>
<tr>
<td><strong>System Cold Start</strong></td>
<td>Lack of data for entire system</td>
<td>External data, crowdsourcing</td>
</tr>
</tbody>
</table>
<h3>Data Sparsity</h3>
<p><strong>Data Sparsity</strong> is the problem where most of the User-Item matrix consists of missing values.</p>
<div class="mermaid">
graph LR
    A[User-Item Matrix] --&gt; B[Rated: 1%]
    A --&gt; C[Unrated: 99%]

    B --&gt; D[Collaborative Filtering Possible]
    C --&gt; E[Recommendation Difficult]

    style A fill:#fff3e0
    style B fill:#c8e6c9
    style C fill:#ffcdd2
    style D fill:#e8f5e9
    style E fill:#ffebee
</div>
<h3>Scalability</h3>
<p><strong>Scalability</strong> is the problem of computational complexity as the number of users and items grows.</p>
<ul>
<li>Number of users: 1 million</li>
<li>Number of items: 100,000</li>
<li>‚Üí User-Item matrix: 100 billion cells</li>
</ul>
<blockquote>
<p><strong>Solutions</strong>: Dimensionality reduction (Matrix Factorization), Approximate Nearest Neighbor (ANN), distributed processing</p>
</blockquote>
<h3>Filter Bubble</h3>
<p><strong>Filter Bubble</strong> is the problem where only similar items are recommended, reducing diversity.</p>
<ul>
<li><strong>Cause</strong>: Excessive personalization</li>
<li><strong>Impact</strong>: Reduced new discoveries, biased information consumption</li>
<li><strong>Solutions</strong>: Consider diversity, introduce serendipity, balance exploration and exploitation</li>
</ul>
<hr/>
<h2>1.5 Datasets and Preprocessing</h2>
<h3>MovieLens Dataset</h3>
<p><strong>MovieLens</strong> is the most widely used dataset in recommendation systems research.</p>
<table>
<thead>
<tr>
<th>Version</th>
<th>Ratings</th>
<th>Users</th>
<th>Movies</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>100K</td>
<td>100,000</td>
<td>943</td>
<td>1,682</td>
<td>Learning, prototyping</td>
</tr>
<tr>
<td>1M</td>
<td>1 million</td>
<td>6,040</td>
<td>3,706</td>
<td>Research, evaluation</td>
</tr>
<tr>
<td>10M</td>
<td>10 million</td>
<td>71,567</td>
<td>10,681</td>
<td>Scalability testing</td>
</tr>
<tr>
<td>25M</td>
<td>25 million</td>
<td>162,541</td>
<td>62,423</td>
<td>Large-scale experiments</td>
</tr>
</tbody>
</table>
<h3>User-Item Matrix</h3>
<p><strong>The User-Item Matrix</strong> is the fundamental data structure for recommendation systems.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: The User-Item Matrixis the fundamental data structure for re

Purpose: Demonstrate data manipulation and preprocessing
Target: Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix

# Create sample data (MovieLens-style)
np.random.seed(42)
n_users = 100
n_items = 50
n_ratings = 500

ratings_data = pd.DataFrame({
    'user_id': np.random.randint(1, n_users + 1, n_ratings),
    'item_id': np.random.randint(1, n_items + 1, n_ratings),
    'rating': np.random.randint(1, 6, n_ratings),
    'timestamp': pd.date_range('2024-01-01', periods=n_ratings, freq='H')
})

# Remove duplicates (keep latest rating for same user-item pair)
ratings_data = ratings_data.sort_values('timestamp').drop_duplicates(
    subset=['user_id', 'item_id'],
    keep='last'
)

print("=== Rating Data ===")
print(ratings_data.head(10))
print(f"\nTotal Ratings: {len(ratings_data)}")
print(f"Unique Users: {ratings_data['user_id'].nunique()}")
print(f"Unique Items: {ratings_data['item_id'].nunique()}")
print(f"Rating Distribution:\n{ratings_data['rating'].value_counts().sort_index()}")

# Build User-Item matrix
user_item_matrix = ratings_data.pivot_table(
    index='user_id',
    columns='item_id',
    values='rating',
    fill_value=0
)

print(f"\n=== User-Item Matrix ===")
print(f"Shape: {user_item_matrix.shape}")
print(f"Density: {(user_item_matrix &gt; 0).sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.1%}")
print(f"\nMatrix Sample (first 5 users √ó 5 items):")
print(user_item_matrix.iloc[:5, :5])

# Convert to sparse matrix (memory efficiency)
sparse_matrix = csr_matrix(user_item_matrix.values)
print(f"\nSparse Matrix Size: {sparse_matrix.data.nbytes / 1024:.2f} KB")
print(f"Dense Matrix Size: {user_item_matrix.values.nbytes / 1024:.2f} KB")
print(f"Memory Reduction Rate: {(1 - sparse_matrix.data.nbytes / user_item_matrix.values.nbytes):.1%}")
</code></pre>
<h3>Train-Test Split Strategies</h3>
<p>For recommendation systems, time-series-aware splitting is important.</p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# 1. Random split (simple but ignores time series)
train_random, test_random = train_test_split(
    ratings_data,
    test_size=0.2,
    random_state=42
)

print("=== 1. Random Split ===")
print(f"Training Data: {len(train_random)} samples")
print(f"Test Data: {len(test_random)} samples")

# 2. Temporal split (more realistic)
ratings_data_sorted = ratings_data.sort_values('timestamp')
split_idx = int(len(ratings_data_sorted) * 0.8)

train_temporal = ratings_data_sorted.iloc[:split_idx]
test_temporal = ratings_data_sorted.iloc[split_idx:]

print("\n=== 2. Temporal Split ===")
print(f"Training Period: {train_temporal['timestamp'].min()} ~ {train_temporal['timestamp'].max()}")
print(f"Test Period: {test_temporal['timestamp'].min()} ~ {test_temporal['timestamp'].max()}")
print(f"Training Data: {len(train_temporal)} samples")
print(f"Test Data: {len(test_temporal)} samples")

# 3. Per-user split (Leave-One-Out)
def leave_one_out_split(data):
    """Put each user's latest rating into test set"""
    train_list = []
    test_list = []

    for user_id, group in data.groupby('user_id'):
        group_sorted = group.sort_values('timestamp')
        if len(group_sorted) &gt; 1:
            train_list.append(group_sorted.iloc[:-1])
            test_list.append(group_sorted.iloc[-1:])
        else:
            train_list.append(group_sorted)

    train = pd.concat(train_list)
    test = pd.concat(test_list) if test_list else pd.DataFrame()

    return train, test

train_loo, test_loo = leave_one_out_split(ratings_data)

print("\n=== 3. Leave-One-Out Split ===")
print(f"Training Data: {len(train_loo)} samples")
print(f"Test Data: {len(test_loo)} samples")
print(f"Test Users: {test_loo['user_id'].nunique()}")
</code></pre>
<h3>Python Preprocessing</h3>
<p>Practical example of data preprocessing for recommendation systems.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import pandas as pd
import numpy as np

class RecommendationDataPreprocessor:
    """Data preprocessing class for recommendation systems"""

    def __init__(self, min_user_ratings=5, min_item_ratings=5):
        self.min_user_ratings = min_user_ratings
        self.min_item_ratings = min_item_ratings
        self.user_mapping = {}
        self.item_mapping = {}

    def filter_rare_users_items(self, data):
        """Filter out users and items with few ratings"""
        print("=== Before Filtering ===")
        print(f"Users: {data['user_id'].nunique()}")
        print(f"Items: {data['item_id'].nunique()}")
        print(f"Ratings: {len(data)}")

        # Filter users
        user_counts = data['user_id'].value_counts()
        valid_users = user_counts[user_counts &gt;= self.min_user_ratings].index
        data = data[data['user_id'].isin(valid_users)]

        # Filter items
        item_counts = data['item_id'].value_counts()
        valid_items = item_counts[item_counts &gt;= self.min_item_ratings].index
        data = data[data['item_id'].isin(valid_items)]

        print("\n=== After Filtering ===")
        print(f"Users: {data['user_id'].nunique()}")
        print(f"Items: {data['item_id'].nunique()}")
        print(f"Ratings: {len(data)}")

        return data

    def create_mappings(self, data):
        """Map user and item IDs to continuous integers"""
        unique_users = sorted(data['user_id'].unique())
        unique_items = sorted(data['item_id'].unique())

        self.user_mapping = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item_mapping = {iid: idx for idx, iid in enumerate(unique_items)}

        data['user_idx'] = data['user_id'].map(self.user_mapping)
        data['item_idx'] = data['item_id'].map(self.item_mapping)

        print("\n=== ID Mapping ===")
        print(f"User ID Range: {data['user_id'].min()} ~ {data['user_id'].max()}")
        print(f"User Index Range: {data['user_idx'].min()} ~ {data['user_idx'].max()}")
        print(f"Item ID Range: {data['item_id'].min()} ~ {data['item_id'].max()}")
        print(f"Item Index Range: {data['item_idx'].min()} ~ {data['item_idx'].max()}")

        return data

    def normalize_ratings(self, data, method='mean'):
        """Normalize rating values"""
        if method == 'mean':
            # Subtract mean
            user_means = data.groupby('user_id')['rating'].transform('mean')
            data['rating_normalized'] = data['rating'] - user_means
        elif method == 'minmax':
            # Scale to [0, 1]
            data['rating_normalized'] = (data['rating'] - data['rating'].min()) / (
                data['rating'].max() - data['rating'].min()
            )

        print(f"\n=== Rating Normalization ({method}) ===")
        print(f"Original Rating Range: [{data['rating'].min()}, {data['rating'].max()}]")
        print(f"Normalized Range: [{data['rating_normalized'].min():.2f}, {data['rating_normalized'].max():.2f}]")

        return data

# Execute preprocessing
preprocessor = RecommendationDataPreprocessor(
    min_user_ratings=3,
    min_item_ratings=3
)

# Filter data
filtered_data = preprocessor.filter_rare_users_items(ratings_data)

# ID mapping
mapped_data = preprocessor.create_mappings(filtered_data)

# Rating normalization
normalized_data = preprocessor.normalize_ratings(mapped_data, method='mean')

print("\n=== Preprocessed Data (sample) ===")
print(normalized_data[['user_id', 'user_idx', 'item_id', 'item_idx',
                        'rating', 'rating_normalized']].head(10))
</code></pre>
<hr/>
<h2>1.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Role of Recommendation Systems</strong></p>
<ul>
<li>Suggest optimal content in an age of information overload</li>
<li>Contribute to revenue growth, engagement, and customer satisfaction</li>
<li>Collaborative filtering, content-based, and hybrid approaches</li>
</ul></li>
<li><p><strong>Types of Recommendation Tasks</strong></p>
<ul>
<li>Explicit vs Implicit Feedback</li>
<li>Rating Prediction, Top-N Recommendation, Ranking</li>
<li>Selecting appropriate methods for each task</li>
</ul></li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li>Precision, Recall, F1: Basic accuracy metrics</li>
<li>NDCG: Evaluating ranking quality</li>
<li>MAP: Average precision evaluation</li>
<li>Coverage, Diversity, Serendipity: Recommendation quality</li>
</ul></li>
<li><p><strong>Key Challenges</strong></p>
<ul>
<li>Cold Start Problem: Handling new users and items</li>
<li>Data Sparsity: Managing sparse data</li>
<li>Scalability: Processing large-scale data</li>
<li>Filter Bubble: Ensuring diversity</li>
</ul></li>
<li><p><strong>Practical Data Processing</strong></p>
<ul>
<li>Utilizing the MovieLens dataset</li>
<li>Building User-Item matrices</li>
<li>Appropriate Train-Test splitting</li>
<li>Building preprocessing pipelines</li>
</ul></li>
</ol>
<h3>Principles of Recommendation System Design</h3>
<table>
<thead>
<tr>
<th>Principle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>User-Centric Design</strong></td>
<td>Prioritize user satisfaction and experience</td>
</tr>
<tr>
<td><strong>Multi-faceted Evaluation</strong></td>
<td>Consider not just accuracy but also diversity and novelty</td>
</tr>
<tr>
<td><strong>Temporal Awareness</strong></td>
<td>Respect time series in splitting and evaluation</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Design to handle large-scale data</td>
</tr>
<tr>
<td><strong>Continuous Improvement</strong></td>
<td>Improve through A/B testing and regular evaluation</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 2, we will learn about <strong>Collaborative Filtering</strong>:</p>
<ul>
<li>User-based collaborative filtering</li>
<li>Item-based collaborative filtering</li>
<li>Similarity computation (cosine similarity, Pearson correlation)</li>
<li>Nearest neighbor search and k-NN</li>
<li>Implementation and evaluation</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Explain the difference between Explicit Feedback and Implicit Feedback, and describe the advantages and disadvantages of each.</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Explicit Feedback</strong>:</p>
<ul>
<li>Definition: Ratings intentionally provided by users (star ratings, likes, reviews)</li>
<li>Advantages: Clear preference information, easy to interpret</li>
<li>Disadvantages: Difficult to collect data, high user burden, limited data volume</li>
</ul>
<p><strong>Implicit Feedback</strong>:</p>
<ul>
<li>Definition: Preferences inferred from user behavior (clicks, watch time, purchases)</li>
<li>Advantages: Abundant data, no user burden, natural behavior</li>
<li>Disadvantages: Ambiguous interpretation (clicks don't necessarily mean preference), negative feedback unclear</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Explicit: Fields where ratings are important (movies, book reviews)</li>
<li>Implicit: Large-scale services (video streaming, e-commerce)</li>
<li>Hybrid: Combine both for improved accuracy</li>
</ul>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Calculate Precision@5 and Recall@5 for the following recommendation results.</p>
<pre><code class="language-python">recommended = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
relevant = [2, 3, 5, 8, 11, 15, 20]
</code></pre>
<details>
<summary>Solution</summary>
<pre><code class="language-python">def precision_recall_at_k(recommended, relevant, k):
    """Calculate Precision@K and Recall@K"""
    recommended_k = recommended[:k]

    # Items that are both recommended and relevant
    hits = len(set(recommended_k) &amp; set(relevant))

    precision = hits / k if k &gt; 0 else 0
    recall = hits / len(relevant) if len(relevant) &gt; 0 else 0

    return precision, recall

recommended = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
relevant = [2, 3, 5, 8, 11, 15, 20]

precision, recall = precision_recall_at_k(recommended, relevant, k=5)

print("=== Calculation Process ===")
print(f"Recommended Items (top 5): {recommended[:5]}")
print(f"Relevant Items: {relevant}")
print(f"Hits: {set(recommended[:5]) &amp; set(relevant)}")
print(f"Hit Count: {len(set(recommended[:5]) &amp; set(relevant))}")
print(f"\nPrecision@5 = {len(set(recommended[:5]) &amp; set(relevant))} / 5 = {precision:.3f}")
print(f"Recall@5 = {len(set(recommended[:5]) &amp; set(relevant))} / {len(relevant)} = {recall:.3f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Calculation Process ===
Recommended Items (top 5): [1, 3, 5, 7, 9]
Relevant Items: [2, 3, 5, 8, 11, 15, 20]
Hits: {3, 5}
Hit Count: 2

Precision@5 = 2 / 5 = 0.400
Recall@5 = 2 / 7 = 0.286
</code></pre>
</details>
<h3>Problem 3 (Difficulty: medium)</h3>
<p>Explain the three types of Cold Start problems (User, Item, System) and propose solutions for each.</p>
<details>
<summary>Solution</summary>
<p><strong>Answer</strong>:</p>
<p><strong>1. User Cold Start (New User Problem)</strong>:</p>
<ul>
<li>Description: New users have no rating history, preferences unknown</li>
<li>Solutions:
<ul>
<li>Recommend popular items (overall popularity)</li>
<li>Utilize demographic information (age, gender, location)</li>
<li>Profiling through initial questionnaires</li>
<li>Utilize social network information</li>
</ul>
</li>
</ul>
<p><strong>2. Item Cold Start (New Item Problem)</strong>:</p>
<ul>
<li>Description: New items have no ratings, cannot be recommended</li>
<li>Solutions:
<ul>
<li>Content-based recommendation (calculate similarity from item features)</li>
<li>Utilize metadata (genre, tags, descriptions)</li>
<li>Prioritize presentation to active users</li>
<li>Initial ratings by experts</li>
</ul>
</li>
</ul>
<p><strong>3. System Cold Start (Overall System Problem)</strong>:</p>
<ul>
<li>Description: Lack of both user and item data at service launch</li>
<li>Solutions:
<ul>
<li>Utilize external data sources (existing review sites)</li>
<li>Initial data collection through crowdsourcing</li>
<li>Expert curation</li>
<li>Transfer Learning (knowledge transfer from other domains)</li>
</ul>
</li>
</ul>
<p><strong>Real-world Examples</strong>:</p>
<ul>
<li>Netflix: Have new users select 3 favorite titles</li>
<li>Spotify: Select favorite artists to create profile</li>
<li>Amazon: Recommend based on browsing history and popular products</li>
</ul>
</details>
<h3>Problem 4 (Difficulty: hard)</h3>
<p>For the following data, build a User-Item matrix, implement temporal splitting (80% training, 20% test), and calculate the matrix density.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following data, build a User-Item matrix, implement 

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import pandas as pd
import numpy as np

np.random.seed(42)
data = pd.DataFrame({
    'user_id': [1, 1, 2, 2, 2, 3, 3, 4, 4, 5],
    'item_id': [10, 20, 10, 30, 40, 20, 30, 10, 50, 40],
    'rating': [5, 4, 3, 5, 2, 4, 5, 3, 4, 5],
    'timestamp': pd.date_range('2024-01-01', periods=10, freq='D')
})
</code></pre>
<details>
<summary>Solution</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: For the following data, build a User-Item matrix, implement 

Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import pandas as pd
import numpy as np

np.random.seed(42)
data = pd.DataFrame({
    'user_id': [1, 1, 2, 2, 2, 3, 3, 4, 4, 5],
    'item_id': [10, 20, 10, 30, 40, 20, 30, 10, 50, 40],
    'rating': [5, 4, 3, 5, 2, 4, 5, 3, 4, 5],
    'timestamp': pd.date_range('2024-01-01', periods=10, freq='D')
})

print("=== Original Data ===")
print(data)

# Build User-Item matrix
user_item_matrix = data.pivot_table(
    index='user_id',
    columns='item_id',
    values='rating',
    fill_value=0
)

print("\n=== User-Item Matrix ===")
print(user_item_matrix)

# Calculate density
total_cells = user_item_matrix.shape[0] * user_item_matrix.shape[1]
non_zero_cells = (user_item_matrix &gt; 0).sum().sum()
density = non_zero_cells / total_cells

print(f"\n=== Matrix Statistics ===")
print(f"Shape: {user_item_matrix.shape}")
print(f"Total Cells: {total_cells}")
print(f"Rated Cells: {non_zero_cells}")
print(f"Density: {density:.1%}")
print(f"Sparsity: {(1 - density):.1%}")

# Temporal split
data_sorted = data.sort_values('timestamp')
split_idx = int(len(data_sorted) * 0.8)

train_data = data_sorted.iloc[:split_idx]
test_data = data_sorted.iloc[split_idx:]

print("\n=== Temporal Split ===")
print(f"Training Data Count: {len(train_data)}")
print(f"Test Data Count: {len(test_data)}")
print(f"\nTraining Period: {train_data['timestamp'].min()} ~ {train_data['timestamp'].max()}")
print(f"Test Period: {test_data['timestamp'].min()} ~ {test_data['timestamp'].max()}")

print("\nTraining Data:")
print(train_data)
print("\nTest Data:")
print(test_data)

# User-Item matrix for training and test data
train_matrix = train_data.pivot_table(
    index='user_id',
    columns='item_id',
    values='rating',
    fill_value=0
)

print("\n=== Training Data User-Item Matrix ===")
print(train_matrix)
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Original Data ===
   user_id  item_id  rating  timestamp
0        1       10       5 2024-01-01
1        1       20       4 2024-01-02
2        2       10       3 2024-01-03
3        2       30       5 2024-01-04
4        2       40       2 2024-01-05
5        3       20       4 2024-01-06
6        3       30       5 2024-01-07
7        4       10       3 2024-01-08
8        4       50       4 2024-01-09
9        5       40       5 2024-01-10

=== User-Item Matrix ===
item_id  10  20  30  40  50
user_id
1         5   4   0   0   0
2         3   0   5   2   0
3         0   4   5   0   0
4         3   0   0   0   4
5         0   0   0   5   0

=== Matrix Statistics ===
Shape: (5, 5)
Total Cells: 25
Rated Cells: 10
Density: 40.0%
Sparsity: 60.0%

=== Temporal Split ===
Training Data Count: 8
Test Data Count: 2

Training Period: 2024-01-01 ~ 2024-01-08
Test Period: 2024-01-09 ~ 2024-01-10

Training Data:
   user_id  item_id  rating  timestamp
0        1       10       5 2024-01-01
1        1       20       4 2024-01-02
2        2       10       3 2024-01-03
3        2       30       5 2024-01-04
4        2       40       2 2024-01-05
5        3       20       4 2024-01-06
6        3       30       5 2024-01-07
7        4       10       3 2024-01-08

Test Data:
   user_id  item_id  rating  timestamp
8        4       50       4 2024-01-09
9        5       40       5 2024-01-10

=== Training Data User-Item Matrix ===
item_id  10  20  30  40
user_id
1         5   4   0   0
2         3   0   5   2
3         0   4   5   0
4         3   0   0   0
</code></pre>
</details>
<h3>Problem 5 (Difficulty: hard)</h3>
<p>Implement a function to calculate NDCG@5 and evaluate the quality of the following recommendation results. Relevance scores are on a 5-level scale (0-4).</p>
<pre><code class="language-python">relevances = [3, 2, 0, 1, 4, 0, 2, 3, 1, 0]  # Relevance in recommendation order
</code></pre>
<details>
<summary>Solution</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np

def dcg_at_k(relevances, k):
    """Calculate DCG@K

    DCG@K = Œ£ (2^rel_i - 1) / log2(i + 1)
    """
    relevances = np.array(relevances[:k])
    if relevances.size:
        # Discount factor for position i: log2(i + 1)
        # Starting from i=1, so log2(2), log2(3), ...
        discounts = np.log2(np.arange(2, relevances.size + 2))
        gains = 2**relevances - 1
        dcg = np.sum(gains / discounts)
        return dcg
    return 0.0

def ndcg_at_k(relevances, k):
    """Calculate NDCG@K

    NDCG@K = DCG@K / IDCG@K
    """
    dcg = dcg_at_k(relevances, k)

    # Ideal DCG: DCG when relevances are sorted in descending order
    ideal_relevances = sorted(relevances, reverse=True)
    idcg = dcg_at_k(ideal_relevances, k)

    if idcg == 0:
        return 0.0

    ndcg = dcg / idcg
    return ndcg

# Example: Evaluate recommendation results
relevances = [3, 2, 0, 1, 4, 0, 2, 3, 1, 0]

print("=== NDCG Evaluation ===")
print(f"Relevance in Recommendation Order: {relevances}")
print(f"Ideal Order: {sorted(relevances, reverse=True)}")

for k in [3, 5, 10]:
    dcg = dcg_at_k(relevances, k)
    ideal_relevances = sorted(relevances, reverse=True)
    idcg = dcg_at_k(ideal_relevances, k)
    ndcg = ndcg_at_k(relevances, k)

    print(f"\n=== K={k} ===")
    print(f"DCG@{k}: {dcg:.3f}")
    print(f"IDCG@{k}: {idcg:.3f}")
    print(f"NDCG@{k}: {ndcg:.3f}")

# Detailed calculation example (K=5)
print("\n=== Detailed Calculation (K=5) ===")
k = 5
rels = relevances[:k]
print(f"Top {k} Relevances: {rels}")

for i, rel in enumerate(rels):
    pos = i + 1
    gain = 2**rel - 1
    discount = np.log2(pos + 1)
    contribution = gain / discount
    print(f"Position {pos}: rel={rel}, gain={gain}, discount={discount:.3f}, contribution={contribution:.3f}")

dcg = dcg_at_k(relevances, k)
print(f"\nDCG@5 = {dcg:.3f}")

ideal_rels = sorted(relevances, reverse=True)[:k]
print(f"\nIdeal Top {k}: {ideal_rels}")
idcg = dcg_at_k(ideal_rels, k)
print(f"IDCG@5 = {idcg:.3f}")

ndcg = ndcg_at_k(relevances, k)
print(f"\nNDCG@5 = {dcg:.3f} / {idcg:.3f} = {ndcg:.3f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== NDCG Evaluation ===
Relevance in Recommendation Order: [3, 2, 0, 1, 4, 0, 2, 3, 1, 0]
Ideal Order: [4, 3, 3, 2, 2, 1, 1, 0, 0, 0]

=== K=3 ===
DCG@3: 7.500
IDCG@3: 11.131
NDCG@3: 0.674

=== K=5 ===
DCG@5: 16.714
IDCG@5: 19.714
NDCG@5: 0.848

=== K=10 ===
DCG@10: 20.344
IDCG@10: 23.344
NDCG@10: 0.871

=== Detailed Calculation (K=5) ===
Top 5 Relevances: [3, 2, 0, 1, 4]
Position 1: rel=3, gain=7, discount=1.000, contribution=7.000
Position 2: rel=2, gain=3, discount=1.585, contribution=1.893
Position 3: rel=0, gain=0, discount=2.000, contribution=0.000
Position 4: rel=1, gain=1, discount=2.322, contribution=0.431
Position 5: rel=4, gain=15, discount=2.585, contribution=5.803

DCG@5 = 15.127

Ideal Top 5: [4, 3, 3, 2, 2]
IDCG@5 = 19.714

NDCG@5 = 15.127 / 19.714 = 0.767
</code></pre>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Ricci, F., Rokach, L., &amp; Shapira, B. (2015). <em>Recommender Systems Handbook</em> (2nd ed.). Springer.</li>
<li>Aggarwal, C. C. (2016). <em>Recommender Systems: The Textbook</em>. Springer.</li>
<li>Falk, K. (2019). <em>Practical Recommender Systems</em>. Manning Publications.</li>
<li>Harper, F. M., &amp; Konstan, J. A. (2015). The MovieLens Datasets: History and Context. <em>ACM Transactions on Interactive Intelligent Systems</em>, 5(4), 1-19.</li>
<li>Koren, Y., Bell, R., &amp; Volinsky, C. (2009). Matrix Factorization Techniques for Recommender Systems. <em>Computer</em>, 42(8), 30-37.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter2-collaborative-filtering.html">Next Chapter: Collaborative Filtering ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operability, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material is subject to change, update, or discontinuation without notice.</li>
<li>The copyright and license of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
