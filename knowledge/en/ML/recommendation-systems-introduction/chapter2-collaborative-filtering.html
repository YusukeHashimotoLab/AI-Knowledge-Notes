<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Collaborative Filtering - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/recommendation-systems-introduction/index.html">Recommendation Systems</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 2: Collaborative Filtering</h1>
<p class="subtitle">Core Technologies for Recommendation Systems that Learn from User Preference Patterns</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code examples: 10</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Collaborative Filtering. You will learn principles of collaborative filtering, similarity metrics (Cosine, and advanced techniques such as SVD++.</p>
<h2>Learning Objectives</h2>
<p>After completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the principles of collaborative filtering and the differences between Memory-based and Model-based approaches</li>
<li>‚úÖ Implement User-based and Item-based collaborative filtering</li>
<li>‚úÖ Select appropriate similarity metrics (Cosine, Pearson)</li>
<li>‚úÖ Master the theory and implementation of matrix factorization (SVD, ALS)</li>
<li>‚úÖ Apply advanced techniques such as SVD++, NMF, and BPR</li>
<li>‚úÖ Build practical recommendation systems using the Surprise library</li>
</ul>
<hr/>
<h2>2.1 Principles of Collaborative Filtering</h2>
<h3>What is Collaborative Filtering?</h3>
<p><strong>Collaborative Filtering</strong> is a technique that makes recommendations by finding users or items with similar preferences based on the behavioral patterns of many users.</p>
<blockquote>
<p>It is based on the assumption that "users with similar preferences will have similar ratings for unknown items."</p>
</blockquote>
<h3>Memory-based vs Model-based</h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Characteristics</th>
<th>Method Examples</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Memory-based</strong></td>
<td>Directly uses rating data</td>
<td>User-based CF, Item-based CF</td>
<td>High interpretability, easy implementation</td>
<td>Scalability challenges</td>
</tr>
<tr>
<td><strong>Model-based</strong></td>
<td>Learns latent factor models</td>
<td>SVD, ALS, Matrix Factorization</td>
<td>High accuracy, scalable</td>
<td>Low interpretability</td>
</tr>
</tbody>
</table>
<h3>Overview of Collaborative Filtering</h3>
<div class="mermaid">
graph TD
    A[Collaborative Filtering] --&gt; B[Memory-based]
    A --&gt; C[Model-based]

    B --&gt; D[User-based CF]
    B --&gt; E[Item-based CF]

    C --&gt; F[Matrix Factorization]
    C --&gt; G[Neural Networks]

    F --&gt; H[SVD]
    F --&gt; I[ALS]
    F --&gt; J[NMF]

    style A fill:#e8f5e9
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#ffebee
    style E fill:#f3e5f5
    style F fill:#fce4ec
</div>
<h3>Representation of Rating Matrix</h3>
<p>The foundation of collaborative filtering is the <strong>Rating Matrix</strong> $R$:</p>
<p>$$
R = \begin{bmatrix}
r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1m} \\
r_{21} &amp; r_{22} &amp; \cdots &amp; r_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
r_{n1} &amp; r_{n2} &amp; \cdots &amp; r_{nm}
\end{bmatrix}
$$</p>
<ul>
<li>$n$: Number of users</li>
<li>$m$: Number of items</li>
<li>$r_{ui}$: Rating of user $u$ for item $i$ (mostly missing values)</li>
</ul>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: $$
R = \begin{bmatrix}
r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1m} \\


Purpose: Demonstrate data manipulation and preprocessing
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd

# Create sample rating matrix
np.random.seed(42)
users = ['Alice', 'Bob', 'Carol', 'David', 'Eve']
items = ['Item1', 'Item2', 'Item3', 'Item4', 'Item5']

# Rating matrix (NaN = not rated)
ratings = pd.DataFrame([
    [5, 3, np.nan, 1, np.nan],
    [4, np.nan, np.nan, 1, np.nan],
    [1, 1, np.nan, 5, 4],
    [1, np.nan, np.nan, 4, 4],
    [np.nan, 1, 5, 4, np.nan]
], columns=items, index=users)

print("=== Rating Matrix ===")
print(ratings)
print(f"\nShape: {ratings.shape}")
print(f"Rated cells: {ratings.notna().sum().sum()}/{ratings.size}")
print(f"Density: {ratings.notna().sum().sum()/ratings.size:.2%}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Rating Matrix ===
       Item1  Item2  Item3  Item4  Item5
Alice    5.0    3.0    NaN    1.0    NaN
Bob      4.0    NaN    NaN    1.0    NaN
Carol    1.0    1.0    NaN    5.0    4.0
David    1.0    NaN    NaN    4.0    4.0
Eve      NaN    1.0    5.0    4.0    NaN

Shape: (5, 5)
Rated cells: 13/25
Density: 52.00%
</code></pre>
<hr/>
<h2>2.2 User-based Collaborative Filtering</h2>
<h3>Basic Principle</h3>
<p><strong>User-based CF</strong> is a method that "recommends items that users with similar preferences like."</p>
<h4>Algorithm Flow</h4>
<ol>
<li><strong>Calculate User Similarity</strong>: Compute similarity between all user pairs</li>
<li><strong>Select Neighbor Users</strong>: Select k users similar to the target user</li>
<li><strong>Predict Rating</strong>: Calculate weighted average from neighbor users' ratings</li>
</ol>
<h3>Similarity Metrics</h3>
<h4>1. Cosine Similarity</h4>
<p>$$
\text{sim}_{\text{cos}}(u, v) = \frac{\sum_{i \in I_{uv}} r_{ui} \cdot r_{vi}}{\sqrt{\sum_{i \in I_{uv}} r_{ui}^2} \cdot \sqrt{\sum_{i \in I_{uv}} r_{vi}^2}}
$$</p>
<ul>
<li>$I_{uv}$: Set of items rated by both users $u$ and $v$</li>
</ul>
<h4>2. Pearson Correlation Coefficient</h4>
<p>$$
\text{sim}_{\text{pearson}}(u, v) = \frac{\sum_{i \in I_{uv}} (r_{ui} - \bar{r}_u)(r_{vi} - \bar{r}_v)}{\sqrt{\sum_{i \in I_{uv}} (r_{ui} - \bar{r}_u)^2} \cdot \sqrt{\sum_{i \in I_{uv}} (r_{vi} - \bar{r}_v)^2}}
$$</p>
<ul>
<li>$\bar{r}_u$: Average rating of user $u$</li>
</ul>
<h3>Rating Prediction Formula</h3>
<p>$$
\hat{r}_{ui} = \bar{r}_u + \frac{\sum_{v \in N(u)} \text{sim}(u, v) \cdot (r_{vi} - \bar{r}_v)}{\sum_{v \in N(u)} |\text{sim}(u, v)|}
$$</p>
<ul>
<li>$N(u)$: Set of neighbor users for user $u$</li>
</ul>
<h3>Complete Implementation Example</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import numpy as np
import pandas as pd
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr

class UserBasedCF:
    def __init__(self, k=3, similarity='cosine'):
        """
        User-based Collaborative Filtering

        Parameters:
        -----------
        k : int
            Number of neighbor users
        similarity : str
            Similarity metric ('cosine' or 'pearson')
        """
        self.k = k
        self.similarity = similarity
        self.ratings = None
        self.user_mean = None

    def fit(self, ratings_df):
        """Train with rating matrix"""
        self.ratings = ratings_df.copy()
        self.user_mean = self.ratings.mean(axis=1)
        return self

    def _compute_similarity(self, user1, user2):
        """Compute similarity between two users"""
        # Extract items rated by both users
        mask = self.ratings.loc[user1].notna() &amp; self.ratings.loc[user2].notna()

        if mask.sum() == 0:
            return 0  # No common ratings

        r1 = self.ratings.loc[user1][mask].values
        r2 = self.ratings.loc[user2][mask].values

        if self.similarity == 'cosine':
            # Cosine similarity
            if np.linalg.norm(r1) == 0 or np.linalg.norm(r2) == 0:
                return 0
            return 1 - cosine(r1, r2)

        elif self.similarity == 'pearson':
            # Pearson correlation coefficient
            if len(r1) &lt; 2:
                return 0
            corr, _ = pearsonr(r1, r2)
            return corr if not np.isnan(corr) else 0

    def predict(self, user, item):
        """Predict rating for specific user-item pair"""
        if user not in self.ratings.index:
            return self.ratings[item].mean()  # Fallback

        # Compute similarity with all users
        similarities = []
        for other_user in self.ratings.index:
            if other_user == user:
                continue
            # Only users who have rated the item
            if pd.notna(self.ratings.loc[other_user, item]):
                sim = self._compute_similarity(user, other_user)
                similarities.append((other_user, sim))

        # Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Select top k neighbors
        neighbors = similarities[:self.k]

        if len(neighbors) == 0:
            return self.user_mean[user]  # Fallback

        # Predict with weighted average
        numerator = 0
        denominator = 0

        for neighbor, sim in neighbors:
            if sim &gt; 0:
                rating = self.ratings.loc[neighbor, item]
                neighbor_mean = self.user_mean[neighbor]
                numerator += sim * (rating - neighbor_mean)
                denominator += abs(sim)

        if denominator == 0:
            return self.user_mean[user]

        prediction = self.user_mean[user] + numerator / denominator

        # Clip to rating range
        return np.clip(prediction, self.ratings.min().min(), self.ratings.max().max())

# Usage example
cf = UserBasedCF(k=2, similarity='cosine')
cf.fit(ratings)

# Prediction
user = 'Alice'
item = 'Item3'
prediction = cf.predict(user, item)

print(f"\n=== User-based CF Prediction ===")
print(f"User: {user}")
print(f"Item: {item}")
print(f"Predicted rating: {prediction:.2f}")

# Predict all unrated items
print(f"\n=== All predictions for {user} ===")
for item in ratings.columns:
    if pd.isna(ratings.loc[user, item]):
        pred = cf.predict(user, item)
        print(f"{item}: {pred:.2f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== User-based CF Prediction ===
User: Alice
Item: Item3
Predicted rating: 4.50

=== All predictions for Alice ===
Item3: 4.50
Item5: 4.00
</code></pre>
<h3>Visualization of Similarity Matrix</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - seaborn&gt;=0.12.0

"""
Example: Visualization of Similarity Matrix

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Compute similarity matrix between all users
cf_cosine = UserBasedCF(k=2, similarity='cosine')
cf_cosine.fit(ratings)

similarity_matrix = pd.DataFrame(
    index=ratings.index,
    columns=ratings.index,
    dtype=float
)

for u1 in ratings.index:
    for u2 in ratings.index:
        if u1 == u2:
            similarity_matrix.loc[u1, u2] = 1.0
        else:
            similarity_matrix.loc[u1, u2] = cf_cosine._compute_similarity(u1, u2)

print("=== User Similarity Matrix (Cosine) ===")
print(similarity_matrix)

# Visualize with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(similarity_matrix.astype(float), annot=True, fmt='.2f',
            cmap='coolwarm', center=0, vmin=-1, vmax=1,
            xticklabels=similarity_matrix.columns,
            yticklabels=similarity_matrix.index)
plt.title('User-User Similarity Matrix (Cosine)', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>2.3 Item-based Collaborative Filtering</h2>
<h3>Basic Principle</h3>
<p><strong>Item-based CF</strong> is a method that "recommends items similar to items the user liked."</p>
<h4>Differences from User-based</h4>
<table>
<thead>
<tr>
<th>Property</th>
<th>User-based CF</th>
<th>Item-based CF</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Similarity Calculation</strong></td>
<td>Between users</td>
<td>Between items</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Depends on number of users</td>
<td>Depends on number of items</td>
</tr>
<tr>
<td><strong>Application Scenarios</strong></td>
<td>Users &lt; Items</td>
<td>Users &gt; Items</td>
</tr>
<tr>
<td><strong>Stability</strong></td>
<td>Highly affected by preference changes</td>
<td>Item characteristics are stable</td>
</tr>
<tr>
<td><strong>Practical Examples</strong></td>
<td>Niche communities</td>
<td>Amazon, Netflix</td>
</tr>
</tbody>
</table>
<h3>Rating Prediction Formula</h3>
<p>$$
\hat{r}_{ui} = \frac{\sum_{j \in N(i)} \text{sim}(i, j) \cdot r_{uj}}{\sum_{j \in N(i)} |\text{sim}(i, j)|}
$$</p>
<ul>
<li>$N(i)$: Set of neighbor items for item $i$ (rated by user $u$)</li>
</ul>
<h3>Complete Implementation Example</h3>
<pre><code class="language-python">class ItemBasedCF:
    def __init__(self, k=3, similarity='cosine'):
        """
        Item-based Collaborative Filtering

        Parameters:
        -----------
        k : int
            Number of neighbor items
        similarity : str
            Similarity metric ('cosine' or 'pearson')
        """
        self.k = k
        self.similarity = similarity
        self.ratings = None

    def fit(self, ratings_df):
        """Train with rating matrix"""
        self.ratings = ratings_df.copy()
        return self

    def _compute_similarity(self, item1, item2):
        """Compute similarity between two items"""
        # Extract users who rated both items
        mask = self.ratings[item1].notna() &amp; self.ratings[item2].notna()

        if mask.sum() == 0:
            return 0  # No common ratings

        r1 = self.ratings[item1][mask].values
        r2 = self.ratings[item2][mask].values

        if self.similarity == 'cosine':
            if np.linalg.norm(r1) == 0 or np.linalg.norm(r2) == 0:
                return 0
            return 1 - cosine(r1, r2)

        elif self.similarity == 'pearson':
            if len(r1) &lt; 2:
                return 0
            corr, _ = pearsonr(r1, r2)
            return corr if not np.isnan(corr) else 0

    def predict(self, user, item):
        """Predict rating for specific user-item pair"""
        if user not in self.ratings.index:
            return self.ratings[item].mean()

        # Compute similarity with other items rated by user
        similarities = []
        for other_item in self.ratings.columns:
            if other_item == item:
                continue
            # Only items rated by the user
            if pd.notna(self.ratings.loc[user, other_item]):
                sim = self._compute_similarity(item, other_item)
                similarities.append((other_item, sim))

        # Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Select top k neighbors
        neighbors = similarities[:self.k]

        if len(neighbors) == 0:
            return self.ratings[item].mean()

        # Predict with weighted average
        numerator = 0
        denominator = 0

        for neighbor, sim in neighbors:
            if sim &gt; 0:
                rating = self.ratings.loc[user, neighbor]
                numerator += sim * rating
                denominator += abs(sim)

        if denominator == 0:
            return self.ratings[item].mean()

        prediction = numerator / denominator

        # Clip to rating range
        return np.clip(prediction, self.ratings.min().min(), self.ratings.max().max())

# Usage example
item_cf = ItemBasedCF(k=2, similarity='cosine')
item_cf.fit(ratings)

user = 'Alice'
item = 'Item3'
prediction = item_cf.predict(user, item)

print(f"\n=== Item-based CF Prediction ===")
print(f"User: {user}")
print(f"Item: {item}")
print(f"Predicted rating: {prediction:.2f}")

# User-based vs Item-based comparison
user_pred = cf.predict(user, item)
item_pred = item_cf.predict(user, item)

print(f"\n=== Method Comparison ===")
print(f"User-based CF: {user_pred:.2f}")
print(f"Item-based CF: {item_pred:.2f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Item-based CF Prediction ===
User: Alice
Item: Item3
Predicted rating: 4.00

=== Method Comparison ===
User-based CF: 4.50
Item-based CF: 4.00
</code></pre>
<h3>Visualization of Item Similarity Matrix</h3>
<pre><code class="language-python"># Compute item similarity matrix
item_similarity_matrix = pd.DataFrame(
    index=ratings.columns,
    columns=ratings.columns,
    dtype=float
)

for i1 in ratings.columns:
    for i2 in ratings.columns:
        if i1 == i2:
            item_similarity_matrix.loc[i1, i2] = 1.0
        else:
            item_similarity_matrix.loc[i1, i2] = item_cf._compute_similarity(i1, i2)

print("=== Item Similarity Matrix (Cosine) ===")
print(item_similarity_matrix)

# Visualize with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(item_similarity_matrix.astype(float), annot=True, fmt='.2f',
            cmap='viridis', center=0.5, vmin=0, vmax=1,
            xticklabels=item_similarity_matrix.columns,
            yticklabels=item_similarity_matrix.index)
plt.title('Item-Item Similarity Matrix (Cosine)', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>2.4 Matrix Factorization</h2>
<h3>Basic Concept</h3>
<p><strong>Matrix factorization</strong> is a technique that approximates the rating matrix $R$ as the product of two low-rank matrices:</p>
<p>$$
R \approx P \times Q^T
$$</p>
<ul>
<li>$P \in \mathbb{R}^{n \times k}$: User latent factor matrix</li>
<li>$Q \in \mathbb{R}^{m \times k}$: Item latent factor matrix</li>
<li>$k$: Number of latent factors (dimensionality reduction parameter)</li>
</ul>
<p>Predicted rating:</p>
<p>$$
\hat{r}_{ui} = p_u \cdot q_i^T = \sum_{f=1}^{k} p_{uf} \cdot q_{if}
$$</p>
<h3>SVD (Singular Value Decomposition)</h3>
<p>Singular value decomposition decomposes a matrix as follows:</p>
<p>$$
R = U \Sigma V^T
$$</p>
<ul>
<li>$U$: Left singular vectors (user latent factors)</li>
<li>$\Sigma$: Diagonal matrix of singular values</li>
<li>$V$: Right singular vectors (item latent factors)</li>
</ul>
<h3>Implementation Using Surprise Library</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Implementation Using Surprise Library

Purpose: Demonstrate machine learning model training and evaluation
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

from surprise import SVD, Dataset, Reader
from surprise.model_selection import cross_validate, train_test_split
from surprise import accuracy
import pandas as pd

# Prepare rating data (long format)
ratings_long = []
for user in ratings.index:
    for item in ratings.columns:
        if pd.notna(ratings.loc[user, item]):
            ratings_long.append({
                'user': user,
                'item': item,
                'rating': ratings.loc[user, item]
            })

df_long = pd.DataFrame(ratings_long)

print("=== Long Format Data ===")
print(df_long.head(10))

# Create Surprise dataset
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df_long[['user', 'item', 'rating']], reader)

# Train-test split
trainset, testset = train_test_split(data, test_size=0.25, random_state=42)

# Train SVD model
svd = SVD(n_factors=2, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)
svd.fit(trainset)

# Predict on test set
predictions = svd.test(testset)

# Evaluation metrics
rmse = accuracy.rmse(predictions, verbose=False)
mae = accuracy.mae(predictions, verbose=False)

print(f"\n=== SVD Model Evaluation ===")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")

# Predict for specific user
user = 'Alice'
item = 'Item3'
prediction = svd.predict(user, item)

print(f"\n=== Individual Prediction ===")
print(f"User: {user}")
print(f"Item: {item}")
print(f"Predicted rating: {prediction.est:.2f}")
print(f"Actual rating: {ratings.loc[user, item]}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Long Format Data ===
    user   item  rating
0  Alice  Item1     5.0
1  Alice  Item2     3.0
2  Alice  Item4     1.0
3    Bob  Item1     4.0
4    Bob  Item4     1.0

=== SVD Model Evaluation ===
RMSE: 0.8452
MAE: 0.6234

=== Individual Prediction ===
User: Alice
Item: Item3
Predicted rating: 4.23
Actual rating: nan
</code></pre>
<h3>ALS (Alternating Least Squares)</h3>
<p>ALS is a method that alternately optimizes $P$ and $Q$. It is suitable for implicit feedback.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: ALS is a method that alternately optimizes $P$ and $Q$. It i

Purpose: Demonstrate machine learning model training and evaluation
Target: Intermediate
Execution time: 30-60 seconds
Dependencies: None
"""

from implicit.als import AlternatingLeastSquares
from scipy.sparse import csr_matrix
import numpy as np

# Convert rating matrix to sparse matrix
# Fill NaN with 0 (for implicit: observed=1, unobserved=0)
ratings_binary = ratings.fillna(0)
ratings_binary[ratings_binary &gt; 0] = 1  # 1 if rated

# User and item index maps
user_to_idx = {user: idx for idx, user in enumerate(ratings.index)}
item_to_idx = {item: idx for idx, item in enumerate(ratings.columns)}

# Create sparse matrix (item x user)
sparse_ratings = csr_matrix(ratings_binary.values.T)

# ALS model
als_model = AlternatingLeastSquares(
    factors=5,
    regularization=0.01,
    iterations=20,
    random_state=42
)

# Train
als_model.fit(sparse_ratings)

print("\n=== ALS Model ===")
print(f"User latent factors: {als_model.user_factors.shape}")
print(f"Item latent factors: {als_model.item_factors.shape}")

# Recommend to specific user
user_idx = user_to_idx['Alice']
recommendations = als_model.recommend(
    user_idx,
    sparse_ratings[user_idx],
    N=3,
    filter_already_liked_items=True
)

print(f"\n=== Recommendations for Alice (ALS) ===")
idx_to_item = {idx: item for item, idx in item_to_idx.items()}
for item_idx, score in recommendations:
    print(f"{idx_to_item[item_idx]}: Score {score:.4f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== ALS Model ===
User latent factors: (5, 5)
Item latent factors: (5, 5)

=== Recommendations for Alice (ALS) ===
Item3: Score 0.2345
Item5: Score 0.1892
</code></pre>
<h3>Model Comparison with Cross-Validation</h3>
<pre><code class="language-python">from surprise import SVD, NMF, KNNBasic
from surprise.model_selection import cross_validate

# Compare multiple algorithms
algorithms = {
    'SVD': SVD(n_factors=5, random_state=42),
    'NMF': NMF(n_factors=5, random_state=42),
    'User-KNN': KNNBasic(sim_options={'user_based': True}),
    'Item-KNN': KNNBasic(sim_options={'user_based': False})
}

results = {}

print("\n=== Cross-Validation (5-fold) ===\n")
for name, algo in algorithms.items():
    cv_results = cross_validate(
        algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=False
    )

    results[name] = {
        'RMSE': cv_results['test_rmse'].mean(),
        'MAE': cv_results['test_mae'].mean()
    }

    print(f"{name}:")
    print(f"  RMSE: {results[name]['RMSE']:.4f} (+/- {cv_results['test_rmse'].std():.4f})")
    print(f"  MAE:  {results[name]['MAE']:.4f} (+/- {cv_results['test_mae'].std():.4f})\n")

# Visualize results
results_df = pd.DataFrame(results).T
print("=== Final Results ===")
print(results_df.sort_values('RMSE'))
</code></pre>
<hr/>
<h2>2.5 Advanced Techniques</h2>
<h3>SVD++ (Considering Implicit Feedback)</h3>
<p>SVD++ considers both explicit ratings and implicit feedback (such as viewing history).</p>
<p>$$
\hat{r}_{ui} = \mu + b_u + b_i + q_i^T \left( p_u + |I_u|^{-0.5} \sum_{j \in I_u} y_j \right)
$$</p>
<ul>
<li>$\mu$: Global average</li>
<li>$b_u$: User bias</li>
<li>$b_i$: Item bias</li>
<li>$I_u$: Set of items rated by user $u$</li>
<li>$y_j$: Latent factors for implicit feedback</li>
</ul>
<pre><code class="language-python">from surprise import SVDpp

# SVD++ model
svdpp = SVDpp(n_factors=5, n_epochs=20, lr_all=0.007, reg_all=0.02, random_state=42)

# Cross-validation
cv_results = cross_validate(svdpp, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)

print("=== SVD++ Evaluation ===")
print(f"RMSE: {cv_results['test_rmse'].mean():.4f} (+/- {cv_results['test_rmse'].std():.4f})")
print(f"MAE: {cv_results['test_mae'].mean():.4f} (+/- {cv_results['test_mae'].std():.4f})")

# Train and predict
trainset = data.build_full_trainset()
svdpp.fit(trainset)

user = 'Alice'
item = 'Item3'
pred = svdpp.predict(user, item)

print(f"\n=== SVD++ Prediction ===")
print(f"{user} ‚Üí {item}: {pred.est:.2f}")
</code></pre>
<h3>NMF (Non-negative Matrix Factorization)</h3>
<p>NMF is matrix factorization with non-negativity constraints:</p>
<p>$$
R \approx P \times Q^T, \quad P, Q \geq 0
$$</p>
<p>It has high interpretability and is suitable for applications like topic modeling.</p>
<pre><code class="language-python">from surprise import NMF

# NMF model
nmf = NMF(n_factors=5, n_epochs=50, random_state=42)

# Train
trainset = data.build_full_trainset()
nmf.fit(trainset)

# Predict
predictions = []
for user in ratings.index:
    for item in ratings.columns:
        if pd.isna(ratings.loc[user, item]):
            pred = nmf.predict(user, item)
            predictions.append({
                'user': user,
                'item': item,
                'prediction': pred.est
            })

pred_df = pd.DataFrame(predictions)

print("\n=== NMF Prediction Results ===")
print(pred_df.pivot(index='user', columns='item', values='prediction'))

# Visualize latent factors
print("\n=== User Latent Factors (NMF) ===")
user_factors_df = pd.DataFrame(
    nmf.pu,  # User latent factors
    index=ratings.index,
    columns=[f'Factor{i+1}' for i in range(nmf.n_factors)]
)
print(user_factors_df)
</code></pre>
<h3>BPR (Bayesian Personalized Ranking)</h3>
<p>BPR is a ranking optimization method for implicit feedback.</p>
<p><strong>Objective Function</strong>:</p>
<p>$$
\text{maximize} \quad \sum_{u,i,j} \ln \sigma(\hat{r}_{ui} - \hat{r}_{uj})
$$</p>
<ul>
<li>$i$: Item that user $u$ interacted with</li>
<li>$j$: Item that user $u$ did not interact with</li>
<li>$\sigma$: Sigmoid function</li>
</ul>
<pre><code class="language-python">from implicit.bpr import BayesianPersonalizedRanking

# BPR model
bpr_model = BayesianPersonalizedRanking(
    factors=10,
    learning_rate=0.01,
    regularization=0.01,
    iterations=100,
    random_state=42
)

# Train (implicit library requires item x user transpose)
bpr_model.fit(sparse_ratings)

print("\n=== BPR Model ===")
print(f"Number of latent factors: {bpr_model.factors}")

# Recommend
user_idx = user_to_idx['Alice']
recommendations = bpr_model.recommend(
    user_idx,
    sparse_ratings[user_idx],
    N=5,
    filter_already_liked_items=True
)

print(f"\n=== Recommendations for Alice (BPR) ===")
for item_idx, score in recommendations:
    print(f"{idx_to_item[item_idx]}: Score {score:.4f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== BPR Model ===
Number of latent factors: 10

=== Recommendations for Alice (BPR) ===
Item3: Score 0.3421
Item5: Score 0.2876
</code></pre>
<h3>Comprehensive Comparison of Methods</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Data Type</th>
<th>Complexity</th>
<th>Interpretability</th>
<th>Accuracy</th>
<th>Application Scenarios</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>User-based CF</strong></td>
<td>Explicit</td>
<td>O(n¬≤m)</td>
<td>High</td>
<td>Medium</td>
<td>Small-scale, few users</td>
</tr>
<tr>
<td><strong>Item-based CF</strong></td>
<td>Explicit</td>
<td>O(nm¬≤)</td>
<td>High</td>
<td>Medium</td>
<td>E-commerce</td>
</tr>
<tr>
<td><strong>SVD</strong></td>
<td>Explicit</td>
<td>O(k¬∑iter¬∑nnz)</td>
<td>Medium</td>
<td>High</td>
<td>Netflix Prize</td>
</tr>
<tr>
<td><strong>SVD++</strong></td>
<td>Explicit + Implicit</td>
<td>O(k¬∑iter¬∑nnz)</td>
<td>Medium</td>
<td>Highest</td>
<td>Hybrid</td>
</tr>
<tr>
<td><strong>ALS</strong></td>
<td>Implicit</td>
<td>O(k¬≤¬∑iter¬∑nnz)</td>
<td>Medium</td>
<td>High</td>
<td>Viewing history, clicks</td>
</tr>
<tr>
<td><strong>NMF</strong></td>
<td>Explicit</td>
<td>O(k¬∑iter¬∑nnz)</td>
<td>High</td>
<td>Medium</td>
<td>Topic recommendation</td>
</tr>
<tr>
<td><strong>BPR</strong></td>
<td>Implicit</td>
<td>O(k¬∑iter¬∑nnz)</td>
<td>Low</td>
<td>High</td>
<td>Ranking optimization</td>
</tr>
</tbody>
</table>
<ul>
<li>$n$: Number of users, $m$: Number of items, $k$: Number of latent factors, $nnz$: Number of non-zero elements</li>
</ul>
<hr/>
<h2>2.6 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><p><strong>Principles of Collaborative Filtering</strong></p>
<ul>
<li>Two approaches: Memory-based (User/Item-based CF) and Model-based (Matrix Factorization)</li>
<li>Selection of similarity metrics (Cosine, Pearson)</li>
<li>Representation of rating matrix and sparsity challenges</li>
</ul></li>
<li><p><strong>User-based CF</strong></p>
<ul>
<li>Recommends based on similar users' preferences</li>
<li>High interpretability but scalability challenges</li>
<li>Effective when number of users is small</li>
</ul></li>
<li><p><strong>Item-based CF</strong></p>
<ul>
<li>Recommends similar items</li>
<li>Practical as item characteristics are stable</li>
<li>Adopted by Amazon, Netflix</li>
</ul></li>
<li><p><strong>Matrix Factorization (SVD, ALS)</strong></p>
<ul>
<li>High-accuracy predictions with latent factor models</li>
<li>Scalable for large-scale data</li>
<li>SVD for explicit, ALS/BPR for implicit</li>
</ul></li>
<li><p><strong>Advanced Techniques</strong></p>
<ul>
<li>SVD++: Leverages implicit feedback</li>
<li>NMF: Improved interpretability with non-negativity constraints</li>
<li>BPR: Specialized for implicit feedback with ranking optimization</li>
</ul></li>
</ol>
<h3>Challenges in Collaborative Filtering</h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Description</th>
<th>Solutions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cold Start Problem</strong></td>
<td>Difficulty recommending for new users/items</td>
<td>Content-based, hybrid approaches</td>
</tr>
<tr>
<td><strong>Sparsity</strong></td>
<td>Sparse rating data makes similarity calculation difficult</td>
<td>Dimensionality reduction, matrix factorization</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Computational cost increases with users/items</td>
<td>Approximation algorithms, distributed processing</td>
</tr>
<tr>
<td><strong>Gray Sheep Problem</strong></td>
<td>Difficulty recommending to users with unique preferences</td>
<td>Hybrid approaches, diversity consideration</td>
</tr>
</tbody>
</table>
<h3>To the Next Chapter</h3>
<p>In Chapter 3, we will learn about <strong>Content-Based Filtering</strong>:</p>
<ul>
<li>Item feature extraction</li>
<li>TF-IDF and text features</li>
<li>User profile construction</li>
<li>Similarity calculation and recommendation</li>
<li>Comparison with collaborative filtering</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Question 1 (Difficulty: easy)</h3>
<p>Explain the differences between User-based CF and Item-based CF from three perspectives: similarity calculation target, scalability, and application scenarios.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<ol>
<li><p><strong>Similarity Calculation Target</strong></p>
<ul>
<li>User-based CF: Calculates similarity between users</li>
<li>Item-based CF: Calculates similarity between items</li>
</ul></li>
<li><p><strong>Scalability</strong></p>
<ul>
<li>User-based CF: O(n¬≤) computational complexity for n users</li>
<li>Item-based CF: O(m¬≤) computational complexity for m items</li>
<li>Generally users &gt;&gt; items, so Item-based is more scalable</li>
</ul></li>
<li><p><strong>Application Scenarios</strong></p>
<ul>
<li>User-based CF: Scenarios with few users and diverse preferences (niche communities)</li>
<li>Item-based CF: Scenarios with many users and stable item characteristics (E-commerce, Netflix)</li>
</ul></li>
</ol>
</details>
<h3>Question 2 (Difficulty: medium)</h3>
<p>For the following rating matrix, manually calculate the cosine similarity between Alice and Bob.</p>
<pre><code>       Item1  Item2  Item3
Alice    5      3      1
Bob      4      2      2
</code></pre>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p>Cosine similarity formula:</p>
<p>$$
\text{sim}_{\text{cos}}(Alice, Bob) = \frac{\sum r_{Alice,i} \cdot r_{Bob,i}}{\sqrt{\sum r_{Alice,i}^2} \cdot \sqrt{\sum r_{Bob,i}^2}}
$$</p>
<p><strong>Calculation</strong>:</p>
<ol>
<li><p>Numerator (dot product):</p>
<pre><code>5√ó4 + 3√ó2 + 1√ó2 = 20 + 6 + 2 = 28
</code></pre></li>
<li><p>Denominator (product of norms):</p>
<pre><code>||Alice|| = ‚àö(5¬≤ + 3¬≤ + 1¬≤) = ‚àö(25 + 9 + 1) = ‚àö35 ‚âà 5.916
||Bob|| = ‚àö(4¬≤ + 2¬≤ + 2¬≤) = ‚àö(16 + 4 + 4) = ‚àö24 ‚âà 4.899
Denominator = 5.916 √ó 4.899 ‚âà 28.98
</code></pre></li>
<li><p>Cosine similarity:</p>
<pre><code>sim = 28 / 28.98 ‚âà 0.966
</code></pre></li>
</ol>
<p><strong>Verification with Python</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Verification with Python:

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np
from scipy.spatial.distance import cosine

alice = np.array([5, 3, 1])
bob = np.array([4, 2, 2])

similarity = 1 - cosine(alice, bob)
print(f"Cosine similarity: {similarity:.4f}")  # 0.9661
</code></pre>
</details>
<h3>Question 3 (Difficulty: medium)</h3>
<p>In matrix factorization, explain the advantages and disadvantages of increasing the number of latent factors $k$.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Advantages</strong>:</p>
<ol>
<li><strong>Improved Representation</strong>: Can capture more complex preference patterns</li>
<li><strong>Higher Accuracy</strong>: Better fitting on training data</li>
<li><strong>Expressing Fine Differences</strong>: Can distinguish subtle preference differences</li>
</ol>
<p><strong>Disadvantages</strong>:</p>
<ol>
<li><strong>Overfitting Risk</strong>: Excessive fitting to training data, reduced generalization</li>
<li><strong>Increased Computational Cost</strong>: Training and inference time increases (proportional to O(k))</li>
<li><strong>Sparsity Issues</strong>: Difficult to estimate many factors with limited data</li>
<li><strong>Reduced Interpretability</strong>: More factors make interpretation harder</li>
</ol>
<p><strong>Appropriate Selection</strong>:</p>
<ul>
<li>Consider data volume, sparsity, computational resources</li>
<li>Select optimal $k$ through cross-validation</li>
<li>Generally $k = 10 \sim 100$ is practical</li>
</ul>
</details>
<h3>Question 4 (Difficulty: hard)</h3>
<p>Using the following MovieLens-style data, build an SVD model and calculate the RMSE.</p>
<pre><code class="language-python">data = pd.DataFrame({
    'user_id': [1, 1, 1, 2, 2, 3, 3, 3, 4, 4],
    'item_id': ['A', 'B', 'C', 'A', 'C', 'B', 'C', 'D', 'A', 'D'],
    'rating': [5, 3, 4, 4, 5, 2, 3, 5, 3, 4]
})
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Using the following MovieLens-style data, build an SVD model

Purpose: Demonstrate machine learning model training and evaluation
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import pandas as pd
from surprise import SVD, Dataset, Reader
from surprise.model_selection import cross_validate, train_test_split
from surprise import accuracy

# Prepare data
data_df = pd.DataFrame({
    'user_id': [1, 1, 1, 2, 2, 3, 3, 3, 4, 4],
    'item_id': ['A', 'B', 'C', 'A', 'C', 'B', 'C', 'D', 'A', 'D'],
    'rating': [5, 3, 4, 4, 5, 2, 3, 5, 3, 4]
})

print("=== Data ===")
print(data_df)

# Create Surprise dataset
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(data_df[['user_id', 'item_id', 'rating']], reader)

# Train-test split
trainset, testset = train_test_split(data, test_size=0.3, random_state=42)

# SVD model
svd = SVD(n_factors=2, n_epochs=30, lr_all=0.005, reg_all=0.02, random_state=42)
svd.fit(trainset)

# Predict and calculate RMSE
predictions = svd.test(testset)
rmse = accuracy.rmse(predictions)

print(f"\n=== Model Evaluation ===")
print(f"RMSE: {rmse:.4f}")

# Cross-validation
cv_results = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)

print(f"\n=== 3-Fold Cross-Validation ===")
print(f"Average RMSE: {cv_results['test_rmse'].mean():.4f} (+/- {cv_results['test_rmse'].std():.4f})")
print(f"Average MAE: {cv_results['test_mae'].mean():.4f} (+/- {cv_results['test_mae'].std():.4f})")

# New predictions
trainset_full = data.build_full_trainset()
svd.fit(trainset_full)

print(f"\n=== New Predictions ===")
test_cases = [(1, 'D'), (2, 'B'), (4, 'C')]
for user, item in test_cases:
    pred = svd.predict(user, item)
    print(f"User {user} ‚Üí Item {item}: {pred.est:.2f}")
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>=== Data ===
   user_id item_id  rating
0        1       A       5
1        1       B       3
2        1       C       4
...

=== Model Evaluation ===
RMSE: 0.7823

=== 3-Fold Cross-Validation ===
Average RMSE: 0.8234 (+/- 0.1245)
Average MAE: 0.6543 (+/- 0.0987)

=== New Predictions ===
User 1 ‚Üí Item D: 4.12
User 2 ‚Üí Item B: 3.45
User 4 ‚Üí Item C: 3.78
</code></pre>
</details>
<h3>Question 5 (Difficulty: hard)</h3>
<p>Explain what the "Cold Start Problem" is in collaborative filtering and describe three solution approaches with specific examples.</p>
<details>
<summary>Sample Answer</summary>
<p><strong>Answer</strong>:</p>
<p><strong>What is the Cold Start Problem</strong>:</p>
<p>The difficulty of making recommendations with collaborative filtering for new users or new items due to the absence of historical behavioral data.</p>
<p><strong>Three Solution Approaches</strong>:</p>
<ol>
<li><p><strong>Combining Content-Based Filtering</strong></p>
<ul>
<li>Example: For a new movie, recommend similar movies using metadata such as genre, director, and actors</li>
<li>Advantage: Can recommend without user behavioral data</li>
<li>Implementation: Create features using TF-IDF or Word2Vec and recommend using cosine similarity</li>
</ul></li>
<li><p><strong>Hybrid Approaches</strong></p>
<ul>
<li>Example: Netflix combines collaborative filtering + content-based + popularity</li>
<li>Advantage: Complements weaknesses of each method</li>
<li>Implementation: Weighted linear combination or stacking</li>
</ul></li>
<li><p><strong>Active Learning (Collecting Initial Ratings)</strong></p>
<ul>
<li>Example: Spotify asks users to select favorite artists during initial registration</li>
<li>Advantage: Understands preferences from few ratings</li>
<li>Implementation: Collect initial ratings from popular items or diverse genres</li>
</ul></li>
</ol>
<p><strong>Additional Approaches</strong>:</p>
<ul>
<li><strong>Popularity-Based Recommendation</strong>: Recommend generally popular items when no rating data exists</li>
<li><strong>Demographic Information</strong>: Estimate similar user groups from age, gender, etc.</li>
<li><strong>Transfer Learning</strong>: Leverage preference data from other domains</li>
</ul>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>Ricci, F., Rokach, L., &amp; Shapira, B. (2015). <em>Recommender Systems Handbook</em> (2nd ed.). Springer.</li>
<li>Koren, Y., Bell, R., &amp; Volinsky, C. (2009). Matrix Factorization Techniques for Recommender Systems. <em>Computer</em>, 42(8), 30-37.</li>
<li>Aggarwal, C. C. (2016). <em>Recommender Systems: The Textbook</em>. Springer.</li>
<li>Rendle, S., Freudenthaler, C., Gantner, Z., &amp; Schmidt-Thieme, L. (2009). BPR: Bayesian Personalized Ranking from Implicit Feedback. <em>UAI 2009</em>.</li>
<li>Hu, Y., Koren, Y., &amp; Volinsky, C. (2008). Collaborative Filtering for Implicit Feedback Datasets. <em>ICDM 2008</em>.</li>
</ol>
<div class="navigation">
<a class="nav-button" href="chapter1-introduction.html">‚Üê Previous Chapter: Introduction to Recommendation Systems</a>
<a class="nav-button" href="chapter3-content-based.html">Next Chapter: Content-Based Filtering ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Authors</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-21</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
