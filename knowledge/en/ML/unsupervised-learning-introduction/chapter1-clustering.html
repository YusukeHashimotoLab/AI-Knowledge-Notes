<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Chapter 1: Clustering Algorithms | Unsupervised Learning</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>MathJax={tex:{inlineMath:[['$','$']],displayMath:[['$$','$$']]}}</script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
</head>
<body><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/unsupervised-learning-introduction/chapter1-clustering.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header><h1>Chapter 1: Clustering Algorithms</h1><p class="subtitle">K-Means, DBSCAN, and Hierarchical Clustering</p></header>
<div class="container">
<div class="breadcrumb"><a href="../index.html">ML Dojo</a> &gt; <a href="index.html">Unsupervised Learning</a> &gt; Ch1</div>
<div class="content">
<h2>1.1 K-Means Clustering</h2>
<p>K-Means partitions data into K clusters by minimizing within-cluster variance.</p>
<div class="definition"><strong>üìê K-Means Objective:</strong>
$$\min_C \sum_{i=1}^K \sum_{x \in C_i} \|x - \mu_i\|^2$$
where $\mu_i$ is centroid of cluster $C_i$</div>
<h3>üíª Code Example 1: K-Means Implementation</h3>
<div class="code-example"><pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.datasets import make_blobs, make_moons
from sklearn.metrics import silhouette_score, davies_bouldin_score

class ClusteringAnalysis:
    &quot;&quot;&quot;Comprehensive clustering analysis&quot;&quot;&quot;
    
    def __init__(self, algorithm='kmeans', n_clusters=3):
        self.algorithm = algorithm
        self.n_clusters = n_clusters
        self.model = None
    
    def fit(self, X):
        &quot;&quot;&quot;Fit clustering model&quot;&quot;&quot;
        if self.algorithm == 'kmeans':
            self.model = KMeans(n_clusters=self.n_clusters, random_state=42)
        elif self.algorithm == 'dbscan':
            self.model = DBSCAN(eps=0.5, min_samples=5)
        elif self.algorithm == 'hierarchical':
            self.model = AgglomerativeClustering(n_clusters=self.n_clusters)
        
        self.labels_ = self.model.fit_predict(X)
        return self
    
    def evaluate(self, X):
        &quot;&quot;&quot;Evaluate clustering quality&quot;&quot;&quot;
        metrics = {}
        
        # Silhouette score
        if len(np.unique(self.labels_)) &gt; 1:
            metrics['silhouette'] = silhouette_score(X, self.labels_)
            metrics['davies_bouldin'] = davies_bouldin_score(X, self.labels_)
        
        # Inertia (K-Means only)
        if hasattr(self.model, 'inertia_'):
            metrics['inertia'] = self.model.inertia_
        
        metrics['n_clusters'] = len(np.unique(self.labels_))
        
        return metrics
    
    def find_optimal_k(self, X, k_range=(2, 10)):
        &quot;&quot;&quot;Find optimal number of clusters using elbow method&quot;&quot;&quot;
        inertias = []
        silhouettes = []
        
        for k in range(k_range[0], k_range[1] + 1):
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(X)
            
            inertias.append(kmeans.inertia_)
            silhouettes.append(silhouette_score(X, labels))
        
        # Plot elbow curve
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        k_values = range(k_range[0], k_range[1] + 1)
        ax1.plot(k_values, inertias, 'bo-', linewidth=2, markersize=8)
        ax1.set_xlabel('Number of Clusters (K)')
        ax1.set_ylabel('Inertia')
        ax1.set_title('Elbow Method')
        ax1.grid(True, alpha=0.3)
        
        ax2.plot(k_values, silhouettes, 'ro-', linewidth=2, markersize=8)
        ax2.set_xlabel('Number of Clusters (K)')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Silhouette Analysis')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return k_values, inertias, silhouettes

# Example usage
# Generate synthetic data
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# K-Means clustering
clustering = ClusteringAnalysis(algorithm='kmeans', n_clusters=4)
clustering.fit(X)

metrics = clustering.evaluate(X)
print(f&quot;Silhouette Score: {metrics['silhouette']:.3f}&quot;)
print(f&quot;Davies-Bouldin Index: {metrics['davies_bouldin']:.3f}&quot;)

# Find optimal K
k_values, inertias, silhouettes = clustering.find_optimal_k(X, k_range=(2, 10))</code></pre></div>
<h2>1.2-1.7 Additional Clustering Methods</h2>
<p>DBSCAN for density-based clustering, hierarchical clustering, Gaussian mixture models, evaluation metrics.</p>
<h3>üíª Code Examples 2-7</h3>
<div class="code-example"><pre><code class="language-python"># DBSCAN implementation and parameter tuning
# Hierarchical clustering with dendrograms
# Gaussian Mixture Models (GMM)
# Cluster evaluation metrics
# Dimensionality reduction + clustering
# Real-world applications
# See complete implementations</code></pre></div>
<h2>üìù Exercises</h2>
<div class="exercise"><ol>
<li>Apply K-Means to Iris dataset and determine optimal K.</li>
<li>Compare K-Means vs DBSCAN on moon-shaped data.</li>
<li>Create dendrogram for hierarchical clustering.</li>
<li>Implement GMM and compare with K-Means.</li>
<li>Evaluate clustering using multiple metrics (silhouette, DB index, Calinski-Harabasz).</li>
</ol></div>
<h2>Summary</h2>
<ul>
<li>K-Means: partitional clustering minimizing within-cluster variance</li>
<li>DBSCAN: density-based, finds arbitrary shapes, handles noise</li>
<li>Hierarchical: creates dendrogram, agglomerative or divisive</li>
<li>GMM: probabilistic clustering with Gaussian distributions</li>
<li>Evaluation: silhouette score, Davies-Bouldin index, elbow method</li>
<li>Applications: customer segmentation, image compression, anomaly detection</li>
</ul>
<div class="nav-buttons">
<a class="nav-button" href="index.html">‚Üê Overview</a>
<a class="nav-button" href="chapter2-dimensionality-reduction.html">Ch2: Dimensionality Reduction ‚Üí</a>
</div>
</div>
</div>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranty, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>

<footer><p>¬© 2025 AI Terakoya - ML Dojo</p></footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
