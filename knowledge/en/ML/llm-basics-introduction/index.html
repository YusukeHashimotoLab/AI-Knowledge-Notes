<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Introduction to LLM Basics - A practical educational series on the mechanisms and applications of Large Language Models">
    <title>Introduction to LLM Basics Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            margin-bottom: 1rem;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Chapter Cards */
        .chapter-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .chapter-card {
            background: white;
            border-radius: 8px;
            padding: 1.5rem;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            transition: all 0.3s;
            border: 2px solid transparent;
        }

        .chapter-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 4px 16px rgba(0,0,0,0.15);
            border-color: var(--secondary-color);
        }

        .chapter-card h3 {
            margin-top: 0;
            font-size: 1.3rem;
        }

        .chapter-card .chapter-meta {
            font-size: 0.85rem;
            color: #666;
            margin-bottom: 1rem;
        }

        .chapter-card.ch1 {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        }

        .chapter-card.ch2 {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
        }

        .chapter-card.ch3 {
            background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
        }

        .chapter-card.ch4 {
            background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
        }

        .chapter-card.ch5 {
            background: linear-gradient(135deg, #fce4ec 0%, #f8bbd0 100%);
        }

        .chapter-card.coming-soon {
            opacity: 0.6;
            cursor: not-allowed;
        }

        .chapter-card.coming-soon:hover {
            transform: none;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-color: transparent;
        }

        .coming-soon-badge {
            display: inline-block;
            background: #666;
            color: white;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-size: 0.75rem;
            margin-left: 0.5rem;
        }

        /* Learning Paths */
        .learning-path {
            background: #f8f9fa;
            border-left: 4px solid var(--secondary-color);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .learning-path h4 {
            margin-top: 0;
            color: var(--secondary-color);
        }

        /* FAQ */
        .faq {
            background: #f8f9fa;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .faq h4 {
            margin-top: 0;
            color: var(--primary-color);
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Locale Switcher */
        .locale-switcher {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: rgba(255, 255, 255, 0.2);
            padding: 0.5rem 1rem;
            border-radius: 20px;
            backdrop-filter: blur(10px);
        }

        .locale-switcher a {
            color: white;
            text-decoration: none;
            font-weight: 500;
            opacity: 0.9;
        }

        .locale-switcher a:hover {
            opacity: 1;
        }

        .locale-switcher .coming-soon-text {
            color: white;
            opacity: 0.6;
            font-size: 0.85rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .chapter-grid {
                grid-template-columns: 1fr;
            }

            .locale-switcher {
                position: static;
                margin-bottom: 1rem;
                text-align: center;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js initialization
        document.addEventListener('DOMContentLoaded', function() {
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
            }
        });
    </script>
</head>
<body>
    <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Introduction to LLM Basics</span>
        </div>
    </nav>

    <header style="position: relative;">
        <div class="locale-switcher">
            <a href="../../../jp/ML/llm-basics-introduction/index.html">JP</a>
        </div>
        <div class="container">
            <h1>ü§ñ Introduction to LLM Basics Series v1.0</h1>
            <p class="subtitle">Understanding the Mechanisms and Applications of Large Language Models - Navigating the ChatGPT Era</p>
            <div class="meta">
                <span>üìñ Total Study Time: 120-150 min</span>
                <span>üìä Level: Beginner to Intermediate</span>
                <span>üíª Code Examples: 30+</span>
                <span>üìù Chapters: 5</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Understanding the Technology Behind ChatGPT - From LLM Fundamentals to Practical Applications</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is a comprehensive 5-chapter practical educational content for systematically learning the <strong>fundamentals of Large Language Models (LLMs)</strong>.</p>

        <p>As AI assistants like <strong>ChatGPT, Claude, and Gemini</strong> become integrated into daily life, understanding the underlying technology is essential for everyone living in the AI era. This series covers everything from the basic principles of LLMs to implementation, evaluation, and practical applications.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>‚úÖ <strong>Modern Approach</strong>: Explanation of latest models including ChatGPT, GPT-4, Claude, LLaMA</li>
            <li>‚úÖ <strong>Practical Code</strong>: Over 30 implementation examples using Hugging Face Transformers</li>
            <li>‚úÖ <strong>Progressive Learning</strong>: Theory ‚Üí Architecture ‚Üí Implementation ‚Üí Evaluation ‚Üí Application</li>
            <li>‚úÖ <strong>Diagrams and Visualization</strong>: Understanding Transformer architecture and Attention mechanisms through diagrams</li>
            <li>‚úÖ <strong>Practical Perspective</strong>: Prompt engineering, fine-tuning, RAG, etc.</li>
        </ul>

        <p><strong>Total Study Time</strong>: 120-150 minutes (including code execution and exercises)</p>

        <h2 id="learning-objectives">Learning Objectives</h2>

        <p>Upon completion of this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>‚úÖ Ability to explain what LLMs are, their history and evolution</li>
            <li>‚úÖ Understanding the basic structure of Transformer architecture</li>
            <li>‚úÖ Knowledge of Self-Attention and Multi-Head Attention mechanisms</li>
            <li>‚úÖ Understanding the roles of tokenization and positional encoding</li>
            <li>‚úÖ Ability to explain the difference between pre-training and fine-tuning</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>‚úÖ Ability to use LLMs with Hugging Face Transformers</li>
            <li>‚úÖ Design effective prompts (prompt engineering)</li>
            <li>‚úÖ Evaluate LLM output quality (BLEU, ROUGE, etc.)</li>
            <li>‚úÖ Implement simple fine-tuning</li>
            <li>‚úÖ Understand and implement RAG (Retrieval-Augmented Generation)</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>‚úÖ Propose appropriate LLM utilization methods for business challenges</li>
            <li>‚úÖ Understand and appropriately address LLM limitations and risks</li>
            <li>‚úÖ Track and understand latest LLM research trends</li>
            <li>‚úÖ Prepared to advance to more sophisticated LLM projects</li>
        </ul>

        <h2 id="learning">How to Learn</h2>

        <h3>Recommended Learning Sequence</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: What is LLM] --> B[Chapter 2: Transformer Architecture]
    B --> C[Chapter 3: LLM Implementation and Applications]
    C --> D[Chapter 4: LLM Evaluation and Improvement]
    D --> E[Chapter 5: Practical LLM Applications]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
        </div>

        <div class="learning-path">
            <h4>üéØ Complete Master Course (All chapters recommended)</h4>
            <p><strong>Target</strong>: Those who want to systematically learn LLMs, with basic machine learning knowledge</p>
            <p><strong>Path</strong>: Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5</p>
            <p><strong>Duration</strong>: 120-150 minutes</p>
            <p><strong>Outcome</strong>: Comprehensive acquisition from LLM theory to implementation, evaluation, and practice</p>
        </div>

        <div class="learning-path">
            <h4>‚ö° Practice-First Course</h4>
            <p><strong>Target</strong>: Those who want to use LLMs immediately, practice-oriented</p>
            <p><strong>Path</strong>: Chapter 1 (overview only) ‚Üí Chapter 3 (implementation) ‚Üí Chapter 5 (practice)</p>
            <p><strong>Duration</strong>: 70-80 minutes</p>
            <p><strong>Outcome</strong>: Basic LLM understanding and practical application skills</p>
        </div>

        <div class="learning-path">
            <h4>üîç Theory-Focused Course</h4>
            <p><strong>Target</strong>: Researchers and engineers who want deep understanding of LLM principles</p>
            <p><strong>Path</strong>: Chapter 1 ‚Üí Chapter 2 (detailed study) ‚Üí Chapter 4 ‚Üí related papers</p>
            <p><strong>Duration</strong>: 90-100 minutes + paper reading</p>
            <p><strong>Outcome</strong>: Deep understanding of Transformer theory and LLM research foundations</p>
        </div>

        <h2 id="prerequisites">Prerequisites</h2>

        <h3>Required Knowledge</h3>
        <ul>
            <li>üìå <strong>Python Basics</strong>: Basic syntax including variables, functions, lists, dictionaries</li>
            <li>üìå <strong>Machine Learning Basics</strong>: Concepts of training and testing, loss functions, optimization</li>
            <li>üìå <strong>Neural Network Basics</strong>: Multi-layer perceptron, activation functions</li>
        </ul>

        <h3>Recommended Knowledge (enhances understanding)</h3>
        <ul>
            <li>üí° Basic concepts of Natural Language Processing (NLP)</li>
            <li>üí° Basics of deep learning frameworks (PyTorch, TensorFlow)</li>
            <li>üí° Linear algebra basics (matrix operations, inner product)</li>
        </ul>

        <h3>Recommended Learning Resources (if prerequisites are lacking)</h3>
        <ul>
            <li>‚Üí <a href="../supervised-learning-introduction/index.html">Supervised Learning Introduction Series</a> - Python, NumPy, Pandas basics</li>
            <li>‚Üí <a href="../neural-networks-introduction/index.html">Neural Networks Introduction</a> - NN basics and PyTorch</li>
            <li>‚Üí <a href="../nlp-introduction/index.html">Natural Language Processing Introduction</a> - Basic NLP concepts</li>
        </ul>

        <h2 id="chapters">Chapter Details</h2>

        <div class="chapter-grid">
            <div class="chapter-card ch1">
                <h3><a href="./chapter-1.html">Chapter 1: What is LLM</a></h3>
                <div class="chapter-meta">
                    üìñ Reading Time: 25-30 min | üíª Code Examples: 5 | üìù Exercises: 3
                </div>
                <h4>Learning Content</h4>
                <ul>
                    <li>Definition and characteristics of LLMs</li>
                    <li>History of LLMs (from BERT, GPT, T5 to ChatGPT)</li>
                    <li>Representative LLMs (GPT-4, Claude, LLaMA, Gemini)</li>
                    <li>Transformer architecture basics</li>
                    <li>Tokenization mechanisms (BPE, WordPiece)</li>
                    <li>LLM use cases and limitations</li>
                </ul>
                <p><strong><a href="./chapter-1.html">Read Chapter 1 ‚Üí</a></strong></p>
            </div>

            <div class="chapter-card ch2 coming-soon">
                <h3>Chapter 2: Transformer Architecture <span class="coming-soon-badge">Coming Soon</span></h3>
                <div class="chapter-meta">
                    üìñ Reading Time: 30-35 min | üíª Code Examples: 8 | üìù Exercises: 4
                </div>
                <h4>Learning Content (planned)</h4>
                <ul>
                    <li>Detailed Self-Attention mechanism</li>
                    <li>Multi-Head Attention structure</li>
                    <li>Positional Encoding</li>
                    <li>Feed-Forward Network layer</li>
                    <li>Layer Normalization and residual connections</li>
                    <li>Encoder-Decoder and Decoder-Only models</li>
                </ul>
            </div>

            <div class="chapter-card ch3 coming-soon">
                <h3>Chapter 3: LLM Implementation and Applications <span class="coming-soon-badge">Coming Soon</span></h3>
                <div class="chapter-meta">
                    üìñ Reading Time: 30-35 min | üíª Code Examples: 10 | üìù Exercises: 5
                </div>
                <h4>Learning Content (planned)</h4>
                <ul>
                    <li>Hugging Face Transformers basics</li>
                    <li>Loading pre-trained models and inference</li>
                    <li>Prompt engineering techniques</li>
                    <li>Few-Shot Learning, Zero-Shot Learning</li>
                    <li>Adjusting text generation parameters (Temperature, Top-k, etc.)</li>
                    <li>Fine-tuning basics</li>
                </ul>
            </div>

            <div class="chapter-card ch4 coming-soon">
                <h3>Chapter 4: LLM Evaluation and Improvement <span class="coming-soon-badge">Coming Soon</span></h3>
                <div class="chapter-meta">
                    üìñ Reading Time: 25-30 min | üíª Code Examples: 6 | üìù Exercises: 4
                </div>
                <h4>Learning Content (planned)</h4>
                <ul>
                    <li>LLM evaluation metrics (BLEU, ROUGE, Perplexity)</li>
                    <li>Human evaluation and benchmarks</li>
                    <li>Bias and fairness issues</li>
                    <li>Countermeasures against hallucinations</li>
                    <li>RLHF (Reinforcement Learning from Human Feedback)</li>
                    <li>Model compression and efficiency</li>
                </ul>
            </div>

            <div class="chapter-card ch5 coming-soon">
                <h3>Chapter 5: Practical LLM Applications <span class="coming-soon-badge">Coming Soon</span></h3>
                <div class="chapter-meta">
                    üìñ Reading Time: 30-35 min | üíª Code Examples: 8 | üìù Exercises: 5
                </div>
                <h4>Learning Content (planned)</h4>
                <ul>
                    <li>Implementing RAG (Retrieval-Augmented Generation)</li>
                    <li>LLM applications using LangChain</li>
                    <li>Vector databases (Pinecone, Chroma)</li>
                    <li>Building agent-based AI</li>
                    <li>Deployment to production environment</li>
                    <li>Cost optimization and security</li>
                </ul>
            </div>
        </div>

        <h2 id="faq">Frequently Asked Questions (FAQ)</h2>

        <div class="faq">
            <h4>Q1: Can I learn LLMs without machine learning knowledge?</h4>
            <p><strong>A:</strong> Basic machine learning knowledge (training, testing, loss functions, etc.) is recommended. If you lack this knowledge, we recommend first studying our "<a href="../supervised-learning-introduction/index.html">Supervised Learning Introduction Series</a>" or "<a href="../neural-networks-introduction/index.html">Neural Networks Introduction</a>".</p>
        </div>

        <div class="faq">
            <h4>Q2: What is the difference between ChatGPT and GPT-4?</h4>
            <p><strong>A:</strong> ChatGPT is an application name, with GPT-3.5 or GPT-4 as the underlying model. GPT-4 is larger and more capable than GPT-3.5, enabling more complex reasoning and longer text understanding. This is explained in detail in Chapter 1.</p>
        </div>

        <div class="faq">
            <h4>Q3: What do I need to run the code?</h4>
            <p><strong>A:</strong> Python 3.8 or higher, and libraries like transformers, torch, numpy, pandas are required. Using Google Colab eliminates the need for environment setup and provides free GPU access. Setup instructions are provided in each chapter.</p>
        </div>

        <div class="faq">
            <h4>Q4: Do I need a high-performance GPU to run LLMs?</h4>
            <p><strong>A:</strong> For inference (using existing models), smaller models can run on CPU. GPUs are recommended for large models or fine-tuning, but you can try for free using Google Colab or Hugging Face Inference API.</p>
        </div>

        <div class="faq">
            <h4>Q5: What should I do after completing this series?</h4>
            <p><strong>A:</strong> You can proceed to more specialized series such as "Transformer Details," "Fine-tuning in Practice," or "RAG Implementation." Also, applying LLMs to actual projects deepens understanding.</p>
        </div>

        <div class="faq">
            <h4>Q6: Are there restrictions on commercial use of LLMs?</h4>
            <p><strong>A:</strong> Licenses vary by model. GPT-4 must comply with OpenAI API terms of service, LLaMA had research-only restrictions, but LLaMA 2 and later allow commercial use. This is explained in detail in each chapter.</p>
        </div>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Ready to begin? Start with Chapter 1 and explore the world of Large Language Models!</p>

        <div class="nav-buttons">
            <a href="../index.html" class="nav-button">‚Üê Machine Learning Top</a>
            <a href="./chapter-1.html" class="nav-button">Chapter 1: What is LLM ‚Üí</a>
        </div>

        <hr>

        <p><strong>Update History</strong></p>
        <ul>
            <li><strong>2025-12-01</strong>: v1.0 Initial release (Chapter 1 only)</li>
        </ul>

        <hr>

        <p><strong>Your LLM learning journey starts here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0), which typically include warranty disclaimers.</li>
        </ul>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
