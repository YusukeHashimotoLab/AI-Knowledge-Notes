<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Chapter 1: What is LLM - Learn the definition, history, and representative models of Large Language Models">
    <title>Chapter 1: What is LLM - LLM Basics Introduction - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            margin-bottom: 1rem;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
        }

        p code, li code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.85rem;
        }

        /* Info boxes */
        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196F3;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .info-box h4 {
            margin-top: 0;
            color: #1976D2;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box h4 {
            margin-top: 0;
            color: #f57c00;
        }

        .example-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .example-box h4 {
            margin-top: 0;
            color: #7b1fa2;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--code-bg);
            font-weight: 600;
            color: var(--primary-color);
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
            text-decoration: none;
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }

        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
            }
        }
    </style>

    <!-- MathJax for equations -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
            }
        });
    </script>
</head>
<body>
    <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="./index.html">LLM Basics Introduction</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

    <header>
        <div class="container">
            <h1>ü§ñ Chapter 1: What is LLM</h1>
            <p class="subtitle">Definition, History, and Future of Large Language Models</p>
            <div class="meta">
                <span>üìñ Reading Time: 25-30 min</span>
                <span>üìä Difficulty: Beginner</span>
                <span>üíª Code Examples: 5</span>
                <span>üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h2 id="intro">Introduction</h2>
        <p>Since 2023, with the advent of <strong>ChatGPT</strong>, AI technology has rapidly permeated into general society. The technology behind ChatGPT is the <strong>Large Language Model (LLM)</strong>.</p>

        <p>In this chapter, we will learn what LLMs are, the history that led to their current form, and what representative models exist.</p>

        <h2 id="definition">1.1 Definition of LLM</h2>

        <h3>What is a Large Language Model (LLM)</h3>
        <p>A <strong>Large Language Model (LLM)</strong> is a deep learning model trained on massive amounts of text data that performs natural language understanding and generation.</p>

        <div class="info-box">
            <h4>üìå Key Characteristics of LLMs</h4>
            <ul>
                <li><strong>Large-scale</strong>: Contains billions to trillions of parameters</li>
                <li><strong>Pre-training</strong>: Trained on large amounts of internet text</li>
                <li><strong>Versatility</strong>: Handles various tasks (summarization, translation, question answering, etc.)</li>
                <li><strong>Few-Shot Learning</strong>: Can learn from a small number of examples</li>
                <li><strong>Context Understanding</strong>: Responses that consider long context</li>
            </ul>
        </div>

        <h3>Basic Structure of LLMs</h3>
        <p>Most modern LLMs are based on the <strong>Transformer</strong> architecture. Transformer is a revolutionary neural network structure announced by Google in 2017.</p>

        <div class="mermaid">
graph TD
    A[Input Text] --> B[Tokenization]
    B --> C[Embedding Layer]
    C --> D[Transformer Layer x N]
    D --> E[Output Layer]
    E --> F[Predicted Text]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#e3f2fd
        </div>

        <h2 id="history">1.2 History of LLMs</h2>

        <h3>Evolution of Language Models</h3>
        <p>Language models have a long history, but they have developed rapidly since 2018.</p>

        <div class="mermaid">
timeline
    title Evolution of LLMs
    2017 : Transformer emerges (Vaswani et al.)
    2018 : BERT (Google), GPT-1 (OpenAI)
    2019 : GPT-2 (OpenAI), T5 (Google)
    2020 : GPT-3 (175 billion parameters)
    2021 : Codex (GitHub Copilot)
    2022 : ChatGPT released (GPT-3.5 based)
    2023 : GPT-4, Claude, LLaMA, Gemini
    2024 : GPT-4 Turbo, Claude 3, LLaMA 3
        </div>

        <h3>Major Milestones</h3>

        <h4>2017: Transformer</h4>
        <p>The <strong>Transformer</strong> proposed in Google's paper "Attention is All You Need" became the foundational architecture for LLMs.</p>
        <ul>
            <li><strong>Innovation</strong>: Self-Attention mechanism computes relationships between all words in a sentence in parallel</li>
            <li><strong>Advantages</strong>: Learning long-range dependencies, acceleration through parallel processing</li>
        </ul>

        <h4>2018: BERT (Bidirectional Encoder Representations from Transformers)</h4>
        <p>A <strong>bidirectional</strong> language model announced by Google. Its ability to consider context from both directions was groundbreaking.</p>
        <ul>
            <li><strong>Feature</strong>: Masked Language Modeling (masking words and predicting them)</li>
            <li><strong>Use Cases</strong>: Text classification, named entity recognition, question answering, etc.</li>
        </ul>

        <h4>2018: GPT-1 (Generative Pre-trained Transformer)</h4>
        <p>A <strong>generative</strong> language model announced by OpenAI. It established the pre-training + fine-tuning approach.</p>
        <ul>
            <li><strong>Parameters</strong>: 117 million</li>
            <li><strong>Feature</strong>: Autoregressive generation predicting the next word</li>
        </ul>

        <h4>2020: GPT-3</h4>
        <p>The third generation of the GPT series. A dramatic increase in parameters enabled Few-Shot Learning.</p>
        <ul>
            <li><strong>Parameters</strong>: 175 billion (approximately 1500 times GPT-1)</li>
            <li><strong>Innovation</strong>: Can execute new tasks with just a few examples</li>
        </ul>

        <h4>2022: ChatGPT</h4>
        <p>A chatbot based on GPT-3.5 and tuned with human feedback. It became the catalyst for the democratization of AI technology.</p>
        <ul>
            <li><strong>Feature</strong>: Tuned with RLHF (Reinforcement Learning from Human Feedback)</li>
            <li><strong>Impact</strong>: Reached 100 million users within 2 months of release</li>
        </ul>

        <h4>2023: GPT-4</h4>
        <p>OpenAI's latest model (at the time of writing). It supports multimodal (text + image) capabilities.</p>
        <ul>
            <li><strong>Improvements</strong>: More accurate reasoning, long-text understanding, enhanced creativity</li>
            <li><strong>Safety</strong>: More robust safety features and ethical considerations</li>
        </ul>

        <h2 id="models">1.3 Representative LLM Models</h2>

        <h3>Comparison of Major LLMs</h3>

        <table>
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Developer</th>
                    <th>Parameters</th>
                    <th>Features</th>
                    <th>Availability</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>GPT-4</strong></td>
                    <td>OpenAI</td>
                    <td>Undisclosed (estimated 1T+)</td>
                    <td>Multimodal, high accuracy</td>
                    <td>Via API</td>
                </tr>
                <tr>
                    <td><strong>Claude 3</strong></td>
                    <td>Anthropic</td>
                    <td>Undisclosed</td>
                    <td>Long-text understanding, safety-focused</td>
                    <td>Via API</td>
                </tr>
                <tr>
                    <td><strong>Gemini</strong></td>
                    <td>Google</td>
                    <td>Undisclosed</td>
                    <td>Multimodal, integrated</td>
                    <td>Via API</td>
                </tr>
                <tr>
                    <td><strong>LLaMA 3</strong></td>
                    <td>Meta</td>
                    <td>8B, 70B, 405B</td>
                    <td>Open source, high efficiency</td>
                    <td>Fully open</td>
                </tr>
                <tr>
                    <td><strong>Mistral</strong></td>
                    <td>Mistral AI</td>
                    <td>7B, 8x7B</td>
                    <td>Small high-performance, MoE</td>
                    <td>Open source</td>
                </tr>
            </tbody>
        </table>

        <div class="info-box">
            <h4>üí° Parameter Notation</h4>
            <ul>
                <li><strong>B</strong>: Billion - Example: 7B = 7 billion parameters</li>
                <li><strong>M</strong>: Million - Example: 340M = 340 million parameters</li>
                <li>More parameters generally mean higher performance, but also increase computational costs</li>
            </ul>
        </div>

        <h3>Details of Each Model</h3>

        <h4>GPT-4 (OpenAI)</h4>
        <ul>
            <li><strong>Release</strong>: March 2023</li>
            <li><strong>Strengths</strong>: Complex reasoning, creative tasks, multimodal support</li>
            <li><strong>Weaknesses</strong>: High cost, API-only, knowledge cutoff exists</li>
            <li><strong>Use Cases</strong>: Code generation, document creation, complex problem solving</li>
        </ul>

        <h4>Claude 3 (Anthropic)</h4>
        <ul>
            <li><strong>Release</strong>: March 2024</li>
            <li><strong>Strengths</strong>: Long-text understanding (200k+ tokens), safety, accuracy</li>
            <li><strong>Model Variants</strong>: Opus (highest performance), Sonnet (balanced), Haiku (fast)</li>
            <li><strong>Use Cases</strong>: Long-text analysis, safety-critical applications</li>
        </ul>

        <h4>Gemini (Google)</h4>
        <ul>
            <li><strong>Release</strong>: December 2023</li>
            <li><strong>Strengths</strong>: Google services integration, multimodal, fast</li>
            <li><strong>Model Variants</strong>: Ultra, Pro, Nano</li>
            <li><strong>Use Cases</strong>: Google Workspace integration, search integration</li>
        </ul>

        <h4>LLaMA 3 (Meta)</h4>
        <ul>
            <li><strong>Release</strong>: April 2024</li>
            <li><strong>Strengths</strong>: Open source, commercially usable, high efficiency</li>
            <li><strong>Sizes</strong>: 8B (small), 70B (medium), 405B (large)</li>
            <li><strong>Use Cases</strong>: Self-hosted deployment, customization, research</li>
        </ul>

        <h2 id="tokenization">1.4 Tokenization Mechanism</h2>

        <h3>What are Tokens</h3>
        <p>LLMs do not process strings directly but split them into units called <strong>tokens</strong>. Tokens can be parts of words, entire words, or punctuation marks.</p>

        <div class="example-box">
            <h4>üîç Tokenization Example</h4>
            <p><strong>Input Text</strong>: "ChatGPT is an amazing AI"</p>
            <p><strong>Token Split</strong>: ["Chat", "G", "PT", " is", " an", " amazing", " AI"]</p>
            <p>‚Üí 7 tokens</p>
        </div>

        <h3>Main Tokenization Methods</h3>

        <h4>1. BPE (Byte Pair Encoding)</h4>
        <ul>
            <li>Used in GPT series</li>
            <li>Repeatedly merges frequently occurring character pairs</li>
            <li>Strong against unknown words (subword splitting)</li>
        </ul>

        <h4>2. WordPiece</h4>
        <ul>
            <li>Used in BERT</li>
            <li>Improved version of BPE</li>
            <li>Selects optimal splits based on likelihood</li>
        </ul>

        <h4>3. SentencePiece</h4>
        <ul>
            <li>Multilingual support</li>
            <li>Language-independent tokenization</li>
            <li>Used in LLaMA, T5, etc.</li>
        </ul>

        <h3>Python Code Example for Tokenization</h3>

        <pre><code class="language-python"># Tokenization using Hugging Face transformers
from transformers import AutoTokenizer

# Load GPT-2 tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Tokenize text
text = "ChatGPT is an amazing AI"
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)
# Example output: ['Chat', 'G', 'PT', ' is', ' an', ' amazing', ' AI']

# Convert to token IDs
token_ids = tokenizer.encode(text)
print("Token IDs:", token_ids)

# Check number of tokens
print(f"Number of tokens: {len(token_ids)}")
</code></pre>

        <div class="warning-box">
            <h4>‚ö†Ô∏è Importance of Token Count</h4>
            <p>Many LLM APIs charge based on <strong>token count</strong>. Additionally, models have maximum token limits (context length).</p>
            <ul>
                <li><strong>GPT-3.5</strong>: 4,096 tokens (approximately 3,000 words)</li>
                <li><strong>GPT-4</strong>: 8,192 tokens, or 32,768 tokens</li>
                <li><strong>Claude 3</strong>: 200,000 tokens (approximately 150,000 words)</li>
            </ul>
        </div>

        <h2 id="architecture">1.5 Fundamentals of Transformer Architecture</h2>

        <h3>Basic Structure of Transformer</h3>
        <p>Transformer consists of Encoder and Decoder, but most LLMs adopt a <strong>Decoder-Only</strong> architecture.</p>

        <div class="mermaid">
graph TD
    A[Input Tokens] --> B[Embedding + Position Encoding]
    B --> C[Multi-Head Self-Attention]
    C --> D[Add & Norm]
    D --> E[Feed-Forward Network]
    E --> F[Add & Norm]
    F --> G[Next Layer or Output]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#e8f5e9
        </div>

        <h3>Main Components</h3>

        <h4>1. Self-Attention</h4>
        <p>A mechanism where each word in a sentence learns its relationship with all other words.</p>
        <ul>
            <li><strong>Query</strong>: The word to focus on</li>
            <li><strong>Key</strong>: The comparison target word</li>
            <li><strong>Value</strong>: The information to retrieve</li>
        </ul>

        <div class="example-box">
            <h4>üîç Self-Attention Example</h4>
            <p><strong>Sentence</strong>: "The cat ate the fish"</p>
            <p><strong>Focusing on "ate"</strong>:</p>
            <ul>
                <li>"cat" ‚Üí high attention (subject)</li>
                <li>"fish" ‚Üí high attention (object)</li>
                <li>"The" ‚Üí moderate attention</li>
                <li>"the" ‚Üí moderate attention</li>
            </ul>
            <p>‚Üí The model automatically learns grammatical structure</p>
        </div>

        <h4>2. Multi-Head Attention</h4>
        <p>Computes attention from multiple different perspectives (heads) and processes them in parallel.</p>
        <ul>
            <li><strong>Advantage</strong>: Learns different types of relationships simultaneously</li>
            <li><strong>Typical number of heads</strong>: 8-16</li>
        </ul>

        <h4>3. Position Encoding</h4>
        <p>Transformer requires explicit word order information due to parallel processing.</p>
        <ul>
            <li><strong>Absolute Position Encoding</strong>: Unique vector for each position</li>
            <li><strong>Relative Position Encoding</strong>: Considers relative distance between words</li>
        </ul>

        <h4>4. Feed-Forward Network</h4>
        <p>A fully connected layer that independently transforms the representation of each token.</p>

        <h3>Decoder-Only vs Encoder-Decoder</h3>

        <table>
            <thead>
                <tr>
                    <th>Architecture</th>
                    <th>Representative Models</th>
                    <th>Features</th>
                    <th>Main Use Cases</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Decoder-Only</strong></td>
                    <td>GPT-3, GPT-4, LLaMA</td>
                    <td>Autoregressive generation</td>
                    <td>Text generation, chat</td>
                </tr>
                <tr>
                    <td><strong>Encoder-Only</strong></td>
                    <td>BERT</td>
                    <td>Bidirectional understanding</td>
                    <td>Text classification, NER</td>
                </tr>
                <tr>
                    <td><strong>Encoder-Decoder</strong></td>
                    <td>T5, BART</td>
                    <td>Input‚ÜíOutput transformation</td>
                    <td>Translation, summarization</td>
                </tr>
            </tbody>
        </table>

        <h2 id="use-cases">1.6 LLM Use Cases</h2>

        <h3>Main Application Areas</h3>

        <h4>1. Content Generation</h4>
        <ul>
            <li>Article and blog post creation</li>
            <li>Marketing copy</li>
            <li>Email draft responses</li>
            <li>Creative writing (novels, poetry, scripts)</li>
        </ul>

        <h4>2. Code Generation and Assistance</h4>
        <ul>
            <li>GitHub Copilot (Codex-based)</li>
            <li>Bug fix suggestions</li>
            <li>Code review</li>
            <li>Documentation generation</li>
        </ul>

        <h4>3. Question Answering and Customer Support</h4>
        <ul>
            <li>FAQ bots</li>
            <li>Technical support</li>
            <li>Internal knowledge base search</li>
        </ul>

        <h4>4. Translation and Summarization</h4>
        <ul>
            <li>Multilingual translation</li>
            <li>Document summarization</li>
            <li>Automatic meeting minutes generation</li>
        </ul>

        <h4>5. Educational Support</h4>
        <ul>
            <li>Learning tutors</li>
            <li>Problem generation</li>
            <li>Grading and feedback</li>
        </ul>

        <h3>Try Using LLM: Simple Code Example</h3>

        <pre><code class="language-python"># Text generation with GPT-2 using Hugging Face transformers
from transformers import pipeline

# Create text generation pipeline
generator = pipeline('text-generation', model='gpt2')

# Generate text with a prompt
prompt = "When thinking about the future of artificial intelligence"
result = generator(
    prompt,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7
)

print(result[0]['generated_text'])
</code></pre>

        <div class="info-box">
            <h4>üí° Parameter Explanation</h4>
            <ul>
                <li><strong>max_length</strong>: Maximum number of tokens to generate</li>
                <li><strong>num_return_sequences</strong>: Number of candidates to generate</li>
                <li><strong>temperature</strong>: Randomness (0=deterministic, 1=creative)</li>
            </ul>
        </div>

        <h2 id="limitations">1.7 Limitations and Challenges of LLMs</h2>

        <h3>Main Challenges</h3>

        <h4>1. Hallucination</h4>
        <p>LLMs can generate non-existent information in a plausible manner.</p>
        <div class="warning-box">
            <h4>‚ö†Ô∏è Example of Hallucination</h4>
            <p><strong>Question</strong>: "Who won the 2024 Nobel Prize in Physics?"</p>
            <p><strong>Example of Incorrect Answer</strong>: "Dr. Taro Yamada won for his research in quantum computing"</p>
            <p>‚Üí Models cannot say "I don't know" and may generate plausible lies</p>
        </div>

        <h4>2. Bias and Fairness</h4>
        <ul>
            <li>Learns social biases contained in training data</li>
            <li>Prejudices related to gender, race, age, etc.</li>
            <li>Need for ethical considerations</li>
        </ul>

        <h4>3. Knowledge Cutoff</h4>
        <ul>
            <li>Does not know information after the training data cutoff date</li>
            <li>Example: GPT-4 (2023 version) does not know events after April 2023</li>
        </ul>

        <h4>4. Computational Cost and Energy</h4>
        <ul>
            <li>Training costs millions to tens of millions of dollars</li>
            <li>Inference also requires high computational resources</li>
            <li>Environmental impact</li>
        </ul>

        <h4>5. Privacy and Security</h4>
        <ul>
            <li>Risk of information leakage from training data</li>
            <li>Potential for misuse (phishing, disinformation)</li>
            <li>Copyright issues</li>
        </ul>

        <h3>Countermeasures and Mitigation Strategies</h3>
        <ul>
            <li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong>: Adopted in ChatGPT, etc.</li>
            <li><strong>RAG (Retrieval-Augmented Generation)</strong>: Integration with external knowledge bases</li>
            <li><strong>Fact-checking Mechanisms</strong>: Verification of generated content</li>
            <li><strong>Transparency and Documentation</strong>: Explicitly stating model limitations</li>
        </ul>

        <h2 id="future">1.8 Future of LLMs</h2>

        <h3>Future Development Directions</h3>

        <h4>1. Multimodal AI</h4>
        <p>Models that integrate understanding and generation of not only text but also images, audio, and video.</p>
        <ul>
            <li>GPT-4V (Vision): Image understanding</li>
            <li>Gemini: Natively multimodal</li>
        </ul>

        <h4>2. More Efficient Models</h4>
        <p>Development of smaller yet high-performance models.</p>
        <ul>
            <li>Mistral 7B: High performance with 7 billion parameters</li>
            <li>Quantization, pruning, distillation</li>
        </ul>

        <h4>3. Agent-type AI</h4>
        <p>AI that can use tools, make plans, and take actions.</p>
        <ul>
            <li>AutoGPT, BabyAGI</li>
            <li>Function Calling</li>
        </ul>

        <h4>4. Personalization</h4>
        <p>AI assistants optimized for individuals.</p>
        <ul>
            <li>Learning user preferences</li>
            <li>Custom GPTs</li>
        </ul>

        <h4>5. Open Source Movement</h4>
        <p>Trend toward more models being released as open source.</p>
        <ul>
            <li>LLaMA, Mistral, Falcon, etc.</li>
            <li>Democratization of research and development</li>
        </ul>

        <h2 id="summary">Summary</h2>

        <p>In this chapter, we learned the fundamentals of Large Language Models (LLMs).</p>

        <div class="info-box">
            <h4>üìå Key Points</h4>
            <ul>
                <li>LLMs are large-scale neural networks based on the <strong>Transformer</strong> architecture</li>
                <li>Rapid development from Transformer's emergence in 2017 to ChatGPT's popularization in 2023</li>
                <li>Various models exist including <strong>GPT-4, Claude, Gemini, LLaMA</strong></li>
                <li><strong>Tokenization</strong> converts strings to numbers for processing</li>
                <li><strong>Self-Attention</strong> enables context understanding</li>
                <li>Various use cases exist but challenges like <strong>hallucination</strong> also persist</li>
                <li>Future development toward multimodal, efficiency, and agent-type AI</li>
            </ul>
        </div>

        <h2 id="exercises">Exercises</h2>

        <div class="example-box">
            <h4>üìù Exercise 1: Basic Knowledge Check</h4>
            <p><strong>Question</strong>: Answer the following questions.</p>
            <ol>
                <li>What does "large-scale" in LLM refer to?</li>
                <li>List two main advantages of the Transformer architecture.</li>
                <li>Explain the difference between Decoder-Only and Encoder-Only models.</li>
            </ol>
        </div>

        <div class="example-box">
            <h4>üìù Exercise 2: Tokenization Practice</h4>
            <p><strong>Task</strong>: Run the following code and compare token counts for different texts.</p>
            <pre><code class="language-python">from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

texts = [
    "Hello",
    "Bonjour",
    "Artificial Intelligence is amazing",
    "‰∫∫Â∑•Áü•ËÉΩ„ÅØÁ¥†Êô¥„Çâ„Åó„ÅÑ"
]

for text in texts:
    tokens = tokenizer.encode(text)
    print(f"'{text}' ‚Üí {len(tokens)} tokens")
</code></pre>
            <p><strong>Analysis</strong>: Are there differences in token counts between different languages? Consider the reasons.</p>
        </div>

        <div class="example-box">
            <h4>üìù Exercise 3: Model Comparison</h4>
            <p><strong>Task</strong>: Choose one from GPT-4, Claude, or LLaMA and research the following:</p>
            <ul>
                <li>Developer and development background</li>
                <li>Main features and strengths</li>
                <li>Usage methods (API, open source, etc.)</li>
                <li>Representative use cases</li>
            </ul>
            <p><strong>Advanced</strong>: Read the official documentation of the chosen model and summarize technical details.</p>
        </div>

        <h2 id="next">Next Chapter</h2>
        <p>In the next chapter, we will study the <strong>Transformer architecture</strong>, the core technology of LLMs, in detail. You will understand the mechanisms of Self-Attention, Multi-Head Attention, position encoding, etc., and experience them with working code.</p>

        <div class="nav-buttons">
            <a href="./index.html" class="nav-button">‚Üê Series Overview</a>
            <a href="./index.html" class="nav-button">Chapter 2: Transformer Architecture (Coming Soon)</a>
        </div>

    </main>

    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
