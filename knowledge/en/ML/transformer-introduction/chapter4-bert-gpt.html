<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: BERT & GPT - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: var(--font-body); line-height: 1.7; color: var(--color-text); background-color: var(--color-bg); font-size: 16px; }
        header { background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; padding: var(--spacing-xl) var(--spacing-md); margin-bottom: var(--spacing-xl); box-shadow: var(--box-shadow); }
        .header-content { max-width: 900px; margin: 0 auto; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: var(--spacing-sm); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; opacity: 0.95; font-weight: 400; margin-bottom: var(--spacing-md); }
        .meta { display: flex; flex-wrap: wrap; gap: var(--spacing-md); font-size: 0.9rem; opacity: 0.9; }
        .meta-item { display: flex; align-items: center; gap: 0.3rem; }
        .container { max-width: 900px; margin: 0 auto; padding: 0 var(--spacing-md) var(--spacing-xl); }
        h2 { font-size: 1.75rem; color: var(--color-primary); margin-top: var(--spacing-xl); margin-bottom: var(--spacing-md); padding-bottom: var(--spacing-xs); border-bottom: 3px solid var(--color-accent); }
        h3 { font-size: 1.4rem; color: var(--color-primary); margin-top: var(--spacing-lg); margin-bottom: var(--spacing-sm); }
        h4 { font-size: 1.1rem; color: var(--color-primary-dark); margin-top: var(--spacing-md); margin-bottom: var(--spacing-sm); }
        p { margin-bottom: var(--spacing-md); color: var(--color-text); }
        a { color: var(--color-link); text-decoration: none; transition: color 0.2s; }
        a:hover { color: var(--color-link-hover); text-decoration: underline; }
        ul, ol { margin-left: var(--spacing-lg); margin-bottom: var(--spacing-md); }
        li { margin-bottom: var(--spacing-xs); color: var(--color-text); }
        pre { background-color: var(--color-code-bg); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); overflow-x: auto; margin-bottom: var(--spacing-md); font-family: var(--font-mono); font-size: 0.9rem; line-height: 1.5; }
        code { font-family: var(--font-mono); font-size: 0.9em; background-color: var(--color-code-bg); padding: 0.2em 0.4em; border-radius: 3px; }
        pre code { background-color: transparent; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: var(--spacing-md); font-size: 0.95rem; }
        th, td { border: 1px solid var(--color-border); padding: var(--spacing-sm); text-align: left; }
        th { background-color: var(--color-bg-alt); font-weight: 600; color: var(--color-primary); }
        blockquote { border-left: 4px solid var(--color-accent); padding-left: var(--spacing-md); margin: var(--spacing-md) 0; color: var(--color-text-light); font-style: italic; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        .mermaid { text-align: center; margin: var(--spacing-lg) 0; background-color: var(--color-bg-alt); padding: var(--spacing-md); border-radius: var(--border-radius); }
        details { background-color: var(--color-bg-alt); border: 1px solid var(--color-border); border-radius: var(--border-radius); padding: var(--spacing-md); margin-bottom: var(--spacing-md); }
        summary { cursor: pointer; font-weight: 600; color: var(--color-primary); user-select: none; padding: var(--spacing-xs); margin: calc(-1 * var(--spacing-md)); padding: var(--spacing-md); border-radius: var(--border-radius); }
        summary:hover { background-color: rgba(123, 44, 191, 0.1); }
        details[open] summary { margin-bottom: var(--spacing-md); border-bottom: 1px solid var(--color-border); }
        .navigation { display: flex; justify-content: space-between; gap: var(--spacing-md); margin: var(--spacing-xl) 0; padding-top: var(--spacing-lg); border-top: 2px solid var(--color-border); }
        .nav-button { flex: 1; padding: var(--spacing-md); background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%); color: white; border-radius: var(--border-radius); text-align: center; font-weight: 600; transition: transform 0.2s, box-shadow 0.2s; box-shadow: var(--box-shadow); }
        .nav-button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); text-decoration: none; }
        footer { margin-top: var(--spacing-xl); padding: var(--spacing-lg) var(--spacing-md); background-color: var(--color-bg-alt); border-top: 1px solid var(--color-border); text-align: center; font-size: 0.9rem; color: var(--color-text-light); }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }

        .project-box { background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 50%); border-radius: var(--border-radius); padding: var(--spacing-lg); margin: var(--spacing-lg) 0; box-shadow: var(--box-shadow); }
        @media (max-width: 768px) { h1 { font-size: 1.5rem; } h2 { font-size: 1.4rem; } h3 { font-size: 1.2rem; } .meta { font-size: 0.85rem; } .navigation { flex-direction: column; } table { font-size: 0.85rem; } th, td { padding: var(--spacing-xs); } }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 4: BERT & GPT</h1>
            <p class="subtitle">Two Pillars of Pre-trained Models: Theory and Practice of Bidirectional Encoders and Autoregressive Generative Models</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading time: 28 min</span>
                <span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
                <span class="meta-item">üíª Code examples: 9</span>
                <span class="meta-item">üìù Exercises: 6</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand BERT's bidirectional encoding and Masked Language Modeling</li>
<li>‚úÖ Understand GPT's autoregressive generation and Causal Masking mechanisms</li>
<li>‚úÖ Implement pre-training tasks (MLM, NSP, CLM) for BERT and GPT</li>
<li>‚úÖ Use both models with the Hugging Face Transformers library</li>
<li>‚úÖ Build task-specific models through fine-tuning</li>
<li>‚úÖ Determine when to use BERT vs GPT and their application scenarios</li>
<li>‚úÖ Complete practical projects: question answering systems and text generation</li>
</ul>

<hr>

<h2>4.1 BERT Architecture</h2>

<h3>4.1.1 BERT's Innovation and Design Philosophy</h3>

<p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) is a pre-trained model announced by Google in 2018 that revolutionized natural language processing.</p>

<table>
<thead>
<tr>
<th>Characteristic</th>
<th>Traditional Models (ELMo, GPT-1, etc.)</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Directionality</strong></td>
<td>Unidirectional (left‚Üíright) or shallow bidirectional</td>
<td>Deep bidirectional (uses both left and right context)</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>RNN, LSTM, shallow Transformer</td>
<td>Transformer Encoder only (12-24 layers)</td>
</tr>
<tr>
<td><strong>Pre-training</strong></td>
<td>Language modeling (next word prediction)</td>
<td>Masked LM + Next Sentence Prediction</td>
</tr>
<tr>
<td><strong>Use Cases</strong></td>
<td>Mainly generation tasks</td>
<td>Classification, extraction, QA and understanding tasks</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>Complex task-specific architecture required</td>
<td>Simple output layer addition only</td>
</tr>
</tbody>
</table>

<h3>4.1.2 Achieving Bidirectionality in BERT</h3>

<p>BERT's most distinctive feature is <strong>bidirectional context understanding</strong>. Traditional language models predicted words sequentially from left to right, but BERT understands each word by looking at the entire sentence.</p>

<div class="mermaid">
graph LR
    subgraph "Traditional Unidirectional Model (GPT-1, etc.)"
        A1[The] --> A2[cat]
        A2 --> A3[sat]
        A3 --> A4[on]
        A4 --> A5[mat]

        style A1 fill:#e74c3c,color:#fff
        style A2 fill:#e74c3c,color:#fff
        style A3 fill:#e74c3c,color:#fff
    end

    subgraph "BERT Bidirectional Model"
        B1[The] <--> B2[cat]
        B2 <--> B3[sat]
        B3 <--> B4[on]
        B4 <--> B5[mat]

        style B2 fill:#27ae60,color:#fff
        style B3 fill:#27ae60,color:#fff
    end
</div>

<blockquote>
<p><strong>Important</strong>: When BERT understands the word "cat" in a sentence, it simultaneously uses both "The" (left context) and "sat on mat" (right context). This allows it to accurately capture word meanings.</p>
</blockquote>

<h3>4.1.3 BERT Architecture Structure</h3>

<p>BERT consists of multiple stacked Transformer Encoder blocks:</p>

<table>
<thead>
<tr>
<th>Model</th>
<th>Layers (L)</th>
<th>Hidden Size (H)</th>
<th>Attention Heads (A)</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT-Base</strong></td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr>
<td><strong>BERT-Large</strong></td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>

<p>Each Transformer Encoder block consists of Multi-Head Attention and Feed-Forward Network, as we learned in Chapter 2:</p>

$$
\text{EncoderBlock}(x) = \text{LayerNorm}(x + \text{FFN}(\text{LayerNorm}(x + \text{MultiHeadAttn}(x))))
$$

<h3>4.1.4 Input Representation: Token + Segment + Position Embeddings</h3>

<p>BERT's input is the sum of three types of embeddings:</p>

<ol>
<li><strong>Token Embeddings</strong>: Word (subword) embedding representations</li>
<li><strong>Segment Embeddings</strong>: Distinguish sentence A and sentence B (for NSP task)</li>
<li><strong>Position Embeddings</strong>: Position information (learnable, different from GPT's Sinusoidal)</li>
</ol>

$$
\text{Input} = \text{TokenEmbed}(x) + \text{SegmentEmbed}(x) + \text{PositionEmbed}(x)
$$

<div class="mermaid">
graph TB
    subgraph "BERT Input Structure"
        T1["[CLS] The cat sat [SEP] on mat [SEP]"]

        T2[Token Embeddings]
        T3[Segment Embeddings]
        T4[Position Embeddings]

        T5[Input to Transformer]

        T1 --> T2
        T1 --> T3
        T1 --> T4

        T2 --> T5
        T3 --> T5
        T4 --> T5

        style T5 fill:#7b2cbf,color:#fff
    end
</div>

<p><strong>Special Tokens</strong>:</p>
<ul>
<li><code>[CLS]</code>: Classification representation for entire sentence (Classification token)</li>
<li><code>[SEP]</code>: Sentence separator (Separator)</li>
<li><code>[MASK]</code>: Mask token for Masked Language Modeling</li>
</ul>

<hr>

<h2>4.2 BERT Pre-training Tasks</h2>

<h3>4.2.1 Masked Language Modeling (MLM)</h3>

<p>MLM is a task that masks part of the input and predicts those words. This allows learning bidirectional context.</p>

<h4>MLM Procedure</h4>

<ol>
<li>Randomly select 15% of input tokens</li>
<li>For selected tokens:
<ul>
<li>80% probability: Replace with <code>[MASK]</code> token</li>
<li>10% probability: Replace with a random different word</li>
<li>10% probability: Keep the original word</li>
</ul>
</li>
<li>Predict the original words at masked positions</li>
</ol>

<p><strong>Example</strong>:</p>
<pre><code>Input: "The cat sat on the mat"
After masking: "The [MASK] sat on the mat"
Target: Predict "cat"
</code></pre>

<h4>Why not mask 100%?</h4>

<p>The <code>[MASK]</code> token doesn't exist during fine-tuning. To reduce the gap between training and deployment, some are kept as random words or original words.</p>

<h3>4.2.2 Next Sentence Prediction (NSP)</h3>

<p>NSP is a task that determines whether two sentences are consecutive. Understanding inter-sentence relationships is important for question answering and natural language inference.</p>

<h4>NSP Structure</h4>

<ul>
<li><strong>IsNext</strong> (50%): Actually consecutive sentence pairs</li>
<li><strong>NotNext</strong> (50%): Randomly selected non-consecutive sentence pairs</li>
</ul>

<p><strong>Example</strong>:</p>
<pre><code>Input A: "The cat sat on the mat."
Input B (IsNext): "It was very comfortable."
Input B (NotNext): "Paris is the capital of France."

BERT input: [CLS] The cat sat on the mat [SEP] It was very comfortable [SEP]
Target: IsNext = True
</code></pre>

<h3>4.2.3 MLM Implementation in PyTorch</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np

class MaskedLanguageModel:
    """BERT-style Masked Language Modeling implementation"""

    def __init__(self, vocab_size, mask_prob=0.15):
        self.vocab_size = vocab_size
        self.mask_prob = mask_prob

        # Special token IDs
        self.MASK_TOKEN_ID = vocab_size - 3
        self.CLS_TOKEN_ID = vocab_size - 2
        self.SEP_TOKEN_ID = vocab_size - 1

    def create_masked_lm_data(self, input_ids):
        """
        Generate masked data for MLM

        Args:
            input_ids: [batch_size, seq_len] input token IDs

        Returns:
            masked_input: Input after applying mask
            labels: Labels for prediction targets (valid only at masked positions, -100 for others)
        """
        batch_size, seq_len = input_ids.shape

        # Initialize labels (-100 is ignored in loss calculation)
        labels = torch.full_like(input_ids, -100)
        masked_input = input_ids.clone()

        for i in range(batch_size):
            # Exclude special tokens and select mask candidates
            special_tokens_mask = (input_ids[i] == self.CLS_TOKEN_ID) | \
                                 (input_ids[i] == self.SEP_TOKEN_ID)

            # Maskable positions
            candidate_indices = torch.where(~special_tokens_mask)[0]

            # Select 15% for masking
            num_to_mask = max(1, int(len(candidate_indices) * self.mask_prob))
            mask_indices = candidate_indices[torch.randperm(len(candidate_indices))[:num_to_mask]]

            for idx in mask_indices:
                labels[i, idx] = input_ids[i, idx]  # Save original word

                rand = random.random()
                if rand < 0.8:
                    # 80%: Replace with [MASK] token
                    masked_input[i, idx] = self.MASK_TOKEN_ID
                elif rand < 0.9:
                    # 10%: Replace with random word
                    random_token = random.randint(0, self.vocab_size - 4)
                    masked_input[i, idx] = random_token
                # 10%: Keep original word (no else needed)

        return masked_input, labels


# Demonstration
print("=== Masked Language Modeling Demo ===\n")

# Parameter settings
vocab_size = 1000
batch_size = 3
seq_len = 10

# Generate dummy input
mlm = MaskedLanguageModel(vocab_size)
input_ids = torch.randint(0, vocab_size - 3, (batch_size, seq_len))

# Add [CLS] at beginning, [SEP] at end
input_ids[:, 0] = mlm.CLS_TOKEN_ID
input_ids[:, -1] = mlm.SEP_TOKEN_ID

print("Original Input IDs (Batch 0):")
print(input_ids[0].numpy())

# Apply MLM mask
masked_input, labels = mlm.create_masked_lm_data(input_ids)

print("\nMasked Input IDs (Batch 0):")
print(masked_input[0].numpy())

print("\nLabels (Batch 0, -100 is ignored):")
print(labels[0].numpy())

# Check masked positions
mask_positions = torch.where(labels[0] != -100)[0]
print(f"\nMasked Positions: {mask_positions.numpy()}")
print(f"Number of masked tokens: {len(mask_positions)} / {seq_len-2} (excluding [CLS] and [SEP])")

for pos in mask_positions:
    original = input_ids[0, pos].item()
    masked = masked_input[0, pos].item()
    target = labels[0, pos].item()

    mask_type = "MASK" if masked == mlm.MASK_TOKEN_ID else \
                "RANDOM" if masked != original else \
                "UNCHANGED"

    print(f"  Position {pos}: Original={original}, Masked={masked} ({mask_type}), Target={target}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Masked Language Modeling Demo ===

Original Input IDs (Batch 0):
[998 453 721 892 156 334 667 289 445 999]

Masked Input IDs (Batch 0):
[998 997 721 542 156 997 667 289 445 999]

Labels (Batch 0, -100 is ignored):
[-100 453 -100 892 -100 334 -100 -100 -100 -100]

Masked Positions: [1 3 5]
Number of masked tokens: 3 / 8 (excluding [CLS] and [SEP])
  Position 1: Original=453, Masked=997 (MASK), Target=453
  Position 3: Original=892, Masked=542 (RANDOM), Target=892
  Position 5: Original=334, Masked=997 (MASK), Target=334
</code></pre>

<hr>

<h2>4.3 BERT Usage Examples</h2>

<h3>4.3.1 Text Classification (Sentiment Analysis)</h3>

<p>An implementation example of sentiment analysis using BERT. The output of the <code>[CLS]</code> token is used for classification.</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # 2-class classification (Positive/Negative)
)

# Set to inference mode
model.eval()

# Sample texts
texts = [
    "I absolutely loved this movie! It was fantastic.",
    "This product is terrible and waste of money.",
    "The service was okay, nothing special."
]

print("=== BERT Sentiment Analysis Demo ===\n")

for text in texts:
    # Tokenize
    inputs = tokenizer(
        text,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=128
    )

    # Inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        predicted_class = torch.argmax(probs, dim=1).item()

    sentiment = "Positive" if predicted_class == 1 else "Negative"
    confidence = probs[0, predicted_class].item()

    print(f"Text: {text}")
    print(f"Sentiment: {sentiment} (Confidence: {confidence:.4f})")
    print(f"Probabilities: Negative={probs[0, 0]:.4f}, Positive={probs[0, 1]:.4f}\n")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== BERT Sentiment Analysis Demo ===

Text: I absolutely loved this movie! It was fantastic.
Sentiment: Positive (Confidence: 0.8234)
Probabilities: Negative=0.1766, Positive=0.8234

Text: This product is terrible and waste of money.
Sentiment: Negative (Confidence: 0.9102)
Probabilities: Negative=0.9102, Positive=0.0898

Text: The service was okay, nothing special.
Sentiment: Negative (Confidence: 0.5621)
Probabilities: Negative=0.5621, Positive=0.4379
</code></pre>

<h3>4.3.2 Named Entity Recognition</h3>

<p>An example of using BERT for Token Classification (labeling each token).</p>

<pre><code class="language-python">from transformers import BertTokenizerFast, BertForTokenClassification
import torch

print("\n=== BERT Named Entity Recognition Demo ===\n")

# Model for NER (pre-trained)
model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
tokenizer = BertTokenizerFast.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name)

model.eval()

# Label mapping
label_list = [
    'O',       # Outside
    'B-MISC', 'I-MISC',  # Miscellaneous
    'B-PER', 'I-PER',    # Person
    'B-ORG', 'I-ORG',    # Organization
    'B-LOC', 'I-LOC'     # Location
]

# Sample text
text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."

# Tokenize
inputs = tokenizer(text, return_tensors='pt', truncation=True)

# Inference
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=2)

# Display tokens and labels
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
predicted_labels = [label_list[pred] for pred in predictions[0].numpy()]

print(f"Text: {text}\n")
print("Token-Level Predictions:")
print(f"{'Token':<15} {'Label':<10}")
print("-" * 25)

for token, label in zip(tokens, predicted_labels):
    if token not in ['[CLS]', '[SEP]', '[PAD]']:
        print(f"{token:<15} {label:<10}")

# Entity extraction
print("\nExtracted Entities:")
current_entity = []
current_label = None

for token, label in zip(tokens, predicted_labels):
    if label.startswith('B-'):
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = [token]
        current_label = label[2:]
    elif label.startswith('I-') and current_label == label[2:]:
        current_entity.append(token)
    else:
        if current_entity:
            print(f"  {current_label}: {' '.join(current_entity)}")
        current_entity = []
        current_label = None

if current_entity:
    print(f"  {current_label}: {' '.join(current_entity)}")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== BERT Named Entity Recognition Demo ===

Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.

Token-Level Predictions:
Token           Label
-------------------------
Apple           B-ORG
Inc             I-ORG
.               O
was             O
founded         O
by              O
Steve           B-PER
Jobs            I-PER
in              O
Cup             B-LOC
##ert           I-LOC
##ino           I-LOC
,               O
California      B-LOC
.               O

Extracted Entities:
  ORG: Apple Inc
  PER: Steve Jobs
  LOC: Cup ##ert ##ino
  LOC: California
</code></pre>

<h3>4.3.3 Question Answering</h3>

<p>A SQuAD-format question answering system, a representative application of BERT.</p>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

print("\n=== BERT Question Answering Demo ===\n")

# BERT model fine-tuned on SQuAD
model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

model.eval()

# Context and questions
context = """
Transformers is a state-of-the-art natural language processing library developed by Hugging Face.
It provides thousands of pretrained models to perform tasks on texts such as classification,
information extraction, question answering, summarization, translation, and text generation.
The library supports PyTorch, TensorFlow, and JAX frameworks.
"""

questions = [
    "Who developed Transformers?",
    "What tasks can Transformers perform?",
    "Which frameworks does the library support?"
]

for question in questions:
    # Tokenize
    inputs = tokenizer(
        question,
        context,
        return_tensors='pt',
        truncation=True,
        max_length=384
    )

    # Inference
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

    # Predict start and end positions
    start_idx = torch.argmax(start_logits)
    end_idx = torch.argmax(end_logits)

    # Extract answer tokens
    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # Confidence score
    start_score = start_logits[0, start_idx].item()
    end_score = end_logits[0, end_idx].item()
    confidence = (start_score + end_score) / 2

    print(f"Question: {question}")
    print(f"Answer: {answer}")
    print(f"Confidence Score: {confidence:.4f}\n")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== BERT Question Answering Demo ===

Question: Who developed Transformers?
Answer: Hugging Face
Confidence Score: 8.2341

Question: What tasks can Transformers perform?
Answer: classification, information extraction, question answering, summarization, translation, and text generation
Confidence Score: 7.9823

Question: Which frameworks does the library support?
Answer: PyTorch, TensorFlow, and JAX
Confidence Score: 9.1247
</code></pre>

<hr>

<h2>4.4 GPT Architecture</h2>

<h3>4.4.1 GPT Design Philosophy: Autoregressive Language Model</h3>

<p><strong>GPT</strong> (Generative Pre-trained Transformer) is an autoregressive language model developed by OpenAI. In contrast to BERT, it specializes in text generation.</p>

<table>
<thead>
<tr>
<th>Characteristic</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Architecture</strong></td>
<td>Transformer Encoder</td>
<td>Transformer Decoder (without Cross-Attention)</td>
</tr>
<tr>
<td><strong>Directionality</strong></td>
<td>Bidirectional</td>
<td>Unidirectional (left‚Üíright)</td>
</tr>
<tr>
<td><strong>Pre-training</strong></td>
<td>MLM + NSP</td>
<td>Causal Language Modeling (next word prediction)</td>
</tr>
<tr>
<td><strong>Attention Mask</strong></td>
<td>None (refers to all tokens)</td>
<td>Causal Mask (hides future tokens)</td>
</tr>
<tr>
<td><strong>Main Use Cases</strong></td>
<td>Classification, extraction, QA</td>
<td>Text generation, dialogue, summarization</td>
</tr>
<tr>
<td><strong>Inference Method</strong></td>
<td>Parallel processing (all tokens simultaneously)</td>
<td>Sequential generation (one token at a time)</td>
</tr>
</tbody>
</table>

<h3>4.4.2 Causal Masking: Attention Without Seeing the Future</h3>

<p>The core of GPT is the <strong>Causal Attention Mask</strong>. Each position can only refer to tokens before itself.</p>

<p><strong>Causal Mask Matrix</strong> (1=can refer, 0=cannot refer):</p>
$$
\text{CausalMask} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

<p>During attention calculation, scores for future tokens are set to $-\infty$, resulting in probability 0 after softmax:</p>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V
$$

<p>where $M$ is the Causal Mask matrix, with masked positions set to $-\infty$.</p>

<h3>4.4.3 Evolution of GPT-1/2/3</h3>

<table>
<thead>
<tr>
<th>Model</th>
<th>Release Year</th>
<th>Layers</th>
<th>Hidden Size</th>
<th>Parameters</th>
<th>Training Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT-1</strong></td>
<td>2018</td>
<td>12</td>
<td>768</td>
<td>117M</td>
<td>BooksCorpus (4.5GB)</td>
</tr>
<tr>
<td><strong>GPT-2</strong></td>
<td>2019</td>
<td>48</td>
<td>1600</td>
<td>1.5B</td>
<td>WebText (40GB)</td>
</tr>
<tr>
<td><strong>GPT-3</strong></td>
<td>2020</td>
<td>96</td>
<td>12288</td>
<td>175B</td>
<td>CommonCrawl (570GB)</td>
</tr>
<tr>
<td><strong>GPT-4</strong></td>
<td>2023</td>
<td>Undisclosed</td>
<td>Undisclosed</td>
<td>Est. 1.7T</td>
<td>Undisclosed (Multimodal)</td>
</tr>
</tbody>
</table>

<p><strong>Main Evolution Points</strong>:</p>
<ul>
<li><strong>Scale Expansion</strong>: Exponential increase in parameters</li>
<li><strong>Few-shot Learning</strong>: From GPT-3 onwards, can adapt to new tasks with just a few examples</li>
<li><strong>In-context Learning</strong>: Learning with prompts only, without fine-tuning</li>
<li><strong>Emergent Abilities</strong>: Abilities that suddenly appear with scale (reasoning, translation, etc.)</li>
</ul>

<h3>4.4.4 Causal Attention Implementation</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class CausalSelfAttention(nn.Module):
    """GPT-style Causal Self-Attention implementation"""

    def __init__(self, embed_size, num_heads):
        super(CausalSelfAttention, self).__init__()
        assert embed_size % num_heads == 0

        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        # Linear transformations for Q, K, V
        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)

        # Output layer
        self.proj = nn.Linear(embed_size, embed_size)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, embed_size]

        Returns:
            output: [batch, seq_len, embed_size]
            attention_weights: [batch, num_heads, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape

        # Compute Q, K, V
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # Split for multi-head: [batch, num_heads, seq_len, head_dim]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # Apply Causal Mask (set upper triangle to -inf)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores = scores.masked_fill(causal_mask, float('-inf'))

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # Weighted sum with Value
        out = torch.matmul(attention_weights, V)

        # Concatenate heads
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)

        # Final projection
        output = self.proj(out)

        return output, attention_weights


# Demonstration
print("=== Causal Self-Attention Demo ===\n")

batch_size = 1
seq_len = 8
embed_size = 64
num_heads = 4

# Dummy input
x = torch.randn(batch_size, seq_len, embed_size)

# Apply Causal Attention
causal_attn = CausalSelfAttention(embed_size, num_heads)
output, attn_weights = causal_attn(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {attn_weights.shape}")

# Visualize Causal Mask
sample_attn = attn_weights[0, 0].detach().numpy()  # 1st batch, 1st head

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Left: Causal Attention weights
ax1 = axes[0]
sns.heatmap(sample_attn,
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'},
            ax=ax1,
            annot=True,
            fmt='.3f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax1.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax1.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax1.set_title('GPT Causal Attention Weights\n(Only lower triangle is valid)', fontsize=13, fontweight='bold')

# Right: Causal Mask structure
causal_mask_viz = np.tril(np.ones((seq_len, seq_len)))
ax2 = axes[1]
sns.heatmap(causal_mask_viz,
            cmap='RdYlGn',
            cbar_kws={'label': '1=Can refer, 0=Masked'},
            ax=ax2,
            annot=True,
            fmt='.0f',
            linewidths=0.5,
            xticklabels=[f't{i+1}' for i in range(seq_len)],
            yticklabels=[f't{i+1}' for i in range(seq_len)])

ax2.set_xlabel('Key Position', fontsize=12, fontweight='bold')
ax2.set_ylabel('Query Position', fontsize=12, fontweight='bold')
ax2.set_title('Causal Mask Structure\n(Hides future tokens)', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nCharacteristics:")
print("‚úì Each position only refers to tokens before (left of) itself")
print("‚úì Lower triangular matrix structure (upper triangle is 0)")
print("‚úì Sequential generation possible without using future information")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Causal Self-Attention Demo ===

Input shape: torch.Size([1, 8, 64])
Output shape: torch.Size([1, 8, 64])
Attention weights shape: torch.Size([1, 4, 8, 8])

Characteristics:
‚úì Each position only refers to tokens before (left of) itself
‚úì Lower triangular matrix structure (upper triangle is 0)
‚úì Sequential generation possible without using future information
</code></pre>

<hr>

<h2>4.5 Text Generation with GPT</h2>

<h3>4.5.1 Autoregressive Generation Mechanism</h3>

<p>GPT generates one token at a time sequentially:</p>

<ol>
<li>Input prompt (input text) to the model</li>
<li>Predict probability distribution of next token</li>
<li>Select next token using sampling strategy</li>
<li>Append selected token to input</li>
<li>Repeat steps 2-4</li>
</ol>

<h3>4.5.2 Sampling Strategies</h3>

<table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Greedy Decoding</strong></td>
<td>Select highest probability token</td>
<td>Deterministic, lots of repetition</td>
</tr>
<tr>
<td><strong>Beam Search</strong></td>
<td>Maintain and explore multiple candidates</td>
<td>High quality but low diversity</td>
</tr>
<tr>
<td><strong>Temperature Sampling</strong></td>
<td>Adjust probability with temperature parameter</td>
<td>Deterministic at T‚Üí0, random at T‚Üí‚àû</td>
</tr>
<tr>
<td><strong>Top-k Sampling</strong></td>
<td>Sample from top k probability tokens</td>
<td>Balance between diversity and quality</td>
</tr>
<tr>
<td><strong>Top-p (Nucleus)</strong></td>
<td>Sample from tokens with cumulative prob ‚â• p</td>
<td>Dynamic vocabulary size adjustment</td>
</tr>
</tbody>
</table>

<h3>4.5.3 Text Generation Implementation with GPT-2</h3>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

print("=== GPT-2 Text Generation Demo ===\n")

# Load GPT-2 model
model_name = 'gpt2'  # 124M parameters
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

model.eval()

# Prompt
prompt = "Artificial intelligence is transforming the world by"

print(f"Prompt: {prompt}\n")
print("=" * 80)

# Generation with different sampling strategies
strategies = [
    {
        'name': 'Greedy Decoding',
        'params': {
            'do_sample': False,
            'max_length': 50
        }
    },
    {
        'name': 'Temperature Sampling (T=0.7)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'temperature': 0.7
        }
    },
    {
        'name': 'Top-k Sampling (k=50)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_k': 50,
            'temperature': 1.0
        }
    },
    {
        'name': 'Top-p Sampling (p=0.9)',
        'params': {
            'do_sample': True,
            'max_length': 50,
            'top_p': 0.9,
            'temperature': 1.0
        }
    }
]

for strategy in strategies:
    # Tokenize
    inputs = tokenizer(prompt, return_tensors='pt')

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            **strategy['params'],
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"\n{strategy['name']}:")
    print(f"{generated_text}")
    print("-" * 80)
</code></pre>

<p><strong>Example Output</strong>:</p>
<pre><code>=== GPT-2 Text Generation Demo ===

Prompt: Artificial intelligence is transforming the world by

================================================================================

Greedy Decoding:
Artificial intelligence is transforming the world by making it easier for people to do things that they would otherwise have to do manually. The most common example is the use of AI to automate tasks such as scheduling, scheduling appointments, and scheduling meetings.

--------------------------------------------------------------------------------

Temperature Sampling (T=0.7):
Artificial intelligence is transforming the world by enabling machines to learn from experience and make decisions without human intervention. From self-driving cars to medical diagnosis systems, AI technologies are revolutionizing industries and improving our daily lives.

--------------------------------------------------------------------------------

Top-k Sampling (k=50):
Artificial intelligence is transforming the world by creating new possibilities in healthcare, education, and entertainment. AI systems can now analyze vast amounts of data, recognize patterns, and provide insights that were previously impossible to obtain.

--------------------------------------------------------------------------------

Top-p Sampling (p=0.9):
Artificial intelligence is transforming the world by automating complex tasks, enhancing decision-making processes, and opening doors to innovations we never thought possible. As AI continues to evolve, its impact on society will only grow stronger.

--------------------------------------------------------------------------------
</code></pre>

<h3>4.5.4 Custom Generation Function Implementation</h3>

<pre><code class="language-python">def generate_text_custom(model, tokenizer, prompt, max_length=50,
                        strategy='top_p', temperature=1.0, top_k=50, top_p=0.9):
    """
    Custom text generation function

    Args:
        model: GPT-2 model
        tokenizer: Tokenizer
        prompt: Input prompt
        max_length: Maximum generation length
        strategy: 'greedy', 'temperature', 'top_k', 'top_p'
        temperature: Temperature parameter
        top_k: k for Top-k sampling
        top_p: p for Top-p sampling

    Returns:
        Generated text
    """
    # Tokenize
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # Generation loop
    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # Get logits for last token
        next_token_logits = logits[0, -1, :]

        # Apply temperature
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # Sampling strategy
        if strategy == 'greedy':
            next_token_id = torch.argmax(next_token_logits).unsqueeze(0)

        elif strategy == 'temperature':
            probs = F.softmax(next_token_logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_k':
            # Top-k masking
            top_k_values, top_k_indices = torch.topk(next_token_logits, top_k)
            next_token_logits_filtered = torch.full_like(next_token_logits, float('-inf'))
            next_token_logits_filtered[top_k_indices] = top_k_values

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        elif strategy == 'top_p':
            # Top-p masking
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            # Find positions where cumulative probability exceeds p
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0

            # Apply mask
            next_token_logits_filtered = next_token_logits.clone()
            next_token_logits_filtered[sorted_indices[sorted_indices_to_remove]] = float('-inf')

            probs = F.softmax(next_token_logits_filtered, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)

        # Append to input
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)

        # Stop at EOS token
        if next_token_id.item() == tokenizer.eos_token_id:
            break

    # Decode
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return generated_text


# Test custom generation function
print("\n=== Custom Generation Function Test ===\n")

prompt = "The future of machine learning is"
print(f"Prompt: {prompt}\n")

for strategy in ['greedy', 'temperature', 'top_k', 'top_p']:
    generated = generate_text_custom(
        model, tokenizer, prompt,
        max_length=30,
        strategy=strategy,
        temperature=0.8,
        top_k=40,
        top_p=0.9
    )
    print(f"{strategy.upper()}: {generated}\n")
</code></pre>

<hr>

<h2>4.6 BERT vs GPT: Comparison and When to Use Each</h2>

<h3>4.6.1 Architecture Comparison</h3>

<div class="mermaid">
graph TB
    subgraph "BERT (Encoder-only)"
        B1[Input: Full sentence] --> B2[Token + Segment + Position Embeddings]
        B2 --> B3[Transformer Encoder √ó 12]
        B3 --> B4[Bidirectional Attention]
        B4 --> B5["[CLS] for Classification<br/>All Tokens for Token-level"]

        style B4 fill:#27ae60,color:#fff
    end

    subgraph "GPT (Decoder-only)"
        G1[Input: Prompt] --> G2[Token + Position Embeddings]
        G2 --> G3[Transformer Decoder √ó 12]
        G3 --> G4[Causal Attention]
        G4 --> G5[Next Token Prediction]
        G5 --> G6[Autoregressive Generation]

        style G4 fill:#e74c3c,color:#fff
    end
</div>

<h3>4.6.2 Performance Comparison Experiment</h3>

<pre><code class="language-python">from transformers import BertModel, GPT2Model, BertTokenizer, GPT2Tokenizer
import torch
import time

print("=== BERT vs GPT Performance Comparison ===\n")

# Load models
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_model = GPT2Model.from_pretrained('gpt2')

bert_model.eval()
gpt2_model.eval()

# Test text
text = "Natural language processing is a fascinating field of artificial intelligence."

# BERT processing
bert_inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)
start_time = time.time()
with torch.no_grad():
    bert_outputs = bert_model(**bert_inputs)
bert_time = time.time() - start_time

# GPT-2 processing
gpt2_inputs = gpt2_tokenizer(text, return_tensors='pt')
start_time = time.time()
with torch.no_grad():
    gpt2_outputs = gpt2_model(**gpt2_inputs)
gpt2_time = time.time() - start_time

# Display results
print("Input Text:", text)
print(f"\nBERT:")
print(f"  Model: bert-base-uncased")
print(f"  Parameters: {sum(p.numel() for p in bert_model.parameters()):,}")
print(f"  Input shape: {bert_inputs['input_ids'].shape}")
print(f"  Output shape: {bert_outputs.last_hidden_state.shape}")
print(f"  Processing time: {bert_time*1000:.2f} ms")
print(f"  [CLS] embedding shape: {bert_outputs.pooler_output.shape}")

print(f"\nGPT-2:")
print(f"  Model: gpt2")
print(f"  Parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}")
print(f"  Input shape: {gpt2_inputs['input_ids'].shape}")
print(f"  Output shape: {gpt2_outputs.last_hidden_state.shape}")
print(f"  Processing time: {gpt2_time*1000:.2f} ms")

# Attention visualization comparison
print("\n" + "="*80)
print("Attention Pattern Comparison")
print("="*80)

# BERT: Can refer to all tokens mutually
print("\nBERT Attention Pattern:")
print("  ‚úì Bidirectional - all tokens can refer to all tokens")
print("  ‚úì Parallel processing - processes all tokens simultaneously")
print("  ‚úì Use cases: classification, NER, QA, sentence encoding")

# GPT: Can only refer to left tokens
print("\nGPT Attention Pattern:")
print("  ‚úì Unidirectional - each token only refers to tokens on its left")
print("  ‚úì Sequential generation - generates one token at a time")
print("  ‚úì Use cases: text generation, dialogue, completion, translation")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== BERT vs GPT Performance Comparison ===

Input Text: Natural language processing is a fascinating field of artificial intelligence.

BERT:
  Model: bert-base-uncased
  Parameters: 109,482,240
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 45.23 ms
  [CLS] embedding shape: torch.Size([1, 768])

GPT-2:
  Model: gpt2
  Parameters: 124,439,808
  Input shape: torch.Size([1, 14])
  Output shape: torch.Size([1, 14, 768])
  Processing time: 38.67 ms

================================================================================
Attention Pattern Comparison
================================================================================

BERT Attention Pattern:
  ‚úì Bidirectional - all tokens can refer to all tokens
  ‚úì Parallel processing - processes all tokens simultaneously
  ‚úì Use cases: classification, NER, QA, sentence encoding

GPT Attention Pattern:
  ‚úì Unidirectional - each token only refers to tokens on its left
  ‚úì Sequential generation - generates one token at a time
  ‚úì Use cases: text generation, dialogue, completion, translation
</code></pre>

<h3>4.6.3 Usage Guide</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>Recommended Model</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sentiment Analysis</strong></td>
<td>BERT</td>
<td>Requires understanding of full sentence context</td>
</tr>
<tr>
<td><strong>Named Entity Recognition</strong></td>
<td>BERT</td>
<td>Token classification, bidirectional context is advantageous</td>
</tr>
<tr>
<td><strong>Question Answering</strong></td>
<td>BERT</td>
<td>Identifying answer spans in text</td>
</tr>
<tr>
<td><strong>Document Classification</strong></td>
<td>BERT</td>
<td>Encode full sentence with [CLS] token</td>
</tr>
<tr>
<td><strong>Text Generation</strong></td>
<td>GPT</td>
<td>Specialized for autoregressive generation</td>
</tr>
<tr>
<td><strong>Dialogue Systems</strong></td>
<td>GPT</td>
<td>Response generation is main task</td>
</tr>
<tr>
<td><strong>Summarization</strong></td>
<td>GPT (or BART)</td>
<td>Generation task, abstractive summarization</td>
</tr>
<tr>
<td><strong>Code Generation</strong></td>
<td>GPT (Codex)</td>
<td>Sequential code generation</td>
</tr>
<tr>
<td><strong>Translation</strong></td>
<td>Both possible</td>
<td>BERT‚ÜíEncoder, GPT‚ÜíDecoder usage</td>
</tr>
</tbody>
</table>

<hr>

<h2>4.7 Practical Projects</h2>

<h3>4.7.1 Project 1: Question Answering System with BERT</h3>

<div class="project-box">
<h4>Goal</h4>
<p>Build a SQuAD-format question answering system that accurately extracts answers from context.</p>

<h4>Implementation Requirements</h4>
<ul>
<li>Use fine-tuned BERT model</li>
<li>Answer extraction for multiple questions</li>
<li>Calculate and display confidence scores</li>
<li>Validate answer plausibility</li>
</ul>
</div>

<pre><code class="language-python">from transformers import BertForQuestionAnswering, BertTokenizer
import torch

class QuestionAnsweringSystem:
    """BERT-based question answering system"""

    def __init__(self, model_name='bert-large-uncased-whole-word-masking-finetuned-squad'):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForQuestionAnswering.from_pretrained(model_name)
        self.model.eval()

    def answer_question(self, question, context, return_confidence=True):
        """
        Extract answer to question

        Args:
            question: Question text
            context: Context (source text for answer)
            return_confidence: Whether to return confidence score

        Returns:
            answer: Extracted answer
            confidence: Confidence score (if return_confidence=True)
        """
        # Tokenize
        inputs = self.tokenizer(
            question,
            context,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )

        # Inference
        with torch.no_grad():
            outputs = self.model(**inputs)

        # Predict start and end positions
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

        start_idx = torch.argmax(start_logits)
        end_idx = torch.argmax(end_logits)

        # Extract answer
        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)

        if return_confidence:
            # Calculate confidence
            start_score = torch.softmax(start_logits, dim=1)[0, start_idx].item()
            end_score = torch.softmax(end_logits, dim=1)[0, end_idx].item()
            confidence = (start_score + end_score) / 2

            return answer, confidence
        else:
            return answer

    def batch_answer(self, qa_pairs):
        """
        Answer multiple questions in batch

        Args:
            qa_pairs: List of [(question, context), ...]

        Returns:
            results: List of [(answer, confidence), ...]
        """
        results = []
        for question, context in qa_pairs:
            answer, confidence = self.answer_question(question, context)
            results.append((answer, confidence))
        return results


# Test the system
print("=== Question Answering System Demo ===\n")

qa_system = QuestionAnsweringSystem()

# Test cases
context = """
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.
"""

questions = [
    "When was the Transformer introduced?",
    "Who introduced the Transformer?",
    "What does the Transformer rely on?",
    "What are the two main components of the Transformer?",
    "What models are based on the Transformer?"
]

print("Context:")
print(context)
print("\n" + "="*80 + "\n")

for i, question in enumerate(questions, 1):
    answer, confidence = qa_system.answer_question(question, context)

    print(f"Q{i}: {question}")
    print(f"A{i}: {answer}")
    print(f"Confidence: {confidence:.4f}")
    print()

# Batch processing demo
print("="*80)
print("\nBatch Processing Demo:")
print("="*80 + "\n")

qa_pairs = [(q, context) for q in questions]
results = qa_system.batch_answer(qa_pairs)

for (question, _), (answer, conf) in zip(qa_pairs, results):
    print(f"Q: {question}")
    print(f"A: {answer} (Conf: {conf:.4f})\n")
</code></pre>

<p><strong>Output</strong>:</p>
<pre><code>=== Question Answering System Demo ===

Context:
The Transformer architecture was introduced in the paper "Attention is All You Need"
by Vaswani et al. in 2017. It relies entirely on self-attention mechanisms to compute
representations of input and output sequences without using recurrent or convolutional layers.
The model achieved state-of-the-art results on machine translation tasks and has since become
the foundation for models like BERT and GPT. The architecture consists of an encoder and a decoder,
each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention
mechanism and a position-wise fully connected feed-forward network.

================================================================================

Q1: When was the Transformer introduced?
A1: 2017
Confidence: 0.9523

Q2: Who introduced the Transformer?
A2: Vaswani et al.
Confidence: 0.8876

Q3: What does the Transformer rely on?
A3: self-attention mechanisms
Confidence: 0.9234

Q4: What are the two main components of the Transformer?
A4: an encoder and a decoder
Confidence: 0.8912

Q5: What models are based on the Transformer?
A5: BERT and GPT
Confidence: 0.9101
</code></pre>

<h3>4.7.2 Project 2: Text Generation App with GPT</h3>

<div class="project-box">
<h4>Goal</h4>
<p>Build a customizable text generation system to experiment with various generation strategies.</p>

<h4>Implementation Requirements</h4>
<ul>
<li>Support multiple sampling strategies</li>
<li>Adjust generation parameters</li>
<li>Practice prompt engineering</li>
<li>Evaluate generation quality</li>
</ul>
</div>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class TextGenerator:
    """GPT-2 based text generation system"""

    def __init__(self, model_name='gpt2-medium'):
        """
        Args:
            model_name: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'
        """
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.model.eval()

        # Set PAD token
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate(self, prompt, max_length=100, strategy='top_p',
                num_return_sequences=1, **kwargs):
        """
        Generate text

        Args:
            prompt: Input prompt
            max_length: Maximum generation length
            strategy: 'greedy', 'beam', 'temperature', 'top_k', 'top_p'
            num_return_sequences: Number of candidates to generate
            **kwargs: Strategy-specific parameters

        Returns:
            List of generated texts
        """
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors='pt')

        # Set parameters according to strategy
        gen_params = {
            'max_length': max_length,
            'num_return_sequences': num_return_sequences,
            'pad_token_id': self.tokenizer.eos_token_id,
            'early_stopping': True
        }

        if strategy == 'greedy':
            gen_params['do_sample'] = False

        elif strategy == 'beam':
            gen_params['num_beams'] = kwargs.get('num_beams', 5)
            gen_params['do_sample'] = False

        elif strategy == 'temperature':
            gen_params['do_sample'] = True
            gen_params['temperature'] = kwargs.get('temperature', 0.7)

        elif strategy == 'top_k':
            gen_params['do_sample'] = True
            gen_params['top_k'] = kwargs.get('top_k', 50)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        elif strategy == 'top_p':
            gen_params['do_sample'] = True
            gen_params['top_p'] = kwargs.get('top_p', 0.9)
            gen_params['temperature'] = kwargs.get('temperature', 1.0)

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(inputs['input_ids'], **gen_params)

        # Decode
        generated_texts = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]

        return generated_texts

    def interactive_generation(self):
        """Interactive generation session"""
        print("=== Interactive Text Generation ===")
        print("Type 'quit' to exit\n")

        while True:
            prompt = input("Prompt: ")
            if prompt.lower() == 'quit':
                break

            # Generation settings
            print("\nGeneration Settings:")
            strategy = input("Strategy (greedy/beam/temperature/top_k/top_p) [top_p]: ") or 'top_p'
            max_length = int(input("Max length [100]: ") or 100)

            # Generate
            outputs = self.generate(prompt, max_length=max_length, strategy=strategy)

            print("\n--- Generated Text ---")
            print(outputs[0])
            print("-" * 80 + "\n")


# Test the system
print("=== Text Generation System Demo ===\n")

generator = TextGenerator(model_name='gpt2')

# Prompt templates
prompts = [
    "In the future of artificial intelligence,",
    "The most important breakthrough in deep learning was",
    "Once upon a time in a distant galaxy,"
]

print("Comparing Different Generation Strategies:\n")
print("="*80 + "\n")

for prompt in prompts:
    print(f"Prompt: {prompt}\n")

    strategies = [
        ('greedy', {}),
        ('top_k', {'top_k': 50, 'temperature': 0.8}),
        ('top_p', {'top_p': 0.9, 'temperature': 0.8})
    ]

    for strategy, params in strategies:
        outputs = generator.generate(
            prompt,
            max_length=60,
            strategy=strategy,
            num_return_sequences=1,
            **params
        )

        print(f"{strategy.upper()}:")
        print(f"{outputs[0]}\n")

    print("="*80 + "\n")

# Multiple candidates generation demo
print("\nMultiple Candidates Generation:")
print("="*80 + "\n")

prompt = "The key to successful machine learning is"
outputs = generator.generate(
    prompt,
    max_length=50,
    strategy='top_p',
    num_return_sequences=3,
    top_p=0.9,
    temperature=0.9
)

for i, output in enumerate(outputs, 1):
    print(f"Candidate {i}:")
    print(output)
    print()
</code></pre>

<p><strong>Example Output</strong>:</p>
<pre><code>=== Text Generation System Demo ===

Comparing Different Generation Strategies:

================================================================================

Prompt: In the future of artificial intelligence,

GREEDY:
In the future of artificial intelligence, we will be able to create a new kind of AI that can do things that we have never done before. We will be able to build systems that can learn from data and make decisions based on that data.

TOP_K:
In the future of artificial intelligence, machines will become increasingly capable of understanding human language, emotions, and intentions. This will revolutionize how we interact with technology and open new possibilities in healthcare, education, and entertainment.

TOP_P:
In the future of artificial intelligence, we can expect to see breakthroughs in areas such as natural language understanding, computer vision, and autonomous decision-making. These advances will transform industries and create opportunities we haven't yet imagined.

================================================================================
</code></pre>

<hr>

<h2>4.8 Summary and Advanced Topics</h2>

<h3>What We Learned in This Chapter</h3>

<table>
<thead>
<tr>
<th>Topic</th>
<th>Key Points</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT</strong></td>
<td>Bidirectional encoder, MLM+NSP, optimal for understanding tasks</td>
</tr>
<tr>
<td><strong>GPT</strong></td>
<td>Autoregressive generation, Causal Masking, optimal for generation tasks</td>
</tr>
<tr>
<td><strong>Pre-training</strong></td>
<td>Train on large-scale data, specialize with fine-tuning</td>
</tr>
<tr>
<td><strong>When to Use Each</strong></td>
<td>BERT for classification/extraction, GPT for generation</td>
</tr>
<tr>
<td><strong>Practical Methods</strong></td>
<td>Hugging Face, sampling strategies, QA systems</td>
</tr>
</tbody>
</table>

<h3>Advanced Topics</h3>

<details>
<summary><strong>RoBERTa: Improved Version of BERT</strong></summary>
<p>An improved version of BERT by Facebook. Removed the NSP task and adopted dynamic masking, larger training data, and longer training time to improve performance.</p>
</details>

<details>
<summary><strong>ALBERT: Parameter Efficiency</strong></summary>
<p>Achieves BERT-equivalent performance with fewer parameters through parameter sharing and factorization. Efficiently trains large-scale models.</p>
</details>

<details>
<summary><strong>GPT-3.5/4: InstructGPT & ChatGPT</strong></summary>
<p>Significantly improved ability to follow user instructions through Instruction Tuning and RLHF (Reinforcement Learning from Human Feedback). Became mainstream for dialogue systems.</p>
</details>

<details>
<summary><strong>Prompt Engineering</strong></summary>
<p>Prompt design techniques to maximize model performance. Few-shot examples, Chain-of-Thought prompting, Role prompting, etc.</p>
</details>

<details>
<summary><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong></summary>
<p>Methods for efficient fine-tuning without updating all parameters: LoRA, Adapter, Prefix Tuning, etc. Essential technology for the large-scale model era.</p>
</details>

<h3>Exercises</h3>

<div class="project-box">
<h4>Exercise 4.1: Sentiment Analysis with BERT Fine-tuning</h4>
<p><strong>Task</strong>: Fine-tune BERT on the IMDB review dataset and build a sentiment analysis model.</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>Data preprocessing and tokenization</li>
<li>Load BERT-Base model and add classification layer</li>
<li>Implement training loop</li>
<li>Evaluate accuracy and F1 score</li>
</ul>
</div>

<div class="project-box">
<h4>Exercise 4.2: Dialogue System with GPT-2</h4>
<p><strong>Task</strong>: Implement a simple dialogue system using GPT-2.</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>Accept user input and manage context</li>
<li>Response generation (multiple sampling strategies)</li>
<li>Maintain and reflect conversation history</li>
<li>Evaluate conversation naturalness</li>
</ul>
</div>

<div class="project-box">
<h4>Exercise 4.3: BERT vs GPT Performance Comparison</h4>
<p><strong>Task</strong>: Compare performance of BERT and GPT on the same task.</p>
<p><strong>Task</strong>: Document classification (news article category classification)</p>
<p><strong>Comparison Items</strong>:</p>
<ul>
<li>Accuracy, F1 score</li>
<li>Training time</li>
<li>Inference speed</li>
<li>Memory usage</li>
</ul>
</div>

<div class="project-box">
<h4>Exercise 4.4: Masked Language Modeling Implementation</h4>
<p><strong>Task</strong>: Implement MLM on a small dataset to reproduce BERT's pre-training.</p>
<p><strong>Implementation Contents</strong>:</p>
<ul>
<li>Mask data generation logic</li>
<li>MLM loss function</li>
<li>Training loop</li>
<li>Evaluate mask prediction accuracy</li>
</ul>
</div>

<div class="project-box">
<h4>Exercise 4.5: Multilingual BERT (mBERT) Application</h4>
<p><strong>Task</strong>: Implement text classification in multiple languages using multilingual BERT.</p>
<p><strong>Languages</strong>: English, Japanese, Chinese</p>
<p><strong>Task</strong>: News article topic classification</p>
</div>

<div class="project-box">
<h4>Exercise 4.6: Code Generation with GPT</h4>
<p><strong>Task</strong>: Build a system that generates Python code from natural language instructions using GPT-2.</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>Design prompt templates</li>
<li>Code generation and syntax validation</li>
<li>Evaluate generation quality</li>
</ul>
</div>

<hr>

<h3>Next Chapter Preview</h3>

<p>In Chapter 5, we will learn about <strong>Vision Transformer (ViT)</strong>. We will explore the innovative approach of applying Transformer architecture to Computer Vision by treating images as "tokens".</p>

<blockquote>
<p><strong>Next Chapter Topics</strong>:<br>
„ÉªVision Transformer architecture<br>
„ÉªImage patch tokenization<br>
„Éª2D extension of Position Embeddings<br>
„ÉªPerformance comparison with CNNs<br>
„ÉªPre-training strategies (ImageNet-21k)<br>
„ÉªImplementation: Image classification with ViT<br>
„ÉªApplications: Object Detection, Segmentation</p>
</blockquote>

        <div class="navigation">
            <a href="chapter3-advanced-topics.html" class="nav-button">‚Üê Chapter 3: Advanced Topics</a>
            <a href="chapter5-vision-transformer.html" class="nav-button">Chapter 5: Vision Transformer ‚Üí</a>
        </div>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content shall follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p>&copy; 2025 AI Terakoya. All rights reserved.</p>
        <p>Chapter 4: BERT & GPT | Transformer Introduction Series</p>
    </footer>

</body>
</html>
