<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Pre-training and Fine-tuning - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'mermaid' }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/transformer-introduction/chapter3-pretraining-finetuning.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 3: Pre-training and Fine-tuning</h1>
<p class="subtitle">Building Task-Specific Models Efficiently with Transfer Learning - From MLM to LoRA</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Practice Problems: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Pre. You will learn importance of pre-training, differences between Causal Language Modeling (CLM), and full-parameter fine-tuning techniques.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understand the importance of pre-training and the principles of Transfer Learning</li>
<li>‚úÖ Master the mechanisms and implementation methods of Masked Language Modeling (MLM)</li>
<li>‚úÖ Understand the differences between Causal Language Modeling (CLM) and MLM</li>
<li>‚úÖ Master the basic usage of the Hugging Face Transformers library</li>
<li>‚úÖ Implement full-parameter fine-tuning techniques</li>
<li>‚úÖ Understand the principles and efficiency of LoRA (Low-Rank Adaptation)</li>
<li>‚úÖ Execute fine-tuning on actual sentiment analysis tasks</li>
<li>‚úÖ Select efficient fine-tuning strategies</li>
</ul>
<hr/>
<h2>3.1 Importance of Pre-training</h2>
<h3>What is Transfer Learning</h3>
<p><strong>Transfer Learning</strong> is a technique that adapts general-purpose models trained on large-scale data to specific tasks. The success of Transformers heavily depends on this approach.</p>
<blockquote>
<p>"By fine-tuning models pre-trained on hundreds of gigabytes of text with thousands to tens of thousands of task-specific data samples, we can efficiently build high-performance task-specialized models."</p>
</blockquote>
<div class="mermaid">
graph LR
    A[Large-scale Text<br/>Hundreds of GB] --&gt; B[Pre-training<br/>MLM/CLM]
    B --&gt; C[General Model<br/>BERT/GPT]
    C --&gt; D1[Fine-tuning<br/>Sentiment Analysis]
    C --&gt; D2[Fine-tuning<br/>Question Answering]
    C --&gt; D3[Fine-tuning<br/>Named Entity Recognition]
    D1 --&gt; E1[Task-Specific<br/>Model 1]
    D2 --&gt; E2[Task-Specific<br/>Model 2]
    D3 --&gt; E3[Task-Specific<br/>Model 3]

    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#e8f5e9
    style D3 fill:#e8f5e9
</div>
<h3>Comparison with Traditional Methods</h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Training Data Size</th>
<th>Computational Cost</th>
<th>Performance</th>
<th>Generalization</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training from Scratch</strong></td>
<td>Large (millions+)</td>
<td>Very High</td>
<td>Data Dependent</td>
<td>Task-Specific</td>
</tr>
<tr>
<td><strong>Feature Extraction Only</strong></td>
<td>Medium (thousands+)</td>
<td>Low</td>
<td>Medium</td>
<td>General Representations</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>Small (hundreds+)</td>
<td>Medium</td>
<td>High</td>
<td>Both Acquired</td>
</tr>
<tr>
<td><strong>LoRA/Adapter</strong></td>
<td>Small (hundreds+)</td>
<td>Very Low</td>
<td>High</td>
<td>Efficient</td>
</tr>
</tbody>
</table>
<h3>Benefits of Pre-training</h3>
<ul>
<li><strong>Language Knowledge Acquisition</strong>: Learning grammar, semantics, and common sense from large-scale data</li>
<li><strong>High Performance with Less Data</strong>: Few-shot learning is possible</li>
<li><strong>Improved Generalization</strong>: Easier to adapt to unseen tasks</li>
<li><strong>Reduced Development Costs</strong>: Significantly more efficient than training from scratch</li>
<li><strong>Knowledge Sharing</strong>: Apply to multiple tasks with a single pre-training</li>
</ul>
<hr/>
<h2>3.2 Pre-training Strategies</h2>
<h3>Masked Language Modeling (MLM)</h3>
<p><strong>MLM</strong> is the pre-training method adopted by BERT, which masks a portion of input tokens (typically 15%) and predicts them.</p>
<p>Masking strategy:</p>
<ul>
<li><strong>80%</strong>: Replace with <code>[MASK]</code> token</li>
<li><strong>10%</strong>: Replace with random token</li>
<li><strong>10%</strong>: Keep original token</li>
</ul>
<div class="mermaid">
graph TB
    subgraph Input["Input Sentence"]
        I1[The] --&gt; I2[cat] --&gt; I3[sat] --&gt; I4[on] --&gt; I5[the] --&gt; I6[mat]
    end

    subgraph Masked["Masking Process (15%)"]
        M1[The] --&gt; M2["[MASK]"] --&gt; M3[sat] --&gt; M4[on] --&gt; M5[the] --&gt; M6["[MASK]"]
    end

    subgraph BERT["BERT Encoder"]
        B1[Transformer] --&gt; B2[Self-Attention] --&gt; B3[Feed Forward]
    end

    subgraph Prediction["Prediction"]
        P1[The] --&gt; P2[cat] --&gt; P3[sat] --&gt; P4[on] --&gt; P5[the] --&gt; P6[mat]
    end

    Input --&gt; Masked
    Masked --&gt; BERT
    BERT --&gt; Prediction

    style M2 fill:#ffebee
    style M6 fill:#ffebee
    style P2 fill:#e8f5e9
    style P6 fill:#e8f5e9
</div>
<p>MLM loss function:</p>
<p>$$
\mathcal{L}_{\text{MLM}} = -\sum_{i \in \text{masked}} \log P(x_i | \mathbf{x}_{\setminus i})
$$</p>
<p>where $\mathbf{x}_{\setminus i}$ represents the context excluding token $i$.</p>
<h3>Next Sentence Prediction (NSP)</h3>
<p><strong>NSP</strong> is an auxiliary task in BERT that determines whether two sentences are consecutive (rarely used in current models).</p>
<table>
<thead>
<tr>
<th>Sentence A</th>
<th>Sentence B</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>The cat sat on the mat.</td>
<td>It was very comfortable.</td>
<td>IsNext (50%)</td>
</tr>
<tr>
<td>The cat sat on the mat.</td>
<td>I love pizza.</td>
<td>NotNext (50%)</td>
</tr>
</tbody>
</table>
<h3>Causal Language Modeling (CLM)</h3>
<p><strong>CLM</strong> is the method adopted by GPT, which predicts the next token from all previous tokens (autoregressive).</p>
<p>CLM loss function:</p>
<p>$$
\mathcal{L}_{\text{CLM}} = -\sum_{i=1}^{n} \log P(x_i | x_{1}, \ldots, x_{i-1})
$$</p>
<div class="mermaid">
graph LR
    A[The] --&gt; B[cat]
    B --&gt; C[sat]
    C --&gt; D[on]
    D --&gt; E[the]
    E --&gt; F[mat]

    A -.predicts.-&gt; B
    B -.predicts.-&gt; C
    C -.predicts.-&gt; D
    D -.predicts.-&gt; E
    E -.predicts.-&gt; F

    style A fill:#e3f2fd
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style D fill:#e8f5e9
    style E fill:#e8f5e9
    style F fill:#e8f5e9
</div>
<h3>Comparison: MLM vs CLM</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>MLM (BERT-style)</th>
<th>CLM (GPT-style)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Context</strong></td>
<td>Bidirectional (both directions)</td>
<td>Unidirectional (left to right)</td>
</tr>
<tr>
<td><strong>Suitable Tasks</strong></td>
<td>Classification, extraction, understanding</td>
<td>Generation, dialogue, continuation</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Can reference all tokens</td>
<td>Future tokens masked</td>
</tr>
<tr>
<td><strong>Training Efficiency</strong></td>
<td>Learns from all tokens</td>
<td>Predicts one token at a time</td>
</tr>
<tr>
<td><strong>Representative Models</strong></td>
<td>BERT, RoBERTa</td>
<td>GPT-2, GPT-3, GPT-4</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.3 Hugging Face Transformers Library</h2>
<h3>Library Overview</h3>
<p><strong>Hugging Face Transformers</strong> is a Python library that makes it easy to use pre-trained Transformer models.</p>
<ul>
<li><strong>100,000+ models</strong>: BERT, GPT, T5, LLaMA, etc.</li>
<li><strong>Unified API</strong>: Consistent usage with AutoModel and AutoTokenizer</li>
<li><strong>Pipeline API</strong>: Execute tasks in one line</li>
<li><strong>Community</strong>: Accelerate development with Model Hub, Datasets, and Trainer</li>
</ul>
<h3>Implementation Example 1: Basic Hugging Face Operations</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

"""
Example: Implementation Example 1: Basic Hugging Face Operations

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
from transformers import AutoTokenizer, AutoModel

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}\n")

print("=== Hugging Face Transformers Basic Operations ===\n")

# Load pre-trained model and tokenizer
model_name = "bert-base-uncased"
print(f"Model: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)

print(f"Vocabulary size: {tokenizer.vocab_size:,}")
print(f"Parameter count: {sum(p.numel() for p in model.parameters()):,}\n")

# Text tokenization
text = "The quick brown fox jumps over the lazy dog."
print(f"Input text: {text}")

# Tokenization (detailed display)
encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
tokens = tokenizer.tokenize(text)

print(f"\nTokens: {tokens}")
print(f"Token IDs: {encoded['input_ids'][0].tolist()}")
print(f"Attention Mask: {encoded['attention_mask'][0].tolist()}\n")

# Input to model
encoded = {k: v.to(device) for k, v in encoded.items()}
with torch.no_grad():
    outputs = model(**encoded)

# Check outputs
last_hidden_state = outputs.last_hidden_state  # [batch, seq_len, hidden_size]
pooler_output = outputs.pooler_output          # [batch, hidden_size]

print(f"Last Hidden State shape: {last_hidden_state.shape}")
print(f"Pooler Output shape: {pooler_output.shape}")
print(f"Hidden Size: {model.config.hidden_size}")
print(f"Attention Heads: {model.config.num_attention_heads}")
print(f"Hidden Layers: {model.config.num_hidden_layers}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Using device: cuda

=== Hugging Face Transformers Basic Operations ===

Model: bert-base-uncased
Vocabulary size: 30,522
Parameter count: 109,482,240

Input text: The quick brown fox jumps over the lazy dog.

Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']
Token IDs: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]
Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

Last Hidden State shape: torch.Size([1, 12, 768])
Pooler Output shape: torch.Size([1, 768])
Hidden Size: 768
Attention Heads: 12
Hidden Layers: 12
</code></pre>
<h3>Implementation Example 2: Simple Inference with Pipeline API</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - transformers&gt;=4.30.0

"""
Example: Implementation Example 2: Simple Inference with Pipeline API

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 10-30 seconds
Dependencies: None
"""

from transformers import pipeline

print("\n=== Pipeline API Demo ===\n")

# Sentiment analysis pipeline
print("--- Sentiment Analysis ---")
sentiment_pipeline = pipeline("sentiment-analysis", device=0 if torch.cuda.is_available() else -1)

texts = [
    "I love this product! It's amazing!",
    "This is the worst experience ever.",
    "It's okay, nothing special."
]

for text in texts:
    result = sentiment_pipeline(text)[0]
    print(f"Text: {text}")
    print(f"  ‚Üí {result['label']}: {result['score']:.4f}\n")

# Text generation pipeline
print("--- Text Generation ---")
generator = pipeline("text-generation", model="gpt2", device=0 if torch.cuda.is_available() else -1)

prompt = "Artificial intelligence will"
generated = generator(prompt, max_length=30, num_return_sequences=2)

print(f"Prompt: {prompt}")
for i, gen in enumerate(generated, 1):
    print(f"  Generated {i}: {gen['generated_text']}")

# Named Entity Recognition
print("\n--- Named Entity Recognition ---")
ner_pipeline = pipeline("ner", aggregation_strategy="simple", device=0 if torch.cuda.is_available() else -1)

text_ner = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text_ner)

print(f"Text: {text_ner}")
for entity in entities:
    print(f"  ‚Üí {entity['word']}: {entity['entity_group']} ({entity['score']:.4f})")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Pipeline API Demo ===

--- Sentiment Analysis ---
Text: I love this product! It's amazing!
  ‚Üí POSITIVE: 0.9998

Text: This is the worst experience ever.
  ‚Üí NEGATIVE: 0.9995

Text: It's okay, nothing special.
  ‚Üí NEUTRAL: 0.7234

--- Text Generation ---
Prompt: Artificial intelligence will
  Generated 1: Artificial intelligence will revolutionize the way we work and live in the coming decades.
  Generated 2: Artificial intelligence will transform industries from healthcare to transportation.

--- Named Entity Recognition ---
Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.
  ‚Üí Apple Inc.: ORG (0.9987)
  ‚Üí Steve Jobs: PER (0.9995)
  ‚Üí Cupertino: LOC (0.9982)
  ‚Üí California: LOC (0.9991)
</code></pre>
<h3>Implementation Example 3: MLM Pre-training Simulation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0
# - transformers&gt;=4.30.0

"""
Example: Implementation Example 3: MLM Pre-training Simulation

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from transformers import BertForMaskedLM
import torch.nn.functional as F

print("\n=== Masked Language Modeling Demo ===\n")

# Load MLM model
mlm_model = BertForMaskedLM.from_pretrained("bert-base-uncased").to(device)
mlm_model.eval()

# Masked text
text_with_mask = "The capital of France is [MASK]."
print(f"Input: {text_with_mask}\n")

# Tokenization
inputs = tokenizer(text_with_mask, return_tensors='pt').to(device)
mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]

# Prediction
with torch.no_grad():
    outputs = mlm_model(**inputs)
    predictions = outputs.logits

# Predictions at [MASK] position
mask_token_logits = predictions[0, mask_token_index, :]
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

print("Top 5 predictions:")
for i, token_id in enumerate(top_5_tokens, 1):
    token = tokenizer.decode([token_id])
    prob = F.softmax(mask_token_logits, dim=1)[0, token_id].item()
    print(f"  {i}. {token}: {prob:.4f}")

# Multiple mask example
print("\n--- Multiple Masks ---")
text_multi_mask = "I love [MASK] learning and [MASK] intelligence."
print(f"Input: {text_multi_mask}\n")

inputs_multi = tokenizer(text_multi_mask, return_tensors='pt').to(device)
mask_indices = torch.where(inputs_multi['input_ids'] == tokenizer.mask_token_id)[1]

with torch.no_grad():
    outputs_multi = mlm_model(**inputs_multi)
    predictions_multi = outputs_multi.logits

for idx, mask_pos in enumerate(mask_indices, 1):
    mask_logits = predictions_multi[0, mask_pos, :]
    top_token_id = torch.argmax(mask_logits).item()
    top_token = tokenizer.decode([top_token_id])
    prob = F.softmax(mask_logits, dim=0)[top_token_id].item()
    print(f"[MASK] {idx}: {top_token} ({prob:.4f})")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Masked Language Modeling Demo ===

Input: The capital of France is [MASK].

Top 5 predictions:
  1. paris: 0.8234
  2. lyon: 0.0456
  3. france: 0.0234
  4. marseille: 0.0189
  5. unknown: 0.0067

--- Multiple Masks ---
Input: I love [MASK] learning and [MASK] intelligence.

[MASK] 1: machine (0.7845)
[MASK] 2: artificial (0.8923)
</code></pre>
<hr/>
<h2>3.4 Fine-tuning Methods</h2>
<h3>Full-Parameter Fine-tuning</h3>
<p><strong>Full-parameter fine-tuning</strong> is a method that updates all parameters of a pre-trained model with task-specific data.</p>
<div class="mermaid">
graph TB
    subgraph Pretrained["Pre-trained Model"]
        P1[Embedding Layer] --&gt; P2[Transformer Layer 1]
        P2 --&gt; P3[Transformer Layer 2]
        P3 --&gt; P4[...]
        P4 --&gt; P5[Transformer Layer 12]
    end

    subgraph TaskHead["Task-Specific Head"]
        T1[Classification Head<br/>Dropout + Linear]
    end

    subgraph FineTuning["Fine-tuning"]
        F1[Update All Parameters]
    end

    P5 --&gt; T1
    P1 -.update.-&gt; F1
    P2 -.update.-&gt; F1
    P3 -.update.-&gt; F1
    P5 -.update.-&gt; F1
    T1 -.update.-&gt; F1

    style F1 fill:#e8f5e9
    style T1 fill:#fff3e0
</div>
<h3>Implementation Example 4: Full-Parameter Fine-tuning for Sentiment Analysis</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - transformers&gt;=4.30.0

"""
Example: Implementation Example 4: Full-Parameter Fine-tuning for Sen

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
import numpy as np

print("\n=== Full-Parameter Fine-tuning ===\n")

# Custom dataset
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Sample data (in practice, use large-scale datasets like IMDb)
train_texts = [
    "This movie is fantastic! I loved every minute.",
    "Terrible film, waste of time and money.",
    "An absolute masterpiece of cinema.",
    "Boring and predictable plot.",
    "One of the best movies I've ever seen!",
    "Disappointing and poorly acted."
] * 100  # Data augmentation simulation

train_labels = [1, 0, 1, 0, 1, 0] * 100  # 1: Positive, 0: Negative

# Dataset and dataloader
train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Build model
num_labels = 2  # Binary classification
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels
).to(device)

print(f"Task: Sentiment Analysis (Binary Classification)")
print(f"Number of labels: {num_labels}")
print(f"Training samples: {len(train_dataset)}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n")

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
num_epochs = 3
num_training_steps = num_epochs * len(train_loader)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0.1 * num_training_steps,
    num_training_steps=num_training_steps
)

print("=== Training Configuration ===")
print(f"Optimizer: AdamW")
print(f"Learning rate: 2e-5")
print(f"Weight Decay: 0.01")
print(f"Epochs: {num_epochs}")
print(f"Batch size: 8")
print(f"Warmup steps: {int(0.1 * num_training_steps)}\n")

# Training loop (simplified)
print("=== Training Started ===")
model.train()

for epoch in range(num_epochs):
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    for batch_idx, batch in enumerate(train_loader):
        # Transfer to GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        logits = outputs.logits

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        # Calculate metrics
        total_loss += loss.item()
        predictions = torch.argmax(logits, dim=1)
        correct_predictions += (predictions == labels).sum().item()
        total_predictions += labels.size(0)

        # Display progress every 10 batches
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = correct_predictions / total_predictions
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, "
                  f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

    epoch_loss = total_loss / len(train_loader)
    epoch_accuracy = correct_predictions / total_predictions
    print(f"\nEpoch {epoch+1} completed: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}\n")

print("Training completed!")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Full-Parameter Fine-tuning ===

Task: Sentiment Analysis (Binary Classification)
Number of labels: 2
Training samples: 600
Total parameters: 109,483,778
Trainable parameters: 109,483,778

=== Training Configuration ===
Optimizer: AdamW
Learning rate: 2e-5
Weight Decay: 0.01
Epochs: 3
Batch size: 8
Warmup steps: 22

=== Training Started ===
Epoch 1/3, Batch 10/75, Loss: 0.6234, Accuracy: 0.6250
Epoch 1/3, Batch 20/75, Loss: 0.5123, Accuracy: 0.7375
Epoch 1/3, Batch 30/75, Loss: 0.3987, Accuracy: 0.8208
Epoch 1/3, Batch 40/75, Loss: 0.2876, Accuracy: 0.8813
Epoch 1/3, Batch 50/75, Loss: 0.2234, Accuracy: 0.9150
Epoch 1/3, Batch 60/75, Loss: 0.1823, Accuracy: 0.9354
Epoch 1/3, Batch 70/75, Loss: 0.1534, Accuracy: 0.9482

Epoch 1 completed: Loss = 0.1423, Accuracy = 0.9517

Epoch 2/3, Batch 10/75, Loss: 0.0876, Accuracy: 0.9750
Epoch 2/3, Batch 20/75, Loss: 0.0723, Accuracy: 0.9813
...

Epoch 3 completed: Loss = 0.0312, Accuracy = 0.9933

Training completed!
</code></pre>
<h3>LoRA (Low-Rank Adaptation) Principles</h3>
<p><strong>LoRA</strong> is an efficient fine-tuning method for large-scale models that applies low-rank decomposition to weight matrices.</p>
<p>Original weight update:</p>
<p>$$
W' = W + \Delta W
$$</p>
<p>In LoRA, $\Delta W$ is decomposed into low-rank:</p>
<p>$$
\Delta W = BA
$$</p>
<p>where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, and $r \ll \min(d, k)$.</p>
<div class="mermaid">
graph LR
    subgraph Original["Original Weight W"]
        O1[d √ó k<br/>109M params]
    end

    subgraph LoRA["LoRA Decomposition"]
        L1[B: d √ó r] --&gt; L2[A: r √ó k]
    end

    subgraph Savings["Parameter Reduction"]
        S1[With r=8<br/>Less than 1%]
    end

    O1 -.frozen.-&gt; O1
    L1 --&gt; S1
    L2 --&gt; S1

    style O1 fill:#e0e0e0
    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style S1 fill:#fff3e0
</div>
<p>Parameter reduction rate:</p>
<p>$$
\text{Reduction rate} = \frac{r(d + k)}{d \times k} \times 100\%
$$</p>
<p>Example: For $d=768$, $k=768$, $r=8$:</p>
<p>$$
\text{Reduction rate} = \frac{8 \times (768 + 768)}{768 \times 768} \times 100\% = 2.08\%
$$</p>
<h3>Implementation Example 5: LoRA Fine-tuning</h3>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model, TaskType

print("\n=== LoRA Fine-tuning ===\n")

# New base model (for LoRA)
base_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
).to(device)

# LoRA configuration
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,  # Sequence Classification
    r=8,                          # LoRA rank
    lora_alpha=16,                # Scaling factor
    lora_dropout=0.1,             # LoRA dropout
    target_modules=["query", "value"],  # Apply to Q, V in attention layers
)

# Create LoRA model
lora_model = get_peft_model(base_model, lora_config)
lora_model.print_trainable_parameters()

# Parameter comparison
total_params = sum(p.numel() for p in lora_model.parameters())
trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)

print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Trainable ratio: {100 * trainable_params / total_params:.2f}%")
print(f"Memory reduction: Approximately {100 - 100 * trainable_params / total_params:.1f}%\n")

# Training with LoRA (code same as full-parameter FT)
print("LoRA training features:")
print("  ‚úì Training speed: Approximately 1.5-2x faster")
print("  ‚úì Memory usage: Approximately 50-70% reduction")
print("  ‚úì Performance: Comparable to full-parameter FT")
print("  ‚úì Model size: Few MB when saved (original model is several GB)")
print("  ‚úì Multi-task: Can switch between multiple LoRA adapters")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== LoRA Fine-tuning ===

trainable params: 294,912 || all params: 109,778,690 || trainable%: 0.2687%

Total parameters: 109,778,690
Trainable parameters: 294,912
Trainable ratio: 0.27%
Memory reduction: Approximately 99.7%

LoRA training features:
  ‚úì Training speed: Approximately 1.5-2x faster
  ‚úì Memory usage: Approximately 50-70% reduction
  ‚úì Performance: Comparable to full-parameter FT
  ‚úì Model size: Few MB when saved (original model is several GB)
  ‚úì Multi-task: Can switch between multiple LoRA adapters
</code></pre>
<h3>Comparison with Adapter Layers</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Trainable Parameters</th>
<th>Inference Speed</th>
<th>Implementation Difficulty</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full-Parameter FT</strong></td>
<td>100%</td>
<td>Standard</td>
<td>Easy</td>
<td>Highest</td>
</tr>
<tr>
<td><strong>Adapter Layers</strong></td>
<td>1-5%</td>
<td>Slightly slower</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>0.1-1%</td>
<td>Standard</td>
<td>Easy</td>
<td>High</td>
</tr>
<tr>
<td><strong>Prefix Tuning</strong></td>
<td>0.01-0.1%</td>
<td>Standard</td>
<td>Difficult</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr/>
<h2>3.5 Practice: Complete Pipeline for Sentiment Analysis</h2>
<h3>Implementation Example 6: Data Preparation and Tokenization</h3>
<pre><code class="language-python">from datasets import load_dataset
from sklearn.model_selection import train_test_split

print("\n=== Complete Sentiment Analysis Pipeline ===\n")

# Dataset loading (using Hugging Face Datasets)
print("--- Dataset Preparation ---")

# Sample dataset (in practice, use IMDb, SST-2, etc.)
sample_data = {
    'text': [
        "This movie exceeded all my expectations!",
        "Absolutely terrible, do not watch.",
        "A brilliant masterpiece of storytelling.",
        "Waste of time, boring from start to finish.",
        "Incredible performances by all actors!",
        "The worst film I've seen this year.",
        "Highly recommend, a must-see!",
        "Disappointing and uninspired."
    ] * 125,  # Scale to 1000 samples
    'label': [1, 0, 1, 0, 1, 0, 1, 0] * 125
}

# Train/Test split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    sample_data['text'],
    sample_data['label'],
    test_size=0.2,
    random_state=42
)

print(f"Training data: {len(train_texts)} samples")
print(f"Test data: {len(test_texts)} samples")
print(f"Label distribution: {sum(train_labels)} Positive, {len(train_labels) - sum(train_labels)} Negative\n")

# Create datasets
train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length=128)
test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_length=128)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

print(f"Training batches: {len(train_loader)}")
print(f"Test batches: {len(test_loader)}\n")

# Token statistics
sample_lengths = []
for text in train_texts[:100]:
    tokens = tokenizer.tokenize(text)
    sample_lengths.append(len(tokens))

print(f"Average token length: {np.mean(sample_lengths):.1f}")
print(f"Maximum token length: {np.max(sample_lengths)}")
print(f"95th percentile: {np.percentile(sample_lengths, 95):.0f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Complete Sentiment Analysis Pipeline ===

--- Dataset Preparation ---
Training data: 800 samples
Test data: 200 samples
Label distribution: 400 Positive, 400 Negative

Training batches: 50
Test batches: 13

Average token length: 8.3
Maximum token length: 12
95th percentile: 11
</code></pre>
<h3>Implementation Example 7: Model Training and Evaluation</h3>
<pre><code class="language-python">from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

print("\n=== Model Training and Evaluation ===\n")

# Initialize model and optimizer
model_ft = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)
optimizer = AdamW(model_ft.parameters(), lr=2e-5, weight_decay=0.01)

# Training function
def train_epoch(model, data_loader, optimizer):
    model.train()
    total_loss = 0
    predictions_list = []
    labels_list = []

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        loss = outputs.loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        predictions = torch.argmax(outputs.logits, dim=1)
        predictions_list.extend(predictions.cpu().numpy())
        labels_list.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(labels_list, predictions_list)
    return avg_loss, accuracy

# Evaluation function
def evaluate(model, data_loader):
    model.eval()
    predictions_list = []
    labels_list = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            predictions = torch.argmax(outputs.logits, dim=1)

            predictions_list.extend(predictions.cpu().numpy())
            labels_list.extend(labels.cpu().numpy())

    accuracy = accuracy_score(labels_list, predictions_list)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels_list, predictions_list, average='binary'
    )

    return accuracy, precision, recall, f1, predictions_list, labels_list

# Execute training
print("--- Training Started ---")
num_epochs = 3

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model_ft, train_loader, optimizer)
    test_acc, test_prec, test_rec, test_f1, _, _ = evaluate(model_ft, test_loader)

    print(f"Epoch {epoch+1}/{num_epochs}:")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"  Test Acc: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\n")

# Final evaluation
print("--- Final Evaluation ---")
final_acc, final_prec, final_rec, final_f1, predictions, true_labels = evaluate(model_ft, test_loader)

print(f"Accuracy: {final_acc:.4f}")
print(f"Precision: {final_prec:.4f}")
print(f"Recall: {final_rec:.4f}")
print(f"F1-Score: {final_f1:.4f}\n")

# Confusion matrix
cm = confusion_matrix(true_labels, predictions)
print("Confusion Matrix:")
print(f"              Predicted")
print(f"              Neg    Pos")
print(f"Actual Neg  [{cm[0,0]:4d}  {cm[0,1]:4d}]")
print(f"       Pos  [{cm[1,0]:4d}  {cm[1,1]:4d}]")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Model Training and Evaluation ===

--- Training Started ---
Epoch 1/3:
  Train Loss: 0.2134, Train Acc: 0.9125
  Test Acc: 0.9400, Precision: 0.9388, Recall: 0.9423, F1: 0.9405

Epoch 2/3:
  Train Loss: 0.0823, Train Acc: 0.9763
  Test Acc: 0.9600, Precision: 0.9608, Recall: 0.9615, F1: 0.9611

Epoch 3/3:
  Train Loss: 0.0412, Train Acc: 0.9900
  Test Acc: 0.9650, Precision: 0.9655, Recall: 0.9663, F1: 0.9659

--- Final Evaluation ---
Accuracy: 0.9650
Precision: 0.9655
Recall: 0.9663
F1-Score: 0.9659

Confusion Matrix:
              Predicted
              Neg    Pos
Actual Neg  [  97    3]
       Pos  [   4   96]
</code></pre>
<h3>Implementation Example 8: Inference Pipeline</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch.nn.functional as F

print("\n=== Inference Pipeline ===\n")

def predict_sentiment(text, model, tokenizer, device):
    """
    Predict sentiment for a single text

    Args:
        text: Input text
        model: Trained model
        tokenizer: Tokenizer
        device: Device

    Returns:
        label: Predicted label (Positive/Negative)
        confidence: Confidence score
    """
    model.eval()

    # Tokenization
    encoding = tokenizer(
        text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    # Inference
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)

    prediction = torch.argmax(probabilities, dim=1).item()
    confidence = probabilities[0, prediction].item()

    label = "Positive" if prediction == 1 else "Negative"
    return label, confidence

# Test sentences
test_sentences = [
    "This is the best movie I have ever seen!",
    "Absolutely horrible, a complete disaster.",
    "It was okay, nothing particularly special.",
    "Mind-blowing performance, highly recommend!",
    "Boring and predictable throughout.",
    "A true cinematic achievement!",
]

print("--- Sentiment Prediction Results ---\n")
for text in test_sentences:
    label, confidence = predict_sentiment(text, model_ft, tokenizer, device)
    print(f"Text: {text}")
    print(f"  ‚Üí Prediction: {label} (Confidence: {confidence:.4f})\n")

# Batch inference
print("--- Batch Inference Performance ---")
import time

batch_texts = test_sentences * 100  # 600 samples
start_time = time.time()

for text in batch_texts:
    _ = predict_sentiment(text, model_ft, tokenizer, device)

elapsed_time = time.time() - start_time
throughput = len(batch_texts) / elapsed_time

print(f"Number of samples: {len(batch_texts)}")
print(f"Processing time: {elapsed_time:.2f} seconds")
print(f"Throughput: {throughput:.1f} samples/second")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>
=== Inference Pipeline ===

--- Sentiment Prediction Results ---

Text: This is the best movie I have ever seen!
  ‚Üí Prediction: Positive (Confidence: 0.9987)

Text: Absolutely horrible, a complete disaster.
  ‚Üí Prediction: Negative (Confidence: 0.9993)

Text: It was okay, nothing particularly special.
  ‚Üí Prediction: Negative (Confidence: 0.6234)

Text: Mind-blowing performance, highly recommend!
  ‚Üí Prediction: Positive (Confidence: 0.9978)

Text: Boring and predictable throughout.
  ‚Üí Prediction: Negative (Confidence: 0.9856)

Text: A true cinematic achievement!
  ‚Üí Prediction: Positive (Confidence: 0.9945)

--- Batch Inference Performance ---
Number of samples: 600
Processing time: 12.34 seconds
Throughput: 48.6 samples/second
</code></pre>
<hr/>
<h2>Best Practices for Fine-tuning</h2>
<h3>Learning Rate Selection</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Recommended Learning Rate</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full-Parameter FT</strong></td>
<td>1e-5 to 5e-5</td>
<td>Fine-tune pre-trained weights</td>
</tr>
<tr>
<td><strong>Task Head Only</strong></td>
<td>1e-4 to 1e-3</td>
<td>Randomly initialized layers need higher LR</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>1e-4 to 3e-4</td>
<td>Training adaptation layers only, slightly higher</td>
</tr>
<tr>
<td><strong>Layer-wise LR</strong></td>
<td>Lower layers: 1e-5, Upper layers: 5e-5</td>
<td>Different learning rates per layer</td>
</tr>
</tbody>
</table>
<h3>Data Augmentation Strategies</h3>
<ul>
<li><strong>Back Translation</strong>: Translate to another language and back</li>
<li><strong>Synonym Replacement</strong>: Replace words with synonyms</li>
<li><strong>Random Deletion/Insertion</strong>: Randomly delete or insert words</li>
<li><strong>Mixup</strong>: Linear interpolation between samples</li>
<li><strong>Paraphrasing</strong>: Reconstruct sentences with paraphrasing models</li>
</ul>
<h3>Preventing Overfitting</h3>
<div class="mermaid">
graph LR
    A[Small Training Data] --&gt; B[High Overfitting Risk]
    B --&gt; C1[Early Stopping]
    B --&gt; C2[Increase Dropout]
    B --&gt; C3[Weight Decay]
    B --&gt; C4[Data Augmentation]
    B --&gt; C5[LoRA/Adapter]

    C1 --&gt; D[Improved Generalization]
    C2 --&gt; D
    C3 --&gt; D
    C4 --&gt; D
    C5 --&gt; D

    style B fill:#ffebee
    style D fill:#e8f5e9
</div>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned about Transformer pre-training and fine-tuning:</p>
<h3>Key Points</h3>
<details>
<summary><strong>1. Power of Transfer Learning</strong></summary>
<ul>
<li>Acquire general language knowledge through large-scale pre-training</li>
<li>Achieve high performance with small amounts of task-specific data</li>
<li>Significantly reduce development costs and resources</li>
<li>Easy application to multiple tasks</li>
</ul>
</details>
<details>
<summary><strong>2. Pre-training Methods</strong></summary>
<ul>
<li><strong>MLM</strong>: Bidirectional context, strong for classification/extraction tasks</li>
<li><strong>CLM</strong>: Unidirectional, optimal for text generation</li>
<li><strong>NSP</strong>: Understanding inter-sentence relationships (less used now)</li>
<li>Choose methods according to tasks</li>
</ul>
</details>
<details>
<summary><strong>3. Hugging Face Transformers</strong></summary>
<ul>
<li>Unified API with AutoModel/AutoTokenizer</li>
<li>One-line inference with Pipeline API</li>
<li>100,000+ pre-trained models</li>
<li>Simplify training with Trainer API</li>
</ul>
</details>
<details>
<summary><strong>4. Efficient Fine-tuning</strong></summary>
<ul>
<li><strong>Full-Parameter FT</strong>: Highest performance, high computational cost</li>
<li><strong>LoRA</strong>: 99%+ parameter reduction, maintained performance</li>
<li><strong>Adapter</strong>: Module addition, slightly slower inference</li>
<li>Choose according to task and resources</li>
</ul>
</details>
<h3>Next Steps</h3>
<p>In the next chapter, we will focus on practical applications of Transformers:</p>
<ul>
<li>Building question-answering systems</li>
<li>Text generation and prompt engineering</li>
<li>Multi-task learning and zero-shot classification</li>
<li>Utilizing large language models (LLMs)</li>
</ul>
<hr/>
<h2>Practice Problems</h2>
<details>
<summary><strong>Problem 1: Choosing Between MLM and CLM</strong></summary>
<p><strong>Question</strong>: For the following tasks, explain which is more appropriate - MLM pre-trained models (BERT) or CLM pre-trained models (GPT), along with reasons.</p>
<ol>
<li>Text classification (sentiment analysis)</li>
<li>Dialogue generation (chatbot)</li>
<li>Named Entity Recognition (NER)</li>
<li>Summarization</li>
</ol>
<p><strong>Sample Answer</strong>:</p>
<p><strong>1. Text Classification (Sentiment Analysis)</strong></p>
<ul>
<li><strong>Appropriate: BERT (MLM)</strong></li>
<li>Reason: Can consider context of all words bidirectionally, understanding overall sentence meaning is important</li>
<li>Can represent entire sentence with [CLS] token representation</li>
</ul>
<p><strong>2. Dialogue Generation (Chatbot)</strong></p>
<ul>
<li><strong>Appropriate: GPT (CLM)</strong></li>
<li>Reason: Generation task that autoregressively predicts next words</li>
<li>Sequential generation from left to right is natural</li>
</ul>
<p><strong>3. Named Entity Recognition (NER)</strong></p>
<ul>
<li><strong>Appropriate: BERT (MLM)</strong></li>
<li>Reason: Classification of each token requires context before and after</li>
<li>Maximizes context information with bidirectional Attention</li>
</ul>
<p><strong>4. Summarization</strong></p>
<ul>
<li><strong>Appropriate: GPT (CLM) or T5 (Seq2Seq)</strong></li>
<li>Reason: Task of generating summary sentences</li>
<li>Autoregressive models are optimal for generation tasks</li>
<li>Encoder-Decoder models like T5 are also excellent</li>
</ul>
</details>
<details>
<summary><strong>Problem 2: LoRA Parameter Reduction Calculation</strong></summary>
<p><strong>Question</strong>: Apply LoRA to the Attention layers (Query, Key, Value, Output) of BERT-base model (hidden_size=768, 12 layers). Calculate the number of trainable parameters for rank r=16.</p>
<p><strong>Sample Answer</strong>:</p>
<p><strong>Original weights</strong>:</p>
<ul>
<li>4 weight matrices (Q, K, V, Output) in each Attention layer</li>
<li>Each weight: 768 √ó 768 = 589,824 parameters</li>
<li>Per layer: 4 √ó 589,824 = 2,359,296 parameters</li>
<li>Total for 12 layers: 12 √ó 2,359,296 = 28,311,552 parameters</li>
</ul>
<p><strong>LoRA additional parameters (r=16)</strong>:</p>
<ul>
<li>For each weight: B (768√ó16) + A (16√ó768)</li>
<li>One LoRA: 768√ó16 + 16√ó768 = 24,576 parameters</li>
<li>4 weights (Q, K, V, Output): 4 √ó 24,576 = 98,304 parameters/layer</li>
<li>Total for 12 layers: 12 √ó 98,304 = 1,179,648 parameters</li>
</ul>
<p><strong>Reduction rate</strong>:</p>
<p>$$
\frac{1,179,648}{28,311,552} \times 100\% = 4.17\%
$$</p>
<p>This means <strong>approximately 96% parameter reduction</strong> can be achieved for Attention layers alone.</p>
</details>
<details>
<summary><strong>Problem 3: Selecting Fine-tuning Strategy</strong></summary>
<p><strong>Question</strong>: For the following three scenarios, select the optimal fine-tuning strategy and explain your reasoning.</p>
<p><strong>Scenario A</strong>: 100,000 training samples, 1 GPU (16GB), need to complete training in 3 days</p>
<p><strong>Scenario B</strong>: 500 training samples, 1 GPU (8GB), overfitting concerns</p>
<p><strong>Scenario C</strong>: Support 20 tasks simultaneously, model size constraints</p>
<p><strong>Sample Answer</strong>:</p>
<p><strong>Scenario A</strong>:</p>
<ul>
<li><strong>Recommended: Full-parameter fine-tuning</strong></li>
<li>Reason: Sufficient data, time available, can pursue highest performance</li>
<li>Feasible on 16GB GPU, convergence within 3 days</li>
</ul>
<p><strong>Scenario B</strong>:</p>
<ul>
<li><strong>Recommended: LoRA + Data Augmentation</strong></li>
<li>Reason: High overfitting risk with small data, LoRA reduces trainable parameters</li>
<li>Feasible on 8GB GPU</li>
<li>Increase effective data amount with data augmentation</li>
</ul>
<p><strong>Scenario C</strong>:</p>
<ul>
<li><strong>Recommended: LoRA (Multi-adapter)</strong></li>
<li>Reason: Support with 1 base model + 20 LoRA adapters</li>
<li>Each adapter is several MB, significantly reduce total capacity</li>
<li>Easy task switching, load adapter at inference time</li>
</ul>
</details>
<details>
<summary><strong>Problem 4: Impact of Pre-training Data</strong></summary>
<p><strong>Question</strong>: When fine-tuning a BERT model that does not include medical literature in its pre-training data for a disease classification task in the medical domain, what challenges can you anticipate? List three or more and propose countermeasures.</p>
<p><strong>Sample Answer</strong>:</p>
<p><strong>Challenge 1: Lack of Domain-Specific Vocabulary</strong></p>
<ul>
<li>Problem: Medical terms (e.g., "diabetes", "myocardial infarction") are subword-segmented and not properly represented</li>
<li>Countermeasure: Perform additional pre-training in medical domain (Domain-Adaptive Pretraining)</li>
</ul>
<p><strong>Challenge 2: Context Understanding Mismatch</strong></p>
<ul>
<li>Problem: Differences in writing style and structure between general text and medical literature</li>
<li>Countermeasure: Use medical BERT models (BioBERT, ClinicalBERT, etc.)</li>
</ul>
<p><strong>Challenge 3: Lack of Specialized Knowledge</strong></p>
<ul>
<li>Problem: Does not understand relationships between diseases or medical causality</li>
<li>Countermeasure: Integrate medical knowledge graphs with Knowledge-enhanced methods</li>
</ul>
<p><strong>Challenge 4: Performance Limitations</strong></p>
<ul>
<li>Problem: General BERT has low performance in specialized domains</li>
<li>Countermeasure: Fine-tune with large amounts of medical data or use domain-specific models</li>
</ul>
</details>
<details>
<summary><strong>Problem 5: Hyperparameter Optimization</strong></summary>
<p><strong>Question</strong>: When fine-tuning BERT for sentiment analysis tasks, explain the impact of the following hyperparameters and propose recommended values.</p>
<ol>
<li>Learning Rate</li>
<li>Batch Size</li>
<li>Number of Warmup Steps</li>
<li>Weight Decay</li>
<li>Number of Epochs</li>
</ol>
<p><strong>Sample Answer</strong>:</p>
<p><strong>1. Learning Rate</strong></p>
<ul>
<li><strong>Impact</strong>: Too high causes divergence, too low causes slow convergence</li>
<li><strong>Recommended Value</strong>: 2e-5 to 5e-5 (full-parameter FT), 1e-4 to 3e-4 (LoRA)</li>
<li><strong>Adjustment</strong>: Gradually decrease with Learning Rate Scheduler</li>
</ul>
<p><strong>2. Batch Size</strong></p>
<ul>
<li><strong>Impact</strong>: Large is stable training but increases memory consumption, small is unstable</li>
<li><strong>Recommended Value</strong>: 16-32 (adjust according to GPU memory)</li>
<li><strong>Technique</strong>: Increase effective batch size with Gradient Accumulation</li>
</ul>
<p><strong>3. Number of Warmup Steps</strong></p>
<ul>
<li><strong>Impact</strong>: Suppresses sudden weight changes in early training, stabilizes learning</li>
<li><strong>Recommended Value</strong>: 10% of total training steps (e.g., 100 steps out of 1000 steps)</li>
<li><strong>Effect</strong>: Particularly effective for small datasets</li>
</ul>
<p><strong>4. Weight Decay</strong></p>
<ul>
<li><strong>Impact</strong>: L2 regularization prevents overfitting</li>
<li><strong>Recommended Value</strong>: 0.01 to 0.1</li>
<li><strong>Note</strong>: Do not apply to LayerNorm or Bias</li>
</ul>
<p><strong>5. Number of Epochs</strong></p>
<ul>
<li><strong>Impact</strong>: Too many causes overfitting, too few causes underfitting</li>
<li><strong>Recommended Value</strong>: 3-5 epochs (fewer for pre-trained models)</li>
<li><strong>Technique</strong>: Stop when validation loss increases with Early Stopping</li>
</ul>
<p><strong>Optimization Priority</strong>: Learning Rate &gt; Batch Size &gt; Warmup &gt; Epochs &gt; Weight Decay</p>
</details>
<hr/>
<div class="navigation">
<a class="nav-button" href="chapter2-architecture.html">‚Üê Chapter 2: Transformer Architecture</a>
<a class="nav-button" href="chapter4-applications.html">Chapter 4: Practical Applications ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is for educational, research, and informational purposes only, and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operational safety, etc.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>Copyright and licensing of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
<p>ML-A03: Introduction to Transformers - Build High-Performance NLP Models with Pre-training and Fine-tuning</p>
</footer>
</body>
</html>
