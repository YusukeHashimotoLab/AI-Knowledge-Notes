<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Transformer Architecture - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "âš ï¸";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">â€º</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">â€º</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Transformer Architectureï¼ˆTransformer Architectureï¼‰</h1>
            <p class="subtitle">Encoder-Decoderã®å®Œå…¨ç†è§£ã¨PyTorchã«ã‚ˆã‚‹å®Ÿè£…</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– Reading Time: 30-35 minutes</span>
                <span class="meta-item">ğŸ“Š Difficulty: Intermediate to Advanced</span>
                <span class="meta-item">ğŸ’» Code Examples: 12</span>
                <span class="meta-item">ğŸ“ Exercises: 5å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h2>Learning Objectives</h2>
<p>ã“ã® ChapterReadã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<ul>
<li>âœ… Transformerã®å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆEncoder-Decoderæ§‹é€ ï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… Encoderã®æ§‹æˆè¦ç´ ï¼ˆMulti-Head Attentionã€FFNã€Layer Normã€Residualï¼‰ã‚’èª¬æ˜ã§ãã‚‹</li>
<li>âœ… Decoderã®ç‰¹å¾´ï¼ˆMasked Self-Attentionã€Cross-Attentionï¼‰ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… PyTorchã§Transformerã‚’å®Œå…¨å®Ÿè£…ã§ãã‚‹</li>
<li>âœ… è‡ªå·±å›å¸°ç”Ÿæˆã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹</li>
<li>âœ… å®Ÿè·µçš„ãªæ©Ÿæ¢°ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹</li>
</ul>

<hr>

<h2>2.1 Transformerå…¨ä½“åƒ</h2>

<h3>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¦‚è¦</h3>

<p><strong>Transformer</strong>ã¯ã€"Attention is All You Need" (Vaswani et al., 2017)ã§ææ¡ˆã•ã‚ŒãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€RNNãƒ»CNNã‚’ä½¿ã‚ãšã€<strong>Attentionæ©Ÿæ§‹ã®ã¿</strong>ã§ç³»åˆ—å¤‰æ›ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    Input["å…¥åŠ›ç³»åˆ—<br/>(Source)"] --> Encoder["Encoder<br/>(Nå±¤ã‚¹ã‚¿ãƒƒã‚¯)"]
    Encoder --> Memory["ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿è¡¨ç¾<br/>(Memory)"]
    Memory --> Decoder["Decoder<br/>(Nå±¤ã‚¹ã‚¿ãƒƒã‚¯)"]
    Target["ç›®æ¨™ç³»åˆ—<br/>(Target)"] --> Decoder
    Decoder --> Output["å‡ºåŠ›ç³»åˆ—<br/>(Prediction)"]

    style Encoder fill:#b3e5fc
    style Decoder fill:#ffab91
    style Memory fill:#fff9c4
</div>

<h3>ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h3>

<table>
<thead>
<tr>
<th>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</th>
<th>å½¹å‰²</th>
<th>ç‰¹å¾´</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>å…¥åŠ›ç³»åˆ—ã‚’æ–‡è„ˆè¡¨ç¾ã«å¤‰æ›</td>
<td>6å±¤ã‚¹ã‚¿ãƒƒã‚¯ã€ä¸¦åˆ—å‡¦ç†å¯èƒ½</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¡¨ç¾ã‹ã‚‰å‡ºåŠ›ç³»åˆ—ã‚’ç”Ÿæˆ</td>
<td>6å±¤ã‚¹ã‚¿ãƒƒã‚¯ã€è‡ªå·±å›å¸°ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹</td>
<td>8ãƒ˜ãƒƒãƒ‰ä¸¦åˆ—å®Ÿè¡Œ</td>
</tr>
<tr>
<td><strong>Feed-Forward Network</strong></td>
<td>å„ä½ç½®ã‚’ç‹¬ç«‹ã«å¤‰æ›</td>
<td>2å±¤MLPï¼ˆReLUæ´»æ€§åŒ–ï¼‰</td>
</tr>
<tr>
<td><strong>Positional Encoding</strong></td>
<td>ä½ç½®æƒ…å ±ã‚’æ³¨å…¥</td>
<td>Sin/Cosé–¢æ•°ãƒ™ãƒ¼ã‚¹</td>
</tr>
<tr>
<td><strong>Layer Normalization</strong></td>
<td>å­¦ç¿’ã‚’å®‰å®šåŒ–</td>
<td>å„ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼å¾Œã«é©ç”¨</td>
</tr>
<tr>
<td><strong>Residual Connection</strong></td>
<td>å‹¾é…æµã‚’æ”¹å–„</td>
<td>Skip connection</td>
</tr>
</tbody>
</table>

<h3>RNNã¨ã®é•ã„</h3>

<table>
<thead>
<tr>
<th>é …ç›®</th>
<th>RNN/LSTM</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>å‡¦ç†æ–¹å¼</strong></td>
<td>é€æ¬¡çš„ï¼ˆsequentialï¼‰</td>
<td>ä¸¦åˆ—çš„ï¼ˆparallelï¼‰</td>
</tr>
<tr>
<td><strong>é•·æœŸä¾å­˜</strong></td>
<td>è·é›¢ãŒé›¢ã‚Œã‚‹ã¨å¼±ã¾ã‚‹</td>
<td>è·é›¢ã«é–¢ä¿‚ãªãç›´æ¥æ¥ç¶š</td>
</tr>
<tr>
<td><strong>è¨ˆç®—è¤‡é›‘åº¦</strong></td>
<td>$O(n)$ hours</td>
<td>$O(1)$ hoursï¼ˆä¸¦åˆ—åŒ–å¯èƒ½ï¼‰</td>
</tr>
<tr>
<td><strong>ãƒ¡ãƒ¢ãƒª</strong></td>
<td>éš ã‚ŒçŠ¶æ…‹ã§åœ§ç¸®</td>
<td>å…¨ä½ç½®ã®æƒ…å ±ã‚’ä¿æŒ</td>
</tr>
<tr>
<td><strong>è¨“ç·´é€Ÿåº¦</strong></td>
<td>é…ã„</td>
<td>é«˜é€Ÿï¼ˆGPUæ´»ç”¨ï¼‰</td>
</tr>
</tbody>
</table>

<blockquote>
<p>ã€ŒTransformerã¯ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚Šã€RNNã‚ˆã‚Š10å€ä»¥ä¸Šé«˜é€Ÿã«è¨“ç·´ã§ãã¾ã™ï¼ã€</p>
</blockquote>

<h3>åŸºæœ¬æ§‹é€ ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

# Transformerã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print("=== Transformerã®åŸºæœ¬è¨­å®š ===")
d_model = 512        # ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
nhead = 8            # Attentionãƒ˜ãƒƒãƒ‰æ•°
num_layers = 6       # Encoder/Decoderã®å±¤æ•°
d_ff = 2048          # Feed-Forwardã®éš ã‚Œå±¤ã‚µã‚¤ã‚º
dropout = 0.1        # Dropoutç‡
max_len = 5000       # æœ€å¤§ç³»åˆ—é•·

print(f"ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ: d_model = {d_model}")
print(f"Attentionãƒ˜ãƒƒãƒ‰æ•°: nhead = {nhead}")
print(f"å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ: d_k = d_v = {d_model // nhead}")
print(f"Encoder/Decoderå±¤æ•°: {num_layers}")
print(f"FFNéš ã‚Œå±¤ã‚µã‚¤ã‚º: {d_ff}")
print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆæ¦‚ç®—ï¼‰: {(num_layers * 2) * (4 * d_model**2 + 2 * d_model * d_ff):,}")

# å…¥å‡ºåŠ›ã‚µã‚¤ã‚ºã®ä¾‹
batch_size = 32
src_len = 20  # ã‚½ãƒ¼ã‚¹ç³»åˆ—é•·
tgt_len = 15  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—é•·

print(f"\n=== å…¥å‡ºåŠ›ä¾‹ ===")
print(f"å…¥åŠ›ï¼ˆã‚½ãƒ¼ã‚¹ï¼‰: ({batch_size}, {src_len}, {d_model})")
print(f"å…¥åŠ›ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰: ({batch_size}, {tgt_len}, {d_model})")
print(f"å‡ºåŠ›: ({batch_size}, {tgt_len}, {d_model})")
</code></pre>

<hr>

<h2>2.2 Encoderæ§‹é€ </h2>

<h3>Encoderã®å½¹å‰²</h3>

<p>Encoderã¯å…¥åŠ›ç³»åˆ—ã‚’ã€å„ä½ç½®ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸé«˜æ¬¡å…ƒè¡¨ç¾ã«å¤‰æ›ã—ã¾ã™ã€‚Nå±¤ï¼ˆé€šå¸¸6å±¤ï¼‰ã®EncoderLayerã‚’ã‚¹ã‚¿ãƒƒã‚¯ã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    Input["å…¥åŠ›åŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"] --> E1["Encoder Layer 1"]
    E1 --> E2["Encoder Layer 2"]
    E2 --> E3["..."]
    E3 --> EN["Encoder Layer N"]
    EN --> Output["ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿è¡¨ç¾"]

    style Input fill:#e1f5ff
    style Output fill:#b3e5fc
</div>

<h3>EncoderLayerã®æ§‹é€ </h3>

<p>å„EncoderLayerã¯ã€ä»¥ä¸‹ã®2ã¤ã®ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<ol>
<li><strong>Multi-Head Self-Attention</strong>ï¼šå…¥åŠ›ç³»åˆ—å†…ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹</li>
<li><strong>Position-wise Feed-Forward Network</strong>ï¼šå„ä½ç½®ã‚’ç‹¬ç«‹ã«å¤‰æ›</li>
</ol>

<p>å„ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã¯ã€<strong>Residual Connection</strong>ã¨<strong>Layer Normalization</strong>ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    X["å…¥åŠ› x"] --> MHA["Multi-Head<br/>Self-Attention"]
    MHA --> Add1["Add & Norm"]
    X --> Add1
    Add1 --> FFN["Feed-Forward<br/>Network"]
    Add1 --> Add2["Add & Norm"]
    FFN --> Add2
    Add2 --> Y["å‡ºåŠ› y"]

    style MHA fill:#b3e5fc
    style FFN fill:#ffccbc
    style Add1 fill:#c5e1a5
    style Add2 fill:#c5e1a5
</div>

<h3>Multi-Head Attentionã®æ•°å¼</h3>

<p>Multi-Head Attentionã¯ã€è¤‡æ•°ã®ç•°ãªã‚‹è¡¨ç¾éƒ¨ minutesç©ºé–“ã§ä¸¦åˆ—ã«Attentionã‚’è¨ˆç®—ã—ã¾ã™ï¼š</p>

$$
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{align}
$$

<p>ã“ã“ã§ï¼š</p>
<ul>
<li>$h$ï¼šãƒ˜ãƒƒãƒ‰æ•°ï¼ˆé€šå¸¸8ï¼‰</li>
<li>$d_k = d_v = d_{\text{model}} / h$ï¼šå„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ</li>
<li>$W_i^Q, W_i^K, W_i^V, W^O$ï¼šå­¦ç¿’å¯èƒ½ãªå°„å½±è¡Œåˆ—</li>
</ul>

<h3>EncoderLayerã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    """Multi-Head Attentionæ©Ÿæ§‹"""
    def __init__(self, d_model, nhead, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % nhead == 0, "d_model must be divisible by nhead"

        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead

        # Q, K, V ã®ç·šå½¢å¤‰æ›
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # å‡ºåŠ›ã®ç·šå½¢å¤‰æ›
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """(batch, seq_len, d_model) -> (batch, nhead, seq_len, d_k)"""
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        """(batch, nhead, seq_len, d_k) -> (batch, seq_len, d_model)"""
        batch_size, nhead, seq_len, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, query, key, value, mask=None):
        """
        query, key, value: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) or (batch, seq_len, seq_len)
        """
        # ç·šå½¢å¤‰æ›
        Q = self.W_q(query)  # (batch, seq_len, d_model)
        K = self.W_k(key)
        V = self.W_v(value)

        # ãƒ˜ãƒƒãƒ‰ã« minuteså‰²
        Q = self.split_heads(Q)  # (batch, nhead, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # scores: (batch, nhead, seq_len, seq_len)

        # ãƒã‚¹ã‚¯é©ç”¨
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Valueã¨ã®ç©
        attn_output = torch.matmul(attn_weights, V)
        # attn_output: (batch, nhead, seq_len, d_k)

        # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ
        attn_output = self.combine_heads(attn_output)
        # attn_output: (batch, seq_len, d_model)

        # å‡ºåŠ›ç·šå½¢å¤‰æ›
        output = self.W_o(attn_output)

        return output, attn_weights


class PositionwiseFeedForward(nn.Module):
    """Position-wise Feed-Forward Network"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class EncoderLayer(nn.Module):
    """Transformer Encoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()

        # Multi-Head Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        x: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        """
        # Self-Attention + Residual + Norm
        attn_output, attn_weights = self.self_attn(x, x, x, mask)
        x = x + self.dropout1(attn_output)  # Residual connection
        x = self.norm1(x)  # Layer normalization

        # Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout2(ffn_output)  # Residual connection
        x = self.norm2(x)  # Layer normalization

        return x, attn_weights


# å‹•ä½œç¢ºèª
print("=== EncoderLayerã®å‹•ä½œç¢ºèª ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
seq_len = 20

encoder_layer = EncoderLayer(d_model, nhead, d_ff)
x = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = encoder_layer(x)

print(f"å…¥åŠ›: {x.shape}")
print(f"å‡ºåŠ›: {output.shape}")
print(f"Attentioné‡ã¿: {attn_weights.shape}")
print("â†’ å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚µã‚¤ã‚ºãŒåŒã˜ï¼ˆæ®‹å·®æ¥ç¶šã®ãŸã‚ï¼‰")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in encoder_layer.parameters())
print(f"\nEncoderLayer ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<h3>å®Œå…¨ãªEncoderã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆSin/Cosï¼‰"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’ä½œæˆ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class TransformerEncoder(nn.Module):
    """å®Œå…¨ãªTransformer Encoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model

        # å˜èªåŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # EncoderLayerã‚’ã‚¹ã‚¿ãƒƒã‚¯
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        """
        src: (batch, src_len) - ãƒˆãƒ¼ã‚¯ãƒ³ID
        src_mask: (batch, 1, src_len) - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        """
        # åŸ‹ã‚è¾¼ã¿ + ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        x = self.embedding(src) * math.sqrt(self.d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
        x = self.pos_encoding(x)

        # å„EncoderLayerã‚’é€šé
        attn_weights_list = []
        for layer in self.layers:
            x, attn_weights = layer(x, src_mask)
            attn_weights_list.append(attn_weights)

        return x, attn_weights_list


# å‹•ä½œç¢ºèª
print("\n=== å®Œå…¨ãªEncoderã®å‹•ä½œç¢ºèª ===")
vocab_size = 10000
encoder = TransformerEncoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 16
src_len = 25
src = torch.randint(0, vocab_size, (batch_size, src_len))

# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ä½œæˆï¼ˆä¾‹ï¼šæœ€å¾Œã®5ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
src_mask = torch.ones(batch_size, 1, src_len)
src_mask[:, :, -5:] = 0

# Encoderå®Ÿè¡Œ
encoder_output, attn_weights_list = encoder(src, src_mask)

print(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {src.shape}")
print(f"Encoderå‡ºåŠ›: {encoder_output.shape}")
print(f"Attentioné‡ã¿ã®æ•°: {len(attn_weights_list)} (å±¤ã”ã¨)")
print(f"å„Attentioné‡ã¿: {attn_weights_list[0].shape}")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in encoder.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<h3>Layer Normalizationã¨Residual Connectionã®é‡è¦æ€§</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

# Layer Normalizationã®åŠ¹æœ
print("=== Layer Normalizationã®åŠ¹æœ ===")

x = torch.randn(32, 20, 512)  # (batch, seq_len, d_model)

# Layer Normalizationå‰
print(f"æ­£è¦åŒ–å‰ - å¹³å‡: {x.mean():.4f}, æ¨™æº–åå·®: {x.std():.4f}")

layer_norm = nn.LayerNorm(512)
x_normalized = layer_norm(x)

# Layer Normalizationå¾Œ
print(f"æ­£è¦åŒ–å¾Œ - å¹³å‡: {x_normalized.mean():.4f}, æ¨™æº–åå·®: {x_normalized.std():.4f}")
print("â†’ å„ã‚µãƒ³ãƒ—ãƒ«ãƒ»ä½ç½®ã§å¹³å‡0ã€æ¨™æº–åå·®1ã«æ­£è¦åŒ–")

# Residual Connectionã®åŠ¹æœ
print("\n=== Residual Connectionã®åŠ¹æœ ===")

class WithoutResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)  # Residualæ¥ç¶šãªã—
        return x

class WithResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)  # Residualæ¥ç¶šã‚ã‚Š
        return x

# å‹¾é…æµã‚’æ¯”è¼ƒ
model_without = WithoutResidual(d_model=512, num_layers=10)
model_with = WithResidual(d_model=512, num_layers=10)

x = torch.randn(1, 512, requires_grad=True)

# Forward + Backward
out_without = model_without(x)
out_without.sum().backward()
grad_without = x.grad.norm().item()

x.grad = None
out_with = model_with(x)
out_with.sum().backward()
grad_with = x.grad.norm().item()

print(f"Residualæ¥ç¶šãªã— - å‹¾é…ãƒãƒ«ãƒ : {grad_without:.6f}")
print(f"Residualæ¥ç¶šã‚ã‚Š - å‹¾é…ãƒãƒ«ãƒ : {grad_with:.6f}")
print("â†’ Residualæ¥ç¶šã«ã‚ˆã‚Šå‹¾é…ãŒæ¶ˆå¤±ã›ãšã€æ·±ã„å±¤ã§ã‚‚å­¦ç¿’å¯èƒ½")
</code></pre>

<hr>

<h2>2.3 Decoderæ§‹é€ </h2>

<h3>Decoderã®å½¹å‰²</h3>

<p>Decoderã¯ã€Encoderã®å‡ºåŠ›ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã¨æ—¢ã«ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’<strong>è‡ªå·±å›å¸°çš„</strong>ã«ç”Ÿæˆã—ã¾ã™ã€‚</p>

<div class="mermaid">
graph TB
    Target["ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—<br/>(ã‚·ãƒ•ãƒˆæ¸ˆã¿)"] --> D1["Decoder Layer 1"]
    Memory["Encoderå‡ºåŠ›<br/>(Memory)"] --> D1
    D1 --> D2["Decoder Layer 2"]
    Memory --> D2
    D2 --> D3["..."]
    Memory --> D3
    D3 --> DN["Decoder Layer N"]
    Memory --> DN
    DN --> Output["å‡ºåŠ›<br/>(æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬)"]

    style Target fill:#e1f5ff
    style Memory fill:#fff9c4
    style Output fill:#ffab91
</div>

<h3>DecoderLayerã®æ§‹é€ </h3>

<p>å„DecoderLayerã¯ã€<strong>3ã¤ã®ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼</strong>ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š</p>

<ol>
<li><strong>Masked Multi-Head Self-Attention</strong>ï¼šæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†ãƒã‚¹ã‚¯</li>
<li><strong>Cross-Attention</strong>ï¼šEncoderã®å‡ºåŠ›ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã‚’å‚ç…§</li>
<li><strong>Position-wise Feed-Forward Network</strong>ï¼šå„ä½ç½®ã‚’ç‹¬ç«‹ã«å¤‰æ›</li>
</ol>

<div class="mermaid">
graph TB
    X["å…¥åŠ› x"] --> MMHA["Masked Multi-Head<br/>Self-Attention"]
    MMHA --> Add1["Add & Norm"]
    X --> Add1

    Add1 --> CA["Cross-Attention<br/>(Encoderå‡ºåŠ›å‚ç…§)"]
    Memory["Encoder Memory"] --> CA
    CA --> Add2["Add & Norm"]
    Add1 --> Add2

    Add2 --> FFN["Feed-Forward<br/>Network"]
    FFN --> Add3["Add & Norm"]
    Add2 --> Add3
    Add3 --> Y["å‡ºåŠ› y"]

    style MMHA fill:#ffab91
    style CA fill:#ce93d8
    style FFN fill:#ffccbc
    style Memory fill:#fff9c4
</div>

<h3>Masked Self-Attentionã®é‡è¦æ€§</h3>

<p><strong>Causal Maskingï¼ˆå› æœãƒã‚¹ã‚¯ï¼‰</strong>ã«ã‚ˆã‚Šã€ä½ç½®$i$ã¯ä½ç½®$i$ä»¥å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’å‚ç…§ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨“ç·´æ™‚ã§ã‚‚æ¨è«–æ™‚ã¨åŒã˜è‡ªå·±å›å¸°çš„ãªæ¡ä»¶ã‚’ä¿ã¡ã¾ã™ã€‚</p>

$$
\text{Mask}_{ij} =
\begin{cases}
0 & \text{if } i < j \text{ (æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³)} \\
1 & \text{if } i \geq j \text{ (éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³)}
\end{cases}
$$

<h3>DecoderLayerã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class DecoderLayer(nn.Module):
    """Transformer Decoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()

        # 1. Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 2. Cross-Attentionï¼ˆEncoderã®å‡ºåŠ›ã‚’å‚ç…§ï¼‰
        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 3. Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, memory, tgt_mask=None, memory_mask=None):
        """
        x: (batch, tgt_len, d_model) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—
        memory: (batch, src_len, d_model) - Encoderã®å‡ºåŠ›
        tgt_mask: (batch, tgt_len, tgt_len) - å› æœãƒã‚¹ã‚¯
        memory_mask: (batch, 1, src_len) - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        """
        # 1. Masked Self-Attention + Residual + Norm
        self_attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)
        x = x + self.dropout1(self_attn_output)
        x = self.norm1(x)

        # 2. Cross-Attention + Residual + Norm
        # Query: Decoderã®å‡ºåŠ›, Key/Value: Encoderã®å‡ºåŠ›
        cross_attn_output, cross_attn_weights = self.cross_attn(x, memory, memory, memory_mask)
        x = x + self.dropout2(cross_attn_output)
        x = self.norm2(x)

        # 3. Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout3(ffn_output)
        x = self.norm3(x)

        return x, self_attn_weights, cross_attn_weights


# å‹•ä½œç¢ºèª
print("=== DecoderLayerã®å‹•ä½œç¢ºèª ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
tgt_len = 15
src_len = 20

decoder_layer = DecoderLayer(d_model, nhead, d_ff)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
tgt = torch.randn(batch_size, tgt_len, d_model)
memory = torch.randn(batch_size, src_len, d_model)

# å› æœãƒã‚¹ã‚¯ä½œæˆ
def create_causal_mask(seq_len):
    """ä¸‹ä¸‰è§’è¡Œåˆ—ï¼ˆæœªæ¥ã‚’ãƒã‚¹ã‚¯ï¼‰"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.unsqueeze(0)  # (1, seq_len, seq_len)

tgt_mask = create_causal_mask(tgt_len)

# Decoderå®Ÿè¡Œ
output, self_attn_weights, cross_attn_weights = decoder_layer(tgt, memory, tgt_mask)

print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¥åŠ›: {tgt.shape}")
print(f"Encoderãƒ¡ãƒ¢ãƒª: {memory.shape}")
print(f"Decoderå‡ºåŠ›: {output.shape}")
print(f"Self-Attentioné‡ã¿: {self_attn_weights.shape}")
print(f"Cross-Attentioné‡ã¿: {cross_attn_weights.shape}")
print("â†’ Cross-Attentionã§Encoderã®æƒ…å ±ã‚’å‚ç…§")
</code></pre>

<h3>å› æœãƒã‚¹ã‚¯ã®å¯è¦–åŒ–</h3>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt

# å› æœãƒã‚¹ã‚¯ã®ä½œæˆã¨å¯è¦–åŒ–
def create_and_visualize_causal_mask(seq_len=10):
    """å› æœãƒã‚¹ã‚¯ã‚’ä½œæˆã—ã¦å¯è¦–åŒ–"""
    mask = torch.tril(torch.ones(seq_len, seq_len))

    print(f"=== å› æœãƒã‚¹ã‚¯ï¼ˆç³»åˆ—é•·={seq_len}ï¼‰ ===")
    print(mask.numpy())
    print("\n1 = å‚ç…§å¯èƒ½ï¼ˆéå»ãƒ»ç¾åœ¨ï¼‰")
    print("0 = å‚ç…§ä¸å¯ï¼ˆæœªæ¥ï¼‰")
    print("\nä¾‹: ä½ç½®3ã¯ä½ç½®0,1,2,3ã®ã¿å‚ç…§å¯èƒ½ï¼ˆä½ç½®4ä»¥é™ã¯è¦‹ãˆãªã„ï¼‰")

    return mask

# ãƒã‚¹ã‚¯ä½œæˆ
causal_mask = create_and_visualize_causal_mask(seq_len=8)

# Attentionã‚¹ã‚³ã‚¢ã¸ã®é©ç”¨ä¾‹
print("\n=== ãƒã‚¹ã‚¯é©ç”¨ã®åŠ¹æœ ===")
scores = torch.randn(8, 8)  # ãƒ©ãƒ³ãƒ€ãƒ ãªAttentionã‚¹ã‚³ã‚¢

print("ãƒã‚¹ã‚¯å‰ã®ã‚¹ã‚³ã‚¢ï¼ˆä¸€éƒ¨ï¼‰:")
print(scores[:4, :4].numpy())

# ãƒã‚¹ã‚¯é©ç”¨ï¼ˆæœªæ¥ã‚’-infã«ï¼‰
masked_scores = scores.masked_fill(causal_mask == 0, float('-inf'))

print("\nãƒã‚¹ã‚¯å¾Œã®ã‚¹ã‚³ã‚¢ï¼ˆä¸€éƒ¨ï¼‰:")
print(masked_scores[:4, :4].numpy())

# Softmaxé©ç”¨
attn_weights = F.softmax(masked_scores, dim=-1)

print("\nSoftmaxå¾Œã®é‡ã¿ï¼ˆä¸€éƒ¨ï¼‰:")
print(attn_weights[:4, :4].numpy())
print("â†’ æœªæ¥ã®ä½ç½®ï¼ˆ-infï¼‰ã®é‡ã¿ã¯0ã«ãªã‚‹")
</code></pre>

<h3>å®Œå…¨ãªDecoderã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import math

class TransformerDecoder(nn.Module):
    """å®Œå…¨ãªTransformer Decoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerDecoder, self).__init__()

        self.d_model = d_model

        # å˜èªåŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # DecoderLayerã‚’ã‚¹ã‚¿ãƒƒã‚¯
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        # å‡ºåŠ›å±¤
        self.fc_out = nn.Linear(d_model, vocab_size)

        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        tgt: (batch, tgt_len) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³ID
        memory: (batch, src_len, d_model) - Encoderã®å‡ºåŠ›
        tgt_mask: (batch, tgt_len, tgt_len) - å› æœãƒã‚¹ã‚¯
        memory_mask: (batch, 1, src_len) - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        """
        # åŸ‹ã‚è¾¼ã¿ + ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        x = self.embedding(tgt) * math.sqrt(self.d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
        x = self.pos_encoding(x)

        # å„DecoderLayerã‚’é€šé
        self_attn_weights_list = []
        cross_attn_weights_list = []

        for layer in self.layers:
            x, self_attn_weights, cross_attn_weights = layer(x, memory, tgt_mask, memory_mask)
            self_attn_weights_list.append(self_attn_weights)
            cross_attn_weights_list.append(cross_attn_weights)

        # èªå½™ã¸ã®å°„å½±
        logits = self.fc_out(x)  # (batch, tgt_len, vocab_size)

        return logits, self_attn_weights_list, cross_attn_weights_list


# å‹•ä½œç¢ºèª
print("\n=== å®Œå…¨ãªDecoderã®å‹•ä½œç¢ºèª ===")
vocab_size = 10000
decoder = TransformerDecoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
batch_size = 16
tgt_len = 20
src_len = 25

tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))
memory = torch.randn(batch_size, src_len, 512)

# å› æœãƒã‚¹ã‚¯
tgt_mask = create_causal_mask(tgt_len)

# Decoderå®Ÿè¡Œ
logits, self_attn_weights, cross_attn_weights = decoder(tgt, memory, tgt_mask)

print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¥åŠ›: {tgt.shape}")
print(f"Encoderãƒ¡ãƒ¢ãƒª: {memory.shape}")
print(f"Decoderå‡ºåŠ›ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰: {logits.shape}")
print(f"â†’ å„ä½ç½®ã§èªå½™å…¨ä½“ã«å¯¾ã™ã‚‹ç¢ºç‡ minuteså¸ƒã‚’å‡ºåŠ›")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in decoder.parameters())
print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
</code></pre>

<hr>

<h2>2.4 å®Œå…¨ãªTransformerãƒ¢ãƒ‡ãƒ«</h2>

<h3>Encoderã¨Decoderã®çµ±åˆ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

class Transformer(nn.Module):
    """å®Œå…¨ãªTransformerãƒ¢ãƒ‡ãƒ«ï¼ˆEncoder-Decoderï¼‰"""
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,
                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,
                 dropout=0.1, max_len=5000):
        super(Transformer, self).__init__()

        # Encoder
        self.encoder = TransformerEncoder(
            src_vocab_size, d_model, nhead, num_encoder_layers,
            d_ff, dropout, max_len
        )

        # Decoder
        self.decoder = TransformerDecoder(
            tgt_vocab_size, d_model, nhead, num_decoder_layers,
            d_ff, dropout, max_len
        )

        self.d_model = d_model

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
        self._reset_parameters()

    def _reset_parameters(self):
        """XavieråˆæœŸåŒ–"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        src: (batch, src_len) - ã‚½ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        tgt: (batch, tgt_len) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³
        src_mask: (batch, 1, src_len) - ã‚½ãƒ¼ã‚¹ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        tgt_mask: (batch, tgt_len, tgt_len) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å› æœãƒã‚¹ã‚¯
        """
        # Encoderã§ã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†
        memory, _ = self.encoder(src, src_mask)

        # Decoderã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ç”Ÿæˆ
        output, _, _ = self.decoder(tgt, memory, tgt_mask, src_mask)

        return output

    def encode(self, src, src_mask=None):
        """Encoderã®ã¿å®Ÿè¡Œï¼ˆæ¨è«–æ™‚ã«ä½¿ç”¨ï¼‰"""
        memory, _ = self.encoder(src, src_mask)
        return memory

    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """Decoderã®ã¿å®Ÿè¡Œï¼ˆæ¨è«–æ™‚ã«ä½¿ç”¨ï¼‰"""
        output, _, _ = self.decoder(tgt, memory, tgt_mask, memory_mask)
        return output


# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
print("=== å®Œå…¨ãªTransformerãƒ¢ãƒ‡ãƒ« ===")
src_vocab_size = 10000
tgt_vocab_size = 8000

model = Transformer(
    src_vocab_size=src_vocab_size,
    tgt_vocab_size=tgt_vocab_size,
    d_model=512,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048,
    dropout=0.1
)

# å‹•ä½œç¢ºèª
batch_size = 16
src_len = 25
tgt_len = 20

src = torch.randint(0, src_vocab_size, (batch_size, src_len))
tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))

# ãƒã‚¹ã‚¯ä½œæˆ
src_mask = torch.ones(batch_size, 1, src_len)
tgt_mask = create_causal_mask(tgt_len)

# Forward pass
output = model(src, tgt, src_mask, tgt_mask)

print(f"ã‚½ãƒ¼ã‚¹å…¥åŠ›: {src.shape}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¥åŠ›: {tgt.shape}")
print(f"ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›: {output.shape}")
print(f"â†’ å‡ºåŠ›ã¯ (batch, tgt_len, tgt_vocab_size) ã®å½¢çŠ¶")

# ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
</code></pre>

<h3>è‡ªå·±å›å¸°ç”Ÿæˆã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn.functional as F

def generate_greedy(model, src, src_mask, max_len, start_token, end_token):
    """
    Greedy Decodingã«ã‚ˆã‚‹ç³»åˆ—ç”Ÿæˆ

    Args:
        model: Transformerãƒ¢ãƒ‡ãƒ«
        src: (batch, src_len) - ã‚½ãƒ¼ã‚¹ç³»åˆ—
        src_mask: (batch, 1, src_len) - ã‚½ãƒ¼ã‚¹ãƒã‚¹ã‚¯
        max_len: æœ€å¤§ç”Ÿæˆé•·
        start_token: é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ID
        end_token: çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID

    Returns:
        generated: (batch, gen_len) - ç”Ÿæˆç³»åˆ—
    """
    model.eval()
    batch_size = src.size(0)
    device = src.device

    # Encoderã§ä¸€åº¦ã ã‘å‡¦ç†
    memory = model.encode(src, src_mask)

    # ç”Ÿæˆç³»åˆ—ã®åˆæœŸåŒ–ï¼ˆé–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
    generated = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)

    # è‡ªå·±å›å¸°çš„ã«ç”Ÿæˆ
    for _ in range(max_len - 1):
        # å› æœãƒã‚¹ã‚¯ä½œæˆ
        tgt_len = generated.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # Decoderå®Ÿè¡Œ
        output = model.decode(generated, memory, tgt_mask, src_mask)

        # æœ€å¾Œã®ä½ç½®ã®äºˆæ¸¬ã‚’å–å¾—
        next_token_logits = output[:, -1, :]  # (batch, vocab_size)

        # Greedyé¸æŠï¼ˆæœ€ã‚‚ç¢ºç‡ãŒé«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # (batch, 1)

        # ç”Ÿæˆç³»åˆ—ã«è¿½åŠ 
        generated = torch.cat([generated, next_token], dim=1)

        # å…¨ã‚µãƒ³ãƒ—ãƒ«ãŒçµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
        if (next_token == end_token).all():
            break

    return generated


def generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5):
    """
    Beam Searchã«ã‚ˆã‚‹ç³»åˆ—ç”Ÿæˆ

    Args:
        model: Transformerãƒ¢ãƒ‡ãƒ«
        src: (1, src_len) - ã‚½ãƒ¼ã‚¹ç³»åˆ—ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º1ï¼‰
        src_mask: (1, 1, src_len)
        max_len: æœ€å¤§ç”Ÿæˆé•·
        start_token: é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ID
        end_token: çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID
        beam_size: ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚º

    Returns:
        best_sequence: (1, gen_len) - æœ€è‰¯ã®ç”Ÿæˆç³»åˆ—
    """
    model.eval()
    device = src.device

    # Encoder
    memory = model.encode(src, src_mask)  # (1, src_len, d_model)
    memory = memory.repeat(beam_size, 1, 1)  # (beam_size, src_len, d_model)

    # ãƒ“ãƒ¼ãƒ åˆæœŸåŒ–
    beams = torch.full((beam_size, 1), start_token, dtype=torch.long, device=device)
    beam_scores = torch.zeros(beam_size, device=device)
    beam_scores[1:] = float('-inf')  # æœ€åˆã¯1ã¤ã®ãƒ“ãƒ¼ãƒ ã®ã¿æœ‰åŠ¹

    finished_beams = []

    for step in range(max_len - 1):
        tgt_len = beams.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # Decoder
        output = model.decode(beams, memory, tgt_mask, src_mask.repeat(beam_size, 1, 1))
        next_token_logits = output[:, -1, :]  # (beam_size, vocab_size)

        # Logç¢ºç‡
        log_probs = F.log_softmax(next_token_logits, dim=-1)

        # ãƒ“ãƒ¼ãƒ ã‚¹ã‚³ã‚¢æ›´æ–°
        vocab_size = log_probs.size(-1)
        scores = beam_scores.unsqueeze(1) + log_probs  # (beam_size, vocab_size)
        scores = scores.view(-1)  # (beam_size * vocab_size)

        # Top-ké¸æŠ
        top_scores, top_indices = scores.topk(beam_size, largest=True)

        # æ–°ã—ã„ãƒ“ãƒ¼ãƒ 
        beam_indices = top_indices // vocab_size
        token_indices = top_indices % vocab_size

        new_beams = []
        new_scores = []

        for i, (beam_idx, token_idx, score) in enumerate(zip(beam_indices, token_indices, top_scores)):
            # ãƒ“ãƒ¼ãƒ ã‚’æ‹¡å¼µ
            new_beam = torch.cat([beams[beam_idx], token_idx.unsqueeze(0)])

            # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ°é”ã—ãŸã‚‰å®Œæˆãƒ“ãƒ¼ãƒ ã«è¿½åŠ 
            if token_idx == end_token:
                finished_beams.append((new_beam, score.item()))
            else:
                new_beams.append(new_beam)
                new_scores.append(score)

        # å®Œæˆãƒ“ãƒ¼ãƒ ãŒå minutesã‚ã‚Œã°çµ‚äº†
        if len(finished_beams) >= beam_size:
            break

        # ãƒ“ãƒ¼ãƒ ãŒæ®‹ã£ã¦ã„ãªã„å ´åˆã‚‚çµ‚äº†
        if len(new_beams) == 0:
            break

        # ãƒ“ãƒ¼ãƒ ã‚’æ›´æ–°
        beams = torch.stack(new_beams)
        beam_scores = torch.tensor(new_scores, device=device)

    # æœ€è‰¯ã®ãƒ“ãƒ¼ãƒ ã‚’é¸æŠ
    if finished_beams:
        best_beam, best_score = max(finished_beams, key=lambda x: x[1])
    else:
        best_beam = beams[0]

    return best_beam.unsqueeze(0)


# å‹•ä½œç¢ºèª
print("\n=== è‡ªå·±å›å¸°ç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ ===")

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿
src_vocab_size = 100
tgt_vocab_size = 100
model = Transformer(src_vocab_size, tgt_vocab_size, d_model=128, nhead=4,
                   num_encoder_layers=2, num_decoder_layers=2)

src = torch.randint(1, src_vocab_size, (1, 10))
src_mask = torch.ones(1, 1, 10)

start_token = 1
end_token = 2
max_len = 20

# Greedy Decoding
with torch.no_grad():
    generated_greedy = generate_greedy(model, src, src_mask, max_len, start_token, end_token)

print(f"ã‚½ãƒ¼ã‚¹ç³»åˆ—: {src.shape}")
print(f"Greedyç”Ÿæˆ: {generated_greedy.shape}")
print(f"ç”Ÿæˆç³»åˆ—: {generated_greedy[0].tolist()}")

# Beam Search
with torch.no_grad():
    generated_beam = generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5)

print(f"\nBeam Searchç”Ÿæˆ: {generated_beam.shape}")
print(f"ç”Ÿæˆç³»åˆ—: {generated_beam[0].tolist()}")
</code></pre>

<hr>

<h2>2.5 å®Ÿè·µï¼šæ©Ÿæ¢°ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ </h2>

<h3>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™</h3>

<pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re

class TranslationDataset(Dataset):
    """ç°¡æ˜“çš„ãªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src = self.src_sentences[idx]
        tgt = self.tgt_sentences[idx]

        # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        src_ids = [self.src_vocab.get(w, self.src_vocab['<unk>']) for w in src.split()]
        tgt_ids = [self.tgt_vocab.get(w, self.tgt_vocab['<unk>']) for w in tgt.split()]

        return torch.tensor(src_ids), torch.tensor(tgt_ids)


def build_vocab(sentences, max_vocab_size=10000):
    """èªå½™ã‚’æ§‹ç¯‰"""
    words = []
    for sent in sentences:
        words.extend(sent.split())

    # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(words)
    most_common = word_counts.most_common(max_vocab_size - 4)  # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ minutesã‚’é™¤ã

    # èªå½™è¾æ›¸ä½œæˆ
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    for word, _ in most_common:
        vocab[word] = len(vocab)

    return vocab


# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯Multi30kã‚„WMTãªã©ã‚’ä½¿ç”¨ï¼‰
src_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need",
    "deep learning is amazing",
    "natural language processing"
]

tgt_sentences = [
    "ç§ ã¯ æ©Ÿæ¢° å­¦ç¿’ ãŒ å¥½ã ã§ã™",
    "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ ã¯ å¼·åŠ› ã§ã™",
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ ãŒ å…¨ã¦ ã§ã™",
    "æ·±å±¤ å­¦ç¿’ ã¯ ç´ æ™´ã‚‰ã—ã„ ã§ã™",
    "è‡ªç„¶ è¨€èª å‡¦ç†"
]

# èªå½™æ§‹ç¯‰
src_vocab = build_vocab(src_sentences)
tgt_vocab = build_vocab(tgt_sentences)

print("=== ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ===")
print(f"ã‚½ãƒ¼ã‚¹èªå½™ã‚µã‚¤ã‚º: {len(src_vocab)}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèªå½™ã‚µã‚¤ã‚º: {len(tgt_vocab)}")
print(f"\nã‚½ãƒ¼ã‚¹èªå½™ï¼ˆä¸€éƒ¨ï¼‰: {list(src_vocab.items())[:10]}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèªå½™ï¼ˆä¸€éƒ¨ï¼‰: {list(tgt_vocab.items())[:10]}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
src_sample, tgt_sample = dataset[0]
print(f"\nã‚µãƒ³ãƒ—ãƒ« 0:")
print(f"ã‚½ãƒ¼ã‚¹: {src_sentences[0]}")
print(f"ã‚½ãƒ¼ã‚¹ID: {src_sample.tolist()}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {tgt_sentences[0]}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆID: {tgt_sample.tolist()}")
</code></pre>

<h3>è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch, src_vocab, tgt_vocab):
    """ãƒãƒƒãƒã®ã‚³ãƒ¬ãƒ¼ãƒˆé–¢æ•°"""
    src_batch, tgt_batch = zip(*batch)

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])

    return src_padded, tgt_padded


def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):
    """ãƒã‚¹ã‚¯ã‚’ä½œæˆ"""
    # ã‚½ãƒ¼ã‚¹ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
    src_mask = (src != src_pad_idx).unsqueeze(1)  # (batch, 1, src_len)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å› æœãƒã‚¹ã‚¯ + ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
    tgt_len = tgt.size(1)
    tgt_mask = create_causal_mask(tgt_len).to(tgt.device)  # (1, tgt_len, tgt_len)
    tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)
    tgt_mask = tgt_mask & tgt_pad_mask

    return src_mask, tgt_mask


def train_epoch(model, dataloader, optimizer, criterion, src_vocab, tgt_vocab, device):
    """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
    model.train()
    total_loss = 0

    for src, tgt in dataloader:
        src, tgt = src.to(device), tgt.to(device)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å…¥åŠ›ã¨æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã« minuteså‰²
        tgt_input = tgt[:, :-1]  # <eos>ã‚’é™¤ã
        tgt_output = tgt[:, 1:]  # <sos>ã‚’é™¤ã

        # ãƒã‚¹ã‚¯ä½œæˆ
        src_mask, tgt_mask = create_masks(src, tgt_input,
                                         src_vocab['<pad>'], tgt_vocab['<pad>'])

        # Forward
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask, tgt_mask)

        # Lossè¨ˆç®—ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç„¡è¦–ï¼‰
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)

        loss = criterion(output, tgt_output)

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


# è¨“ç·´è¨­å®š
print("\n=== ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ ===")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = Transformer(
    src_vocab_size=len(src_vocab),
    tgt_vocab_size=len(tgt_vocab),
    d_model=256,
    nhead=8,
    num_encoder_layers=3,
    num_decoder_layers=3,
    d_ff=1024,
    dropout=0.1
).to(device)

# DataLoader
from functools import partial
loader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    collate_fn=partial(collate_fn, src_vocab=src_vocab, tgt_vocab=tgt_vocab)
)

# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])
optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# è¨“ç·´
num_epochs = 10
for epoch in range(num_epochs):
    loss = train_epoch(model, loader, optimizer, criterion, src_vocab, tgt_vocab, device)
    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

print("\nè¨“ç·´å®Œäº†ï¼")
</code></pre>

<h3>ç¿»è¨³æ¨è«–</h3>

<pre><code class="language-python">import torch

def translate(model, src_sentence, src_vocab, tgt_vocab, device, max_len=50):
    """æ–‡ã‚’ç¿»è¨³"""
    model.eval()

    # ã‚½ãƒ¼ã‚¹æ–‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
    src_tokens = src_sentence.split()
    src_ids = [src_vocab.get(w, src_vocab['<unk>']) for w in src_tokens]
    src = torch.tensor(src_ids).unsqueeze(0).to(device)  # (1, src_len)

    # ã‚½ãƒ¼ã‚¹ãƒã‚¹ã‚¯
    src_mask = torch.ones(1, 1, src.size(1)).to(device)

    # Greedy Decodingã§ç”Ÿæˆ
    with torch.no_grad():
        generated = generate_greedy(
            model, src, src_mask, max_len,
            start_token=tgt_vocab['<sos>'],
            end_token=tgt_vocab['<eos>']
        )

    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å˜èªã«å¤‰æ›
    idx_to_word = {v: k for k, v in tgt_vocab.items()}
    translated = [idx_to_word.get(idx.item(), '<unk>') for idx in generated[0]]

    # <sos>ã¨<eos>ã‚’é™¤å»
    if translated[0] == '<sos>':
        translated = translated[1:]
    if '<eos>' in translated:
        eos_idx = translated.index('<eos>')
        translated = translated[:eos_idx]

    return ' '.join(translated)


# ç¿»è¨³ãƒ†ã‚¹ãƒˆ
print("\n=== ç¿»è¨³ãƒ†ã‚¹ãƒˆ ===")

test_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need"
]

for src_sent in test_sentences:
    translated = translate(model, src_sent, src_vocab, tgt_vocab, device)
    print(f"ã‚½ãƒ¼ã‚¹: {src_sent}")
    print(f"ç¿»è¨³: {translated}")
    print()

print("â†’ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å®Œç’§ã§ã¯ãªã„ãŒã€åŸºæœ¬çš„ãªç¿»è¨³æ©Ÿèƒ½ã‚’å®Ÿè£…")
</code></pre>

<hr>

<h2>2.6 Transformerã®å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</h2>

<h3>å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—</h3>

<p>Transformerã®è¨“ç·´ã§ã¯ã€<strong>ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</strong>ãŒé‡è¦ã§ã™ã€‚åˆæœŸã¯å­¦ç¿’ç‡ã‚’å°ã•ãä¿ã¡ã€å¾ã€…ã«å¢—ã‚„ã—ã¦ã‹ã‚‰æ¸›è¡°ã•ã›ã¾ã™ã€‚</p>

$$
\text{lr}(step) = d_{\text{model}}^{-0.5} \cdot \min(step^{-0.5}, step \cdot \text{warmup\_steps}^{-1.5})
$$

<pre><code class="language-python">import torch.optim as optim

class NoamOpt:
    """Noamå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆè«–æ–‡å®Ÿè£…ï¼‰"""
    def __init__(self, d_model, warmup_steps, optimizer):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.optimizer = optimizer
        self._step = 0
        self._rate = 0

    def step(self):
        """1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°"""
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        """ç¾åœ¨ã®å­¦ç¿’ç‡ã‚’è¨ˆç®—"""
        if step is None:
            step = self._step
        return (self.d_model ** (-0.5)) * min(step ** (-0.5),
                                               step * self.warmup_steps ** (-1.5))

# ä½¿ç”¨ä¾‹
print("=== Noamå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© ===")
d_model = 512
warmup_steps = 4000

optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
scheduler = NoamOpt(d_model, warmup_steps, optimizer)

# å­¦ç¿’ç‡ã®æ¨ç§»ã‚’å¯è¦–åŒ–
steps = list(range(1, 20000))
lrs = [scheduler.rate(step) for step in steps]

print(f"åˆæœŸå­¦ç¿’ç‡ï¼ˆstep=1ï¼‰: {lrs[0]:.6f}")
print(f"ãƒ”ãƒ¼ã‚¯å­¦ç¿’ç‡ï¼ˆstep={warmup_steps}ï¼‰: {lrs[warmup_steps-1]:.6f}")
print(f"å¾ŒæœŸå­¦ç¿’ç‡ï¼ˆstep=20000ï¼‰: {lrs[-1]:.6f}")
print("â†’ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§å¾ã€…ã«å¢—åŠ ã€ãã®å¾Œæ¸›è¡°")
</code></pre>

<h3>Label Smoothing</h3>

<p><strong>Label Smoothing</strong>ã¯ã€æ­£è§£ãƒ©ãƒ™ãƒ«ã®ç¢ºç‡ã‚’1ã§ã¯ãªã0.9ç¨‹åº¦ã«ã—ã€ä»–ã®ã‚¯ãƒ©ã‚¹ã«å°‘ã—ç¢ºç‡ã‚’ minutesæ•£ã•ã›ã‚‹æ­£å‰‡åŒ–æ‰‹æ³•ã§ã™ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class LabelSmoothingLoss(nn.Module):
    """Label Smoothingä»˜ãCross Entropy Loss"""
    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing

    def forward(self, pred, target):
        """
        pred: (batch * seq_len, num_classes) - ãƒ­ã‚¸ãƒƒãƒˆ
        target: (batch * seq_len) - æ­£è§£ãƒ©ãƒ™ãƒ«
        """
        # Log-softmax
        log_probs = F.log_softmax(pred, dim=-1)

        # æ­£è§£ä½ç½®ã®ç¢ºç‡ã‚’å–å¾—
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)

        # å…¨ã‚¯ãƒ©ã‚¹ã®å¹³å‡logç¢ºç‡
        smooth_loss = -log_probs.mean(dim=-1)

        # çµ„ã¿åˆã‚ã›
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        # ignore_indexã‚’ãƒã‚¹ã‚¯
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float()
            loss = (loss * mask).sum() / mask.sum()
        else:
            loss = loss.mean()

        return loss


# æ¯”è¼ƒ
print("\n=== Label Smoothingã®åŠ¹æœ ===")

num_classes = 10
criterion_normal = nn.CrossEntropyLoss()
criterion_smooth = LabelSmoothingLoss(num_classes, smoothing=0.1)

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
pred = torch.randn(32, num_classes)
target = torch.randint(0, num_classes, (32,))

loss_normal = criterion_normal(pred, target)
loss_smooth = criterion_smooth(pred, target)

print(f"é€šå¸¸ã®Cross Entropy Loss: {loss_normal.item():.4f}")
print(f"Label Smoothing Loss: {loss_smooth.item():.4f}")
print("â†’ Label Smoothingã¯éä¿¡ã‚’é˜²ãã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Š")
</code></pre>

<h3>Mixed Precision Training</h3>

<pre><code class="language-python">import torch
from torch.cuda.amp import autocast, GradScaler

# Mixed Precision Trainingï¼ˆGPUåˆ©ç”¨æ™‚ï¼‰
if torch.cuda.is_available():
    print("\n=== Mixed Precision Trainingã®ä¾‹ ===")

    device = torch.device('cuda')
    model = model.to(device)

    scaler = GradScaler()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®ä¸€éƒ¨
    for src, tgt in loader:
        src, tgt = src.to(device), tgt.to(device)
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        optimizer.zero_grad()

        # Mixed Precisionã§è¨ˆç®—
        with autocast():
            output = model(src, tgt_input)
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            loss = criterion(output, tgt_output)

        # ã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸå‹¾é…ã§é€†ä¼æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print("â†’ FP16è¨ˆç®—ã§é«˜é€ŸåŒ–ï¼†ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆæœ€å¤§2å€é«˜é€Ÿï¼‰")
else:
    print("\n=== Mixed Precision Training ===")
    print("GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
</code></pre>

<hr>

<h2>Exercises</h2>

<details>
<summary><strong>æ¼”ç¿’1ï¼šMulti-Head Attentionã®ãƒ˜ãƒƒãƒ‰æ•°ã®å½±éŸ¿</strong></summary>

<p>ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆ1, 2, 4, 8, 16ï¼‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€æ€§èƒ½ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: ãƒ˜ãƒƒãƒ‰æ•°ã‚’å¤‰ãˆã¦è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
# TODO: åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€æ€§èƒ½ãƒ»è¨“ç·´ hoursãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ¯”è¼ƒ
# TODO: Attentionå¯è¦–åŒ–ã§ãƒ˜ãƒƒãƒ‰ã”ã¨ã®å½¹å‰² minutesæ‹…ã‚’ minutesæ
# ãƒ’ãƒ³ãƒˆ: ãƒ˜ãƒƒãƒ‰æ•°ãŒå¤šã„ã»ã©æ€§èƒ½å‘ä¸Šã™ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚‚å¢—åŠ 
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’2ï¼šPositional Encodingã®å®Ÿé¨“</strong></summary>

<p>Sin/Cosä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: 2ç¨®é¡ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…
# 1. Sin/Cosï¼ˆå›ºå®šï¼‰
# 2. nn.Embeddingï¼ˆå­¦ç¿’å¯èƒ½ï¼‰

# TODO: åŒã˜ã‚¿ã‚¹ã‚¯ã§æ€§èƒ½æ¯”è¼ƒ
# TODO: ç³»åˆ—é•·ã®æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ï¼ˆè¨“ç·´ã‚ˆã‚Šé•·ã„ç³»åˆ—ã§ãƒ†ã‚¹ãƒˆï¼‰
# æœŸå¾…: Sin/Cosã¯ä»»æ„é•·ã«æ±åŒ–å¯èƒ½
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’3ï¼šå› æœãƒã‚¹ã‚¯ã®å¯è¦–åŒ–</strong></summary>

<p>Decoderã®å› æœãƒã‚¹ã‚¯ãŒAttentioné‡ã¿ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt

# TODO: DecoderLayerã®Self-Attentioné‡ã¿ã‚’å–å¾—
# TODO: ãƒã‚¹ã‚¯æœ‰ã‚Šãƒ»ç„¡ã—ã§Attentioné‡ã¿ã‚’å¯è¦–åŒ–
# TODO: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ãˆãªã„ã“ã¨ã‚’ç¢ºèª
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’4ï¼šBeam Searchã®ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºæœ€é©åŒ–</strong></summary>

<p>ç•°ãªã‚‹ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºï¼ˆ1, 3, 5, 10, 20ï¼‰ã§ç¿»è¨³å“è³ªã¨é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import time

# TODO: ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦ç¿»è¨³ã‚’å®Ÿè¡Œ
# TODO: BLEU ã‚¹ã‚³ã‚¢ã€ç”Ÿæˆ hoursã‚’è¨ˆæ¸¬
# TODO: ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚º vs å“è³ªãƒ»é€Ÿåº¦ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
# æœŸå¾…: ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚º5-10ã§å“è³ªã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„
</code></pre>

</details>

<details>
<summary><strong>æ¼”ç¿’5ï¼šLayeræ•°ã®å½±éŸ¿ã‚’èª¿æŸ»</strong></summary>

<p>Encoder/Decoderã®å±¤æ•°ï¼ˆ1, 2, 4, 6, 12ï¼‰ã‚’å¤‰ãˆã¦æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚</p>

<pre><code class="language-python">import torch
import torch.nn as nn

# TODO: ç•°ãªã‚‹å±¤æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
# TODO: è¨“ç·´Lossã€æ¤œè¨¼Lossã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€è¨“ç·´ hoursã‚’è¨˜éŒ²
# TODO: å±¤æ•° vs æ€§èƒ½ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
#  minutesæ: æ·±ã™ãã‚‹ã¨éå­¦ç¿’ãƒ»è¨“ç·´ hourså¢—ã€æµ…ã™ãã‚‹ã¨è¡¨ç¾åŠ›ä¸è¶³
</code></pre>

</details>

<hr>

<h2>Summary</h2>

<p>ã“ã® Chapterã§ã¯ã€Transformerã®å®Œå…¨ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å­¦ã³ã¾ã—ãŸã€‚</p>

<h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3>

<ul>
<li><strong>Transformerã®æ§‹é€ </strong>ï¼šEncoder-Decoderã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€6å±¤ã‚¹ã‚¿ãƒƒã‚¯</li>
<li><strong>Encoder</strong>ï¼šMulti-Head Self-Attention + FFNã€ä¸¦åˆ—å‡¦ç†å¯èƒ½</li>
<li><strong>Decoder</strong>ï¼šMasked Self-Attention + Cross-Attention + FFNã€è‡ªå·±å›å¸°ç”Ÿæˆ</li>
<li><strong>Multi-Head Attention</strong>ï¼šè¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹</li>
<li><strong>Positional Encoding</strong>ï¼šSin/Cosé–¢æ•°ã§ä½ç½®æƒ…å ±ã‚’æ³¨å…¥</li>
<li><strong>Residual + Layer Norm</strong>ï¼šæ·±ã„å±¤ã§ã‚‚å­¦ç¿’ã‚’å®‰å®šåŒ–</li>
<li><strong>å› æœãƒã‚¹ã‚¯</strong>ï¼šæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†åˆ¶å¾¡</li>
<li><strong>è‡ªå·±å›å¸°ç”Ÿæˆ</strong>ï¼šGreedy Decodingã€Beam Search</li>
<li><strong>å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯</strong>ï¼šã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã€Label Smoothingã€Mixed Precision</li>
<li><strong>å®Ÿè·µ</strong>ï¼šæ©Ÿæ¢°ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨å®Ÿè£…</li>
</ul>

<h3>Next Steps</h3>

<p>æ¬¡ Chapterã§ã¯ã€<strong>Transformerã®å­¦ç¿’ã¨æœ€é©åŒ–</strong>ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚åŠ¹ç‡çš„ãªè¨“ç·´æ‰‹æ³•ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã€è©•ä¾¡æŒ‡æ¨™ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã€å®Ÿç”¨çš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç¿’å¾—ã—ã¾ã™ã€‚</p>

<div class="navigation">
    <a href="chapter1-attention.html" class="nav-button">â† Chapter 1 Chapterï¼šAttentionæ©Ÿæ§‹</a>
    <a href="chapter3-training.html" class="nav-button">Chapter 3 Chapterï¼šå­¦ç¿’ã¨æœ€é©åŒ– â†’</a>
</div>

</main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
    <p>&copy; 2024 AI Terakoya. All rights reserved.</p>
</footer>

</body>
</html>
