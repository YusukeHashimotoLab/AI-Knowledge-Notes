<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 2: Transformer Architecture - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Transformer Architecture - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../ML/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/ML/transformer-introduction/chapter2-architecture.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 2: Transformer Architecture</h1>
<p class="subtitle">Complete Understanding of Encoder-Decoder and PyTorch Implementation</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 30-35 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 12</span>
<span class="meta-item">üìù Exercises: 5</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers Transformer Architecture. You will learn Fully implementing Transformer in PyTorch.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Understanding the overall Transformer architecture (Encoder-Decoder structure)</li>
<li>‚úÖ Explaining the components of the Encoder (Multi-Head Attention, FFN, Layer Norm, Residual)</li>
<li>‚úÖ Understanding the features of the Decoder (Masked Self-Attention, Cross-Attention)</li>
<li>‚úÖ Fully implementing Transformer in PyTorch</li>
<li>‚úÖ Understanding the mechanism of autoregressive generation</li>
<li>‚úÖ Building a practical machine translation system</li>
</ul>
<hr/>
<h2>2.1 Transformer Overview</h2>
<h3>Architecture Overview</h3>
<p>The <strong>Transformer</strong> is a revolutionary architecture proposed in "Attention is All You Need" (Vaswani et al., 2017), which achieves sequence-to-sequence transformation using <strong>only Attention mechanisms</strong> without RNNs or CNNs.</p>
<div class="mermaid">
graph TB
    Input["Input Sequence<br/>(Source)"] --&gt; Encoder["Encoder<br/>(N-layer Stack)"]
    Encoder --&gt; Memory["Encoded Representation<br/>(Memory)"]
    Memory --&gt; Decoder["Decoder<br/>(N-layer Stack)"]
    Target["Target Sequence<br/>(Target)"] --&gt; Decoder
    Decoder --&gt; Output["Output Sequence<br/>(Prediction)"]

    style Encoder fill:#b3e5fc
    style Decoder fill:#ffab91
    style Memory fill:#fff9c4
</div>
<h3>Main Components</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Role</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder</strong></td>
<td>Transforms input sequence into contextual representation</td>
<td>6-layer stack, parallelizable</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>Generates output sequence from encoded representation</td>
<td>6-layer stack, autoregressive generation</td>
</tr>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>Captures dependencies from multiple perspectives</td>
<td>8 heads in parallel</td>
</tr>
<tr>
<td><strong>Feed-Forward Network</strong></td>
<td>Transforms each position independently</td>
<td>2-layer MLP (ReLU activation)</td>
</tr>
<tr>
<td><strong>Positional Encoding</strong></td>
<td>Injects positional information</td>
<td>Sin/Cos function-based</td>
</tr>
<tr>
<td><strong>Layer Normalization</strong></td>
<td>Stabilizes training</td>
<td>Applied after each sublayer</td>
</tr>
<tr>
<td><strong>Residual Connection</strong></td>
<td>Improves gradient flow</td>
<td>Skip connection</td>
</tr>
</tbody>
</table>
<h3>Differences from RNN</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>RNN/LSTM</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Processing Method</strong></td>
<td>Sequential</td>
<td>Parallel</td>
</tr>
<tr>
<td><strong>Long-term Dependencies</strong></td>
<td>Weakens with distance</td>
<td>Direct connection regardless of distance</td>
</tr>
<tr>
<td><strong>Computational Complexity</strong></td>
<td>$O(n)$ time</td>
<td>$O(1)$ time (parallelizable)</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Compressed in hidden state</td>
<td>Retains information from all positions</td>
</tr>
<tr>
<td><strong>Training Speed</strong></td>
<td>Slow</td>
<td>Fast (GPU utilization)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>"Transformers can train more than 10 times faster than RNNs through parallel processing!"</p>
</blockquote>
<h3>Basic Structure Visualization</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Basic Structure Visualization

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import torch.nn as nn
import math

# Basic Transformer parameters
print("=== Basic Transformer Configuration ===")
d_model = 512        # Model dimension
nhead = 8            # Number of Attention heads
num_layers = 6       # Number of Encoder/Decoder layers
d_ff = 2048          # Feed-Forward hidden layer size
dropout = 0.1        # Dropout rate
max_len = 5000       # Maximum sequence length

print(f"Model dimension: d_model = {d_model}")
print(f"Number of Attention heads: nhead = {nhead}")
print(f"Dimension per head: d_k = d_v = {d_model // nhead}")
print(f"Encoder/Decoder layers: {num_layers}")
print(f"FFN hidden layer size: {d_ff}")
print(f"Total parameters (estimate): {(num_layers * 2) * (4 * d_model**2 + 2 * d_model * d_ff):,}")

# Input/output size example
batch_size = 32
src_len = 20  # Source sequence length
tgt_len = 15  # Target sequence length

print(f"\n=== Input/Output Example ===")
print(f"Input (source): ({batch_size}, {src_len}, {d_model})")
print(f"Input (target): ({batch_size}, {tgt_len}, {d_model})")
print(f"Output: ({batch_size}, {tgt_len}, {d_model})")
</code></pre>
<hr/>
<h2>2.2 Encoder Structure</h2>
<h3>Role of the Encoder</h3>
<p>The Encoder transforms the input sequence into high-dimensional representations that consider the context of each position. It stacks N layers (typically 6) of EncoderLayer.</p>
<div class="mermaid">
graph TB
    Input["Input Embedding + Positional Encoding"] --&gt; E1["Encoder Layer 1"]
    E1 --&gt; E2["Encoder Layer 2"]
    E2 --&gt; E3["..."]
    E3 --&gt; EN["Encoder Layer N"]
    EN --&gt; Output["Encoded Representation"]

    style Input fill:#e1f5ff
    style Output fill:#b3e5fc
</div>
<h3>EncoderLayer Structure</h3>
<p>Each EncoderLayer consists of two sublayers:</p>
<ol>
<li><strong>Multi-Head Self-Attention</strong>: Captures dependencies within the input sequence</li>
<li><strong>Position-wise Feed-Forward Network</strong>: Transforms each position independently</li>
</ol>
<p><strong>Residual Connection</strong> and <strong>Layer Normalization</strong> are applied to each sublayer.</p>
<div class="mermaid">
graph TB
    X["Input x"] --&gt; MHA["Multi-Head<br/>Self-Attention"]
    MHA --&gt; Add1["Add &amp; Norm"]
    X --&gt; Add1
    Add1 --&gt; FFN["Feed-Forward<br/>Network"]
    Add1 --&gt; Add2["Add &amp; Norm"]
    FFN --&gt; Add2
    Add2 --&gt; Y["Output y"]

    style MHA fill:#b3e5fc
    style FFN fill:#ffccbc
    style Add1 fill:#c5e1a5
    style Add2 fill:#c5e1a5
</div>
<h3>Multi-Head Attention Formula</h3>
<p>Multi-Head Attention computes attention in parallel across multiple different representational subspaces:</p>

$$
\begin{align}
\text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &amp;= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
\text{Attention}(Q, K, V) &amp;= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{align}
$$

<p>Where:</p>
<ul>
<li>$h$: Number of heads (typically 8)</li>
<li>$d_k = d_v = d_{\text{model}} / h$: Dimension per head</li>
<li>$W_i^Q, W_i^K, W_i^V, W^O$: Learnable projection matrices</li>
</ul>
<h3>EncoderLayer Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    """Multi-Head Attention mechanism"""
    def __init__(self, d_model, nhead, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % nhead == 0, "d_model must be divisible by nhead"

        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead

        # Linear transformations for Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Output linear transformation
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """(batch, seq_len, d_model) -&gt; (batch, nhead, seq_len, d_k)"""
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        """(batch, nhead, seq_len, d_k) -&gt; (batch, seq_len, d_model)"""
        batch_size, nhead, seq_len, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, query, key, value, mask=None):
        """
        query, key, value: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) or (batch, seq_len, seq_len)
        """
        # Linear transformation
        Q = self.W_q(query)  # (batch, seq_len, d_model)
        K = self.W_k(key)
        V = self.W_v(value)

        # Split into heads
        Q = self.split_heads(Q)  # (batch, nhead, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # scores: (batch, nhead, seq_len, seq_len)

        # Apply mask
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Multiply with Value
        attn_output = torch.matmul(attn_weights, V)
        # attn_output: (batch, nhead, seq_len, d_k)

        # Combine heads
        attn_output = self.combine_heads(attn_output)
        # attn_output: (batch, seq_len, d_model)

        # Output linear transformation
        output = self.W_o(attn_output)

        return output, attn_weights


class PositionwiseFeedForward(nn.Module):
    """Position-wise Feed-Forward Network"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class EncoderLayer(nn.Module):
    """Transformer Encoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()

        # Multi-Head Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        x: (batch, seq_len, d_model)
        mask: (batch, 1, seq_len) - padding mask
        """
        # Self-Attention + Residual + Norm
        attn_output, attn_weights = self.self_attn(x, x, x, mask)
        x = x + self.dropout1(attn_output)  # Residual connection
        x = self.norm1(x)  # Layer normalization

        # Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout2(ffn_output)  # Residual connection
        x = self.norm2(x)  # Layer normalization

        return x, attn_weights


# Verification
print("=== EncoderLayer Verification ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
seq_len = 20

encoder_layer = EncoderLayer(d_model, nhead, d_ff)
x = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = encoder_layer(x)

print(f"Input: {x.shape}")
print(f"Output: {output.shape}")
print(f"Attention weights: {attn_weights.shape}")
print("‚Üí Input and output sizes are the same (due to residual connections)")

# Parameter count
total_params = sum(p.numel() for p in encoder_layer.parameters())
print(f"\nEncoderLayer parameter count: {total_params:,}")
</code></pre>
<h3>Complete Encoder Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """Positional Encoding (Sin/Cos)"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)

        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class TransformerEncoder(nn.Module):
    """Complete Transformer Encoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model

        # Word embedding
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Positional encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # Stack EncoderLayers
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        """
        src: (batch, src_len) - token IDs
        src_mask: (batch, 1, src_len) - padding mask
        """
        # Embedding + scaling
        x = self.embedding(src) * math.sqrt(self.d_model)

        # Add positional encoding
        x = self.pos_encoding(x)

        # Pass through each EncoderLayer
        attn_weights_list = []
        for layer in self.layers:
            x, attn_weights = layer(x, src_mask)
            attn_weights_list.append(attn_weights)

        return x, attn_weights_list


# Verification
print("\n=== Complete Encoder Verification ===")
vocab_size = 10000
encoder = TransformerEncoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# Dummy data
batch_size = 16
src_len = 25
src = torch.randint(0, vocab_size, (batch_size, src_len))

# Create padding mask (example: last 5 tokens are padding)
src_mask = torch.ones(batch_size, 1, src_len)
src_mask[:, :, -5:] = 0

# Execute Encoder
encoder_output, attn_weights_list = encoder(src, src_mask)

print(f"Input tokens: {src.shape}")
print(f"Encoder output: {encoder_output.shape}")
print(f"Number of attention weights: {len(attn_weights_list)} (per layer)")
print(f"Each attention weight: {attn_weights_list[0].shape}")

# Parameter count
total_params = sum(p.numel() for p in encoder.parameters())
print(f"\nTotal parameters: {total_params:,}")
</code></pre>
<h3>Importance of Layer Normalization and Residual Connection</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Importance of Layer Normalization and Residual Connection

Purpose: Demonstrate neural network implementation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn

# Effect of Layer Normalization
print("=== Effect of Layer Normalization ===")

x = torch.randn(32, 20, 512)  # (batch, seq_len, d_model)

# Before Layer Normalization
print(f"Before normalization - Mean: {x.mean():.4f}, Std: {x.std():.4f}")

layer_norm = nn.LayerNorm(512)
x_normalized = layer_norm(x)

# After Layer Normalization
print(f"After normalization - Mean: {x_normalized.mean():.4f}, Std: {x_normalized.std():.4f}")
print("‚Üí Normalized to mean 0, std 1 for each sample and position")

# Effect of Residual Connection
print("\n=== Effect of Residual Connection ===")

class WithoutResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)  # No residual connection
        return x

class WithResidual(nn.Module):
    def __init__(self, d_model, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)  # With residual connection
        return x

# Compare gradient flow
model_without = WithoutResidual(d_model=512, num_layers=10)
model_with = WithResidual(d_model=512, num_layers=10)

x = torch.randn(1, 512, requires_grad=True)

# Forward + Backward
out_without = model_without(x)
out_without.sum().backward()
grad_without = x.grad.norm().item()

x.grad = None
out_with = model_with(x)
out_with.sum().backward()
grad_with = x.grad.norm().item()

print(f"Without residual connection - gradient norm: {grad_without:.6f}")
print(f"With residual connection - gradient norm: {grad_with:.6f}")
print("‚Üí Residual connections prevent gradient vanishing, enabling training of deep layers")
</code></pre>
<hr/>
<h2>2.3 Decoder Structure</h2>
<h3>Role of the Decoder</h3>
<p>The Decoder <strong>autoregressively</strong> generates the next token from the Encoder output (memory) and the tokens already generated.</p>
<div class="mermaid">
graph TB
    Target["Target Sequence<br/>(Shifted)"] --&gt; D1["Decoder Layer 1"]
    Memory["Encoder Output<br/>(Memory)"] --&gt; D1
    D1 --&gt; D2["Decoder Layer 2"]
    Memory --&gt; D2
    D2 --&gt; D3["..."]
    Memory --&gt; D3
    D3 --&gt; DN["Decoder Layer N"]
    Memory --&gt; DN
    DN --&gt; Output["Output<br/>(Next Token Prediction)"]

    style Target fill:#e1f5ff
    style Memory fill:#fff9c4
    style Output fill:#ffab91
</div>
<h3>DecoderLayer Structure</h3>
<p>Each DecoderLayer consists of <strong>three sublayers</strong>:</p>
<ol>
<li><strong>Masked Multi-Head Self-Attention</strong>: Masks future tokens to prevent looking ahead</li>
<li><strong>Cross-Attention</strong>: References the Encoder output (memory)</li>
<li><strong>Position-wise Feed-Forward Network</strong>: Transforms each position independently</li>
</ol>
<div class="mermaid">
graph TB
    X["Input x"] --&gt; MMHA["Masked Multi-Head<br/>Self-Attention"]
    MMHA --&gt; Add1["Add &amp; Norm"]
    X --&gt; Add1

    Add1 --&gt; CA["Cross-Attention<br/>(Reference Encoder Output)"]
    Memory["Encoder Memory"] --&gt; CA
    CA --&gt; Add2["Add &amp; Norm"]
    Add1 --&gt; Add2

    Add2 --&gt; FFN["Feed-Forward<br/>Network"]
    FFN --&gt; Add3["Add &amp; Norm"]
    Add2 --&gt; Add3
    Add3 --&gt; Y["Output y"]

    style MMHA fill:#ffab91
    style CA fill:#ce93d8
    style FFN fill:#ffccbc
    style Memory fill:#fff9c4
</div>
<h3>Importance of Masked Self-Attention</h3>
<p><strong>Causal Masking</strong> ensures that position $i$ can only reference tokens at positions up to and including $i$. This maintains the same autoregressive conditions during training as during inference.</p>

$$
\text{Mask}_{ij} =
\begin{cases}
0 &amp; \text{if } i &lt; j \text{ (future tokens)} \\
1 &amp; \text{if } i \geq j \text{ (past tokens)}
\end{cases}
$$

<h3>DecoderLayer Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class DecoderLayer(nn.Module):
    """Transformer Decoder Layer"""
    def __init__(self, d_model, nhead, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()

        # 1. Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 2. Cross-Attention (reference Encoder output)
        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)

        # 3. Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, memory, tgt_mask=None, memory_mask=None):
        """
        x: (batch, tgt_len, d_model) - target sequence
        memory: (batch, src_len, d_model) - Encoder output
        tgt_mask: (batch, tgt_len, tgt_len) - causal mask
        memory_mask: (batch, 1, src_len) - padding mask
        """
        # 1. Masked Self-Attention + Residual + Norm
        self_attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)
        x = x + self.dropout1(self_attn_output)
        x = self.norm1(x)

        # 2. Cross-Attention + Residual + Norm
        # Query: Decoder output, Key/Value: Encoder output
        cross_attn_output, cross_attn_weights = self.cross_attn(x, memory, memory, memory_mask)
        x = x + self.dropout2(cross_attn_output)
        x = self.norm2(x)

        # 3. Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout3(ffn_output)
        x = self.norm3(x)

        return x, self_attn_weights, cross_attn_weights


# Verification
print("=== DecoderLayer Verification ===")
d_model = 512
nhead = 8
d_ff = 2048
batch_size = 32
tgt_len = 15
src_len = 20

decoder_layer = DecoderLayer(d_model, nhead, d_ff)

# Dummy data
tgt = torch.randn(batch_size, tgt_len, d_model)
memory = torch.randn(batch_size, src_len, d_model)

# Create causal mask
def create_causal_mask(seq_len):
    """Lower triangular matrix (mask future)"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.unsqueeze(0)  # (1, seq_len, seq_len)

tgt_mask = create_causal_mask(tgt_len)

# Execute Decoder
output, self_attn_weights, cross_attn_weights = decoder_layer(tgt, memory, tgt_mask)

print(f"Target input: {tgt.shape}")
print(f"Encoder memory: {memory.shape}")
print(f"Decoder output: {output.shape}")
print(f"Self-Attention weights: {self_attn_weights.shape}")
print(f"Cross-Attention weights: {cross_attn_weights.shape}")
print("‚Üí Cross-Attention references Encoder information")
</code></pre>
<h3>Causal Mask Visualization</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import matplotlib.pyplot as plt

# Create and visualize causal mask
def create_and_visualize_causal_mask(seq_len=10):
    """Create and visualize causal mask"""
    mask = torch.tril(torch.ones(seq_len, seq_len))

    print(f"=== Causal Mask (seq_len={seq_len}) ===")
    print(mask.numpy())
    print("\n1 = Accessible (past/present)")
    print("0 = Inaccessible (future)")
    print("\nExample: Position 3 can only reference positions 0,1,2,3 (cannot see 4 onwards)")

    return mask

# Create mask
causal_mask = create_and_visualize_causal_mask(seq_len=8)

# Example of applying to Attention scores
print("\n=== Effect of Mask Application ===")
scores = torch.randn(8, 8)  # Random Attention scores

print("Scores before masking (partial):")
print(scores[:4, :4].numpy())

# Apply mask (future to -inf)
masked_scores = scores.masked_fill(causal_mask == 0, float('-inf'))

print("\nScores after masking (partial):")
print(masked_scores[:4, :4].numpy())

# Apply Softmax
attn_weights = F.softmax(masked_scores, dim=-1)

print("\nWeights after Softmax (partial):")
print(attn_weights[:4, :4].numpy())
print("‚Üí Weights for future positions (-inf) become 0")
</code></pre>
<h3>Complete Decoder Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import math

class TransformerDecoder(nn.Module):
    """Complete Transformer Decoder"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super(TransformerDecoder, self).__init__()

        self.d_model = d_model

        # Word embedding
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Positional encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # Stack DecoderLayers
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, nhead, d_ff, dropout)
            for _ in range(num_layers)
        ])

        # Output layer
        self.fc_out = nn.Linear(d_model, vocab_size)

        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        tgt: (batch, tgt_len) - target token IDs
        memory: (batch, src_len, d_model) - Encoder output
        tgt_mask: (batch, tgt_len, tgt_len) - causal mask
        memory_mask: (batch, 1, src_len) - padding mask
        """
        # Embedding + scaling
        x = self.embedding(tgt) * math.sqrt(self.d_model)

        # Add positional encoding
        x = self.pos_encoding(x)

        # Pass through each DecoderLayer
        self_attn_weights_list = []
        cross_attn_weights_list = []

        for layer in self.layers:
            x, self_attn_weights, cross_attn_weights = layer(x, memory, tgt_mask, memory_mask)
            self_attn_weights_list.append(self_attn_weights)
            cross_attn_weights_list.append(cross_attn_weights)

        # Project to vocabulary
        logits = self.fc_out(x)  # (batch, tgt_len, vocab_size)

        return logits, self_attn_weights_list, cross_attn_weights_list


# Verification
print("\n=== Complete Decoder Verification ===")
vocab_size = 10000
decoder = TransformerDecoder(vocab_size, d_model=512, nhead=8, num_layers=6)

# Dummy data
batch_size = 16
tgt_len = 20
src_len = 25

tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))
memory = torch.randn(batch_size, src_len, 512)

# Causal mask
tgt_mask = create_causal_mask(tgt_len)

# Execute Decoder
logits, self_attn_weights, cross_attn_weights = decoder(tgt, memory, tgt_mask)

print(f"Target input: {tgt.shape}")
print(f"Encoder memory: {memory.shape}")
print(f"Decoder output (logits): {logits.shape}")
print(f"‚Üí Outputs probability distribution over entire vocabulary at each position")

# Parameter count
total_params = sum(p.numel() for p in decoder.parameters())
print(f"\nTotal parameters: {total_params:,}")
</code></pre>
<hr/>
<h2>2.4 Complete Transformer Model</h2>
<h3>Integration of Encoder and Decoder</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn

class Transformer(nn.Module):
    """Complete Transformer Model (Encoder-Decoder)"""
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,
                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,
                 dropout=0.1, max_len=5000):
        super(Transformer, self).__init__()

        # Encoder
        self.encoder = TransformerEncoder(
            src_vocab_size, d_model, nhead, num_encoder_layers,
            d_ff, dropout, max_len
        )

        # Decoder
        self.decoder = TransformerDecoder(
            tgt_vocab_size, d_model, nhead, num_decoder_layers,
            d_ff, dropout, max_len
        )

        self.d_model = d_model

        # Parameter initialization
        self._reset_parameters()

    def _reset_parameters(self):
        """Xavier initialization"""
        for p in self.parameters():
            if p.dim() &gt; 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        src: (batch, src_len) - source tokens
        tgt: (batch, tgt_len) - target tokens
        src_mask: (batch, 1, src_len) - source padding mask
        tgt_mask: (batch, tgt_len, tgt_len) - target causal mask
        """
        # Process source with Encoder
        memory, _ = self.encoder(src, src_mask)

        # Generate target with Decoder
        output, _, _ = self.decoder(tgt, memory, tgt_mask, src_mask)

        return output

    def encode(self, src, src_mask=None):
        """Execute Encoder only (used during inference)"""
        memory, _ = self.encoder(src, src_mask)
        return memory

    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """Execute Decoder only (used during inference)"""
        output, _, _ = self.decoder(tgt, memory, tgt_mask, memory_mask)
        return output


# Create model
print("=== Complete Transformer Model ===")
src_vocab_size = 10000
tgt_vocab_size = 8000

model = Transformer(
    src_vocab_size=src_vocab_size,
    tgt_vocab_size=tgt_vocab_size,
    d_model=512,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048,
    dropout=0.1
)

# Verification
batch_size = 16
src_len = 25
tgt_len = 20

src = torch.randint(0, src_vocab_size, (batch_size, src_len))
tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))

# Create masks
src_mask = torch.ones(batch_size, 1, src_len)
tgt_mask = create_causal_mask(tgt_len)

# Forward pass
output = model(src, tgt, src_mask, tgt_mask)

print(f"Source input: {src.shape}")
print(f"Target input: {tgt.shape}")
print(f"Model output: {output.shape}")
print(f"‚Üí Output shape is (batch, tgt_len, tgt_vocab_size)")

# Total parameter count
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
</code></pre>
<h3>Autoregressive Generation Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn.functional as F

def generate_greedy(model, src, src_mask, max_len, start_token, end_token):
    """
    Greedy Decoding for sequence generation

    Args:
        model: Transformer model
        src: (batch, src_len) - source sequence
        src_mask: (batch, 1, src_len) - source mask
        max_len: maximum generation length
        start_token: start token ID
        end_token: end token ID

    Returns:
        generated: (batch, gen_len) - generated sequence
    """
    model.eval()
    batch_size = src.size(0)
    device = src.device

    # Process with Encoder once
    memory = model.encode(src, src_mask)

    # Initialize generated sequence (start token)
    generated = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)

    # Generate autoregressively
    for _ in range(max_len - 1):
        # Create causal mask
        tgt_len = generated.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # Execute Decoder
        output = model.decode(generated, memory, tgt_mask, src_mask)

        # Get prediction at last position
        next_token_logits = output[:, -1, :]  # (batch, vocab_size)

        # Greedy selection (highest probability token)
        next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # (batch, 1)

        # Add to generated sequence
        generated = torch.cat([generated, next_token], dim=1)

        # Stop if all samples reach end token
        if (next_token == end_token).all():
            break

    return generated


def generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5):
    """
    Beam Search for sequence generation

    Args:
        model: Transformer model
        src: (1, src_len) - source sequence (batch size 1)
        src_mask: (1, 1, src_len)
        max_len: maximum generation length
        start_token: start token ID
        end_token: end token ID
        beam_size: beam size

    Returns:
        best_sequence: (1, gen_len) - best generated sequence
    """
    model.eval()
    device = src.device

    # Encoder
    memory = model.encode(src, src_mask)  # (1, src_len, d_model)
    memory = memory.repeat(beam_size, 1, 1)  # (beam_size, src_len, d_model)

    # Initialize beams
    beams = torch.full((beam_size, 1), start_token, dtype=torch.long, device=device)
    beam_scores = torch.zeros(beam_size, device=device)
    beam_scores[1:] = float('-inf')  # Only first beam is active initially

    finished_beams = []

    for step in range(max_len - 1):
        tgt_len = beams.size(1)
        tgt_mask = create_causal_mask(tgt_len).to(device)

        # Decoder
        output = model.decode(beams, memory, tgt_mask, src_mask.repeat(beam_size, 1, 1))
        next_token_logits = output[:, -1, :]  # (beam_size, vocab_size)

        # Log probabilities
        log_probs = F.log_softmax(next_token_logits, dim=-1)

        # Update beam scores
        vocab_size = log_probs.size(-1)
        scores = beam_scores.unsqueeze(1) + log_probs  # (beam_size, vocab_size)
        scores = scores.view(-1)  # (beam_size * vocab_size)

        # Top-k selection
        top_scores, top_indices = scores.topk(beam_size, largest=True)

        # New beams
        beam_indices = top_indices // vocab_size
        token_indices = top_indices % vocab_size

        new_beams = []
        new_scores = []

        for i, (beam_idx, token_idx, score) in enumerate(zip(beam_indices, token_indices, top_scores)):
            # Extend beam
            new_beam = torch.cat([beams[beam_idx], token_idx.unsqueeze(0)])

            # Add to finished beams if end token is reached
            if token_idx == end_token:
                finished_beams.append((new_beam, score.item()))
            else:
                new_beams.append(new_beam)
                new_scores.append(score)

        # Stop if enough finished beams
        if len(finished_beams) &gt;= beam_size:
            break

        # Stop if no beams remaining
        if len(new_beams) == 0:
            break

        # Update beams
        beams = torch.stack(new_beams)
        beam_scores = torch.tensor(new_scores, device=device)

    # Select best beam
    if finished_beams:
        best_beam, best_score = max(finished_beams, key=lambda x: x[1])
    else:
        best_beam = beams[0]

    return best_beam.unsqueeze(0)


# Verification
print("\n=== Autoregressive Generation Test ===")

# Dummy model and data
src_vocab_size = 100
tgt_vocab_size = 100
model = Transformer(src_vocab_size, tgt_vocab_size, d_model=128, nhead=4,
                   num_encoder_layers=2, num_decoder_layers=2)

src = torch.randint(1, src_vocab_size, (1, 10))
src_mask = torch.ones(1, 1, 10)

start_token = 1
end_token = 2
max_len = 20

# Greedy Decoding
with torch.no_grad():
    generated_greedy = generate_greedy(model, src, src_mask, max_len, start_token, end_token)

print(f"Source sequence: {src.shape}")
print(f"Greedy generation: {generated_greedy.shape}")
print(f"Generated sequence: {generated_greedy[0].tolist()}")

# Beam Search
with torch.no_grad():
    generated_beam = generate_beam_search(model, src, src_mask, max_len, start_token, end_token, beam_size=5)

print(f"\nBeam Search generation: {generated_beam.shape}")
print(f"Generated sequence: {generated_beam[0].tolist()}")
</code></pre>
<hr/>
<h2>2.5 Practice: Machine Translation System</h2>
<h3>Dataset Preparation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import re

class TranslationDataset(Dataset):
    """Simple translation dataset"""
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src = self.src_sentences[idx]
        tgt = self.tgt_sentences[idx]

        # Convert to token IDs
        src_ids = [self.src_vocab.get(w, self.src_vocab['&lt;unk&gt;']) for w in src.split()]
        tgt_ids = [self.tgt_vocab.get(w, self.tgt_vocab['&lt;unk&gt;']) for w in tgt.split()]

        return torch.tensor(src_ids), torch.tensor(tgt_ids)


def build_vocab(sentences, max_vocab_size=10000):
    """Build vocabulary"""
    words = []
    for sent in sentences:
        words.extend(sent.split())

    # Count frequencies
    word_counts = Counter(words)
    most_common = word_counts.most_common(max_vocab_size - 4)  # Exclude special tokens

    # Create vocabulary dictionary
    vocab = {'&lt;pad&gt;': 0, '&lt;sos&gt;': 1, '&lt;eos&gt;': 2, '&lt;unk&gt;': 3}
    for word, _ in most_common:
        vocab[word] = len(vocab)

    return vocab


# Dummy data (in practice, use Multi30k, WMT, etc.)
src_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need",
    "deep learning is amazing",
    "natural language processing"
]

tgt_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need",
    "deep learning is amazing",
    "natural language processing"
]

# Build vocabulary
src_vocab = build_vocab(src_sentences)
tgt_vocab = build_vocab(tgt_sentences)

print("=== Translation Dataset Preparation ===")
print(f"Source vocabulary size: {len(src_vocab)}")
print(f"Target vocabulary size: {len(tgt_vocab)}")
print(f"\nSource vocabulary (partial): {list(src_vocab.items())[:10]}")
print(f"Target vocabulary (partial): {list(tgt_vocab.items())[:10]}")

# Create dataset
dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)

# Check sample
src_sample, tgt_sample = dataset[0]
print(f"\nSample 0:")
print(f"Source: {src_sentences[0]}")
print(f"Source ID: {src_sample.tolist()}")
print(f"Target: {tgt_sentences[0]}")
print(f"Target ID: {tgt_sample.tolist()}")
&lt;/unk&gt;&lt;/eos&gt;&lt;/sos&gt;&lt;/pad&gt;&lt;/unk&gt;&lt;/unk&gt;</code></pre>
<h3>Training Loop Implementation</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch, src_vocab, tgt_vocab):
    """Batch collate function"""
    src_batch, tgt_batch = zip(*batch)

    # Padding
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['&lt;pad&gt;'])
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['&lt;pad&gt;'])

    return src_padded, tgt_padded


def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):
    """Create masks"""
    # Source padding mask
    src_mask = (src != src_pad_idx).unsqueeze(1)  # (batch, 1, src_len)

    # Target causal mask + padding mask
    tgt_len = tgt.size(1)
    tgt_mask = create_causal_mask(tgt_len).to(tgt.device)  # (1, tgt_len, tgt_len)
    tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)
    tgt_mask = tgt_mask &amp; tgt_pad_mask

    return src_mask, tgt_mask


def train_epoch(model, dataloader, optimizer, criterion, src_vocab, tgt_vocab, device):
    """Train one epoch"""
    model.train()
    total_loss = 0

    for src, tgt in dataloader:
        src, tgt = src.to(device), tgt.to(device)

        # Split target into input and teacher data
        tgt_input = tgt[:, :-1]  # Exclude &lt;eos&gt;
        tgt_output = tgt[:, 1:]  # Exclude &lt;sos&gt;

        # Create masks
        src_mask, tgt_mask = create_masks(src, tgt_input,
                                         src_vocab['&lt;pad&gt;'], tgt_vocab['&lt;pad&gt;'])

        # Forward
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask, tgt_mask)

        # Calculate loss (ignore padding)
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)

        loss = criterion(output, tgt_output)

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


# Training configuration
print("\n=== Translation Model Training ===")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Create model
model = Transformer(
    src_vocab_size=len(src_vocab),
    tgt_vocab_size=len(tgt_vocab),
    d_model=256,
    nhead=8,
    num_encoder_layers=3,
    num_decoder_layers=3,
    d_ff=1024,
    dropout=0.1
).to(device)

# DataLoader
from functools import partial
loader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    collate_fn=partial(collate_fn, src_vocab=src_vocab, tgt_vocab=tgt_vocab)
)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['&lt;pad&gt;'])
optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# Training
num_epochs = 10
for epoch in range(num_epochs):
    loss = train_epoch(model, loader, optimizer, criterion, src_vocab, tgt_vocab, device)
    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

print("\nTraining complete!")
&lt;/pad&gt;&lt;/pad&gt;&lt;/pad&gt;&lt;/sos&gt;&lt;/eos&gt;&lt;/pad&gt;&lt;/pad&gt;</code></pre>
<h3>Translation Inference</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch

def translate(model, src_sentence, src_vocab, tgt_vocab, device, max_len=50):
    """Translate sentence"""
    model.eval()

    # Convert source sentence to token IDs
    src_tokens = src_sentence.split()
    src_ids = [src_vocab.get(w, src_vocab['&lt;unk&gt;']) for w in src_tokens]
    src = torch.tensor(src_ids).unsqueeze(0).to(device)  # (1, src_len)

    # Source mask
    src_mask = torch.ones(1, 1, src.size(1)).to(device)

    # Generate with Greedy Decoding
    with torch.no_grad():
        generated = generate_greedy(
            model, src, src_mask, max_len,
            start_token=tgt_vocab['&lt;sos&gt;'],
            end_token=tgt_vocab['&lt;eos&gt;']
        )

    # Convert token IDs to words
    idx_to_word = {v: k for k, v in tgt_vocab.items()}
    translated = [idx_to_word.get(idx.item(), '&lt;unk&gt;') for idx in generated[0]]

    # Remove &lt;sos&gt; and &lt;eos&gt;
    if translated[0] == '&lt;sos&gt;':
        translated = translated[1:]
    if '&lt;eos&gt;' in translated:
        eos_idx = translated.index('&lt;eos&gt;')
        translated = translated[:eos_idx]

    return ' '.join(translated)


# Translation test
print("\n=== Translation Test ===")

test_sentences = [
    "i love machine learning",
    "transformers are powerful",
    "attention is all you need"
]

for src_sent in test_sentences:
    translated = translate(model, src_sent, src_vocab, tgt_vocab, device)
    print(f"Source: {src_sent}")
    print(f"Translation: {translated}")
    print()

print("‚Üí Not perfect due to small dataset, but basic translation functionality implemented")
&lt;/eos&gt;&lt;/eos&gt;&lt;/sos&gt;&lt;/eos&gt;&lt;/sos&gt;&lt;/unk&gt;&lt;/eos&gt;&lt;/sos&gt;&lt;/unk&gt;</code></pre>
<hr/>
<h2>2.6 Transformer Training Techniques</h2>
<h3>Learning Rate Warmup</h3>
<p>For Transformer training, a <strong>warmup scheduler</strong> is important. It keeps the learning rate small initially, gradually increases it, and then decays.</p>

$$
\text{lr}(step) = d_{\text{model}}^{-0.5} \cdot \min(step^{-0.5}, step \cdot \text{warmup\_steps}^{-1.5})
$$

<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch.optim as optim

class NoamOpt:
    """Noam learning rate scheduler (paper implementation)"""
    def __init__(self, d_model, warmup_steps, optimizer):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.optimizer = optimizer
        self._step = 0
        self._rate = 0

    def step(self):
        """Update one step"""
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        """Calculate current learning rate"""
        if step is None:
            step = self._step
        return (self.d_model ** (-0.5)) * min(step ** (-0.5),
                                               step * self.warmup_steps ** (-1.5))

# Usage example
print("=== Noam Learning Rate Scheduler ===")
d_model = 512
warmup_steps = 4000

optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
scheduler = NoamOpt(d_model, warmup_steps, optimizer)

# Visualize learning rate progression
steps = list(range(1, 20000))
lrs = [scheduler.rate(step) for step in steps]

print(f"Initial learning rate (step=1): {lrs[0]:.6f}")
print(f"Peak learning rate (step={warmup_steps}): {lrs[warmup_steps-1]:.6f}")
print(f"Late learning rate (step=20000): {lrs[-1]:.6f}")
print("‚Üí Gradually increases during warmup, then decays")
</code></pre>
<h3>Label Smoothing</h3>
<p><strong>Label Smoothing</strong> is a regularization technique that sets the probability of the correct label to around 0.9 instead of 1, distributing some probability to other classes.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F

class LabelSmoothingLoss(nn.Module):
    """Cross Entropy Loss with Label Smoothing"""
    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing

    def forward(self, pred, target):
        """
        pred: (batch * seq_len, num_classes) - logits
        target: (batch * seq_len) - correct labels
        """
        # Log-softmax
        log_probs = F.log_softmax(pred, dim=-1)

        # Get probability at correct position
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)

        # Average log probability over all classes
        smooth_loss = -log_probs.mean(dim=-1)

        # Combine
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        # Mask ignore_index
        if self.ignore_index &gt;= 0:
            mask = (target != self.ignore_index).float()
            loss = (loss * mask).sum() / mask.sum()
        else:
            loss = loss.mean()

        return loss


# Comparison
print("\n=== Effect of Label Smoothing ===")

num_classes = 10
criterion_normal = nn.CrossEntropyLoss()
criterion_smooth = LabelSmoothingLoss(num_classes, smoothing=0.1)

# Dummy data
pred = torch.randn(32, num_classes)
target = torch.randint(0, num_classes, (32,))

loss_normal = criterion_normal(pred, target)
loss_smooth = criterion_smooth(pred, target)

print(f"Normal Cross Entropy Loss: {loss_normal.item():.4f}")
print(f"Label Smoothing Loss: {loss_smooth.item():.4f}")
print("‚Üí Label Smoothing prevents overconfidence and improves generalization")
</code></pre>
<h3>Mixed Precision Training</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Mixed Precision Training

Purpose: Demonstrate optimization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
from torch.cuda.amp import autocast, GradScaler

# Mixed Precision Training (when GPU is available)
if torch.cuda.is_available():
    print("\n=== Mixed Precision Training Example ===")

    device = torch.device('cuda')
    model = model.to(device)

    scaler = GradScaler()

    # Part of training loop
    for src, tgt in loader:
        src, tgt = src.to(device), tgt.to(device)
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        optimizer.zero_grad()

        # Compute with Mixed Precision
        with autocast():
            output = model(src, tgt_input)
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            loss = criterion(output, tgt_output)

        # Backpropagation with scaled gradients
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print("‚Üí FP16 computation accelerates and reduces memory (up to 2x faster)")
else:
    print("\n=== Mixed Precision Training ===")
    print("GPU not available, skipping")
</code></pre>
<hr/>
<h2>Exercises</h2>
<details>
<summary><strong>Exercise 1: Effect of Multi-Head Attention Head Count</strong></summary>
<p>Train models with different head counts (1, 2, 4, 8, 16) and compare performance and computational cost.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Train models with different head counts (1, 2, 4, 8, 16) and

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn

# Exercise: Create multiple models with different head counts
# Exercise: Train on same data and compare performance, training time, parameter count
# Exercise: Analyze role distribution across heads using Attention visualization
# Hint: More heads improve performance but increase computational cost
</code></pre>
</details>
<details>
<summary><strong>Exercise 2: Positional Encoding Experiments</strong></summary>
<p>Compare Sin/Cos positional encoding with learnable positional embeddings.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Compare Sin/Cos positional encoding with learnable positiona

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn

# Exercise: Implement two types of positional encoding
# 1. Sin/Cos (fixed)
# 2. nn.Embedding (learnable)

# Exercise: Compare performance on same task
# Exercise: Evaluate generalization to longer sequences (test on sequences longer than training)
# Expected: Sin/Cos can generalize to arbitrary lengths
</code></pre>
</details>
<details>
<summary><strong>Exercise 3: Causal Mask Visualization</strong></summary>
<p>Visualize how the Decoder's causal mask affects Attention weights.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Visualize how the Decoder's causal mask affects Attention we

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 2-5 seconds
Dependencies: None
"""

import torch
import matplotlib.pyplot as plt

# Exercise: Visualize Attention weights with and without mask
</code></pre>
</details>
<details>
<summary><strong>Exercise 4: Beam Search Beam Size Optimization</strong></summary>
<p>Compare translation quality and speed with different beam sizes (1, 3, 5, 10, 20).</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Compare translation quality and speed with different beam si

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
import time

# Exercise: Create graph of beam size vs quality and speed
# Expected: Beam size 5-10 provides good balance of quality and speed
</code></pre>
</details>
<details>
<summary><strong>Exercise 5: Investigate Effect of Layer Count</strong></summary>
<p>Compare performance with different Encoder/Decoder layer counts (1, 2, 4, 6, 12).</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Compare performance with different Encoder/Decoder layer cou

Purpose: Demonstrate machine learning model training and evaluation
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

import torch
import torch.nn as nn

# Exercise: Create models with different layer counts
# Exercise: Record training loss, validation loss, parameter count, training time
# Exercise: Create graph of layer count vs performance
# Analysis: Too deep causes overfitting and longer training, too shallow lacks expressiveness
</code></pre>
</details>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned the complete architecture of the Transformer.</p>
<h3>Key Points</h3>
<p><strong>1. Transformer Structure</strong> ‚Äî Encoder-Decoder architecture with 6-layer stacks.</p>
<p><strong>2. Encoder</strong> ‚Äî Multi-Head Self-Attention + FFN, fully parallelizable.</p>
<p><strong>3. Decoder</strong> ‚Äî Masked Self-Attention + Cross-Attention + FFN for autoregressive generation.</p>
<p><strong>4. Multi-Head Attention</strong> ‚Äî Captures dependencies from multiple perspectives simultaneously.</p>
<p><strong>5. Positional Encoding</strong> ‚Äî Injects positional information using Sin/Cos functions.</p>
<p><strong>6. Residual + Layer Norm</strong> ‚Äî Stabilizes training even in deep layers.</p>
<p><strong>7. Causal Mask</strong> ‚Äî Controls attention to prevent seeing future tokens.</p>
<p><strong>8. Autoregressive Generation</strong> ‚Äî Greedy Decoding and Beam Search strategies.</p>
<p><strong>9. Training Techniques</strong> ‚Äî Warmup scheduling, Label Smoothing, and Mixed Precision training.</p>
<p><strong>10. Practice</strong> ‚Äî Complete implementation of a machine translation system.</p>
<h3>Next Steps</h3>
<p>In the next chapter, we will learn about <strong>Training and Optimization of Transformers</strong>. You will master practical techniques including efficient training methods, data augmentation, evaluation metrics, and hyperparameter tuning.</p>
<div class="navigation">
<a class="nav-button" href="chapter1-self-attention.html">‚Üê Chapter 1: Self-Attention and Multi-Head Attention</a>
<a class="nav-button" href="chapter3-pretraining-finetuning.html">Chapter 3: Pre-training and Fine-tuning ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University bear no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>The creator and Tohoku University bear no liability for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p>¬© 2024 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
