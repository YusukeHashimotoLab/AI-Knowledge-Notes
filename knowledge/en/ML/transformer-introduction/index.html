<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Transformer Introduction Series - Complete guide from Attention mechanism to large language models">
    <title>Transformer Introduction Series v1.0 - AI Terakoya</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        // Mermaid.js Converter - Converts markdown-style mermaid code blocks to renderable divs
        document.addEventListener('DOMContentLoaded', function() {
            // Find all code blocks with class="language-mermaid"
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                // Create a new div with mermaid class
                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                // Replace the pre element with the new div
                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            // Re-initialize mermaid after conversion
            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Machine Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Transformer</span>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>‚ö° Transformer Introduction Series v1.0</h1>
            <p style="font-size: 1.1rem; margin-top: 0.5rem; opacity: 0.95;">From Attention Mechanism to Large Language Models</p>
            <div class="meta">
                <span>üìñ Total Learning Time: 120-150 minutes</span>
                <span>üìä Level: Intermediate to Advanced</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><strong>Systematically master the Transformer architecture that forms the foundation of modern NLP</strong></p>

        <h2 id="overview">Series Overview</h2>
        <p>This series is a practical educational content consisting of 5 chapters that allows you to learn the Transformer architecture systematically from the basics.</p>

        <p><strong>Transformer</strong> is the most revolutionary architecture in natural language processing (NLP) and forms the foundation of modern large language models (LLMs) such as BERT, GPT, and ChatGPT. By mastering parallel-processable sequence modeling through Self-Attention mechanism, learning diverse relationships through Multi-Head Attention, incorporating positional information through Positional Encoding, and transfer learning through pre-training and fine-tuning, you can understand and build state-of-the-art NLP systems. From the mechanisms of Self-Attention and Multi-Head to Transformer architecture, BERT/GPT, and large language models, we provide systematic knowledge.</p>

        <p><strong>Features:</strong></p>
        <ul>
            <li>‚úÖ <strong>From Basics to Cutting Edge</strong>: Systematic learning from Attention mechanism to large-scale models like GPT-4</li>
            <li>‚úÖ <strong>Implementation-Focused</strong>: Over 40 executable PyTorch code examples and practical techniques</li>
            <li>‚úÖ <strong>Intuitive Understanding</strong>: Understand operational principles through Attention visualization and architecture diagrams</li>
            <li>‚úÖ <strong>Full Hugging Face Compliance</strong>: Latest implementation methods using industry-standard libraries</li>
            <li>‚úÖ <strong>Practical Applications</strong>: Application to practical tasks such as sentiment analysis, question answering, and text generation</li>
        </ul>

        <p><strong>Total Learning Time</strong>: 120-150 minutes (including code execution and exercises)</p>

        <h2 id="learning">How to Learn</h2>

        <h3>Recommended Learning Order</h3>

        <div class="mermaid">
graph TD
    A[Chapter 1: Self-Attention and Multi-Head Attention] --> B[Chapter 2: Transformer Architecture]
    B --> C[Chapter 3: Pre-training and Fine-tuning]
    C --> D[Chapter 4: BERT and GPT]
    D --> E[Chapter 5: Large Language Models]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

        <p><strong>For Beginners (completely new to Transformer):</strong><br>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (all chapters recommended)<br>
        - Duration: 120-150 minutes</p>

        <p><strong>For Intermediate Learners (with RNN/Attention experience):</strong><br>
        - Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br>
        - Duration: 90-110 minutes</p>

        <p><strong>For Specific Topic Enhancement:</strong><br>
        - Attention mechanism: Chapter 1 (focused study)<br>
        - BERT/GPT: Chapter 4 (focused study)<br>
        - LLM/Prompting: Chapter 5 (focused study)<br>
        - Duration: 25-30 minutes per chapter</p>

        <h2 id="chapters">Chapter Details</h2>

        <h3><a href="./chapter1-self-attention.html">Chapter 1: Self-Attention and Multi-Head Attention</a></h3>
        <p><strong>Difficulty</strong>: Intermediate<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Attention Fundamentals</strong> - Attention mechanism in RNN, alignment</li>
            <li><strong>Self-Attention Principles</strong> - Query, Key, Value, similarity calculation by dot product</li>
            <li><strong>Scaled Dot-Product Attention</strong> - Scaling, Softmax, weighted sum</li>
            <li><strong>Multi-Head Attention</strong> - Multiple Attention heads, parallel processing</li>
            <li><strong>Visualization and Implementation</strong> - PyTorch implementation, Attention map visualization</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the operational principles of Self-Attention</li>
            <li>‚úÖ Explain the roles of Query, Key, and Value</li>
            <li>‚úÖ Calculate Scaled Dot-Product Attention</li>
            <li>‚úÖ Understand the benefits of Multi-Head Attention</li>
            <li>‚úÖ Implement Self-Attention in PyTorch</li>
        </ul>

        <p><strong><a href="./chapter1-self-attention.html">Read Chapter 1 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter2-transformer-architecture.html">Chapter 2: Transformer Architecture</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Overall Encoder-Decoder Structure</strong> - 6-layer stack, residual connections</li>
            <li><strong>Positional Encoding</strong> - Positional information embedding, sin/cos functions</li>
            <li><strong>Feed-Forward Network</strong> - Position-wise fully connected layers</li>
            <li><strong>Layer Normalization</strong> - Normalization layer, training stabilization</li>
            <li><strong>Masked Self-Attention</strong> - Masking future information in Decoder</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the overall structure of Transformer</li>
            <li>‚úÖ Explain the role of Positional Encoding</li>
            <li>‚úÖ Understand the effects of residual connections and Layer Norm</li>
            <li>‚úÖ Explain the necessity of Masked Self-Attention</li>
            <li>‚úÖ Implement Transformer in PyTorch</li>
        </ul>

        <p><strong><a href="./chapter2-transformer-architecture.html">Read Chapter 2 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter3-pretraining-finetuning.html">Chapter 3: Pre-training and Fine-tuning</a></h3>
        <p><strong>Difficulty</strong>: Intermediate to Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Transfer Learning Concept</strong> - Importance of pre-training, domain adaptation</li>
            <li><strong>Pre-training Tasks</strong> - Masked Language Model, Next Sentence Prediction</li>
            <li><strong>Fine-tuning Strategies</strong> - Full/partial layer updates, learning rate settings</li>
            <li><strong>Data Efficiency</strong> - High performance with small data, Few-shot Learning</li>
            <li><strong>Hugging Face Transformers</strong> - Practical library usage</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand the benefits of transfer learning</li>
            <li>‚úÖ Explain the design philosophy of pre-training tasks</li>
            <li>‚úÖ Select appropriate fine-tuning strategies</li>
            <li>‚úÖ Use the Hugging Face library</li>
            <li>‚úÖ Fine-tune models on custom tasks</li>
        </ul>

        <p><strong><a href="./chapter3-pretraining-finetuning.html">Read Chapter 3 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter4-bert-gpt.html">Chapter 4: BERT and GPT</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 25-30 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>BERT Structure</strong> - Encoder-only, bidirectional context</li>
            <li><strong>BERT Pre-training</strong> - Masked LM, Next Sentence Prediction</li>
            <li><strong>GPT Structure</strong> - Decoder-only, autoregressive model</li>
            <li><strong>GPT Pre-training</strong> - Language modeling, next token prediction</li>
            <li><strong>Comparison of BERT and GPT</strong> - Task characteristics, selection criteria</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand BERT's bidirectionality</li>
            <li>‚úÖ Explain the learning mechanism of Masked LM</li>
            <li>‚úÖ Understand GPT's autoregressive nature</li>
            <li>‚úÖ Appropriately choose between BERT and GPT</li>
            <li>‚úÖ Implement sentiment analysis and question answering</li>
        </ul>

        <p><strong><a href="./chapter4-bert-gpt.html">Read Chapter 4 ‚Üí</a></strong></p>

        <hr>

        <h3><a href="./chapter5-large-language-models.html">Chapter 5: Large Language Models</a></h3>
        <p><strong>Difficulty</strong>: Advanced<br>
        <strong>Reading Time</strong>: 30-35 minutes<br>
        <strong>Code Examples</strong>: 8</p>

        <h4>Learning Content</h4>
        <ol>
            <li><strong>Scaling Laws</strong> - Relationship between model size, data volume, and compute</li>
            <li><strong>GPT-3 and GPT-4</strong> - Ultra-large-scale models, Emergent Abilities</li>
            <li><strong>Prompt Engineering</strong> - Few-shot, Chain-of-Thought</li>
            <li><strong>In-Context Learning</strong> - Learning without fine-tuning</li>
            <li><strong>Latest Trends</strong> - Instruction Tuning, RLHF, ChatGPT</li>
        </ol>

        <h4>Learning Objectives</h4>
        <ul>
            <li>‚úÖ Understand scaling laws</li>
            <li>‚úÖ Explain the concept of Emergent Abilities</li>
            <li>‚úÖ Design effective prompts</li>
            <li>‚úÖ Utilize In-Context Learning</li>
            <li>‚úÖ Understand the latest LLM trends</li>
        </ul>

        <p><strong><a href="./chapter5-large-language-models.html">Read Chapter 5 ‚Üí</a></strong></p>

        <hr>

        <h2 id="outcomes">Overall Learning Outcomes</h2>

        <p>Upon completing this series, you will acquire the following skills and knowledge:</p>

        <h3>Knowledge Level (Understanding)</h3>
        <ul>
            <li>‚úÖ Explain the mechanisms of Self-Attention and Multi-Head Attention</li>
            <li>‚úÖ Understand the Transformer architecture</li>
            <li>‚úÖ Explain pre-training and fine-tuning strategies</li>
            <li>‚úÖ Understand the differences between BERT and GPT and how to use them</li>
            <li>‚úÖ Explain the principles and applications of large language models</li>
        </ul>

        <h3>Practical Skills (Doing)</h3>
        <ul>
            <li>‚úÖ Implement Transformer in PyTorch</li>
            <li>‚úÖ Fine-tune using Hugging Face Transformers</li>
            <li>‚úÖ Implement sentiment analysis and question answering with BERT</li>
            <li>‚úÖ Implement text generation with GPT</li>
            <li>‚úÖ Design effective prompts</li>
        </ul>

        <h3>Application Ability (Applying)</h3>
        <ul>
            <li>‚úÖ Select appropriate models for new NLP tasks</li>
            <li>‚úÖ Efficiently utilize pre-trained models</li>
            <li>‚úÖ Apply the latest LLM technologies to practical work</li>
            <li>‚úÖ Optimize performance through prompt engineering</li>
        </ul>

        <hr>

        <h2 id="prerequisites">Prerequisites</h2>

        <p>To effectively learn this series, it is desirable to have the following knowledge:</p>

        <h3>Required (Must Have)</h3>
        <ul>
            <li>‚úÖ <strong>Python Basics</strong>: Variables, functions, classes, loops, conditionals</li>
            <li>‚úÖ <strong>NumPy Basics</strong>: Array operations, broadcasting, basic mathematical functions</li>
            <li>‚úÖ <strong>Deep Learning Fundamentals</strong>: Neural networks, backpropagation, gradient descent</li>
            <li>‚úÖ <strong>PyTorch Basics</strong>: Tensor operations, nn.Module, Dataset and DataLoader</li>
            <li>‚úÖ <strong>Linear Algebra Basics</strong>: Matrix operations, dot product, shape transformation</li>
        </ul>

        <h3>Recommended (Nice to Have)</h3>
        <ul>
            <li>üí° <strong>RNN/LSTM</strong>: Recurrent neural networks, Attention mechanism</li>
            <li>üí° <strong>NLP Fundamentals</strong>: Tokenization, vocabulary, embeddings</li>
            <li>üí° <strong>Optimization Algorithms</strong>: Adam, learning rate scheduling, Warmup</li>
            <li>üí° <strong>GPU Environment</strong>: Basic understanding of CUDA</li>
        </ul>

        <p><strong>Recommended Prior Learning</strong>:</p>
        <ul>
            <!-- Content in preparation <li>üìö <a href="../deep-learning-basics/">Deep Learning Fundamentals Series</a> - Basics of neural networks</li>
            <li>üìö <a href="../pytorch-introduction/">PyTorch Introduction Series</a> - Basic PyTorch operations</li>
            <li>üìö <a href="../rnn-introduction/">RNN Introduction Series</a> - Recurrent networks and Attention</li> -->
        </ul>

        <hr>

        <h2 id="tech">Technologies and Tools Used</h2>

        <h3>Main Libraries</h3>
        <ul>
            <li><strong>PyTorch 2.0+</strong> - Deep learning framework</li>
            <li><strong>transformers 4.30+</strong> - Hugging Face Transformers library</li>
            <li><strong>tokenizers 0.13+</strong> - Fast tokenizer</li>
            <li><strong>datasets 2.12+</strong> - Dataset library</li>
            <li><strong>NumPy 1.24+</strong> - Numerical computation</li>
            <li><strong>Matplotlib 3.7+</strong> - Visualization</li>
            <li><strong>scikit-learn 1.3+</strong> - Data preprocessing and evaluation metrics</li>
        </ul>

        <h3>Development Environment</h3>
        <ul>
            <li><strong>Python 3.8+</strong> - Programming language</li>
            <li><strong>Jupyter Notebook / Lab</strong> - Interactive development environment</li>
            <li><strong>Google Colab</strong> - GPU environment (free to use)</li>
            <li><strong>CUDA 11.8+ / cuDNN</strong> - GPU acceleration (recommended)</li>
        </ul>

        <h3>Datasets</h3>
        <ul>
            <li><strong>GLUE</strong> - Natural language understanding benchmark</li>
            <li><strong>SQuAD</strong> - Question answering dataset</li>
            <li><strong>WikiText</strong> - Language modeling dataset</li>
            <li><strong>IMDb</strong> - Sentiment analysis dataset</li>
        </ul>

        <hr>

        <h2 id="start">Let's Get Started!</h2>
        <p>Are you ready? Begin with Chapter 1 and master Transformer technology!</p>

        <p><strong><a href="./chapter1-self-attention.html">Chapter 1: Self-Attention and Multi-Head Attention ‚Üí</a></strong></p>

        <hr>

        <h2 id="next">Next Steps</h2>

        <p>After completing this series, we recommend proceeding to the following topics:</p>

        <h3>Advanced Learning</h3>
        <ul>
            <li>üìö <strong>Vision Transformer (ViT)</strong>: Transformer application to image processing</li>
            <li>üìö <strong>Multimodal Learning</strong>: CLIP, Flamingo, GPT-4V</li>
            <li>üìö <strong>Efficiency Techniques</strong>: Model compression, distillation, quantization</li>
            <li>üìö <strong>Integration with Reinforcement Learning</strong>: RLHF, Constitutional AI</li>
        </ul>

        <h3>Related Series</h3>
        <ul>
            <li>üéØ <a href="../nlp-advanced/">Advanced Natural Language Processing</a> - Sentiment analysis, question answering, summarization</li>
            <li>üéØ <a href="../llm-applications/">LLM Application Development</a> - RAG, agents, tool use</li>
            <li>üéØ <a href="../prompt-engineering/">Prompt Engineering</a> - Practical prompt design</li>
        </ul>

        <h3>Practical Projects</h3>
        <ul>
            <li>üöÄ Sentiment Analysis API - Real-time sentiment analysis with BERT</li>
            <li>üöÄ Question Answering System - Document retrieval and answer generation</li>
            <li>üöÄ Chatbot - GPT-based dialogue system</li>
            <li>üöÄ Text Summarization Tool - Automatic news article summarization</li>
        </ul>

        <hr>

        <p><strong>Update History</strong></p>
        <ul>
            <li><strong>2025-10-21</strong>: v1.0 Initial release</li>
        </ul>

        <hr>

        <p><strong>Your Transformer learning journey begins here!</strong></p>

    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is for educational, research, and informational purposes only, and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
