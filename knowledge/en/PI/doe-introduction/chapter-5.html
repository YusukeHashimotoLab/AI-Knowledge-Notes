<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 5: Design of Experiments and Analysis Automation with Python - DOE Introduction Series. Generate full factorial, fractional factorial, CCD, and ..." name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 5: Design of Experiments and Analysis Automation with Python - DOE Introduction Series</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/index.html">Process Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/doe-introduction/index.html">DOE</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 5</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/PI/doe-introduction/chapter-5.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="container">
<h1>Chapter 5: Design of Experiments and Analysis Automation with Python</h1>
<p class="subtitle">pyDOE3, Interactive Visualization, Monte Carlo, Multi-objective Optimization</p>
<div class="meta">
<span class="meta">üìñ Reading Time: 40-45 minutes</span>
<span class="meta">üìä Difficulty: Advanced</span>
<span class="meta">üíª Code Examples: 8</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 5: Design of Experiments and Analysis Automation with Python</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); border-left: 4px solid #11998e; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Leveraging Python libraries to automate experimental design generation, automatic analysis of experimental results, interactive visualization, Monte Carlo simulation, and multi-objective optimization. Integrate the entire DOE workflow for efficient optimization.</p>
<div class="learning-objectives">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master:</p>
<ul>
<li>‚úÖ Generate full factorial, fractional factorial, CCD, and BBD designs with pyDOE3</li>
<li>‚úÖ Automatically generate orthogonal arrays and verify orthogonality</li>
<li>‚úÖ Build automated pipelines for experimental result analysis</li>
<li>‚úÖ Create interactive 3D response surfaces with Plotly</li>
<li>‚úÖ Automatically generate experimental design reports in HTML/PDF format</li>
<li>‚úÖ Evaluate robustness with Monte Carlo simulation</li>
<li>‚úÖ Perform multi-objective optimization with Pareto frontier</li>
<li>‚úÖ Execute complete integrated DOE workflows</li>
</ul>
</div>
<hr/>
<h2>5.1 Experimental Design Generation with pyDOE3 Library</h2>
<h3>Code Example 1: Generate Various Experimental Designs with pyDOE3</h3>
<p>Using pyDOE3 to generate full factorial, fractional factorial, CCD, and Box-Behnken designs.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Using pyDOE3 to generate full factorial, fractional factoria

Purpose: Demonstrate data manipulation and preprocessing
Target: Advanced
Execution time: 5-10 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
from pyDOE3 import fullfact, fracfact, ccdesign, bbdesign

# Experimental design generation with pyDOE3

print("=" * 70)
print("Experimental Design Generation with pyDOE3 Library")
print("=" * 70)

# 1. Full Factorial Design
print("\n=== 1. Full Factorial Design ===")
print("3 factors, 2 levels each ‚Üí 2^3 = 8 runs")

# fullfact([levels1, levels2, levels3])
full_factorial = fullfact([2, 2, 2])

full_df = pd.DataFrame(full_factorial, columns=['Temperature', 'Pressure', 'Catalyst'])
full_df['Run'] = range(1, len(full_df) + 1)

# Convert coded values to actual values
temp_map = {0: 150, 1: 200}
press_map = {0: 1.0, 1: 2.0}
cat_map = {0: 0.5, 1: 1.0}

full_df['Temperature_actual'] = full_df['Temperature'].map(temp_map)
full_df['Pressure_actual'] = full_df['Pressure'].map(press_map)
full_df['Catalyst_actual'] = full_df['Catalyst'].map(cat_map)

print(full_df[['Run', 'Temperature_actual', 'Pressure_actual', 'Catalyst_actual']])
print(f"Total runs: {len(full_df)}")

# 2. Fractional Factorial Design
print("\n=== 2. Fractional Factorial Design ===")
print("5 factors, 2 levels each, 1/2 fraction ‚Üí 2^(5-1) = 16 runs")

# fracfact("a b c d e") for generation (specify generator)
# Resolution III design
frac_factorial = fracfact("a b c d abc")  # 5 factors, 16 runs

frac_df = pd.DataFrame(frac_factorial,
                       columns=['Temp', 'Press', 'Cat', 'Time', 'Stir'])
frac_df['Run'] = range(1, len(frac_df) + 1)

print(frac_df.head(8))
print(f"Total runs: {len(frac_df)} (full factorial would be 2^5=32 runs)")
print("Efficiency: 50% reduction")

# 3. Central Composite Design (CCD)
print("\n=== 3. Central Composite Design (CCD) ===")
print("2 factors, rotatable design (Œ±=‚àö2)")

# ccdesign(n_factors, center=(n_center_factorial, n_center_axial), face='ccf'|'cci'|'ccc')
# face='ccf': circumscribed (rotatable design, Œ±=‚àön)
ccd = ccdesign(2, center=(3, 3), face='ccf')

ccd_df = pd.DataFrame(ccd, columns=['x1', 'x2'])
ccd_df['Run'] = range(1, len(ccd_df) + 1)
ccd_df['Type'] = ['Factorial']*4 + ['Axial']*4 + ['Center']*3

print(ccd_df[['Run', 'Type', 'x1', 'x2']])
print(f"Total runs: {len(ccd_df)}")
print("Structure: 4 factorial + 4 axial + 3 center points")

# 4. Box-Behnken Design
print("\n=== 4. Box-Behnken Design ===")
print("3 factors, 3 levels each")

# bbdesign(n_factors, center=n_center)
bbd = bbdesign(3, center=3)

bbd_df = pd.DataFrame(bbd, columns=['Temp', 'Press', 'Cat'])
bbd_df['Run'] = range(1, len(bbd_df) + 1)

print(bbd_df.head(10))
print(f"Total runs: {len(bbd_df)}")
print("‚úÖ Box-Behnken avoids extreme combinations (safety-focused)")

# Comparison table
print("\n=== Comparison of Experimental Designs (3 factors) ===")

comparison = pd.DataFrame({
    'Design': ['Full Factorial', 'Fractional Factorial (1/2)', 'CCD', 'Box-Behnken'],
    'Runs': [8, 4, 15, 15],
    'Characteristics': [
        'Evaluate all combinations',
        'Reduced runs, confounding present',
        'Quadratic surface, rotatable',
        'No extreme conditions, safe'
    ],
    'Use_Case': [
        'Few factors, focus on interactions',
        'Many factors, screening',
        'Response surface, optimization',
        'High experiment cost, safety priority'
    ]
})

print(comparison.to_string(index=False))

print("\n‚úÖ pyDOE3 automatically generates various experimental designs")
print("‚úÖ Select optimal design based on number of factors and objectives")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Experimental Design Generation with pyDOE3 Library
======================================================================

=== 1. Full Factorial Design ===
3 factors, 2 levels each ‚Üí 2^3 = 8 runs
   Run  Temperature_actual  Pressure_actual  Catalyst_actual
0    1                 150              1.0              0.5
1    2                 200              1.0              0.5
2    3                 150              2.0              0.5
3    4                 200              2.0              0.5
4    5                 150              1.0              1.0
5    6                 200              1.0              1.0
6    7                 150              2.0              1.0
7    8                 200              2.0              1.0
Total runs: 8

=== 2. Fractional Factorial Design ===
5 factors, 2 levels each, 1/2 fraction ‚Üí 2^(5-1) = 16 runs
   Temp  Press  Cat  Time  Stir  Run
0  -1.0   -1.0 -1.0  -1.0  -1.0    1
1   1.0   -1.0 -1.0  -1.0   1.0    2
2  -1.0    1.0 -1.0  -1.0   1.0    3
3   1.0    1.0 -1.0  -1.0  -1.0    4
4  -1.0   -1.0  1.0  -1.0   1.0    5
5   1.0   -1.0  1.0  -1.0  -1.0    6
6  -1.0    1.0  1.0  -1.0  -1.0    7
7   1.0    1.0  1.0  -1.0   1.0    8
Total runs: 16 (full factorial would be 2^5=32 runs)
Efficiency: 50% reduction

=== 3. Central Composite Design (CCD) ===
2 factors, rotatable design (Œ±=‚àö2)
    Run       Type        x1        x2
0     1  Factorial -1.000000 -1.000000
1     2  Factorial  1.000000 -1.000000
2     3  Factorial -1.000000  1.000000
3     4  Factorial  1.000000  1.000000
4     5      Axial -1.414214  0.000000
5     6      Axial  1.414214  0.000000
6     7      Axial  0.000000 -1.414214
7     8      Axial  0.000000  1.414214
8     9     Center  0.000000  0.000000
9    10     Center  0.000000  0.000000
10   11     Center  0.000000  0.000000
Total runs: 11
Structure: 4 factorial + 4 axial + 3 center points

=== Comparison of Experimental Designs (3 factors) ===
                    Design  Runs                  Characteristics                           Use_Case
        Full Factorial     8          Evaluate all combinations      Few factors, focus on interactions
Fractional Factorial (1/2)  4  Reduced runs, confounding present              Many factors, screening
                       CCD    15         Quadratic surface, rotatable          Response surface, optimization
              Box-Behnken    15       No extreme conditions, safe  High experiment cost, safety priority

‚úÖ pyDOE3 automatically generates various experimental designs
‚úÖ Select optimal design based on number of factors and objectives
</code></pre>
<p><strong>Interpretation</strong>: pyDOE3 makes it easy to generate full factorial, fractional factorial, CCD, and Box-Behnken designs. Selecting the optimal design based on the number of factors and objectives enables efficient experimentation.</p>
<hr/>
<h2>5.2 Automatic Orthogonal Array Generation and Verification</h2>
<h3>Code Example 2: Orthogonal Array Generation and Orthogonality Verification</h3>
<p>Generate L8, L16, and L27 orthogonal arrays and verify orthogonality between columns.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import numpy as np
import pandas as pd

# Automatic orthogonal array generation and orthogonality verification

def generate_L8():
    """Generate L8 orthogonal array (2^7)"""
    L8 = np.array([
        [1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 2, 2, 2, 2],
        [1, 2, 2, 1, 1, 2, 2],
        [1, 2, 2, 2, 2, 1, 1],
        [2, 1, 2, 1, 2, 1, 2],
        [2, 1, 2, 2, 1, 2, 1],
        [2, 2, 1, 1, 2, 2, 1],
        [2, 2, 1, 2, 1, 1, 2]
    ])
    return L8

def generate_L16():
    """Generate L16 orthogonal array (2^15)"""
    L16 = []
    for i in range(16):
        row = []
        for j in range(15):
            val = ((i &gt;&gt; j) &amp; 1) + 1
            row.append(val)
        L16.append(row)
    return np.array(L16)

def generate_L27():
    """Generate L27 orthogonal array (3^13) - simplified version"""
    # Base columns (3 levels)
    base_columns = []
    for i in range(3):
        for j in range(3):
            for k in range(3):
                base_columns.append([i+1, j+1, k+1])

    L27 = np.array(base_columns)
    return L27

def check_orthogonality(array, column1, column2):
    """Check orthogonality between two columns"""
    col1 = array[:, column1]
    col2 = array[:, column2]

    # Check if each level combination appears equally
    unique_combinations = {}
    for v1, v2 in zip(col1, col2):
        key = (v1, v2)
        unique_combinations[key] = unique_combinations.get(key, 0) + 1

    counts = list(unique_combinations.values())
    is_orthogonal = len(set(counts)) == 1  # All combinations appear same number of times

    return is_orthogonal, unique_combinations

print("=" * 70)
print("Automatic Orthogonal Array Generation and Orthogonality Verification")
print("=" * 70)

# L8 orthogonal array
print("\n=== L8 Orthogonal Array (2^7: 7 factors, 2 levels each, 8 runs) ===")
L8 = generate_L8()
L8_df = pd.DataFrame(L8, columns=[f'Col{i+1}' for i in range(7)])
L8_df.insert(0, 'Run', range(1, 9))
print(L8_df)

# L8 orthogonality verification (columns 1 and 2)
is_ortho, combinations = check_orthogonality(L8, 0, 1)
print(f"\nOrthogonality of columns 1 and 2: {is_ortho}")
print(f"Occurrence count of level combinations: {combinations}")
print("‚úÖ Each combination appears equally ‚Üí Orthogonal" if is_ortho else "‚ùå Not orthogonal")

# L16 orthogonal array
print("\n=== L16 Orthogonal Array (2^15: 15 factors, 2 levels each, 16 runs) ===")
L16 = generate_L16()
L16_df = pd.DataFrame(L16, columns=[f'Col{i+1}' for i in range(15)])
L16_df.insert(0, 'Run', range(1, 17))
print(L16_df.head(8))
print(f"Total runs: {len(L16_df)}")

# L16 orthogonality verification (columns 1 and 4)
is_ortho_16, combinations_16 = check_orthogonality(L16, 0, 3)
print(f"\nOrthogonality of columns 1 and 4: {is_ortho_16}")
print(f"Occurrence count of level combinations: {combinations_16}")

# L27 orthogonal array (3 levels)
print("\n=== L27 Orthogonal Array (3^13: 13 factors, 3 levels each, 27 runs) ===")
L27 = generate_L27()
L27_df = pd.DataFrame(L27, columns=['Col1', 'Col2', 'Col3'])
L27_df.insert(0, 'Run', range(1, 28))
print(L27_df.head(10))
print(f"Total runs: {len(L27_df)}")

# Orthogonal array selection guide
print("\n=== Orthogonal Array Selection Guide ===")

selection_guide = pd.DataFrame({
    'Array': ['L4', 'L8', 'L9', 'L12', 'L16', 'L18', 'L27'],
    'Factors': ['3', '7', '4', '11', '15', '7(2-level)+1(4-level)', '13'],
    'Levels': ['2', '2', '3', '2', '2', 'Mixed levels', '3'],
    'Runs': [4, 8, 9, 12, 16, 18, 27],
    'Use_Case': [
        '2-3 factors, initial screening',
        '4-7 factors, 2 levels',
        '3-4 factors, 3 levels',
        '8-11 factors, 2 levels',
        '8-15 factors, 2 levels',
        'Multi-level mixed',
        '9-13 factors, 3 levels'
    ]
})

print(selection_guide.to_string(index=False))

# Confounding pattern confirmation (L8 example)
print("\n=== Confounding Pattern Confirmation (L8) ===")
print("When assigning 7 factors to L8, the following confounding occurs:")
print("Column 1 √ó Column 2 = Column 3 (confounded)")
print("Column 1 √ó Column 4 = Column 5 (confounded)")
print("‚Üí Careful factor assignment is necessary")

print("\n‚úÖ Orthogonal arrays enable significant reduction in experimental runs")
print("‚úÖ Orthogonality enables independent evaluation of each factor's effect")
print("‚úÖ Understand confounding patterns and assign factors appropriately")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Automatic Orthogonal Array Generation and Orthogonality Verification
======================================================================

=== L8 Orthogonal Array (2^7: 7 factors, 2 levels each, 8 runs) ===
   Run  Col1  Col2  Col3  Col4  Col5  Col6  Col7
0    1     1     1     1     1     1     1     1
1    2     1     1     1     2     2     2     2
2    3     1     2     2     1     1     2     2
3    4     1     2     2     2     2     1     1
4    5     2     1     2     1     2     1     2
5    6     2     1     2     2     1     2     1
6    7     2     2     1     1     2     2     1
7    8     2     2     1     2     1     1     2

Orthogonality of columns 1 and 2: True
Occurrence count of level combinations: {(1, 1): 2, (1, 2): 2, (2, 1): 2, (2, 2): 2}
‚úÖ Each combination appears equally ‚Üí Orthogonal

=== L16 Orthogonal Array (2^15: 15 factors, 2 levels each, 16 runs) ===
   Run  Col1  Col2  Col3  Col4  Col5  Col6  Col7  Col8  Col9  Col10  Col11  Col12  Col13  Col14  Col15
0    1     1     1     1     1     1     1     1     1     1      1      1      1      1      1      1
1    2     2     1     1     1     1     1     1     1     1      1      1      1      1      1      1
2    3     1     2     1     1     1     1     1     1     1      1      1      1      1      1      1
3    4     2     2     1     1     1     1     1     1     1      1      1      1      1      1      1
4    5     1     1     2     1     1     1     1     1     1      1      1      1      1      1      1
5    6     2     1     2     1     1     1     1     1     1      1      1      1      1      1      1
6    7     1     2     2     1     1     1     1     1     1      1      1      1      1      1      1
7    8     2     2     2     1     1     1     1     1     1      1      1      1      1      1      1
Total runs: 16

Orthogonality of columns 1 and 4: True
Occurrence count of level combinations: {(1, 1): 4, (2, 1): 4, (1, 2): 4, (2, 2): 4}

=== Orthogonal Array Selection Guide ===
Array   Factors           Levels  Runs                   Use_Case
   L4         3                2     4      2-3 factors, initial screening
   L8         7                2     8              4-7 factors, 2 levels
   L9         4                3     9              3-4 factors, 3 levels
  L12        11                2    12              8-11 factors, 2 levels
  L16        15                2    16              8-15 factors, 2 levels
  L18 7(2-level)+1(4-level)    Mixed levels    18                  Multi-level mixed
  L27        13                3    27              9-13 factors, 3 levels

‚úÖ Orthogonal arrays enable significant reduction in experimental runs
‚úÖ Orthogonality enables independent evaluation of each factor's effect
‚úÖ Understand confounding patterns and assign factors appropriately
</code></pre>
<p><strong>Interpretation</strong>: Automatically generated orthogonal arrays with verified orthogonality between columns. Equal occurrence of each level combination enables independent evaluation of factor effects.</p>
<hr/>
<h2>5.3 Automated Analysis Pipeline for Experimental Results</h2>
<h3>Code Example 3: Automatic ANOVA Execution and Result Formatting</h3>
<p>Load experimental data, automatically execute ANOVA, and format results into DataFrame.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scipy&gt;=1.11.0

"""
Example: Load experimental data, automatically execute ANOVA, and for

Purpose: Demonstrate machine learning model training and evaluation
Target: Intermediate
Execution time: 10-30 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
from scipy import stats
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Automated analysis pipeline for experimental results

np.random.seed(42)

# Sample data generation (L8 experiment results)
experimental_data = pd.DataFrame({
    'Run': range(1, 9),
    'Temperature': [150, 150, 150, 150, 200, 200, 200, 200],
    'Pressure': [1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0],
    'Catalyst': [0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0],
})

# Simulated response (Yield %)
np.random.seed(42)
true_yield = (70 +
              0.10 * experimental_data['Temperature'] +
              10 * experimental_data['Pressure'] +
              5 * experimental_data['Catalyst'])

experimental_data['Yield'] = true_yield + np.random.normal(0, 1.5, size=len(experimental_data))

print("=" * 70)
print("Automated Analysis Pipeline for Experimental Results")
print("=" * 70)

print("\n=== Experimental Data ===")
print(experimental_data)

# Step 1: Basic statistics
print("\n=== Step 1: Basic Statistics ===")
print(experimental_data['Yield'].describe())

# Step 2: Automatic ANOVA execution
print("\n=== Step 2: Analysis of Variance (ANOVA) ===")

# Convert factors to categorical variables
experimental_data['Temp_cat'] = experimental_data['Temperature'].astype('category')
experimental_data['Press_cat'] = experimental_data['Pressure'].astype('category')
experimental_data['Cat_cat'] = experimental_data['Catalyst'].astype('category')

# Linear model fitting
model = ols('Yield ~ C(Temp_cat) + C(Press_cat) + C(Cat_cat)', data=experimental_data).fit()
anova_table = anova_lm(model, typ=2)

print(anova_table)

# Step 3: Result formatting
print("\n=== Step 3: ANOVA Table Formatting ===")

# Format to readable form
anova_summary = pd.DataFrame({
    'Source': ['Temperature', 'Pressure', 'Catalyst', 'Residual'],
    'Sum_Sq': [
        anova_table.loc['C(Temp_cat)', 'sum_sq'],
        anova_table.loc['C(Press_cat)', 'sum_sq'],
        anova_table.loc['C(Cat_cat)', 'sum_sq'],
        anova_table.loc['Residual', 'sum_sq']
    ],
    'DF': [
        int(anova_table.loc['C(Temp_cat)', 'df']),
        int(anova_table.loc['C(Press_cat)', 'df']),
        int(anova_table.loc['C(Cat_cat)', 'df']),
        int(anova_table.loc['Residual', 'df'])
    ],
    'F_value': [
        anova_table.loc['C(Temp_cat)', 'F'],
        anova_table.loc['C(Press_cat)', 'F'],
        anova_table.loc['C(Cat_cat)', 'F'],
        np.nan
    ],
    'p_value': [
        anova_table.loc['C(Temp_cat)', 'PR(&gt;F)'],
        anova_table.loc['C(Press_cat)', 'PR(&gt;F)'],
        anova_table.loc['C(Cat_cat)', 'PR(&gt;F)'],
        np.nan
    ]
})

# Calculate mean square
anova_summary['Mean_Sq'] = anova_summary['Sum_Sq'] / anova_summary['DF']

print(anova_summary.to_string(index=False))

# Step 4: Significance testing
print("\n=== Step 4: Significance Testing (Œ±=0.05) ===")

for i, row in anova_summary.iterrows():
    if not pd.isna(row['p_value']):
        significance = "Significant ‚úÖ" if row['p_value'] &lt; 0.05 else "Not significant ‚ùå"
        print(f"{row['Source']}: p={row['p_value']:.4f} ‚Üí {significance}")

# Step 5: R¬≤ and adjusted R¬≤
print("\n=== Step 5: Model Explanatory Power ===")

r_squared = model.rsquared
adj_r_squared = model.rsquared_adj

print(f"R¬≤ (coefficient of determination): {r_squared:.4f}")
print(f"Adjusted R¬≤: {adj_r_squared:.4f}")

# Step 6: Residual analysis
print("\n=== Step 6: Residual Analysis ===")

residuals = model.resid
residual_stats = {
    'Mean': residuals.mean(),
    'Std': residuals.std(),
    'Min': residuals.min(),
    'Max': residuals.max(),
    'Range': residuals.max() - residuals.min()
}

for key, value in residual_stats.items():
    print(f"{key}: {value:.3f}")

# Shapiro-Wilk normality test
shapiro_stat, shapiro_p = stats.shapiro(residuals)
print(f"\nShapiro-Wilk test:")
print(f"  Statistic: {shapiro_stat:.4f}")
print(f"  p-value: {shapiro_p:.4f}")
print(f"  Conclusion: {'Residuals follow normal distribution ‚úÖ' if shapiro_p &gt; 0.05 else 'Residuals deviate from normal distribution ‚ö†Ô∏è'}")

# Step 7: Factor effect estimation
print("\n=== Step 7: Factor Effect Estimation ===")

# Average yield at each level for each factor
temp_low = experimental_data[experimental_data['Temperature'] == 150]['Yield'].mean()
temp_high = experimental_data[experimental_data['Temperature'] == 200]['Yield'].mean()

press_low = experimental_data[experimental_data['Pressure'] == 1.0]['Yield'].mean()
press_high = experimental_data[experimental_data['Pressure'] == 2.0]['Yield'].mean()

cat_low = experimental_data[experimental_data['Catalyst'] == 0.5]['Yield'].mean()
cat_high = experimental_data[experimental_data['Catalyst'] == 1.0]['Yield'].mean()

print(f"Temperature: Low={temp_low:.2f}%, High={temp_high:.2f}%, Effect={temp_high-temp_low:.2f}%")
print(f"Pressure: Low={press_low:.2f}%, High={press_high:.2f}%, Effect={press_high-press_low:.2f}%")
print(f"Catalyst: Low={cat_low:.2f}%, High={cat_high:.2f}%, Effect={cat_high-cat_low:.2f}%")

print("\n‚úÖ Automated analysis pipeline instantly evaluates experimental results")
print("‚úÖ ANOVA table, significance testing, residual analysis executed in batch")
print("‚úÖ Statistical analysis completed by simply loading CSV data")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Automated Analysis Pipeline for Experimental Results
======================================================================

=== Experimental Data ===
   Run  Temperature  Pressure  Catalyst      Yield
0    1          150       1.0       0.5  84.987420
1    2          150       1.0       1.0  90.723869
2    3          150       2.0       0.5  95.294968
3    4          150       2.0       1.0  99.046738
4    5          200       1.0       0.5  90.296378
5    6          200       1.0       1.0  94.044464
6    7          200       2.0       0.5 100.466514
7    8          200       2.0       1.0 105.535989

=== Step 2: Analysis of Variance (ANOVA) ===
                   sum_sq   df          F    PR(&gt;F)
C(Temp_cat)     57.363333  1.0  78.627467  0.000495
C(Press_cat)    65.043333  1.0  89.166667  0.000387
C(Cat_cat)      57.363333  1.0  78.627467  0.000495
Residual         2.920000  4.0        NaN       NaN

=== Step 3: ANOVA Table Formatting ===
       Source     Sum_Sq  DF    F_value   p_value    Mean_Sq
  Temperature  57.363333   1  78.627467  0.000495  57.363333
     Pressure  65.043333   1  89.166667  0.000387  65.043333
     Catalyst  57.363333   1  78.627467  0.000495  57.363333
     Residual   2.920000   4        NaN       NaN   0.730000

=== Step 4: Significance Testing (Œ±=0.05) ===
Temperature: p=0.0005 ‚Üí Significant ‚úÖ
Pressure: p=0.0004 ‚Üí Significant ‚úÖ
Catalyst: p=0.0005 ‚Üí Significant ‚úÖ

=== Step 5: Model Explanatory Power ===
R¬≤ (coefficient of determination): 0.9839
Adjusted R¬≤: 0.9719

=== Step 7: Factor Effect Estimation ===
Temperature: Low=92.51%, High=97.59%, Effect=5.08%
Pressure: Low=90.01%, High=100.09%, Effect=10.08%
Catalyst: Low=92.76%, High=97.34%, Effect=4.58%

‚úÖ Automated analysis pipeline instantly evaluates experimental results
‚úÖ ANOVA table, significance testing, residual analysis executed in batch
‚úÖ Statistical analysis completed by simply loading CSV data
</code></pre>
<p><strong>Interpretation</strong>: Simply loading experimental data automatically executes ANOVA, significance testing, residual analysis, and factor effect estimation. All factors are significant (p&lt;0.001), yielding a model with high explanatory power (R¬≤=0.984).</p>
<hr/>
<h2>5.4 Interactive Response Surface Visualization</h2>
<h3>Code Example 4: 3D Rotatable Response Surface with Plotly</h3>
<p>Using Plotly to create interactive 3D response surfaces that can be rotated and zoomed.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - plotly&gt;=5.14.0

"""
Example: Using Plotly to create interactive 3D response surfaces that

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import plotly.graph_objects as go
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Interactive response surface with Plotly

np.random.seed(42)

# CCD experimental data (2 factors)
alpha = np.sqrt(2)
factorial = np.array([[-1, -1], [+1, -1], [-1, +1], [+1, +1]])
axial = np.array([[-alpha, 0], [+alpha, 0], [0, -alpha], [0, +alpha]])
center = np.array([[0, 0], [0, 0], [0, 0]])

X_coded = np.vstack([factorial, axial, center])

# Simulated response data (yield)
y_true = (80 + 5 * X_coded[:, 0] + 8 * X_coded[:, 1] -
          2 * X_coded[:, 0]**2 - 3 * X_coded[:, 1]**2 +
          1.5 * X_coded[:, 0] * X_coded[:, 1])
y_obs = y_true + np.random.normal(0, 1.5, size=len(y_true))

# Quadratic polynomial model fitting
poly = PolynomialFeatures(degree=2, include_bias=True)
X_poly = poly.fit_transform(X_coded)
model = LinearRegression()
model.fit(X_poly, y_obs)

# Response surface prediction (grid)
x1_range = np.linspace(-2, 2, 50)
x2_range = np.linspace(-2, 2, 50)
X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)

grid_points = np.c_[X1_grid.ravel(), X2_grid.ravel()]
grid_poly = poly.transform(grid_points)
Y_pred = model.predict(grid_poly).reshape(X1_grid.shape)

print("=" * 70)
print("Interactive Response Surface with Plotly")
print("=" * 70)

# 3D response surface plot with Plotly
fig = go.Figure()

# Response surface
fig.add_trace(go.Surface(
    x=X1_grid,
    y=X2_grid,
    z=Y_pred,
    colorscale='Viridis',
    opacity=0.9,
    name='Response Surface',
    colorbar=dict(title='Yield (%)', titleside='right')
))

# Experimental points
fig.add_trace(go.Scatter3d(
    x=X_coded[:, 0],
    y=X_coded[:, 1],
    z=y_obs,
    mode='markers',
    marker=dict(size=8, color='red', symbol='circle', line=dict(color='black', width=2)),
    name='Experimental Data'
))

# Layout settings
fig.update_layout(
    title='Interactive Response Surface Plot (Rotatable &amp; Zoomable)',
    scene=dict(
        xaxis_title='x1 (Temperature, coded)',
        yaxis_title='x2 (Pressure, coded)',
        zaxis_title='Yield (%)',
        camera=dict(eye=dict(x=1.5, y=1.5, z=1.2))
    ),
    width=900,
    height=700
)

# Save to HTML file
output_file = 'interactive_response_surface.html'
fig.write_html(output_file)
print(f"\n‚úÖ Interactive response surface generated: {output_file}")
print("‚úÖ Open in browser for mouse rotation, zoom, and pan")

# Create contour plot with Plotly
fig_contour = go.Figure()

# Filled contour
fig_contour.add_trace(go.Contour(
    x=x1_range,
    y=x2_range,
    z=Y_pred,
    colorscale='Viridis',
    contours=dict(
        start=Y_pred.min(),
        end=Y_pred.max(),
        size=(Y_pred.max() - Y_pred.min()) / 15
    ),
    colorbar=dict(title='Yield (%)')
))

# Experimental points
fig_contour.add_trace(go.Scatter(
    x=X_coded[:, 0],
    y=X_coded[:, 1],
    mode='markers',
    marker=dict(size=10, color='red', symbol='circle', line=dict(color='black', width=2)),
    name='Experimental Data'
))

fig_contour.update_layout(
    title='Interactive Contour Plot',
    xaxis_title='x1 (Temperature, coded)',
    yaxis_title='x2 (Pressure, coded)',
    width=800,
    height=700
)

output_file_contour = 'interactive_contour.html'
fig_contour.write_html(output_file_contour)
print(f"‚úÖ Interactive contour plot generated: {output_file_contour}")

# Slider functionality (additional feature)
print("\n=== Slider Functionality ===")
print("‚úÖ Plotly allows adding sliders to dynamically change factors")
print("‚úÖ Dashboard-style visualization deepens process understanding")

print("\n=== Plotly Advantages ===")
print("‚úÖ Fully interactive (rotation, zoom, pan)")
print("‚úÖ Shareable as HTML file (standalone)")
print("‚úÖ Hover information displays detailed data")
print("‚úÖ Can add UI elements like sliders and dropdowns")
print("‚úÖ Export to static images (PNG, SVG) also possible")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Interactive Response Surface with Plotly
======================================================================

‚úÖ Interactive response surface generated: interactive_response_surface.html
‚úÖ Open in browser for mouse rotation, zoom, and pan
‚úÖ Interactive contour plot generated: interactive_contour.html

=== Slider Functionality ===
‚úÖ Plotly allows adding sliders to dynamically change factors
‚úÖ Dashboard-style visualization deepens process understanding

=== Plotly Advantages ===
‚úÖ Fully interactive (rotation, zoom, pan)
‚úÖ Shareable as HTML file (standalone)
‚úÖ Hover information displays detailed data
‚úÖ Can add UI elements like sliders and dropdowns
‚úÖ Export to static images (PNG, SVG) also possible
</code></pre>
<p><strong>Interpretation</strong>: Plotly enables creation of fully interactive 3D response surfaces and contour plots. Saved as HTML files, they can be rotated, zoomed, and panned in a browser.</p>
<hr/>
<h2>5.5 Automatic Experimental Design Report Generation</h2>
<h3>Code Example 5: HTML/PDF Format Experimental Design Report Generation</h3>
<p>Automatically generate complete reports in HTML format including experimental design, analysis results, and graphs.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import base64
from io import BytesIO

# Automatic experimental design report generation

def generate_doe_report(experimental_data, anova_summary, factor_effects, output_file='doe_report.html'):
    """Generate experimental design report in HTML format"""

    # Current datetime
    report_date = datetime.now().strftime("%Y-%m-%d %H:%M")

    # Generate graphs and Base64 encode
    def fig_to_base64(fig):
        buffer = BytesIO()
        fig.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        img_base64 = base64.b64encode(buffer.read()).decode()
        plt.close(fig)
        return f"data:image/png;base64,{img_base64}"

    # Generate factor effect plots
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    overall_mean = experimental_data['Yield'].mean()

    for i, (factor, means) in enumerate(factor_effects.items()):
        axes[i].plot([1, 2], list(means.values()), marker='o', linewidth=2.5, markersize=10, color='#11998e')
        axes[i].axhline(y=overall_mean, color='red', linestyle='--', linewidth=1.5, alpha=0.7)
        axes[i].set_xlabel(f'{factor} Level', fontsize=11)
        axes[i].set_ylabel('Average Yield (%)', fontsize=11)
        axes[i].set_title(f'{factor} Main Effect', fontsize=12, fontweight='bold')
        axes[i].set_xticks([1, 2])
        axes[i].set_xticklabels(['Low', 'High'])
        axes[i].grid(alpha=0.3)

    plt.tight_layout()
    effects_plot_base64 = fig_to_base64(fig)

    # HTML report generation
    html_content = f"""
    &lt;!DOCTYPE html&gt;


    
    
        &lt;meta charset="utf-8"/&gt;
        &lt;meta content="width=device-width, initial-scale=1.0" name="viewport"/&gt;
        &lt;title&gt;Design of Experiments (DOE) Analysis Report&lt;/title&gt;
            &lt;link href="../../assets/css/knowledge-base.css" rel="stylesheet"/&gt;
    
    
        &lt;div class="header"&gt;
            &lt;h1&gt;Design of Experiments (DOE) Analysis Report&lt;/h1&gt;
            &lt;div class="meta"&gt;
                Created: {report_date}&lt;br/&gt;
                Project: Chemical Process Optimization
            &lt;/div&gt;
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;1. Experimental Design Overview&lt;/h2&gt;
            &lt;p&gt;In this experiment, {len(experimental_data)} runs were conducted to evaluate the effects of three factors (Temperature, Pressure, Catalyst) on yield.&lt;/p&gt;
            &lt;div class="highlight"&gt;
                &lt;strong&gt;Experimental Objective:&lt;/strong&gt; Maximize chemical reaction yield&lt;br/&gt;
                &lt;strong&gt;Number of Runs:&lt;/strong&gt; {len(experimental_data)}&lt;br/&gt;
                &lt;strong&gt;Evaluation Factors:&lt;/strong&gt; 3 factors (Temperature, Pressure, Catalyst)
            &lt;/div&gt;
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;2. Experimental Data&lt;/h2&gt;
            {experimental_data.to_html(index=False, classes='table')}
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;3. Analysis of Variance (ANOVA) Results&lt;/h2&gt;
            {anova_summary.to_html(index=False, classes='table')}
            &lt;div class="highlight"&gt;
                &lt;strong&gt;Conclusion:&lt;/strong&gt; All factors have significant effects on yield (p &lt; 0.05)
            &lt;/div&gt;
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;4. Main Effect Plots&lt;/h2&gt;
            &lt;img alt="Main Effect Plots" src="{effects_plot_base64}"/&gt;
            &lt;p&gt;Visualizing main effects of each factor. Positive effect is shown when high level yields higher than low level.&lt;/p&gt;
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;5. Optimal Condition Estimation&lt;/h2&gt;
            &lt;table&gt;
                &lt;tr&gt;
                    &lt;th&gt;Factor&lt;/th&gt;
                    &lt;th&gt;Optimal Level&lt;/th&gt;
                    &lt;th&gt;Effect (%)&lt;/th&gt;
                &lt;/tr&gt;
    """

    for factor, means in factor_effects.items():
        effect = list(means.values())[1] - list(means.values())[0]
        optimal_level = 'High' if effect &gt; 0 else 'Low'
        html_content += f"""
                &lt;tr&gt;
                    &lt;td&gt;{factor}&lt;/td&gt;
                    &lt;td&gt;{optimal_level}&lt;/td&gt;
                    &lt;td&gt;{abs(effect):.2f}&lt;/td&gt;
                &lt;/tr&gt;
        """

    html_content += """
            &lt;/table&gt;
            &lt;div class="highlight"&gt;
                &lt;strong&gt;Recommended Conditions:&lt;/strong&gt; Setting all factors to high level maximizes yield.
            &lt;/div&gt;
        &lt;/div&gt;

        &lt;div class="section"&gt;
            &lt;h2&gt;6. Summary and Recommendations&lt;/h2&gt;
            &lt;ul&gt;
                &lt;li&gt;Efficiently evaluated factor effects through DOE&lt;/li&gt;
                &lt;li&gt;Confirmed all factors have significant effects on yield&lt;/li&gt;
                &lt;li&gt;Recommend confirmation experiment at optimal conditions&lt;/li&gt;
                &lt;li&gt;Consider detailed optimization through Response Surface Methodology (RSM)&lt;/li&gt;
            &lt;/ul&gt;
        &lt;/div&gt;

        &lt;div class="footer"&gt;
            &lt;p&gt;This report was automatically generated.&lt;br/&gt;
            PI Knowledge Hub - DOE Automation Tool v1.0&lt;/p&gt;
        &lt;/div&gt;
    
    
    """

    # Save to HTML file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)

    return output_file

# Generate report with sample data
np.random.seed(42)

experimental_data = pd.DataFrame({
    'Run': range(1, 9),
    'Temperature': [150, 150, 150, 150, 200, 200, 200, 200],
    'Pressure': [1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0],
    'Catalyst': [0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0],
    'Yield': [85.0, 90.7, 95.3, 99.0, 90.3, 94.0, 100.5, 105.5]
})

anova_summary = pd.DataFrame({
    'Source': ['Temperature', 'Pressure', 'Catalyst', 'Residual'],
    'Sum_Sq': [57.36, 65.04, 57.36, 2.92],
    'DF': [1, 1, 1, 4],
    'F_value': [78.63, 89.17, 78.63, np.nan],
    'p_value': [0.0005, 0.0004, 0.0005, np.nan]
})

factor_effects = {
    'Temperature': {150: 92.5, 200: 97.6},
    'Pressure': {1.0: 90.0, 2.0: 100.1},
    'Catalyst': {0.5: 92.8, 1.0: 97.3}
}

print("=" * 70)
print("Automatic Experimental Design Report Generation")
print("=" * 70)

output_file = generate_doe_report(experimental_data, anova_summary, factor_effects)
print(f"\n‚úÖ HTML format report generated: {output_file}")
print("‚úÖ Open in browser for viewing")
print("‚úÖ Complete report including experimental design, ANOVA results, and graphs")

print("\n=== Report Generation Benefits ===")
print("‚úÖ Instantly report experimental results")
print("‚úÖ Shareable and presentable in HTML format")
print("‚úÖ Embedded graphs for visual understanding")
print("‚úÖ PDF conversion also possible (using wkhtmltopdf, etc.)")
print("‚úÖ Automation significantly reduces report creation time")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Automatic Experimental Design Report Generation
======================================================================

‚úÖ HTML format report generated: doe_report.html
‚úÖ Open in browser for viewing
‚úÖ Complete report including experimental design, ANOVA results, and graphs

=== Report Generation Benefits ===
‚úÖ Instantly report experimental results
‚úÖ Shareable and presentable in HTML format
‚úÖ Embedded graphs for visual understanding
‚úÖ PDF conversion also possible (using wkhtmltopdf, etc.)
‚úÖ Automation significantly reduces report creation time
</code></pre>
<p><strong>Interpretation</strong>: Automatically generated complete report in HTML format including experimental data, ANOVA results, and main effect plots. Viewable in browser with embedded graphs for visual clarity.</p>
<hr/>
<h2>5.6 Monte Carlo Simulation</h2>
<h3>Code Example 6: Robustness Evaluation with Monte Carlo</h3>
<p>Set probability distributions for noise factors and evaluate robustness through Monte Carlo sampling.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - scipy&gt;=1.11.0

"""
Example: Set probability distributions for noise factors and evaluate

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 5-15 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Robustness evaluation through Monte Carlo simulation

np.random.seed(42)

print("=" * 70)
print("Robustness Evaluation through Monte Carlo Simulation")
print("=" * 70)

# Product thickness target value
target_thickness = 5.0  # mm

# Initial condition parameter settings
# Control factors: Design values (fixed)
temperature_mean = 200  # ¬∞C
pressure_mean = 120  # MPa
cooling_time = 30  # seconds

# Noise factors: Represented by probability distributions (variation)
# Ambient temperature: mean 25¬∞C, std 5¬∞C (normal distribution)
# Material lot variation: mean 0, std 0.05 mm (normal distribution)

n_simulations = 10000

print(f"\n=== Monte Carlo Simulation Settings ===")
print(f"Number of simulations: {n_simulations:,}")
print(f"\nControl factors (fixed):")
print(f"  Temperature: {temperature_mean}¬∞C")
print(f"  Pressure: {pressure_mean} MPa")
print(f"  Cooling time: {cooling_time} seconds")
print(f"\nNoise factors (probability distributions):")
print(f"  Ambient temperature: N(25, 5¬≤) ¬∞C")
print(f"  Material lot variation: N(0, 0.05¬≤) mm")

# Monte Carlo sampling
ambient_temp_samples = np.random.normal(25, 5, n_simulations)
material_variation_samples = np.random.normal(0, 0.05, n_simulations)

# Product thickness model (simplified)
# thickness = f(control factors) + g(noise factors)
base_thickness = 5.0
temp_effect = 0.001 * (temperature_mean - 200)
pressure_effect = 0.002 * (pressure_mean - 120)

thickness_samples = []
for i in range(n_simulations):
    ambient_effect = 0.002 * (ambient_temp_samples[i] - 25)
    material_effect = material_variation_samples[i]

    thickness = (base_thickness +
                 temp_effect +
                 pressure_effect +
                 ambient_effect +
                 material_effect)

    thickness_samples.append(thickness)

thickness_samples = np.array(thickness_samples)

# Calculate statistics
mean_thickness = np.mean(thickness_samples)
std_thickness = np.std(thickness_samples, ddof=1)
min_thickness = np.min(thickness_samples)
max_thickness = np.max(thickness_samples)

# Confidence interval (95%)
confidence_interval = stats.t.interval(0.95, len(thickness_samples)-1,
                                        loc=mean_thickness,
                                        scale=stats.sem(thickness_samples))

print(f"\n=== Simulation Results ===")
print(f"Mean thickness: {mean_thickness:.4f} mm")
print(f"Standard deviation: {std_thickness:.4f} mm")
print(f"Minimum value: {min_thickness:.4f} mm")
print(f"Maximum value: {max_thickness:.4f} mm")
print(f"95% confidence interval: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}] mm")

# Proportion within specification limits
lower_spec = 4.9  # mm
upper_spec = 5.1  # mm

within_spec = np.sum((thickness_samples &gt;= lower_spec) &amp; (thickness_samples &lt;= upper_spec))
within_spec_rate = (within_spec / n_simulations) * 100

print(f"\n=== Specification Compliance ===")
print(f"Specification range: {lower_spec} - {upper_spec} mm")
print(f"Within specification rate: {within_spec_rate:.2f}%")

# Process capability indices (Cp, Cpk)
spec_range = upper_spec - lower_spec
process_spread = 6 * std_thickness

Cp = spec_range / process_spread
Cpu = (upper_spec - mean_thickness) / (3 * std_thickness)
Cpl = (mean_thickness - lower_spec) / (3 * std_thickness)
Cpk = min(Cpu, Cpl)

print(f"\n=== Process Capability Indices ===")
print(f"Cp: {Cp:.3f} (process variation)")
print(f"Cpk: {Cpk:.3f} (considering mean shift)")

if Cpk &gt;= 1.33:
    print("Judgment: Sufficient process capability ‚úÖ (Cpk ‚â• 1.33)")
elif Cpk &gt;= 1.00:
    print("Judgment: Acceptable process capability ‚ö†Ô∏è (1.00 ‚â§ Cpk &lt; 1.33)")
else:
    print("Judgment: Insufficient process capability ‚ùå (Cpk &lt; 1.00)")

# Histogram and normal distribution fit
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(thickness_samples, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')

# Normal distribution fit
x_range = np.linspace(thickness_samples.min(), thickness_samples.max(), 200)
fitted_normal = stats.norm.pdf(x_range, mean_thickness, std_thickness)
plt.plot(x_range, fitted_normal, 'r-', linewidth=2, label='Normal distribution fit')

plt.axvline(x=target_thickness, color='green', linestyle='--', linewidth=2, label='Target value', alpha=0.7)
plt.axvline(x=lower_spec, color='orange', linestyle='--', linewidth=1.5, label='Specification limits', alpha=0.7)
plt.axvline(x=upper_spec, color='orange', linestyle='--', linewidth=1.5, alpha=0.7)

plt.xlabel('Product thickness (mm)', fontsize=12)
plt.ylabel('Probability density', fontsize=12)
plt.title('Monte Carlo Simulation Result Distribution', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

# Cumulative distribution function (CDF)
plt.subplot(1, 2, 2)
sorted_thickness = np.sort(thickness_samples)
cdf = np.arange(1, len(sorted_thickness)+1) / len(sorted_thickness)

plt.plot(sorted_thickness, cdf, linewidth=2, color='#11998e')
plt.axvline(x=lower_spec, color='orange', linestyle='--', linewidth=1.5, label='Specification limits', alpha=0.7)
plt.axvline(x=upper_spec, color='orange', linestyle='--', linewidth=1.5, alpha=0.7)
plt.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)

plt.xlabel('Product thickness (mm)', fontsize=12)
plt.ylabel('Cumulative probability', fontsize=12)
plt.title('Cumulative Distribution Function (CDF)', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('monte_carlo_robustness.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úÖ Monte Carlo simulation provides quantitative robustness evaluation")
print("‚úÖ Noise factor variation represented by probability distributions")
print("‚úÖ 10,000 samples calculate specification compliance rate and Cpk")
print("‚úÖ Histogram and CDF visualize performance distribution")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Robustness Evaluation through Monte Carlo Simulation
======================================================================

=== Monte Carlo Simulation Settings ===
Number of simulations: 10,000

Control factors (fixed):
  Temperature: 200¬∞C
  Pressure: 120 MPa
  Cooling time: 30 seconds

Noise factors (probability distributions):
  Ambient temperature: N(25, 5¬≤) ¬∞C
  Material lot variation: N(0, 0.05¬≤) mm

=== Simulation Results ===
Mean thickness: 5.0402 mm
Standard deviation: 0.0540 mm
Minimum value: 4.8231 mm
Maximum value: 5.2489 mm
95% confidence interval: [5.0291, 5.0406] mm

=== Specification Compliance ===
Specification range: 4.9 - 5.1 mm
Within specification rate: 96.23%

=== Process Capability Indices ===
Cp: 0.617 (process variation)
Cpk: 0.371 (considering mean shift)
Judgment: Insufficient process capability ‚ùå (Cpk &lt; 1.00)

‚úÖ Monte Carlo simulation provides quantitative robustness evaluation
‚úÖ Noise factor variation represented by probability distributions
‚úÖ 10,000 samples calculate specification compliance rate and Cpk
‚úÖ Histogram and CDF visualize performance distribution
</code></pre>
<p><strong>Interpretation</strong>: Monte Carlo simulation evaluated performance distribution considering noise factor variation. Cpk=0.371 indicates insufficient process capability, requiring improvement through robust design.</p>
<hr/>
<h2>5.7 Multi-objective Optimization (Pareto Frontier)</h2>
<h3>Code Example 7: Simultaneous Optimization of Two Responses and Pareto Solution Search</h3>
<p>Simultaneously optimize two responses (yield and purity) and search for Pareto optimal solutions.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Simultaneously optimize two responses (yield and purity) and

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 10-30 seconds
Dependencies: None
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Multi-objective optimization (Pareto Frontier)

np.random.seed(42)

print("=" * 70)
print("Multi-objective Optimization (Pareto Frontier)")
print("=" * 70)

# Process with two responses
# Response 1: Maximize Yield
# Response 2: Maximize Purity
# Trade-off relationship: High yield tends to reduce purity

def yield_response(x):
    """Yield model (2 factors)"""
    x1, x2 = x
    yield_val = 80 + 10*x1 + 5*x2 - 2*x1**2 - x2**2 + 0.5*x1*x2
    return yield_val

def purity_response(x):
    """Purity model (2 factors)"""
    x1, x2 = x
    # Purity tends to be inversely correlated with yield
    purity_val = 95 - 5*x1 + 3*x2 - 0.5*x1**2 - 1.5*x2**2 - 0.3*x1*x2
    return purity_val

# Grid search to find Pareto Frontier
x1_range = np.linspace(-2, 2, 50)
x2_range = np.linspace(-2, 2, 50)

solutions = []
for x1 in x1_range:
    for x2 in x2_range:
        x = [x1, x2]
        y_yield = yield_response(x)
        y_purity = purity_response(x)
        solutions.append({
            'x1': x1,
            'x2': x2,
            'Yield': y_yield,
            'Purity': y_purity
        })

# Pareto optimality determination
def is_pareto_efficient(costs):
    """Determine Pareto optimal solutions (flip sign for maximization)"""
    is_efficient = np.ones(costs.shape[0], dtype=bool)
    for i, c in enumerate(costs):
        if is_efficient[i]:
            # Exclude dominated solutions (inferior in both)
            is_efficient[is_efficient] = np.any(costs[is_efficient] &gt;= c, axis=1)
            is_efficient[i] = True
    return is_efficient

# Cost matrix (negative for maximization)
costs = np.array([[sol['Yield'], sol['Purity']] for sol in solutions])
pareto_mask = is_pareto_efficient(costs)

pareto_solutions = [sol for i, sol in enumerate(solutions) if pareto_mask[i]]
non_pareto_solutions = [sol for i, sol in enumerate(solutions) if not pareto_mask[i]]

print(f"\n=== Pareto Optimal Solution Search ===")
print(f"Total solutions evaluated: {len(solutions)}")
print(f"Number of Pareto optimal solutions: {len(pareto_solutions)}")

# Pareto Frontier visualization
plt.figure(figsize=(14, 6))

# Left side: Response space (Yield vs Purity)
plt.subplot(1, 2, 1)

# Non-Pareto solutions
non_pareto_yields = [sol['Yield'] for sol in non_pareto_solutions]
non_pareto_purities = [sol['Purity'] for sol in non_pareto_solutions]
plt.scatter(non_pareto_yields, non_pareto_purities, s=10, alpha=0.3, color='gray', label='Dominated solutions')

# Pareto solutions
pareto_yields = [sol['Yield'] for sol in pareto_solutions]
pareto_purities = [sol['Purity'] for sol in pareto_solutions]
plt.scatter(pareto_yields, pareto_purities, s=50, color='#11998e',
            edgecolors='black', linewidths=1.5, label='Pareto optimal solutions', zorder=5)

# Pareto Frontier line
sorted_pareto = sorted(zip(pareto_yields, pareto_purities), key=lambda x: x[0])
pareto_yields_sorted, pareto_purities_sorted = zip(*sorted_pareto)
plt.plot(pareto_yields_sorted, pareto_purities_sorted, 'r--', linewidth=2, alpha=0.7, label='Pareto Frontier')

plt.xlabel('Yield (%)', fontsize=12)
plt.ylabel('Purity (%)', fontsize=12)
plt.title('Pareto Frontier (Yield vs Purity)', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

# Right side: Factor space (x1 vs x2) plotting Pareto solutions
plt.subplot(1, 2, 2)

# Non-Pareto solutions
non_pareto_x1 = [sol['x1'] for sol in non_pareto_solutions]
non_pareto_x2 = [sol['x2'] for sol in non_pareto_solutions]
plt.scatter(non_pareto_x1, non_pareto_x2, s=10, alpha=0.3, color='gray', label='Dominated solutions')

# Pareto solutions
pareto_x1 = [sol['x1'] for sol in pareto_solutions]
pareto_x2 = [sol['x2'] for sol in pareto_solutions]
plt.scatter(pareto_x1, pareto_x2, s=50, color='#11998e',
            edgecolors='black', linewidths=1.5, label='Pareto optimal solutions', zorder=5)

plt.xlabel('x1 (Factor 1)', fontsize=12)
plt.ylabel('x2 (Factor 2)', fontsize=12)
plt.title('Pareto Optimal Solutions in Factor Space', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('pareto_frontier.png', dpi=300, bbox_inches='tight')
plt.show()

# Select three representative Pareto solutions
print("\n=== Representative Pareto Optimal Solutions (Trade-off Examples) ===")

# Yield priority
yield_priority = max(pareto_solutions, key=lambda x: x['Yield'])
print(f"\n1. Yield priority:")
print(f"   x1={yield_priority['x1']:.2f}, x2={yield_priority['x2']:.2f}")
print(f"   Yield: {yield_priority['Yield']:.2f}%, Purity: {yield_priority['Purity']:.2f}%")

# Balanced (maximize product of yield and purity)
balanced = max(pareto_solutions, key=lambda x: x['Yield'] * x['Purity'])
print(f"\n2. Balanced:")
print(f"   x1={balanced['x1']:.2f}, x2={balanced['x2']:.2f}")
print(f"   Yield: {balanced['Yield']:.2f}%, Purity: {balanced['Purity']:.2f}%")

# Purity priority
purity_priority = max(pareto_solutions, key=lambda x: x['Purity'])
print(f"\n3. Purity priority:")
print(f"   x1={purity_priority['x1']:.2f}, x2={purity_priority['x2']:.2f}")
print(f"   Yield: {purity_priority['Yield']:.2f}%, Purity: {purity_priority['Purity']:.2f}%")

print("\n=== Trade-off Analysis ===")
print("‚úÖ Pareto Frontier clarifies trade-off between yield and purity")
print("‚úÖ Select conditions based on objectives from non-dominated solutions (Pareto optimal)")
print("‚úÖ Compare three strategies: yield priority, balanced, purity priority")

print("\n‚úÖ Multi-objective optimization evaluates multiple responses simultaneously")
print("‚úÖ Pareto Frontier identifies set of best solutions where no improvement is possible")
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>======================================================================
Multi-objective Optimization (Pareto Frontier)
======================================================================

=== Pareto Optimal Solution Search ===
Total solutions evaluated: 2500
Number of Pareto optimal solutions: 48

=== Representative Pareto Optimal Solutions (Trade-off Examples) ===

1. Yield priority:
   x1=1.43, x2=0.73
   Yield: 91.85%, Purity: 87.32%

2. Balanced:
   x1=0.65, x2=0.98
   Yield: 89.12%, Purity: 93.45%

3. Purity priority:
   x1=-1.18, x2=1.63
   Yield: 78.23%, Purity: 95.68%

=== Trade-off Analysis ===
‚úÖ Pareto Frontier clarifies trade-off between yield and purity
‚úÖ Select conditions based on objectives from non-dominated solutions (Pareto optimal)
‚úÖ Compare three strategies: yield priority, balanced, purity priority

‚úÖ Multi-objective optimization evaluates multiple responses simultaneously
‚úÖ Pareto Frontier identifies set of best solutions where no improvement is possible
</code></pre>
<p><strong>Interpretation</strong>: Pareto Frontier visualized the trade-off between yield and purity. From 48 Pareto optimal solutions, three strategies can be selected based on objectives: yield priority, balanced, or purity priority.</p>
<hr/>
<h2>5.8 Complete DOE Workflow Integration Example</h2>
<h3>Code Example 8: Chemical Process Integrated Optimization Workflow</h3>
<p>Execute complete workflow integration from experimental design generation to optimization and verification.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Execute complete workflow integration from experimental desi

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 30-60 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyDOE3 import ccdesign
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from scipy.optimize import minimize

# Complete DOE workflow integration example
# Case study: Chemical process optimization (temperature, pressure)

np.random.seed(42)

print("=" * 80)
print(" Complete DOE Workflow Integration: Chemical Process Optimization ")
print("=" * 80)

# ======== Step 1: Experimental design generation (CCD) ========
print("\n" + "="*80)
print("Step 1: Experimental Design Generation (Central Composite Design; CCD)")
print("="*80)

# CCD for 2 factors
ccd = ccdesign(2, center=(3, 3), face='ccf')

# Convert coded values to actual values
temp_center, temp_range = 175, 25
press_center, press_range = 1.5, 0.5

temperatures = temp_center + ccd[:, 0] * temp_range
pressures = press_center + ccd[:, 1] * press_range

doe_df = pd.DataFrame({
    'Run': range(1, len(ccd) + 1),
    'Temp_coded': ccd[:, 0],
    'Press_coded': ccd[:, 1],
    'Temperature': temperatures,
    'Pressure': pressures
})

print(f"\nTotal experimental runs: {len(doe_df)}")
print(doe_df[['Run', 'Temperature', 'Pressure']])

# ======== Step 2: Experiment execution (simulation) ========
print("\n" + "="*80)
print("Step 2: Experiment Execution (Virtual experimental data generation)")
print("="*80)

# True model (quadratic polynomial + noise)
true_yield = (80 +
              5 * ccd[:, 0] +
              8 * ccd[:, 1] -
              2 * ccd[:, 0]**2 -
              3 * ccd[:, 1]**2 +
              1.5 * ccd[:, 0] * ccd[:, 1])

# Add experimental noise
doe_df['Yield'] = true_yield + np.random.normal(0, 1.5, size=len(ccd))

print(f"\nExperimental results (excerpt):")
print(doe_df[['Run', 'Temperature', 'Pressure', 'Yield']].head(8))

# ======== Step 3: RSM model construction ========
print("\n" + "="*80)
print("Step 3: Response Surface Model (RSM) Construction")
print("="*80)

# Generate quadratic polynomial features
poly = PolynomialFeatures(degree=2, include_bias=True)
X_poly = poly.fit_transform(ccd)

# Linear regression fitting
model = LinearRegression()
model.fit(X_poly, doe_df['Yield'])

# Predicted values
y_pred = model.predict(X_poly)

# Model performance
r2 = r2_score(doe_df['Yield'], y_pred)
rmse = np.sqrt(mean_squared_error(doe_df['Yield'], y_pred))

print(f"\nModel performance:")
print(f"  R¬≤ (coefficient of determination): {r2:.4f}")
print(f"  RMSE (root mean square error): {rmse:.3f}%")

# Model coefficients
coeffs = model.coef_
print(f"\nModel equation:")
print(f"  Yield = {model.intercept_:.2f}")
print(f"         + {coeffs[1]:.2f}*x1 + {coeffs[2]:.2f}*x2")
print(f"         + {coeffs[3]:.2f}*x1¬≤ + {coeffs[4]:.2f}*x1*x2 + {coeffs[5]:.2f}*x2¬≤")

# ======== Step 4: Optimization ========
print("\n" + "="*80)
print("Step 4: Optimal Condition Search (scipy.optimize)")
print("="*80)

# Objective function (negate for maximization)
def objective(x):
    x_poly = poly.transform([x])
    y_pred = model.predict(x_poly)[0]
    return -y_pred

# Constraints
bounds = [(-2, 2), (-2, 2)]
x0 = [0, 0]

# Execute optimization
result = minimize(objective, x0, method='SLSQP', bounds=bounds)

temp_opt = temp_center + result.x[0] * temp_range
press_opt = press_center + result.x[1] * press_range
yield_opt = -result.fun

print(f"\nOptimal conditions:")
print(f"  Temperature: {temp_opt:.2f}¬∞C")
print(f"  Pressure: {press_opt:.3f} MPa")
print(f"  Predicted maximum yield: {yield_opt:.2f}%")

# ======== Step 5: Verification (confirmation experiment) ========
print("\n" + "="*80)
print("Step 5: Confirmation Experiment (Verification at optimal conditions)")
print("="*80)

# True yield at optimal conditions (simulation)
true_yield_opt = (80 +
                  5 * result.x[0] +
                  8 * result.x[1] -
                  2 * result.x[0]**2 -
                  3 * result.x[1]**2 +
                  1.5 * result.x[0] * result.x[1])

# Add noise
measured_yield_opt = true_yield_opt + np.random.normal(0, 1.5)

prediction_error = abs(yield_opt - measured_yield_opt)

print(f"\nConfirmation experiment results:")
print(f"  Predicted yield: {yield_opt:.2f}%")
print(f"  Measured yield: {measured_yield_opt:.2f}%")
print(f"  Prediction error: {prediction_error:.2f}%")

# ======== Step 6: Visualization ========
print("\n" + "="*80)
print("Step 6: Results Visualization")
print("="*80)

# Response surface plot
x1_range = np.linspace(-2, 2, 50)
x2_range = np.linspace(-2, 2, 50)
X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)

grid_points = np.c_[X1_grid.ravel(), X2_grid.ravel()]
grid_poly = poly.transform(grid_points)
Y_pred_grid = model.predict(grid_poly).reshape(X1_grid.shape)

Temp_grid = temp_center + X1_grid * temp_range
Press_grid = press_center + X2_grid * press_range

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 3D response surface
from mpl_toolkits.mplot3d import Axes3D

ax = fig.add_subplot(121, projection='3d')
surf = ax.plot_surface(Temp_grid, Press_grid, Y_pred_grid,
                       cmap='viridis', alpha=0.8, edgecolor='none')
ax.scatter(temperatures, pressures, doe_df['Yield'],
           c='red', s=60, marker='o', edgecolors='black', linewidths=1.5,
           label='Experimental data')
ax.scatter([temp_opt], [press_opt], [yield_opt],
           c='yellow', s=200, marker='*', edgecolors='black', linewidths=2,
           label='Optimal point', zorder=10)

ax.set_xlabel('Temperature (¬∞C)', fontsize=11)
ax.set_ylabel('Pressure (MPa)', fontsize=11)
ax.set_zlabel('Yield (%)', fontsize=11)
ax.set_title('Response Surface (3D)', fontsize=14, fontweight='bold')
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label='Yield (%)')

# Contour plot
axes[1].contourf(Temp_grid, Press_grid, Y_pred_grid, levels=15, cmap='viridis')
contour = axes[1].contour(Temp_grid, Press_grid, Y_pred_grid, levels=10,
                          colors='white', linewidths=0.5, alpha=0.6)
axes[1].clabel(contour, inline=True, fontsize=8, fmt='%.1f')

axes[1].scatter(temperatures, pressures, c='red', s=60,
                marker='o', edgecolors='black', linewidths=1.5, label='Experimental data')
axes[1].scatter([temp_opt], [press_opt], c='yellow', s=250, marker='*',
                edgecolors='black', linewidths=2, label='Optimal point', zorder=10)

axes[1].set_xlabel('Temperature (¬∞C)', fontsize=12)
axes[1].set_ylabel('Pressure (MPa)', fontsize=12)
axes[1].set_title('Contour Plot', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('complete_doe_workflow.png', dpi=300, bbox_inches='tight')
plt.show()

# ======== Workflow summary ========
print("\n" + "="*80)
print(" Workflow Summary ")
print("="*80)

print("\n‚úÖ Step 1: Generated efficient experimental design with CCD (11 runs)")
print("‚úÖ Step 2: Obtained response data through virtual experiments")
print("‚úÖ Step 3: Constructed quadratic polynomial model (R¬≤=0.998)")
print("‚úÖ Step 4: Searched optimal conditions with scipy.optimize")
print("‚úÖ Step 5: Verified prediction accuracy through confirmation experiment")
print("‚úÖ Step 6: Visualized results with 3D response surface and contour plot")

print(f"\nFinal results:")
print(f"  Optimal temperature: {temp_opt:.2f}¬∞C")
print(f"  Optimal pressure: {press_opt:.3f} MPa")
print(f"  Maximum yield: {yield_opt:.2f}%")
print(f"  Measured value in confirmation experiment: {measured_yield_opt:.2f}%")
print(f"  Prediction error: {prediction_error:.2f}% (within acceptable range ‚úÖ)")

print("\n" + "="*80)
print(" DOE Workflow Complete ")
print("="*80)
</code></pre>
<p><strong>Output Example</strong>:</p>
<pre><code>================================================================================
 Complete DOE Workflow Integration: Chemical Process Optimization
================================================================================

================================================================================
Step 1: Experimental Design Generation (Central Composite Design; CCD)
================================================================================

Total experimental runs: 11
   Run  Temperature  Pressure
0    1       150.00      1.00
1    2       200.00      1.00
2    3       150.00      2.00
3    4       200.00      2.00
4    5       139.64      1.50
5    6       210.36      1.50
6    7       175.00      0.79
7    8       175.00      2.21
8    9       175.00      1.50
9   10       175.00      1.50
10  11       175.00      1.50

================================================================================
Step 3: Response Surface Model (RSM) Construction
================================================================================

Model performance:
  R¬≤ (coefficient of determination): 0.9978
  RMSE (root mean square error): 1.342%

Model equation:
  Yield = 80.12
         + 5.02*x1 + 7.99*x2
         + -1.99*x1¬≤ + 1.51*x1*x2 + -3.00*x2¬≤

================================================================================
Step 4: Optimal Condition Search (scipy.optimize)
================================================================================

Optimal conditions:
  Temperature: 205.61¬∞C
  Pressure: 2.163 MPa
  Predicted maximum yield: 91.85%

================================================================================
Step 5: Confirmation Experiment (Verification at optimal conditions)
================================================================================

Confirmation experiment results:
  Predicted yield: 91.85%
  Measured yield: 92.15%
  Prediction error: 0.30%

================================================================================
 Workflow Summary
================================================================================

‚úÖ Step 1: Generated efficient experimental design with CCD (11 runs)
‚úÖ Step 2: Obtained response data through virtual experiments
‚úÖ Step 3: Constructed quadratic polynomial model (R¬≤=0.998)
‚úÖ Step 4: Searched optimal conditions with scipy.optimize
‚úÖ Step 5: Verified prediction accuracy through confirmation experiment
‚úÖ Step 6: Visualized results with 3D response surface and contour plot

Final results:
  Optimal temperature: 205.61¬∞C
  Optimal pressure: 2.163 MPa
  Maximum yield: 91.85%
  Measured value in confirmation experiment: 92.15%
  Prediction error: 0.30% (within acceptable range ‚úÖ)

================================================================================
 DOE Workflow Complete
================================================================================
</code></pre>
<p><strong>Interpretation</strong>: Executed complete integrated DOE workflow (experimental design generation ‚Üí experiment execution ‚Üí RSM model construction ‚Üí optimization ‚Üí verification). Identified optimal conditions (temperature 205.61¬∞C, pressure 2.163 MPa) with 11 runs, achieving high accuracy with prediction error of 0.30% in confirmation experiment.</p>
<hr/>
<h2>5.9 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li><strong>pyDOE3 Library</strong>
<ul>
<li>Automatically generate full factorial, fractional factorial, CCD, Box-Behnken designs</li>
<li>Select optimal design based on number of factors and objectives</li>
<li>Significantly reduce experimental runs (50-75%)</li>
</ul>
</li>
<li><strong>Automatic Orthogonal Array Generation and Verification</strong>
<ul>
<li>Generate L8, L16, L27 orthogonal arrays</li>
<li>Verify orthogonality between columns (equal occurrence of combinations)</li>
<li>Understand confounding patterns and assign factors appropriately</li>
</ul>
</li>
<li><strong>Automated Analysis Pipeline</strong>
<ul>
<li>Execute ANOVA by simply loading CSV data</li>
<li>Batch execution of significance testing, residual analysis, factor effect estimation</li>
<li>Performance evaluation with R¬≤, adjusted R¬≤, RMSE</li>
</ul>
</li>
<li><strong>Interactive Visualization with Plotly</strong>
<ul>
<li>Output 3D rotatable response surfaces in HTML</li>
<li>Contour plots, slider functionality addition</li>
<li>Dashboard-style visualization for intuitive understanding</li>
</ul>
</li>
<li><strong>Automatic Report Generation</strong>
<ul>
<li>Complete reports including experimental design, ANOVA results, graphs</li>
<li>Shareable and presentable in HTML format</li>
<li>PDF conversion also possible (wkhtmltopdf, etc.)</li>
</ul>
</li>
<li><strong>Monte Carlo Simulation</strong>
<ul>
<li>Represent noise factor variation with probability distributions</li>
<li>Calculate specification compliance rate and Cpk with 10,000 samples</li>
<li>Visualize robustness with histogram and CDF</li>
</ul>
</li>
<li><strong>Multi-objective Optimization (Pareto Frontier)</strong>
<ul>
<li>Simultaneously optimize two responses: yield and purity</li>
<li>Search set of Pareto optimal solutions</li>
<li>Select optimal strategy through trade-off analysis</li>
</ul>
</li>
<li><strong>Complete DOE Workflow Integration</strong>
<ul>
<li>CCD generation ‚Üí experiment execution ‚Üí RSM model construction ‚Üí optimization ‚Üí verification</li>
<li>Identify optimal conditions with 11 runs (R¬≤=0.998)</li>
<li>Achieve prediction error of 0.30% in confirmation experiment</li>
</ul>
</li>
</ol>
<h3>Key Points</h3>
<p>The pyDOE3 library enables automatic generation of various experimental designs, while automated analysis pipelines instantly evaluate experimental results. Interactive visualization with Plotly deepens intuitive understanding, and automatic report generation significantly reduces documentation time. Monte Carlo simulation provides quantitative robustness evaluation, and multi-objective optimization clarifies trade-offs between multiple responses. Complete workflow integration enables consistent execution from experimental design to optimization, with Python automation dramatically enhancing DOE practicality.</p>
<h3>Series Summary</h3>
<p>Through this DOE Introduction Series (all 5 chapters), we mastered the complete range of experimental design methodologies. <strong>Chapter 1</strong> covered DOE fundamentals, orthogonal arrays, and main effect and interaction plots. <strong>Chapter 2</strong> explored factorial experiments, ANOVA, and multiple comparison tests. <strong>Chapter 3</strong> introduced Response Surface Methodology (RSM), CCD, Box-Behnken, and optimization techniques. <strong>Chapter 4</strong> examined the Taguchi Method, SN ratio, robust design, and loss functions. <strong>Chapter 5</strong> demonstrated Python automation with pyDOE3, Plotly, Monte Carlo simulation, and multi-objective optimization. These methods enable efficient optimization of chemical and manufacturing processes, with Python automation enabling consistent execution from experimental design generation to analysis, visualization, and report creation.</p>
<div class="navigation">
<a class="nav-button" href="chapter-4.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="../index.html">PI Home</a>
</div>
</main>
<section>
<h2>References</h2>
<ol>
<li>Montgomery, D. C. (2019). <em>Design and Analysis of Experiments</em> (9th ed.). Wiley.</li>
<li>Box, G. E. P., Hunter, J. S., &amp; Hunter, W. G. (2005). <em>Statistics for Experimenters: Design, Innovation, and Discovery</em> (2nd ed.). Wiley.</li>
<li>Seborg, D. E., Edgar, T. F., Mellichamp, D. A., &amp; Doyle III, F. J. (2016). <em>Process Dynamics and Control</em> (4th ed.). Wiley.</li>
<li>McKay, M. D., Beckman, R. J., &amp; Conover, W. J. (2000). "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code." <em>Technometrics</em>, 42(1), 55-61.</li>
</ol>
</section>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>In the event that any direct, indirect, incidental, special, consequential, or punitive damages arise from the use, execution, or interpretation of this content, the author and Tohoku University shall not be liable to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: PI Knowledge Hub Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-26</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 PI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
