<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Introduction to Bayesian Optimization Series - Complete Guide from Gaussian Processes to Acquisition Functions" name="description"/>
<title>Introduction to Bayesian Optimization Series v1.0 - PI Knowledge Hub</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Mermaid for diagrams -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/index.html">Process Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/bayesian-optimization/index.html">Bayesian Optimization</a>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/PI/bayesian-optimization/index.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>üî¨ Introduction to Bayesian Optimization Series v1.0</h1>
<div class="meta">
<span>üìñ Reading Time: 140-170 min</span>
<span>üìä Level: Advanced</span>
<span>üíª Code Examples: 35</span>
</div>
</div>
</header>
<main class="container">
<h1 id="v10">Introduction to Bayesian Optimization Series v1.0</h1>
<p><strong>From Gaussian Processes to Acquisition Functions - Practical Guide for Chemical Process Optimization</strong></p>
<h2 id="_1">Series Overview</h2>
<p>This series is a comprehensive 5-chapter educational content designed to progressively teach Bayesian optimization from fundamentals to practice. You will master Gaussian process modeling, acquisition functions, constrained optimization, and multi-objective optimization techniques, enabling you to implement optimization for real chemical processes (reaction conditions, catalyst design, process parameters).</p>
<p><strong>Features:</strong><br/>
- ‚úÖ <strong>Practice-Oriented</strong>: 35 executable Python code examples<br/>
- ‚úÖ <strong>Systematic Structure</strong>: Progressive 5-chapter structure from fundamental theory to industrial applications<br/>
- ‚úÖ <strong>Industrial Applications</strong>: Complete implementations for reaction condition optimization, catalyst screening, and process design<br/>
- ‚úÖ <strong>Latest Technologies</strong>: GPyOpt, BoTorch, scikit-optimize, and GPy integration frameworks</p>
<p><strong>Total Learning Time</strong>: 140-170 minutes (including code execution and exercises)</p>
<hr/>
<h2 id="_2">How to Progress Through This Series</h2>
<h3 id="_3">Recommended Learning Sequence</h3>
<div class="mermaid">
flowchart TD
    A[Chapter 1: Fundamentals of Bayesian Optimization] --&gt; B[Chapter 2: Gaussian Process Modeling]
    B --&gt; C[Chapter 3: Design and Implementation of Acquisition Functions]
    C --&gt; D[Chapter 4: Constrained and Multi-Objective Optimization]
    D --&gt; E[Chapter 5: Case Studies - Chemical Process Optimization]

    style A fill:#e8f5e9
    style B fill:#c8e6c9
    style C fill:#a5d6a7
    style D fill:#81c784
    style E fill:#66bb6a
</div>
<p><strong>For Beginners (First Time Learning Bayesian Optimization):</strong><br/>
- Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br/>
- Duration: 140-170 minutes</p>
<p><strong>For Optimization Practitioners (Experience with Grid Search/Genetic Algorithms):</strong><br/>
- Chapter 1 (Quick Review) ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br/>
- Duration: 110-140 minutes</p>
<p><strong>For Machine Learning Practitioners (Knowledge of Gaussian Process Regression):</strong><br/>
- Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br/>
- Duration: 70-90 minutes</p>
<hr/>
<h2 id="_4">Chapter Details</h2>
<div class="chapter-card">
<h3 class="chapter-title"><a href="chapter-1.html">Chapter 1: Fundamentals of Bayesian Optimization</a></h3>
<div class="chapter-meta">
<span>üìñ Reading Time: 30-35 min</span>
<span>üíª Code Examples: 7</span>
<span>üìä Difficulty: Advanced</span>
</div>
<h4>Learning Content</h4>
<ol>
<li><strong>Black-Box Optimization Problems</strong>
<ul>
<li>Evaluation cost of objective functions</li>
<li>Cases where analytical gradients are unavailable</li>
<li>Constraints on number of experiments</li>
<li>Comparison with grid search</li>
</ul>
</li>
<li><strong>Principles of Bayesian Optimization</strong>
<ul>
<li>Sequential Design strategy</li>
<li>Surrogate Models</li>
<li>Exploration vs Exploitation tradeoff</li>
<li>Convergence guarantees of Bayesian optimization</li>
</ul>
</li>
<li><strong>Basic Bayesian Optimization Loop</strong>
<ul>
<li>Initial sampling</li>
<li>Training surrogate models</li>
<li>Next point selection via acquisition functions</li>
<li>Iterative observation and updating</li>
</ul>
</li>
<li><strong>Application Examples in Chemical Processes</strong>
<ul>
<li>Optimization of reaction temperature and pressure</li>
<li>Catalyst composition exploration</li>
<li>Process parameter tuning</li>
<li>Integration with design of experiments</li>
</ul>
</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Formulate black-box optimization problems</li>
<li>‚úÖ Understand Bayesian optimization principles and Sequential Design strategy</li>
<li>‚úÖ Explain the Exploration vs Exploitation tradeoff</li>
<li>‚úÖ Implement basic Bayesian optimization loops</li>
<li>‚úÖ Compare performance with grid search and random search</li>
<li>‚úÖ Understand concrete examples of chemical process optimization</li>
</ul>
<p><strong><a href="chapter-1.html">Read Chapter 1 ‚Üí</a></strong></p>
</div>
<div class="chapter-card">
<h3 class="chapter-title"><a href="chapter-2.html">Chapter 2: Gaussian Process Modeling</a></h3>
<div class="chapter-meta">
<span>üìñ Reading Time: 35-40 min</span>
<span>üíª Code Examples: 7</span>
<span>üìä Difficulty: Advanced</span>
</div>
<h4>Learning Content</h4>
<ol>
<li><strong>Fundamentals of Gaussian Process Regression</strong>
<ul>
<li>Definition and properties of Gaussian processes</li>
<li>Mean functions and covariance functions (kernels)</li>
<li>Computation of posterior distributions</li>
<li>Predictive distributions and uncertainty quantification</li>
</ul>
</li>
<li><strong>Selection of Kernel Functions</strong>
<ul>
<li>RBF (Radial Basis Function) kernel</li>
<li>Mat√©rn kernel</li>
<li>Rational Quadratic kernel</li>
<li>Kernel combinations (sum and product)</li>
</ul>
</li>
<li><strong>Hyperparameter Optimization</strong>
<ul>
<li>Maximum Likelihood Estimation (MLE)</li>
<li>Maximum A Posteriori (MAP) estimation</li>
<li>Computation of log marginal likelihood</li>
<li>Gradient-based optimization</li>
</ul>
</li>
<li><strong>Practical Aspects of Gaussian Processes</strong>
<ul>
<li>Multi-output Gaussian processes</li>
<li>Sparse Gaussian processes (computational efficiency)</li>
<li>Handling noisy data</li>
<li>Model validation and diagnostics</li>
</ul>
</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand mathematical foundations of Gaussian process regression</li>
<li>‚úÖ Implement and appropriately select major kernel functions</li>
<li>‚úÖ Optimize hyperparameters using MLE/MAP</li>
<li>‚úÖ Compute predictive distributions and uncertainty</li>
<li>‚úÖ Implement multi-output GP and sparse GP</li>
<li>‚úÖ Diagnose and validate Gaussian process models</li>
</ul>
<p><strong><a href="chapter-2.html">Read Chapter 2 ‚Üí</a></strong></p>
</div>
<div class="chapter-card">
<h3 class="chapter-title"><a href="chapter-3.html">Chapter 3: Design and Implementation of Acquisition Functions</a></h3>
<div class="chapter-meta">
<span>üìñ Reading Time: 30-35 min</span>
<span>üíª Code Examples: 7</span>
<span>üìä Difficulty: Advanced</span>
</div>
<h4>Learning Content</h4>
<ol>
<li><strong>Major Acquisition Functions</strong>
<ul>
<li>Probability of Improvement (PI)</li>
<li>Expected Improvement (EI)</li>
<li>Upper Confidence Bound (UCB)</li>
<li>Entropy Search (ES)</li>
</ul>
</li>
<li><strong>Optimization of Acquisition Functions</strong>
<ul>
<li>Gradient-based optimization (L-BFGS-B)</li>
<li>Multi-start strategy</li>
<li>Optimization in discrete spaces</li>
<li>Acquisition functions for parallel evaluation</li>
</ul>
</li>
<li><strong>Batch Bayesian Optimization</strong>
<ul>
<li>q-Expected Improvement (qEI)</li>
<li>Local Penalization</li>
<li>Constant Liar strategy</li>
<li>Parallel experimental design</li>
</ul>
</li>
<li><strong>Comparison and Selection of Acquisition Functions</strong>
<ul>
<li>Comparison of convergence rates</li>
<li>Adjusting exploration-exploitation balance</li>
<li>Selection based on problem characteristics</li>
<li>Hybrid strategies</li>
</ul>
</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Implement major acquisition functions (PI, EI, UCB, ES)</li>
<li>‚úÖ Optimize acquisition functions using gradient-based methods</li>
<li>‚úÖ Implement batch Bayesian optimization</li>
<li>‚úÖ Compare and evaluate acquisition function performance</li>
<li>‚úÖ Select acquisition functions based on problem characteristics</li>
</ul>
<p><strong><a href="chapter-3.html">Read Chapter 3 ‚Üí</a></strong></p>
</div>
<div class="chapter-card">
<h3 class="chapter-title"><a href="chapter-4.html">Chapter 4: Constrained and Multi-Objective Optimization</a></h3>
<div class="chapter-meta">
<span>üìñ Reading Time: 30-35 min</span>
<span>üíª Code Examples: 7</span>
<span>üìä Difficulty: Advanced</span>
</div>
<h4>Learning Content</h4>
<ol>
<li><strong>Constrained Bayesian Optimization</strong>
<ul>
<li>Modeling constraint functions</li>
<li>Constrained Expected Improvement (CEI)</li>
<li>Probability of Feasibility (PoF)</li>
<li>Unknown Constraints</li>
</ul>
</li>
<li><strong>Multi-Objective Bayesian Optimization</strong>
<ul>
<li>Pareto frontier</li>
<li>Expected Hypervolume Improvement (EHVI)</li>
<li>ParEGO (Pareto Efficient Global Optimization)</li>
<li>Scalarization methods</li>
</ul>
</li>
<li><strong>High-Dimensional Bayesian Optimization</strong>
<ul>
<li>Dimensionality reduction (Random Embedding)</li>
<li>Trust Region Bayesian Optimization (TuRBO)</li>
<li>Additive models</li>
<li>Feature Selection</li>
</ul>
</li>
<li><strong>Practical Optimization Strategies</strong>
<ul>
<li>Early Stopping criteria</li>
<li>Budget Allocation</li>
<li>Transfer Learning</li>
<li>Multi-fidelity optimization</li>
</ul>
</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Implement constrained Bayesian optimization</li>
<li>‚úÖ Find Pareto frontiers in multi-objective optimization</li>
<li>‚úÖ Apply dimensionality reduction techniques for high-dimensional problems</li>
<li>‚úÖ Implement Early Stopping and Budget Allocation</li>
<li>‚úÖ Understand multi-fidelity optimization</li>
</ul>
<p><strong><a href="chapter-4.html">Read Chapter 4 ‚Üí</a></strong></p>
</div>
<div class="chapter-card">
<h3 class="chapter-title"><a href="chapter-5.html">Chapter 5: Case Studies - Chemical Process Optimization</a></h3>
<div class="chapter-meta">
<span>üìñ Reading Time: 35-40 min</span>
<span>üíª Code Examples: 7</span>
<span>üìä Difficulty: Advanced</span>
</div>
<h4>Learning Content</h4>
<ol>
<li><strong>Case Study 1: Reaction Condition Optimization</strong>
<ul>
<li>Simultaneous optimization of temperature, pressure, and residence time</li>
<li>Tradeoffs between yield and selectivity</li>
<li>Consideration of safety constraints</li>
<li>Minimization of experimental costs</li>
</ul>
</li>
<li><strong>Case Study 2: Catalyst Screening</strong>
<ul>
<li>Efficient exploration of composition space</li>
<li>Multi-objective optimization (activity, selectivity, stability)</li>
<li>Mixed discrete and continuous variables</li>
<li>Knowledge transfer via Transfer Learning</li>
</ul>
</li>
<li><strong>Case Study 3: Process Design Optimization</strong>
<ul>
<li>Optimization of distillation column stages and reflux ratio</li>
<li>Economic minimization (CAPEX + OPEX)</li>
<li>Environmental constraints (CO2 emissions)</li>
<li>Robustness evaluation</li>
</ul>
</li>
<li><strong>Deployment to Industrial Implementation</strong>
<ul>
<li>Integration with laboratory automation</li>
<li>Real-time optimization</li>
<li>Integration with Digital Twins</li>
<li>Deployment best practices</li>
</ul>
</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Implement multi-variable simultaneous optimization of reaction conditions</li>
<li>‚úÖ Perform efficient screening of catalyst compositions</li>
<li>‚úÖ Practice economic optimization of process design</li>
<li>‚úÖ Integrate with laboratory automation systems</li>
<li>‚úÖ Complete real process Bayesian optimization projects</li>
</ul>
<p><strong><a href="chapter-5.html">Read Chapter 5 ‚Üí</a></strong></p>
</div>
<hr/>
<h2 id="_5">Overall Learning Outcomes</h2>
<p>Upon completing this series, you will acquire the following skills and knowledge:</p>
<h3>Knowledge Level (Understanding)</h3>
<ul>
<li>‚úÖ Understand the theoretical foundations of Bayesian optimization</li>
<li>‚úÖ Know the mathematical principles of Gaussian process modeling</li>
<li>‚úÖ Understand characteristics and appropriate use of major acquisition functions</li>
<li>‚úÖ Know constrained and multi-objective optimization techniques</li>
<li>‚úÖ Understand application patterns for chemical process optimization</li>
</ul>
<h3>Practical Skills (Doing)</h3>
<ul>
<li>‚úÖ Implement Gaussian process models and appropriately select kernels</li>
<li>‚úÖ Implement acquisition functions (PI, EI, UCB, ES)</li>
<li>‚úÖ Implement constrained and multi-objective Bayesian optimization</li>
<li>‚úÖ Design parallel experiments using batch Bayesian optimization</li>
<li>‚úÖ Utilize GPyOpt, BoTorch, and scikit-optimize libraries</li>
<li>‚úÖ Perform model diagnostics and performance evaluation</li>
</ul>
<h3>Application Ability (Applying)</h3>
<ul>
<li>‚úÖ Optimize real chemical processes</li>
<li>‚úÖ Solve optimization problems for reaction conditions and catalyst compositions</li>
<li>‚úÖ Find Pareto solutions in multi-objective optimization</li>
<li>‚úÖ Integrate with laboratory automation systems</li>
<li>‚úÖ Lead Bayesian optimization projects as a process engineer</li>
</ul>
<hr/>
<h2 id="faq">FAQ (Frequently Asked Questions)</h2>
<h3>Q1: What level of mathematical prerequisite knowledge is required?</h3>
<p><strong>A</strong>: Basic knowledge of linear algebra (matrix operations, eigenvalues), probability and statistics (Gaussian distribution, Bayes' theorem), and calculus (gradient computation) is required. It is assumed that you have completed undergraduate-level mathematics in science and engineering.</p>
<h3>Q2: What are the differences from grid search and genetic algorithms?</h3>
<p><strong>A</strong>: Bayesian optimization specializes in finding optimal solutions with a small number of evaluations. Grid search is exhaustive but requires enormous evaluations, while genetic algorithms require many evaluations. Bayesian optimization is most effective when evaluation costs are high (experiments, simulations).</p>
<h3>Q3: Which Python libraries are needed?</h3>
<p><strong>A</strong>: Primarily uses NumPy, SciPy, scikit-learn, GPyOpt, BoTorch (PyTorch), GPy, Matplotlib, and Ax. All can be installed via pip.</p>
<h3>Q4: What is the relationship with the Process Optimization Series?</h3>
<p><strong>A</strong>: By applying Bayesian optimization techniques from this series to optimization problem formulations learned in the Process Optimization Series, you can significantly reduce the number of experiments. Combining both series enables mastery of efficient process design workflows.</p>
<h3>Q5: Can this be applied to actual chemical processes?</h3>
<p><strong>A</strong>: Yes. Chapter 5 covers complete workflows for real process applications through practical case studies. However, careful verification of safety and process constraints is necessary during implementation.</p>
<hr/>
<h2 id="_6">Next Steps</h2>
<h3>Recommended Actions After Completing the Series</h3>
<p><strong>Immediate (Within 1 Week):</strong><br/>
1. ‚úÖ Publish Chapter 5 case studies on GitHub<br/>
2. ‚úÖ Evaluate Bayesian optimization opportunities in your company's processes<br/>
3. ‚úÖ Try techniques on simple 1D optimization problems</p>
<p><strong>Short-term (1-3 Months):</strong><br/>
1. ‚úÖ Validate Bayesian optimization with experimental data<br/>
2. ‚úÖ Consider integration with laboratory automation systems<br/>
3. ‚úÖ Launch multi-objective optimization projects<br/>
4. ‚úÖ Practice knowledge transfer via Transfer Learning</p>
<p><strong>Long-term (6+ Months):</strong><br/>
1. ‚úÖ Integration of Digital Twins and Bayesian optimization<br/>
2. ‚úÖ Real-time process optimization<br/>
3. ‚úÖ Conference presentations and paper writing<br/>
4. ‚úÖ Career development as a Bayesian optimization specialist</p>
<hr/>
<h2 id="_7">Feedback and Support</h2>
<h3>About This Series</h3>
<p>This series was created under Dr. Yusuke Hashimoto at Tohoku University as part of the PI Knowledge Hub project.</p>
<p><strong>Creation Date</strong>: October 26, 2025<br/>
<strong>Version</strong>: 1.0</p>
<h3>We Welcome Your Feedback</h3>
<p>We welcome your feedback to improve this series:</p>
<ul>
<li><strong>Typos, errors, technical mistakes</strong>: Please report via GitHub repository Issues</li>
<li><strong>Improvement suggestions</strong>: New topics, additional code examples desired, etc.</li>
<li><strong>Questions</strong>: Sections that were difficult to understand, areas needing additional explanation</li>
<li><strong>Success stories</strong>: Projects using what you learned from this series</li>
</ul>
<p><strong>Contact</strong>: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<hr/>
<h2 id="_8">License and Terms of Use</h2>
<p>This series is published under the <strong>CC BY 4.0</strong> (Creative Commons Attribution 4.0 International) license.</p>
<p><strong>What You Can Do:</strong><br/>
- ‚úÖ Free viewing and downloading<br/>
- ‚úÖ Use for educational purposes (classes, study sessions, etc.)<br/>
- ‚úÖ Modification and derivative works (translation, summarization, etc.)</p>
<p><strong>Conditions:</strong><br/>
- üìå Author credit must be provided<br/>
- üìå Modifications must be indicated<br/>
- üìå Contact required before commercial use</p>
<p>Details: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License Full Text</a></p>
<hr/>
<h2 id="_9">Let's Get Started!</h2>
<p>Are you ready? Start with Chapter 1 and begin your journey into the world of Bayesian optimization!</p>
<p><strong><a href="chapter-1.html">Chapter 1: Fundamentals of Bayesian Optimization ‚Üí</a></strong></p>
<hr/>
<p><strong>Update History</strong></p>
<ul>
<li><strong>2025-10-26</strong>: v1.0 Initial Release</li>
</ul>
<hr/>
<p><strong>Your journey to learn Bayesian optimization starts here!</strong></p>
<div class="nav-buttons">
<a class="nav-button" href="../index.html">‚Üê Back to Process Informatics Dojo Top</a>
</div>
</main>
<section>
            <h2>References</h2>
            <ol>
                <li>Montgomery, D. C. (2019). <em>Design and Analysis of Experiments</em> (9th ed.). Wiley.</li>
                <li>Box, G. E. P., Hunter, J. S., &amp; Hunter, W. G. (2005). <em>Statistics for Experimenters: Design, Innovation, and Discovery</em> (2nd ed.). Wiley.</li>
                <li>Seborg, D. E., Edgar, T. F., Mellichamp, D. A., &amp; Doyle III, F. J. (2016). <em>Process Dynamics and Control</em> (4th ed.). Wiley.</li>
                <li>McKay, M. D., Beckman, R. J., &amp; Conover, W. J. (2000). "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code." <em>Technometrics</em>, 42(1), 55-61.</li>
            </ol>
        </section>

<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operability, or safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without prior notice.</li>
<li>The copyright and license of this content follow the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<div class="container">
<p>¬© 2025 PI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
