<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Chapter 2: Gaussian Process Modeling - Bayesian Optimization Introduction Series" name="description"/>
<title>Chapter 2: Gaussian Process Modeling - Bayesian Optimization Introduction | PI Terakoya</title>
<style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.8; color: #333; background: #f5f5f5;
        }
        header {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; padding: 2rem 1rem; text-align: center;
        }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        .subtitle { opacity: 0.9; font-size: 1.1rem; }
        .container { max-width: 1200px; margin: 2rem auto; padding: 0 1rem; }
        .back-link {
            display: inline-block; margin-bottom: 2rem; padding: 0.5rem 1rem;
            background: white; color: #11998e; text-decoration: none;
            border-radius: 6px; font-weight: 600;
        }
        .content-box {
            background: white; padding: 2rem; border-radius: 12px;
            margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #11998e; margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem; border-bottom: 3px solid #11998e;
        }
        h3 { color: #2c3e50; margin: 1.5rem 0 1rem 0; }
        h4 { color: #2c3e50; margin: 1rem 0 0.5rem 0; }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 2rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.5rem; }
        pre {
            background: #1e1e1e; color: #d4d4d4; padding: 1.5rem;
            border-radius: 8px; overflow-x: auto; margin: 1rem 0;
            border-left: 4px solid #11998e;
        }
        code {
            font-family: 'Courier New', monospace; font-size: 0.9rem;
        }
        .key-point {
            background: #e8f5e9; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #4caf50; margin: 1rem 0;
        }
        .tech-note {
            background: #e3f2fd; padding: 1rem; border-radius: 6px;
            border-left: 4px solid #2196f3; margin: 1rem 0;
        }
        .formula {
            background: #f0f7ff; padding: 1rem; border-radius: 6px;
            margin: 1rem 0; overflow-x: auto;
        }
        table {
            width: 100%; border-collapse: collapse; margin: 1rem 0;
        }
        th, td {
            border: 1px solid #ddd; padding: 0.75rem; text-align: left;
        }
        th {
            background: #11998e; color: white; font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        .nav-buttons {
            display: flex; justify-content: space-between; margin-top: 3rem;
        }
        .nav-buttons a {
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white; text-decoration: none; border-radius: 6px;
            font-weight: 600;
        }
        footer {
            background: #2c3e50; color: white; text-align: center;
            padding: 2rem 1rem; margin-top: 4rem;
        }
        @media (max-width: 768px) {
            h1 { font-size: 1.6rem; }
            .container { padding: 0 0.5rem; }
            pre { padding: 1rem; }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">:</span><a href="../../PI/index.html">Process Informatics</a><span class="breadcrumb-separator">:</span><a href="../../PI/bayesian-optimization/index.html">Bayesian Optimization</a><span class="breadcrumb-separator">:</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav>
<header>
<div class="container">
<h1>Chapter 2: Gaussian Process Modeling</h1>
<p class="subtitle">Powerful Regression Method for Quantifying Uncertainty</p>
<div class="meta">
<span class="meta">=Ã– Reading Time: 35-40 minutes</span>
<span class="meta">=ÃŠ Difficulty: Advanced</span>
<span class="meta">=Â» Code Examples: 7</span>
</div>
</div>
</header>
<main class="container">
<h2 id="introduction">Introduction</h2>
<p>Gaussian Processes (GP) are probabilistic modeling methods that form the core of Bayesian optimization. Unlike traditional regression methods, GPs can quantify <strong>prediction uncertainty</strong> in addition to predicted values, enabling optimization of the exploration-exploitation trade-off.</p>
<p>In this chapter, we will learn practical implementation using chemical process data, starting from 1D regression through various kernel functions, hyperparameter optimization, and model validation.</p>
<div class="callout">
<div class="callout-title">=Â¡ Key Points of This Chapter</div>
<ul>
<li>Gaussian processes are completely defined by mean function and covariance function (kernel)</li>
<li>Kernel selection should be adjusted according to problem smoothness</li>
<li>Hyperparameters can be automatically optimized by Maximum Likelihood Estimation (MLE)</li>
<li>Prediction uncertainty can be visualized as confidence intervals</li>
</ul>
</div>
<h2 id="gp-basics">2.1 Fundamentals of Gaussian Process Regression</h2>
<h3>2.1.1 Mathematical Definition</h3>
<p>A Gaussian process is a stochastic process where <strong>function values at any finite set of points follow a joint Gaussian distribution</strong>:</p>
<pre><code>f(x) ~ GP(m(x), k(x, x'))

m(x) = E[f(x)]                    # Mean function
k(x, x') = E[(f(x) - m(x))(f(x') - m(x'))]  # Covariance function (kernel)</code></pre>
<p>Given observed data <code>D = {(x_i, y_i)}</code>, the predictive distribution at a new point <code>x*</code> is:</p>
<pre><code>f(x*) | D ~ N(Â¼(x*), ÃƒÂ²(x*))

Â¼(x*) = k(x*, X) [K + Ãƒ_nÂ² I]{Â¹ y
ÃƒÂ²(x*) = k(x*, x*) - k(x*, X) [K + Ãƒ_nÂ² I]{Â¹ k(X, x*)</code></pre>
<h3 id="example-1">Example 1: 1D Gaussian Process Regression (Chemical Reaction Yield)</h3>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# ===================================
# 1D chemical reaction yield data (Temperature vs Yield)
# ===================================
# Experimental data (Temperature [Â°C] â€™ Yield [%])
X_train = np.array([50, 70, 90, 110, 130]).reshape(-1, 1)
y_train = np.array([45, 62, 78, 71, 52])  # Optimal temperature around 90Â°C

# Build Gaussian process model
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10,
                               alpha=1e-2, random_state=42)

# Model training (automatic hyperparameter optimization)
gp.fit(X_train, y_train)

# Prediction (with uncertainty)
X_test = np.linspace(40, 140, 100).reshape(-1, 1)
y_pred, sigma = gp.predict(X_test, return_std=True)

# Visualization
plt.figure(figsize=(10, 6))
plt.plot(X_test, y_pred, 'b-', label='GP prediction mean', linewidth=2)
plt.fill_between(X_test.ravel(),
                 y_pred - 1.96*sigma,
                 y_pred + 1.96*sigma,
                 alpha=0.3, label='95% confidence interval')
plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='k', label='Experimental data')
plt.xlabel('Temperature [Â°C]', fontsize=12)
plt.ylabel('Yield [%]', fontsize=12)
plt.title('Reaction Yield Prediction with Gaussian Process', fontsize=14)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Optimized kernel: {gp.kernel_}")
print(f"Log marginal likelihood: {gp.log_marginal_likelihood_value_:.2f}")

# Expected output:
# Optimized kernel: 31.6**2 * RBF(length_scale=15.8)
# Log marginal likelihood: -12.45
# Plot: Yield peak around 90Â°C, uncertainty increases at edges</code></pre>
<div class="callout">
<div class="callout-title">=
 Explanation: Meaning of Confidence Intervals</div>
<p>The <strong>95% confidence interval</strong> (Â¼ Â± 1.96Ãƒ) indicates that the true yield falls within this range with 95% probability. In regions far from experimental data (around 40Â°C, 140Â°C), uncertainty increases and the interval widens. This is the key to <strong>promoting exploration of unexplored regions</strong> in Bayesian optimization.</p>
</div>
<h2 id="kernels">2.2 Kernel Function Selection</h2>
<h3>2.2.1 Properties of Major Kernels</h3>
<table>
<thead>
<tr>
<th>Kernel</th>
<th>Formula</th>
<th>Smoothness</th>
<th>Application Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RBF (Squared Exponential)</strong></td>
<td>k(x, x') = ÃƒÂ² exp(-||x - x'||Â² / (2Â²))</td>
<td>Infinitely differentiable</td>
<td>Temperature-yield relationship</td>
</tr>
<tr>
<td><strong>MatÃ©rn (Â½=1.5)</strong></td>
<td>k(x, x') = ÃƒÂ² (1 + 3r/) exp(-3r/)</td>
<td>Once differentiable</td>
<td>Pressure-flow relationship</td>
</tr>
<tr>
<td><strong>MatÃ©rn (Â½=2.5)</strong></td>
<td>k(x, x') = ÃƒÂ² (1 + 5r/ + 5rÂ²/3Â²) exp(-5r/)</td>
<td>Twice differentiable</td>
<td>Catalyst activity curve</td>
</tr>
<tr>
<td><strong>Rational Quadratic</strong></td>
<td>k(x, x') = ÃƒÂ² (1 + rÂ²/(2Â±Â²))^(-Â±)</td>
<td>Scale mixture of RBF</td>
<td>Multi-scale phenomena</td>
</tr>
</tbody>
</table>
<h3 id="example-2">Example 2: RBF Kernel (Smooth Reaction Curve)</h3>
<pre><code>from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# ===================================
# RBF Kernel: Infinitely differentiable (very smooth)
# ===================================
# Yield data with simultaneous temperature-pressure changes
X_train = np.array([[60, 1.0], [80, 1.5], [100, 2.0],
                    [80, 2.5], [60, 2.0]]) # [TemperatureÂ°C, Pressure MPa]
y_train = np.array([50, 68, 85, 72, 58])

# Define RBF kernel
kernel_rbf = C(1.0) * RBF(length_scale=[10.0, 0.5],
                           length_scale_bounds=(1e-2, 1e3))

gp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, n_restarts_optimizer=15)
gp_rbf.fit(X_train, y_train)

# Grid for contour plot
temp_range = np.linspace(50, 110, 50)
pressure_range = np.linspace(0.8, 3.0, 50)
T, P = np.meshgrid(temp_range, pressure_range)
X_grid = np.c_[T.ravel(), P.ravel()]

y_pred_grid, sigma_grid = gp_rbf.predict(X_grid, return_std=True)
Y_pred = y_pred_grid.reshape(T.shape)
Sigma = sigma_grid.reshape(T.shape)

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Prediction mean
contour1 = axes[0].contourf(T, P, Y_pred, levels=15, cmap='viridis')
axes[0].scatter(X_train[:, 0], X_train[:, 1], c='red', s=150,
                edgecolors='white', linewidths=2, label='Experimental points')
axes[0].set_xlabel('Temperature [Â°C]')
axes[0].set_ylabel('Pressure [MPa]')
axes[0].set_title('RBF Kernel: Predicted Yield [%]')
plt.colorbar(contour1, ax=axes[0])
axes[0].legend()

# Uncertainty
contour2 = axes[1].contourf(T, P, Sigma, levels=15, cmap='Reds')
axes[1].scatter(X_train[:, 0], X_train[:, 1], c='blue', s=150,
                edgecolors='white', linewidths=2, label='Experimental points')
axes[1].set_xlabel('Temperature [Â°C]')
axes[1].set_ylabel('Pressure [MPa]')
axes[1].set_title('RBF Kernel: Prediction Standard Deviation [%]')
plt.colorbar(contour2, ax=axes[1])
axes[1].legend()

plt.tight_layout()
plt.show()

print(f"Optimized length scale: {gp_rbf.kernel_.k2.length_scale}")
print(f"Temperature direction: {gp_rbf.kernel_.k2.length_scale[0]:.2f}Â°C")
print(f"Pressure direction: {gp_rbf.kernel_.k2.length_scale[1]:.2f} MPa")

# Expected output:
# Optimized length scale: [12.34  0.68]
# Temperature direction: 12.34Â°C
# Pressure direction: 0.68 MPa
# â€™ Yield characteristics more sensitive to pressure changes than temperature</code></pre>
<h3 id="example-3">Example 3: MatÃ©rn Kernel (Â½=1.5)</h3>
<pre><code>from sklearn.gaussian_process.kernels import Matern

# ===================================
# MatÃ©rn Kernel (Â½=1.5): Once differentiable
# Moderate smoothness based on physical laws
# ===================================
kernel_matern15 = C(1.0) * Matern(length_scale=10.0, nu=1.5)

gp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15,
                                        n_restarts_optimizer=10)
gp_matern15.fit(X_train, y_train)

# Comparison on 1D cross-section (fixed pressure=2.0 MPa)
X_test_1d = np.column_stack([np.linspace(50, 110, 100),
                              np.full(100, 2.0)])
y_pred_matern, sigma_matern = gp_matern15.predict(X_test_1d, return_std=True)

plt.figure(figsize=(10, 6))
plt.plot(X_test_1d[:, 0], y_pred_matern, 'g-',
         label='MatÃ©rn(Â½=1.5)', linewidth=2)
plt.fill_between(X_test_1d[:, 0],
                 y_pred_matern - 1.96*sigma_matern,
                 y_pred_matern + 1.96*sigma_matern,
                 alpha=0.3, color='green')
plt.scatter(X_train[:, 0], y_train, c='red', s=100,
            edgecolors='k', label='Experimental data')
plt.xlabel('Temperature [Â°C] (Pressure=2.0 MPa fixed)')
plt.ylabel('Yield [%]')
plt.title('Prediction with MatÃ©rn Kernel (Â½=1.5)')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"MatÃ©rn(Â½=1.5) kernel: {gp_matern15.kernel_}")
print(f"Log marginal likelihood: {gp_matern15.log_marginal_likelihood_value_:.2f}")

# Expected output:
# MatÃ©rn(Â½=1.5) kernel: 1.2**2 * Matern(length_scale=11.5, nu=1.5)
# Log marginal likelihood: -10.23
# â€™ Slightly rougher prediction than RBF (robust to experimental noise)</code></pre>
<h3 id="example-4">Example 4: MatÃ©rn Kernel (Â½=2.5)</h3>
<pre><code># ===================================
# MatÃ©rn Kernel (Â½=2.5): Twice differentiable
# Intermediate smoothness between RBF and Â½=1.5
# ===================================
kernel_matern25 = C(1.0) * Matern(length_scale=10.0, nu=2.5)

gp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25,
                                        n_restarts_optimizer=10)
gp_matern25.fit(X_train, y_train)

y_pred_matern25, sigma_matern25 = gp_matern25.predict(X_test_1d, return_std=True)

# Comparison plot of three kernels
plt.figure(figsize=(12, 6))

plt.plot(X_test_1d[:, 0], y_pred_grid[:100], 'b-',
         label='RBF ( times differentiable)', linewidth=2)
plt.plot(X_test_1d[:, 0], y_pred_matern, 'g--',
         label='MatÃ©rn(Â½=1.5)', linewidth=2)
plt.plot(X_test_1d[:, 0], y_pred_matern25, 'orange',
         linestyle='-.', label='MatÃ©rn(Â½=2.5)', linewidth=2)

plt.scatter(X_train[:, 0], y_train, c='red', s=150,
            edgecolors='k', linewidths=2, label='Experimental data', zorder=10)

plt.xlabel('Temperature [Â°C] (Pressure=2.0 MPa fixed)', fontsize=12)
plt.ylabel('Yield [%]', fontsize=12)
plt.title('Comparison of Kernel Functions (Smoothness Differences)', fontsize=14)
plt.legend(fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Comparison of log marginal likelihood (higher is better)
print("\n=== Kernel Performance Comparison ===")
print(f"RBF:           {gp_rbf.log_marginal_likelihood_value_:.2f}")
print(f"MatÃ©rn(Â½=1.5): {gp_matern15.log_marginal_likelihood_value_:.2f}")
print(f"MatÃ©rn(Â½=2.5): {gp_matern25.log_marginal_likelihood_value_:.2f}")

# Expected output:
# === Kernel Performance Comparison ===
# RBF:           -9.87
# MatÃ©rn(Â½=1.5): -10.23
# MatÃ©rn(Â½=2.5): -9.95
# â€™ RBF has highest likelihood (best for this data)</code></pre>
<div class="callout">
<div class="callout-title">=ÃŠ Kernel Selection Guidelines</div>
<ul>
<li><strong>RBF</strong>: Very smooth response (temperature-yield, concentration-activity)</li>
<li><strong>MatÃ©rn(Â½=1.5)</strong>: Large experimental noise, physical constraints present</li>
<li><strong>MatÃ©rn(Â½=2.5)</strong>: Balanced type (recommended default)</li>
<li><strong>Rational Quadratic</strong>: Multiple scale variations (multi-stage reactions)</li>
</ul>
</div>
<h3 id="example-5">Example 5: Rational Quadratic Kernel</h3>
<pre><code>from sklearn.gaussian_process.kernels import RationalQuadratic

# ===================================
# Rational Quadratic Kernel:
# Infinite mixture of RBF kernels with different length scales
# Effective for modeling multi-scale phenomena
# ===================================
kernel_rq = C(1.0) * RationalQuadratic(length_scale=10.0, alpha=1.0)

gp_rq = GaussianProcessRegressor(kernel=kernel_rq, n_restarts_optimizer=10)
gp_rq.fit(X_train, y_train)

y_pred_rq, sigma_rq = gp_rq.predict(X_test_1d, return_std=True)

plt.figure(figsize=(10, 6))
plt.plot(X_test_1d[:, 0], y_pred_rq, 'purple', linewidth=2,
         label='Rational Quadratic')
plt.fill_between(X_test_1d[:, 0],
                 y_pred_rq - 1.96*sigma_rq,
                 y_pred_rq + 1.96*sigma_rq,
                 alpha=0.3, color='purple')
plt.scatter(X_train[:, 0], y_train, c='red', s=100,
            edgecolors='k', label='Experimental data')
plt.xlabel('Temperature [Â°C] (Pressure=2.0 MPa fixed)')
plt.ylabel('Yield [%]')
plt.title('Prediction with Rational Quadratic Kernel')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Optimized kernel: {gp_rq.kernel_}")
print(f"Â± (scale mixture degree): {gp_rq.kernel_.k2.alpha:.2f}")
print(f"Log marginal likelihood: {gp_rq.log_marginal_likelihood_value_:.2f}")

# Expected output:
# Optimized kernel: 1.15**2 * RationalQuadratic(alpha=0.85, length_scale=12.3)
# Â± (scale mixture degree): 0.85
# Log marginal likelihood: -10.10
# â€™ Â±&lt;1: short-range correlation dominant, converges to RBF as Â±â€™</code></pre>
<h2 id="hyperparameter-optimization">2.3 Hyperparameter Optimization</h2>
<h3>2.3.1 Principles of Maximum Likelihood Estimation (MLE)</h3>
<p>The hyperparameters <code>Â¸ = {ÃƒÂ², , Ãƒ_nÂ²}</code> of a Gaussian process are optimized by maximizing the log marginal likelihood:</p>
<pre><code>log p(y | X, Â¸) = -1/2 y^T [K + Ãƒ_nÂ² I]{Â¹ y
                  - 1/2 log|K + Ãƒ_nÂ² I|
                  - n/2 log(2Ã€)

Optimization: Â¸* = argmax_Â¸ log p(y | X, Â¸)</code></pre>
<h3 id="example-6">Example 6: Hyperparameter Optimization (MLE with scipy)</h3>
<pre><code>from scipy.optimize import minimize
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import numpy as np

# ===================================
# Manual hyperparameter optimization implementation
# (Understanding internal processing of scikit-learn)
# ===================================
# Data
X = np.array([[60], [80], [100], [80], [60]])
y = np.array([50, 68, 85, 72, 58])

def negative_log_marginal_likelihood(theta, X, y):
    """Negative log marginal likelihood (for minimization)

    Args:
        theta: [log(ÃƒÂ²), log(), log(Ãƒ_nÂ²)]
        X: Input data (n, d)
        y: Output data (n,)

    Returns:
        -log p(y | X, Â¸)
    """
    sigma_f = np.exp(theta[0])  # Signal variance
    length_scale = np.exp(theta[1])  # Length scale
    sigma_n = np.exp(theta[2])  # Noise standard deviation

    # Compute kernel matrix
    n = X.shape[0]
    K = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            r_sq = np.sum((X[i] - X[j])**2)
            K[i, j] = sigma_f**2 * np.exp(-r_sq / (2 * length_scale**2))

    # Add noise
    K_y = K + sigma_n**2 * np.eye(n)

    # Compute log marginal likelihood
    try:
        L = np.linalg.cholesky(K_y)  # Cholesky decomposition (numerically stable)
        alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))

        log_likelihood = (-0.5 * y.dot(alpha)
                         - np.sum(np.log(np.diag(L)))
                         - n/2 * np.log(2*np.pi))

        return -log_likelihood  # Negative for minimization
    except np.linalg.LinAlgError:
        return 1e10  # Penalty for numerically unstable cases

# Initial values (log scale)
theta_init = np.log([1.0, 10.0, 1.0])  # [ÃƒÂ², , Ãƒ_nÂ²]

# Execute optimization (L-BFGS-B method)
result = minimize(negative_log_marginal_likelihood, theta_init,
                  args=(X, y), method='L-BFGS-B',
                  bounds=[(-5, 5), (-2, 5), (-5, 2)])  # Bounds in log space

# Optimal hyperparameters
theta_opt = result.x
sigma_f_opt = np.exp(theta_opt[0])
length_scale_opt = np.exp(theta_opt[1])
sigma_n_opt = np.exp(theta_opt[2])

print("=== Optimization Results ===")
print(f"Signal variance ÃƒÂ²: {sigma_f_opt:.3f}")
print(f"Length scale : {length_scale_opt:.2f}Â°C")
print(f"Noise standard deviation Ãƒ_n: {sigma_n_opt:.3f}%")
print(f"\nMaximum log marginal likelihood: {-result.fun:.2f}")
print(f"Optimization steps: {result.nit}")

# Comparison with scikit-learn
kernel_sklearn = C(1.0) * RBF(10.0)
gp_sklearn = GaussianProcessRegressor(kernel=kernel_sklearn,
                                       n_restarts_optimizer=10)
gp_sklearn.fit(X, y)

print(f"\n=== scikit-learn Results (Comparison) ===")
print(f"Optimized kernel: {gp_sklearn.kernel_}")
print(f"Log marginal likelihood: {gp_sklearn.log_marginal_likelihood_value_:.2f}")

# Expected output:
# === Optimization Results ===
# Signal variance ÃƒÂ²: 156.234
# Length scale : 14.87Â°C
# Noise standard deviation Ãƒ_n: 2.145%
#
# Maximum log marginal likelihood: -8.92
# Optimization steps: 23
#
# === scikit-learn Results (Comparison) ===
# Optimized kernel: 12.5**2 * RBF(length_scale=14.9)
# Log marginal likelihood: -8.91
# â€™ Manual implementation and scikit-learn nearly match</code></pre>
<div class="callout">
<div class="callout-title">â„¢ Hyperparameter Interpretation</div>
<ul>
<li><strong>ÃƒÂ² (signal variance)</strong>: Function amplitude (larger means larger variation)</li>
<li><strong> (length scale)</strong>: Correlation distance (larger means smoother)</li>
<li><strong>Ãƒ_nÂ² (noise variance)</strong>: Observation error (inverse of experimental precision)</li>
</ul>
</div>
<h2 id="model-validation">2.4 Uncertainty Quantification and Confidence Intervals</h2>
<h3>2.4.1 Interpretation of Predictive Distribution</h3>
<p>From the Gaussian process predictive distribution <code>f(x*) ~ N(Â¼, ÃƒÂ²)</code>, the following information is obtained:</p>
<ul>
<li><strong>Â¼(x*)</strong>: Predicted value (expected value)</li>
<li><strong>Ãƒ(x*)</strong>: Prediction uncertainty (standard deviation)</li>
<li><strong>95% confidence interval</strong>: [Â¼ - 1.96Ãƒ, Â¼ + 1.96Ãƒ]</li>
<li><strong>99% confidence interval</strong>: [Â¼ - 2.58Ãƒ, Â¼ + 2.58Ãƒ]</li>
</ul>
<h3 id="example-7">Example 7: Model Validation (Cross-Validation &amp; Residual Analysis)</h3>
<pre><code>from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# ===================================
# Comprehensive Gaussian process model validation
# ===================================
# Generate larger dataset (catalyst activity data)
np.random.seed(42)
X_full = np.random.uniform(50, 150, 30).reshape(-1, 1)  # Temperature [Â°C]
y_true = 50 + 40 * np.exp(-(X_full.ravel() - 100)**2 / 400)  # True function
y_full = y_true + np.random.normal(0, 3, 30)  # Add noise

# Gaussian process model
kernel = C(1.0) * RBF(10.0)
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15,
                               alpha=1e-2)

# === 1. Cross-validation (5-fold CV) ===
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(gp, X_full, y_full, cv=kfold,
                             scoring='neg_mean_squared_error')
cv_rmse = np.sqrt(-cv_scores)

print("=== Cross-Validation Results ===")
print(f"5-fold CV RMSE: {cv_rmse.mean():.2f} Â± {cv_rmse.std():.2f}%")

# === 2. Model training and prediction ===
gp.fit(X_full, y_full)
y_pred, sigma = gp.predict(X_full, return_std=True)

# === 3. Performance metrics ===
rmse = np.sqrt(mean_squared_error(y_full, y_pred))
r2 = r2_score(y_full, y_pred)

print(f"\n=== Training Data Performance ===")
print(f"RMSE: {rmse:.2f}%")
print(f"RÂ² score: {r2:.3f}")

# === 4. Residual analysis ===
residuals = y_full - y_pred
standardized_residuals = residuals / sigma

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# (a) Predicted vs Observed
axes[0, 0].scatter(y_full, y_pred, alpha=0.6, s=80)
axes[0, 0].plot([y_full.min(), y_full.max()],
                [y_full.min(), y_full.max()],
                'r--', linewidth=2, label='Ideal line')
axes[0, 0].set_xlabel('Observed value [%]')
axes[0, 0].set_ylabel('Predicted value [%]')
axes[0, 0].set_title(f'Prediction Accuracy (RÂ²={r2:.3f})')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# (b) Residual plot
axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=80)
axes[0, 1].axhline(0, color='r', linestyle='--', linewidth=2)
axes[0, 1].axhline(1.96*residuals.std(), color='orange',
                   linestyle=':', label='Â±1.96Ãƒ')
axes[0, 1].axhline(-1.96*residuals.std(), color='orange', linestyle=':')
axes[0, 1].set_xlabel('Predicted value [%]')
axes[0, 1].set_ylabel('Residual [%]')
axes[0, 1].set_title('Residual Plot (Homoscedasticity Check)')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# (c) Histogram of standardized residuals
axes[1, 0].hist(standardized_residuals, bins=15, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(0, color='r', linestyle='--', linewidth=2)
x_norm = np.linspace(-3, 3, 100)
y_norm = 30 / (2*np.pi)**0.5 * np.exp(-x_norm**2 / 2)  # Normal distribution
axes[1, 0].plot(x_norm, y_norm, 'r-', linewidth=2, label='Standard normal distribution')
axes[1, 0].set_xlabel('Standardized residual')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].set_title('Residual Normality Check')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# (d) Prediction uncertainty calibration
X_test_dense = np.linspace(50, 150, 100).reshape(-1, 1)
y_pred_dense, sigma_dense = gp.predict(X_test_dense, return_std=True)

axes[1, 1].plot(X_test_dense, y_pred_dense, 'b-', linewidth=2, label='GP prediction')
axes[1, 1].fill_between(X_test_dense.ravel(),
                        y_pred_dense - 1.96*sigma_dense,
                        y_pred_dense + 1.96*sigma_dense,
                        alpha=0.3, label='95% confidence interval')
axes[1, 1].scatter(X_full, y_full, c='red', s=60,
                  edgecolors='k', label='Experimental data')
axes[1, 1].set_xlabel('Temperature [Â°C]')
axes[1, 1].set_ylabel('Activity [%]')
axes[1, 1].set_title('Uncertainty Calibration')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# === 5. Coverage probability (calibration check) ===
# What percentage of observed values fall within 95% confidence interval
in_interval = np.abs(residuals) &lt;= 1.96 * sigma
coverage = np.mean(in_interval) * 100

print(f"\n=== Uncertainty Calibration ===")
print(f"95% confidence interval coverage: {coverage:.1f}%")
print(f"Expected: 95.0%")
print(f"Assessment: {' Good' if 90 &lt;= coverage &lt;= 100 else 'Â  Needs adjustment'}")

# Expected output:
# === Cross-Validation Results ===
# 5-fold CV RMSE: 3.42 Â± 0.87%
#
# === Training Data Performance ===
# RMSE: 2.98%
# RÂ² score: 0.923
#
# === Uncertainty Calibration ===
# 95% confidence interval coverage: 93.3%
# Expected: 95.0%
# Assessment:  Good</code></pre>
<div class="callout">
<div class="callout-title">=, Model Validation Checklist</div>
<ol>
<li><strong>Cross-validation</strong>: Confirm generalization performance (is CV RMSE close to training RMSE)</li>
<li><strong>Homoscedasticity of residuals</strong>: Is residual plot a horizontal random scatter</li>
<li><strong>Normality of residuals</strong>: Does histogram approximate a normal distribution</li>
<li><strong>Uncertainty calibration</strong>: Is 95% interval coverage between 90-100%</li>
</ol>
</div>
<h2 id="summary">Chapter Summary</h2>
<h3>Important Learning Points</h3>
<table>
<thead>
<tr>
<th>Topic</th>
<th>Key Points</th>
<th>Practical Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gaussian Process Definition</strong></td>
<td>Completely defined by mean function and covariance function</td>
<td>Start with RBF kernel in scikit-learn</td>
</tr>
<tr>
<td><strong>Kernel Selection</strong></td>
<td>RBF (smooth), MatÃ©rn (intermediate), RQ (multi-scale)</td>
<td>Compare performance with log marginal likelihood</td>
</tr>
<tr>
<td><strong>Hyperparameter Optimization</strong></td>
<td>Automatic optimization with MLE (L-BFGS-B method)</td>
<td>Recommend n_restarts_optimizer=10-15</td>
</tr>
<tr>
<td><strong>Uncertainty Quantification</strong></td>
<td>95% confidence interval = Â¼ Â± 1.96Ãƒ</td>
<td>Large uncertainty in unexplored regionsâ€™promote exploration</td>
</tr>
<tr>
<td><strong>Model Validation</strong></td>
<td>Cross-validation, residual analysis, coverage check</td>
<td>Target RÂ²&gt;0.9, coverage 90-100%</td>
</tr>
</tbody>
</table>
<h3>Implementation Best Practices</h3>
<ol>
<li><strong>Data normalization</strong>: Normalize inputs to [0, 1] or standardize (stabilizes  optimization)</li>
<li><strong>Kernel selection</strong>: Try RBF first, improve robustness with MatÃ©rn</li>
<li><strong>Noise estimation</strong>: Consider observation error with <code>alpha=1e-2</code></li>
<li><strong>Multiple starts</strong>: Avoid local optima with <code>n_restarts_optimizer=10-15</code></li>
<li><strong>Numerical stability</strong>: Use Cholesky decomposition (avoid direct matrix inversion)</li>
</ol>
<h2 id="learning-objectives">Learning Objectives Check</h2>
<p>Upon completing this chapter, you will be able to:</p>
<h3>Basic Understanding</h3>
<ul>
<li> Explain that Gaussian processes are defined by mean function and kernel</li>
<li> Calculate mean and uncertainty from predictive distribution</li>
<li> Explain the properties of RBF, MatÃ©rn, and Rational Quadratic kernels</li>
</ul>
<h3>Practical Skills</h3>
<ul>
<li> Implement Gaussian process models with scikit-learn</li>
<li> Optimize hyperparameters with MLE</li>
<li> Visualize prediction uncertainty (confidence interval plots)</li>
<li> Validate models with cross-validation and residual analysis</li>
</ul>
<h3>Application Ability</h3>
<ul>
<li> Select appropriate kernels for chemical process data</li>
<li> Build GP models with multi-dimensional inputs (temperature, pressure, concentration)</li>
<li> Quantitatively assess model reliability</li>
</ul>
<h2 id="exercises">Practice Problems</h2>
<h3>Easy (Basic Confirmation)</h3>
<p><strong>Q1</strong>: In the Gaussian process predictive distribution <code>f(x*) ~ N(Â¼, ÃƒÂ²)</code>, which formula calculates the 95% confidence interval?</p>
<details>
<summary>Show answer</summary>
<p><strong>Correct answer</strong>: [Â¼ - 1.96Ãƒ, Â¼ + 1.96Ãƒ]</p>
<p><strong>Explanation</strong>: The 95% interval of a normal distribution is mean Â± 1.96Ã—standard deviation. 99% interval is Â± 2.58Ãƒ, 68% interval is Â± 1Ãƒ.</p>
</details>
<p><strong>Q2</strong>: How does the prediction curve change when the length scale <code></code> of the RBF kernel is increased?</p>
<details>
<summary>Show answer</summary>
<p><strong>Correct answer</strong>: It becomes smoother</p>
<p><strong>Explanation</strong>:  represents the correlation distance; larger values mean distant points are more strongly correlated. As â€™, it converges to a constant function.</p>
</details>
<h3>Medium (Application)</h3>
<p><strong>Q3</strong>: You built a GP model with log marginal likelihood of -10.5 from 5 experimental data points. Is this a good model?</p>
<details>
<summary>Show answer</summary>
<p><strong>Answer</strong>: <strong>Relative comparison</strong> is necessary.</p>
<p><strong>Explanation</strong>: Log marginal likelihood is used for relative comparison between different kernels, not in absolute terms. For example, if RBF has -10.5 and MatÃ©rn has -12.3, RBF is superior. The absolute value of likelihood increases with data size n, so comparison across different datasets is not possible.</p>
</details>
<p><strong>Q4</strong>: Cross-validation RMSE is 3.5%, training data RMSE is 1.8%. Are there problems with this model?</p>
<details>
<summary>Show answer</summary>
<p><strong>Answer</strong>: <strong>Signs of overfitting</strong> are present.</p>
<p><strong>Explanation</strong>: When CV score is significantly worse than training score, there's a possibility of overfitting. Countermeasures: (1) increase noise term <code>alpha</code>, (2) use simpler kernel, (3) increase data size.</p>
</details>
<h3>Hard (Advanced)</h3>
<p><strong>Q5</strong>: When building a GP model with 3D input of temperature, pressure, and concentration, explain why different length scales should be used for each dimension.</p>
<details>
<summary>Show answer</summary>
<p><strong>Answer</strong>: <strong>Automatic Relevance Determination (ARD)</strong> enables automatic learning of the importance of each input dimension.</p>
<p><strong>Implementation example</strong>:</p>
<pre><code>kernel = C(1.0) * RBF(length_scale=[10.0, 1.0, 5.0],
                   length_scale_bounds=(1e-2, 1e3))
# [Temperature 10Â°C, Pressure 1.0MPa, Concentration 5% initial length scales]</code></pre>
<p><strong>Interpretation</strong>: After optimization, if pressure's length scale is small at 0.5, it indicates yield is sensitive to pressure changes. If temperature is large at 50, temperature dependence is low.</p>
</details>
<h2 id="next-steps">Next Steps</h2>
<p>You can now quantify uncertainty with Gaussian process models. In Chapter 3, we will learn about <strong>acquisition functions that intelligently select the next experimental point</strong> using this uncertainty.</p>
<p><strong>What you'll learn in Chapter 3:</strong></p>
<ul>
<li>Expected Improvement (EI): Selecting the most promising point</li>
<li>Upper Confidence Bound (UCB): Balancing exploration-exploitation</li>
<li>Probability of Improvement (PI): Simple improvement probability</li>
<li>Batch Bayesian optimization: Parallel experiment design</li>
</ul>
<div class="nav-buttons">
<a class="nav-button secondary" href="./index.html">ï¿½ Series Index</a>
<a class="nav-button" href="./chapter-3.html">Chapter 3: Acquisition Functions â€™</a>
</div>
<h2 id="references">References</h2>
<ol>
<li>Rasmussen, C. E., &amp; Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</li>
<li>Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." <em>Proceedings of the IEEE</em>, 104(1), 148-175.</li>
<li>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</li>
<li>MatÃ©rn, B. (1960). <em>Spatial Variation</em>. Springer.</li>
</ol>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is for educational, research, and informational purposes only and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, functionality, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University are not liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>Copyright and licensing of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<div class="container">
<p>Â© 2025 PI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
