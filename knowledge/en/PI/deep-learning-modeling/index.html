<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Introduction to Deep Learning for Process Modeling Series - Complete guide from RNN/LSTM, Transformer, CNN, Autoencoder to Reinforcement Learning">
    <title>Introduction to Deep Learning for Process Modeling Series v1.0 - PI Knowledge Hub</title>

    <!-- CSS Styling -->
            <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/index.html">Process Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/deep-learning-modeling/index.html">Deep Learning Modeling</a>
        </div>
    </nav>

        <header>
        <div class="container">
            <h1>üß† Introduction to Deep Learning for Process Modeling Series v1.0</h1>
            <div class="meta">
                <span>üìñ Reading Time: 150-180 minutes</span>
                <span>üìä Level: Advanced</span>
                <span>üíª Code Examples: 40</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="v10">Introduction to Deep Learning for Process Modeling Series v1.0</h1>
<p><strong>From RNN/LSTM, Transformer, CNN, Autoencoder to Reinforcement Learning - Cutting-edge AI Technologies for Process Engineering</strong></p>

<h2 id="_1">Series Overview</h2>
<p>This series provides comprehensive educational content for applying deep learning to process modeling. Learn practical methods to apply cutting-edge neural network architectures to chemical process engineering, from time series prediction, image analysis, and anomaly detection to process control optimization.</p>

<p><strong>Features:</strong><br />
- ‚úÖ <strong>State-of-the-art Technology</strong>: Complete implementation of RNN/LSTM, Transformer, CNN, VAE, GAN, and Reinforcement Learning<br />
- ‚úÖ <strong>Practice-Oriented</strong>: 40 executable Python code examples (PyTorch/TensorFlow/Keras)<br />
- ‚úÖ <strong>Industrial Applications</strong>: Process data time series prediction, image-based quality control, automatic control optimization<br />
- ‚úÖ <strong>Systematic Structure</strong>: 5-chapter structure for step-by-step learning from basic theory to implementation and industrial deployment</p>

<p><strong>Total Learning Time</strong>: 150-180 minutes (including code execution and exercises)</p>

<hr />

<h2 id="_2">How to Progress Through Learning</h2>

<h3 id="_3">Recommended Learning Order</h3>

<div class="mermaid">
flowchart TD
    A[Chapter 1: Time Series Prediction with RNN/LSTM] --> B[Chapter 2: Process Data Analysis with Transformer Models]
    B --> C[Chapter 3: Image-based Process Analysis with CNN]
    C --> D[Chapter 4: Autoencoders and Generative Models]
    D --> E[Chapter 5: Process Control Optimization with Reinforcement Learning]

    style A fill:#e8f5e9
    style B fill:#c8e6c9
    style C fill:#a5d6a7
    style D fill:#81c784
    style E fill:#66bb6a
</div>

<p><strong>For Beginners (First time learning deep learning):</strong><br />
- Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br />
- Duration: 150-180 minutes</p>

<p><strong>For Machine Learning Practitioners (Basic NN knowledge):</strong><br />
- Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br />
- Duration: 120-150 minutes</p>

<p><strong>For Deep Learning Experts (CV/NLP implementation experience):</strong><br />
- Chapter 1 (quick review) ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5<br />
- Duration: 90-120 minutes</p>

<hr />

<h2 id="_4">Prerequisites</h2>

<p>To maximize the value of this series, the following knowledge is assumed:</p>

<h3>Required</h3>

<ul>
<li>‚úÖ <strong>Python</strong>: Basic operations with NumPy, Pandas, Matplotlib, scikit-learn</li>
<li>‚úÖ <strong>Machine Learning Fundamentals</strong>: Supervised learning, loss functions, gradient descent, overfitting</li>
<li>‚úÖ <strong>Process Engineering Basics</strong>: Process variables, control loops, chemical reaction kinetics</li>
<li>‚úÖ <strong>Mathematical Fundamentals</strong>: Linear algebra (matrix operations), calculus (partial derivatives, gradients), probability and statistics</li>
</ul>

<h3>Recommended</h3>

<ul>
<li>üî∂ <strong>PyTorch/TensorFlow</strong>: Experience implementing basic neural networks</li>
<li>üî∂ <strong>Time Series Analysis</strong>: Basics of ARIMA, state space models, frequency analysis</li>
<li>üî∂ <strong>Control Theory</strong>: Concepts of PID control, MPC (Model Predictive Control)</li>
<li>üî∂ <strong>Image Processing</strong>: Basic OpenCV operations, understanding convolution operations</li>
</ul>

<hr />

<h2 id="_5">Chapter Details</h2>

<div class="chapter-card">
    <h3 class="chapter-title"><a href="./chapter-1.html">Chapter 1: Time Series Prediction with RNN/LSTM</a></h3>
    <div class="chapter-meta">
        <span>üìñ Reading Time: 30-35 minutes</span>
        <span>üíª Code Examples: 8</span>
        <span>üìä Difficulty: Advanced</span>
    </div>

    <h4>Learning Content</h4>
    <ol>
        <li><strong>Fundamentals of Recurrent Neural Networks (RNN)</strong>
            <ul>
                <li>Time series data representation and sequence modeling</li>
                <li>Basic RNN architecture and vanishing gradient problem</li>
                <li>Backpropagation Through Time (BPTT)</li>
                <li>Characteristics and preprocessing of process time series data</li>
            </ul>
        </li>
        <li><strong>LSTM (Long Short-Term Memory) and GRU</strong>
            <ul>
                <li>LSTM cell structure (input, forget, and output gates)</li>
                <li>Comparison with GRU (Gated Recurrent Unit)</li>
                <li>Bidirectional LSTM</li>
                <li>Hyperparameter tuning (number of layers, hidden layer size, dropout)</li>
            </ul>
        </li>
        <li><strong>Implementation of Process Time Series Prediction</strong>
            <ul>
                <li>Multivariate time series prediction (simultaneous prediction of temperature, pressure, flow rate)</li>
                <li>Multi-step ahead prediction (5 minutes, 10 minutes ahead)</li>
                <li>Encoder-Decoder architecture</li>
                <li>Visualization of important variables with Attention mechanism</li>
            </ul>
        </li>
        <li><strong>Practical Application: Reactor Temperature Prediction</strong>
            <ul>
                <li>Dataset preparation (scaling, sequencing)</li>
                <li>LSTM model implementation with PyTorch</li>
                <li>Early Stopping and learning curve visualization</li>
                <li>Prediction accuracy evaluation (RMSE, MAE, R¬≤)</li>
            </ul>
        </li>
    </ol>

    <h4>Learning Objectives</h4>
    <ul>
        <li>‚úÖ Understand basic principles of RNN and vanishing gradient problem</li>
        <li>‚úÖ Explain the mechanisms and applications of LSTM/GRU</li>
        <li>‚úÖ Preprocess and sequence process time series data</li>
        <li>‚úÖ Implement LSTM models with PyTorch</li>
        <li>‚úÖ Implement multi-step ahead prediction</li>
        <li>‚úÖ Visualize important variables with Attention mechanism</li>
    </ul>

    <p><strong><a href="./chapter-1.html">Read Chapter 1 ‚Üí</a></strong></p>
</div>

<div class="chapter-card">
    <h3 class="chapter-title"><a href="./chapter-2.html">Chapter 2: Process Data Analysis with Transformer Models</a></h3>
    <div class="chapter-meta">
        <span>üìñ Reading Time: 30-35 minutes</span>
        <span>üíª Code Examples: 8</span>
        <span>üìä Difficulty: Advanced</span>
    </div>

    <h4>Learning Content</h4>
    <ol>
        <li><strong>Fundamentals of Transformer Architecture</strong>
            <ul>
                <li>Principles of Self-Attention mechanism</li>
                <li>Multi-Head Attention and scaled dot-product</li>
                <li>Position information embedding with Positional Encoding</li>
                <li>Feed-Forward Network and residual connections</li>
            </ul>
        </li>
        <li><strong>Time Series Transformer and Temporal Fusion Transformer</strong>
            <ul>
                <li>Applying Transformer to time series data</li>
                <li>Temporal Fusion Transformer (TFT) architecture</li>
                <li>Feature importance with Variable Selection Network</li>
                <li>Multi-horizon prediction and Quantile Regression</li>
            </ul>
        </li>
        <li><strong>Informer: Long-term Time Series Prediction</strong>
            <ul>
                <li>Computational efficiency with ProbSparse Self-Attention</li>
                <li>Self-Attention Distilling mechanism</li>
                <li>Learning long-term dependencies (48-hour ahead prediction)</li>
                <li>Performance comparison with LSTM</li>
            </ul>
        </li>
        <li><strong>Practical Application: Process Anomaly Early Detection</strong>
            <ul>
                <li>Learning anomaly patterns in multivariate process data</li>
                <li>Identifying anomaly causes with Attention weights</li>
                <li>Real-time anomaly scoring</li>
                <li>Threshold setting and false positive suppression</li>
            </ul>
        </li>
    </ol>

    <h4>Learning Objectives</h4>
    <ul>
        <li>‚úÖ Understand mathematical principles of Self-Attention mechanism</li>
        <li>‚úÖ Implement Transformer architecture</li>
        <li>‚úÖ Apply Temporal Fusion Transformer</li>
        <li>‚úÖ Implement long-term time series prediction with Informer</li>
        <li>‚úÖ Identify anomaly causes with Attention visualization</li>
        <li>‚úÖ Appropriately choose between LSTM and Transformer</li>
    </ul>

    <p><strong><a href="./chapter-2.html">Read Chapter 2 ‚Üí</a></strong></p>
</div>

<div class="chapter-card">
    <h3 class="chapter-title"><a href="./chapter-3.html">Chapter 3: Image-based Process Analysis with CNN</a></h3>
    <div class="chapter-meta">
        <span>üìñ Reading Time: 30-35 minutes</span>
        <span>üíª Code Examples: 8</span>
        <span>üìä Difficulty: Advanced</span>
    </div>

    <h4>Learning Content</h4>
    <ol>
        <li><strong>Fundamentals of Convolutional Neural Networks (CNN)</strong>
            <ul>
                <li>Roles of convolutional, pooling, and fully connected layers</li>
                <li>Feature maps and Receptive Field</li>
                <li>Selection of padding, stride, and kernel size</li>
                <li>Batch Normalization, Dropout, Data Augmentation</li>
            </ul>
        </li>
        <li><strong>Major CNN Architectures</strong>
            <ul>
                <li>ResNet: Deepening with residual connections</li>
                <li>Characteristics of VGG, Inception, EfficientNet</li>
                <li>Transfer Learning and utilizing pre-trained models</li>
                <li>Application to process images (small data countermeasures)</li>
            </ul>
        </li>
        <li><strong>Image-based Quality Control and Segmentation</strong>
            <ul>
                <li>Product quality classification (good/defective)</li>
                <li>Visualizing judgment basis with Grad-CAM</li>
                <li>Semantic segmentation with U-Net</li>
                <li>Defect area detection and quantification</li>
            </ul>
        </li>
        <li><strong>Practical Application: Particle Size Distribution Estimation from Crystal Images</strong>
            <ul>
                <li>Preprocessing and data augmentation of microscope images</li>
                <li>Particle size prediction model with CNN</li>
                <li>Particle counting with segmentation</li>
                <li>Correlation evaluation and accuracy verification with experimental values</li>
            </ul>
        </li>
    </ol>

    <h4>Learning Objectives</h4>
    <ul>
        <li>‚úÖ Understand basic CNN structure and convolution operations</li>
        <li>‚úÖ Implement major architectures like ResNet</li>
        <li>‚úÖ Appropriately apply Transfer Learning</li>
        <li>‚úÖ Visualize judgment basis with Grad-CAM</li>
        <li>‚úÖ Implement segmentation with U-Net</li>
        <li>‚úÖ Design and implement process image analysis tasks</li>
    </ul>

    <p><strong><a href="./chapter-3.html">Read Chapter 3 ‚Üí</a></strong></p>
</div>

<div class="chapter-card">
    <h3 class="chapter-title"><a href="./chapter-4.html">Chapter 4: Autoencoders and Generative Models</a></h3>
    <div class="chapter-meta">
        <span>üìñ Reading Time: 30-35 minutes</span>
        <span>üíª Code Examples: 8</span>
        <span>üìä Difficulty: Advanced</span>
    </div>

    <h4>Learning Content</h4>
    <ol>
        <li><strong>Fundamentals of Autoencoders (AE)</strong>
            <ul>
                <li>Roles of encoder and decoder</li>
                <li>Latent variables and dimensionality reduction</li>
                <li>Anomaly detection with reconstruction error</li>
                <li>Denoising Autoencoder and robustness improvement</li>
            </ul>
        </li>
        <li><strong>Variational Autoencoder (VAE)</strong>
            <ul>
                <li>Probabilistic latent variables and KL divergence</li>
                <li>Reparameterization trick</li>
                <li>Structuring latent space and sampling</li>
                <li>Conditional generation with Conditional VAE</li>
            </ul>
        </li>
        <li><strong>Generative Adversarial Networks (GAN)</strong>
            <ul>
                <li>Adversarial learning between Generator and Discriminator</li>
                <li>DCGAN (Deep Convolutional GAN) implementation</li>
                <li>Mode Collapse and countermeasures</li>
                <li>Stabilizing learning with Wasserstein GAN</li>
            </ul>
        </li>
        <li><strong>Practical Application: Process Anomaly Detection and Data Augmentation</strong>
            <ul>
                <li>Anomaly detection system with Autoencoder</li>
                <li>Generating normal operating conditions with VAE</li>
                <li>Data augmentation with GAN (synthetic data generation)</li>
                <li>Anomaly scoring and alert settings</li>
            </ul>
        </li>
    </ol>

    <h4>Learning Objectives</h4>
    <ul>
        <li>‚úÖ Understand principles of autoencoders and application to anomaly detection</li>
        <li>‚úÖ Structure latent space with VAE</li>
        <li>‚úÖ Generate high-quality synthetic data with GAN</li>
        <li>‚úÖ Implement reconstruction error-based anomaly detection</li>
        <li>‚úÖ Improve model performance with data augmentation</li>
        <li>‚úÖ Integrate into process monitoring systems</li>
    </ul>

    <p><strong><a href="./chapter-4.html">Read Chapter 4 ‚Üí</a></strong></p>
</div>

<div class="chapter-card">
    <h3 class="chapter-title"><a href="./chapter-5.html">Chapter 5: Process Control Optimization with Reinforcement Learning</a></h3>
    <div class="chapter-meta">
        <span>üìñ Reading Time: 30-40 minutes</span>
        <span>üíª Code Examples: 8</span>
        <span>üìä Difficulty: Advanced</span>
    </div>

    <h4>Learning Content</h4>
    <ol>
        <li><strong>Fundamentals of Reinforcement Learning</strong>
            <ul>
                <li>Markov Decision Process (MDP) and Bellman equation</li>
                <li>Definition of state, action, reward, policy</li>
                <li>Value function and Q-function</li>
                <li>Exploration vs Exploitation</li>
            </ul>
        </li>
        <li><strong>Deep Q-Network (DQN) and Its Evolution</strong>
            <ul>
                <li>Principles of Q-Learning and DQN</li>
                <li>Experience Replay and target network</li>
                <li>Double DQN, Dueling DQN, Prioritized Experience Replay</li>
                <li>Control in discrete action spaces</li>
            </ul>
        </li>
        <li><strong>Actor-Critic Algorithms</strong>
            <ul>
                <li>Policy Gradient and REINFORCE algorithm</li>
                <li>A3C (Asynchronous Advantage Actor-Critic)</li>
                <li>PPO (Proximal Policy Optimization)</li>
                <li>Control in continuous action spaces (continuous adjustment of temperature, flow rate)</li>
            </ul>
        </li>
        <li><strong>Practical Application: Automatic Control of Batch Reactor</strong>
            <ul>
                <li>Building simulation environment (OpenAI Gym style)</li>
                <li>Reward function design (yield maximization, energy minimization)</li>
                <li>Control policy learning with PPO</li>
                <li>Performance comparison with PID control</li>
                <li>Consideration of safety constraints and risk management</li>
            </ul>
        </li>
    </ol>

    <h4>Learning Objectives</h4>
    <ul>
        <li>‚úÖ Understand basic concepts of reinforcement learning and MDP</li>
        <li>‚úÖ Solve discrete control problems with DQN</li>
        <li>‚úÖ Learn continuous control policies with PPO</li>
        <li>‚úÖ Formulate process control problems with reinforcement learning</li>
        <li>‚úÖ Appropriately design reward functions</li>
        <li>‚úÖ Compare and evaluate with conventional control methods</li>
    </ul>

    <p><strong><a href="./chapter-5.html">Read Chapter 5 ‚Üí</a></strong></p>
</div>

<hr />

<h2 id="_6">Overall Learning Outcomes</h2>

<p>Upon completing this series, you will acquire the following skills and knowledge:</p>

<h3>Knowledge Level (Understanding)</h3>

<ul>
<li>‚úÖ Understand principles of major deep learning architectures (RNN/LSTM, Transformer, CNN, VAE, GAN, RL)</li>
<li>‚úÖ Know strengths and limitations of deep learning in process modeling</li>
<li>‚úÖ Understand methods for time series prediction, image analysis, anomaly detection, control optimization</li>
<li>‚úÖ Know hyperparameter tuning and overfitting countermeasures</li>
<li>‚úÖ Understand model interpretability and visualization methods (Attention, Grad-CAM)</li>
</ul>

<h3>Practical Skills (Doing)</h3>

<ul>
<li>‚úÖ Implement various neural networks with PyTorch/TensorFlow</li>
<li>‚úÖ Build prediction models with process time series data</li>
<li>‚úÖ Develop image-based quality control systems</li>
<li>‚úÖ Implement anomaly detection with autoencoders</li>
<li>‚úÖ Learn process control policies with reinforcement learning</li>
<li>‚úÖ Appropriately evaluate and visualize model performance</li>
</ul>

<h3>Application Ability (Applying)</h3>

<ul>
<li>‚úÖ Apply deep learning to actual chemical processes</li>
<li>‚úÖ Select optimal models according to problem characteristics</li>
<li>‚úÖ Build robust models even with small or noisy data</li>
<li>‚úÖ Deploy to real-time systems</li>
<li>‚úÖ Lead AI projects as a process engineer</li>
</ul>

<hr />

<h2 id="faq">FAQ (Frequently Asked Questions)</h2>

<h3>Q1: Should I use PyTorch or TensorFlow?</h3>

<p><strong>A</strong>: This series mainly uses PyTorch (high flexibility in research). However, the same concepts can be implemented with TensorFlow/Keras. Consider TensorFlow if industrial deployment is a priority.</p>

<h3>Q2: Is a GPU environment essential?</h3>

<p><strong>A</strong>: Small datasets can be trained on CPU, but GPU is recommended for practical training time. Consider using Google Colab (free GPU) or AWS/Azure GPU instances.</p>

<h3>Q3: How to choose between deep learning and traditional statistical models (ARIMA, state space models)?</h3>

<p><strong>A</strong>: Deep learning is strong with large data and complex nonlinear patterns, but statistical models are effective for small data or when interpretability is important. Hybrid approaches combining both are also effective.</p>

<h3>Q4: What should I be careful about when deploying to actual processes?</h3>

<p><strong>A</strong>: Important points include: (1) Model interpretability and accountability, (2) Consideration of safety constraints, (3) Real-time performance, (4) Model update and retraining strategy, (5) Fallback mechanism for anomalies. These are covered in detail in Chapter 5.</p>

<h3>Q5: How much data is needed?</h3>

<p><strong>A</strong>: It varies by task, but for time series prediction, thousands to tens of thousands of samples are typical; for image classification, hundreds to thousands per class. Transfer Learning and Data Augmentation can handle small data situations.</p>

<hr />

<h2 id="_7">Next Steps</h2>

<h3>Recommended Actions After Series Completion</h3>

<p><strong>Immediate (Within 1 week):</strong><br />
1. ‚úÖ Publish implemented code on GitHub<br />
2. ‚úÖ Prototype prediction model with company process data<br />
3. ‚úÖ Test skills in Kaggle competitions (time series prediction, image classification)</p>

<p><strong>Short-term (1-3 months):</strong><br />
1. ‚úÖ Build anomaly detection system for actual processes<br />
2. ‚úÖ Implement quality control with Transfer Learning for small data<br />
3. ‚úÖ Develop real-time prediction system prototype<br />
4. ‚úÖ Present at conferences (AIChE, SCEJ, etc.)</p>

<p><strong>Long-term (6 months+):</strong><br />
1. ‚úÖ Build integrated system of Digital Twin and AI<br />
2. ‚úÖ Demonstrate autonomous process with reinforcement learning<br />
3. ‚úÖ Launch AI R&D division<br />
4. ‚úÖ Develop career as AI specialist</p>

<hr />

<h2 id="_8">Integration with Related Series</h2>

<p>Combining with the following Process Informatics Dojo series will help you acquire more comprehensive process AI capabilities:</p>

<ul>
<li><strong>Bayesian Optimization Series</strong>: Apply to hyperparameter tuning of deep learning</li>
<li><strong>Process Monitoring Series</strong>: Combine with advanced anomaly detection using deep learning</li>
<li><strong>Process Control Series</strong>: Fusion of reinforcement learning and conventional control (Model Predictive Control + RL)</li>
<li><strong>Statistical Quality Control Series</strong>: Integration with image-based quality control</li>
</ul>

<hr />

<h2 id="_9">Feedback and Support</h2>

<h3>About This Series</h3>

<p>This series was created as part of the PI Knowledge Hub project under Dr. Yusuke Hashimoto at Tohoku University.</p>

<p><strong>Created</strong>: October 26, 2025<br />
<strong>Version</strong>: 1.0</p>

<h3>We Welcome Your Feedback</h3>

<p>We welcome your feedback to improve this series:</p>

<ul>
<li><strong>Typos, errors, technical mistakes</strong>: Please report via GitHub repository Issues</li>
<li><strong>Improvement suggestions</strong>: New architectures, code examples you'd like added, etc.</li>
<li><strong>Questions</strong>: Parts that were difficult to understand, sections needing additional explanation</li>
<li><strong>Success stories</strong>: Projects using what you learned from this series</li>
</ul>

<p><strong>Contact</strong>: yusuke.hashimoto.b8@tohoku.ac.jp</p>

<hr />

<h2 id="_10">License and Terms of Use</h2>

<p>This series is published under <strong>CC BY 4.0</strong> (Creative Commons Attribution 4.0 International) license.</p>

<p><strong>What you can do:</strong><br />
- ‚úÖ Free viewing and downloading<br />
- ‚úÖ Use for educational purposes (classes, study groups, etc.)<br />
- ‚úÖ Modification and derivative works (translation, summarization, etc.)</p>

<p><strong>Conditions:</strong><br />
- üìå Author credit display required<br />
- üìå Indicate if modifications were made<br />
- üìå Contact in advance for commercial use</p>

<p>Details: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License Full Text</a></p>

<hr />

<h2 id="_11">Let's Get Started!</h2>

<p>Are you ready? Start with Chapter 1 and learn the fusion of deep learning and process modeling!</p>

<p><strong><a href="./chapter-1.html">Chapter 1: Time Series Prediction with RNN/LSTM ‚Üí</a></strong></p>

<hr />

<p><strong>Update History</strong></p>

<ul>
<li><strong>2025-10-26</strong>: v1.0 Initial release</li>
</ul>

<hr />

<p><strong>Your Process AI learning journey starts here!</strong></p>

        <div class="nav-buttons">
            <a href="../index.html" class="nav-button">‚Üê Back to Process Informatics Dojo Top</a>
        </div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>In the event of direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, the author and Tohoku University shall not be liable to the maximum extent permitted by applicable law.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>Copyright and license of this content shall follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <div class="container">
            <p>&copy; 2025 PI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
