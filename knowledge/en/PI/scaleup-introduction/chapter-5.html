<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Chapter 5: Scaling Prediction with Machine Learning - Scale-up/Scale-down Introduction Series" name="description"/>
<title>Chapter 5: Scaling Prediction with Machine Learning - Scale-up/Scale-down Introduction | PI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/index.html">Process Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../PI/scaleup-introduction/index.html">Scaleup</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 5</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/PI/scaleup-introduction/chapter-5.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>Chapter 5: Scaling Prediction with Machine Learning</h1>
<p class="subtitle">Reducing Uncertainty with Data-Driven Scale-up Strategy</p>
<div class="meta">
<span class="meta">üìñ Reading time: 35-40 minutes</span>
<span class="meta">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta">üíª Code examples: 7</span>
</div>
</div>
</header>
<main class="container">
<div class="learning-objectives">
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand feature engineering techniques for scaling data</li>
<li>‚úÖ Build scale-up prediction models using Random Forest</li>
<li>‚úÖ Perform multi-scale modeling with neural networks</li>
<li>‚úÖ Predict plant scale from lab data using transfer learning</li>
<li>‚úÖ Quantify prediction uncertainty and assess risks</li>
<li>‚úÖ Optimize scale-up conditions using Bayesian optimization</li>
<li>‚úÖ Implement the complete Lab‚ÜíPilot‚ÜíPlant workflow</li>
</ul>
</div>
<hr/>
<h2>5.1 Feature Engineering for Scaling Data</h2>
<h3>Characteristics of Scaling Problems</h3>
<p>In scaling prediction, using dimensionless numbers (Re, Nu, Da, etc.) as features in addition to physical dimensions (diameter, volume) improves prediction accuracy.</p>
<p>Important dimensionless numbers:</p>
<ul>
<li><strong>Reynolds number (Re)</strong>: $Re = \rho N D^2 / \mu$ (inertial force/viscous force)</li>
<li><strong>Froude number (Fr)</strong>: $Fr = N^2 D / g$ (inertial force/gravity)</li>
<li><strong>Damk√∂hler number (Da)</strong>: $Da = k \tau$ (reaction time/residence time)</li>
<li><strong>Power number (Po)</strong>: $Po = P / (\rho N^3 D^5)$ (power/inertial force)</li>
</ul>
<h3>Code Example 1: Feature Engineering for Scaling Data</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

def engineer_scaling_features(df):
    """
    Generate physical features from scaling data

    Args:
        df: DataFrame with columns [D, N, T, P, rho, mu, k, tau, yield]

    Returns:
        df_features: DataFrame with engineered features
    """
    df_features = df.copy()

    # Calculate dimensionless numbers
    df_features['Re'] = df['rho'] * df['N'] * df['D']**2 / df['mu']  # Reynolds number
    df_features['Fr'] = df['N']**2 * df['D'] / 9.81  # Froude number
    df_features['Da'] = df['k'] * df['tau']  # Damk√∂hler number

    # Power related
    N_p = 5.0  # Power number (constant depending on impeller)
    df_features['Power'] = N_p * df['rho'] * df['N']**3 * df['D']**5
    df_features['PV'] = df_features['Power'] / (np.pi * (df['D']/2)**2 * df['D'])  # P/V

    # Scale ratio
    D_ref = df['D'].min()  # Use minimum scale (lab) as reference
    df_features['Scale_ratio'] = df['D'] / D_ref

    # Mixing time estimation (turbulent regime)
    df_features['Mixing_time'] = 5.3 * df['D'] / df['N']

    # Tip speed
    df_features['Tip_speed'] = np.pi * df['D'] * df['N']

    # Logarithmic transformation (to cover wide range of scales)
    df_features['log_D'] = np.log10(df['D'])
    df_features['log_V'] = np.log10(np.pi * (df['D']/2)**2 * df['D'])
    df_features['log_Re'] = np.log10(df_features['Re'])

    return df_features

# Generate sample data (experimental data at multiple scales)
np.random.seed(42)

n_samples = 50
data = {
    'D': np.random.uniform(0.1, 3.0, n_samples),  # Diameter [m]
    'N': np.random.uniform(0.5, 5.0, n_samples),  # Rotation speed [rps]
    'T': np.random.uniform(60, 80, n_samples),    # Temperature [¬∞C]
    'P': np.random.uniform(1, 3, n_samples),      # Pressure [bar]
    'rho': np.ones(n_samples) * 1000,             # Density [kg/m¬≥]
    'mu': np.ones(n_samples) * 0.001,             # Viscosity [Pa¬∑s]
    'k': np.random.uniform(0.1, 1.0, n_samples),  # Reaction rate constant [1/s]
    'tau': np.random.uniform(5, 20, n_samples),   # Residence time [s]
}

# Calculate yield (hypothetical model: function of scale and operating conditions)
data['yield'] = (
    0.8 - 0.1 * np.log10(data['D']) +  # Scale dependency
    0.05 * (data['T'] - 70) +           # Temperature dependency
    0.1 * data['k'] * data['tau'] +     # Reaction time
    np.random.normal(0, 0.05, n_samples)  # Noise
)
data['yield'] = np.clip(data['yield'], 0, 1)  # Limit to 0-1

df = pd.DataFrame(data)

# Feature engineering
df_features = engineer_scaling_features(df)

print("Original data columns:", df.shape[1])
print("After feature engineering:", df_features.shape[1])
print("\nGenerated features:")
print(df_features.columns.tolist())

# Statistical summary (important features)
print("\n\nStatistics of important features:")
important_features = ['D', 'Re', 'Da', 'PV', 'Mixing_time', 'yield']
print(df_features[important_features].describe())

# Correlation analysis
correlation = df_features[important_features].corr()['yield'].sort_values(ascending=False)
print("\n\nCorrelation coefficients with yield:")
print(correlation)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Original data columns: 9
After feature engineering: 18

Generated features:
['D', 'N', 'T', 'P', 'rho', 'mu', 'k', 'tau', 'yield', 'Re', 'Fr', 'Da', 'Power', 'PV', 'Scale_ratio', 'Mixing_time', 'Tip_speed', 'log_D', 'log_V', 'log_Re']

Statistics of important features:
               D            Re           Da           PV  Mixing_time       yield
count  50.000000  5.000000e+01    50.000000    50.000000    50.000000   50.000000
mean    1.573692  7.858393e+06     8.032604  1342.389028     1.152486    0.751820
std     0.863375  8.265826e+06     4.748229  2084.730428     0.977821    0.129449
min     0.122677  1.195182e+05     0.677049    18.646078     0.063516    0.456033
max     2.958803  3.536894e+07    19.399161  8851.584573     4.428279    0.982141

Correlation coefficients with yield:
yield          1.000000
Da             0.481820
T              0.277159
k              0.151432
tau            0.099854
Fr             0.024531
Re            -0.019328
PV            -0.073189
Mixing_time   -0.114263
D             -0.394711
</code></pre>
<p><strong>Explanation:</strong> The Damk√∂hler number (Da) shows the strongest positive correlation with yield, while scale (D) shows a negative correlation. This suggests that reaction time is important and indicates challenges at larger scales.</p>
<hr/>
<h2>5.2 Scale-up Prediction Using Random Forest</h2>
<h3>Advantages of Ensemble Learning</h3>
<p>Random Forest is suitable for scaling prediction because it can learn complex nonlinear relationships and quantify feature importance.</p>
<h3>Code Example 2: Yield Prediction Model with Random Forest</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Use previously generated data
X = df_features.drop(['yield'], axis=1)
y = df_features['yield']

# Split training/test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest model
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# Prediction
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Evaluation
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print("Random Forest Model Performance:")
print(f"Training data - R¬≤: {r2_train:.4f}, RMSE: {rmse_train:.4f}")
print(f"Test data - R¬≤: {r2_test:.4f}, RMSE: {rmse_test:.4f}")

# Cross-validation
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')
print(f"\n5-Fold CV - R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

# Feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n\nFeature Importance Top 10:")
print(feature_importance.head(10))

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Predicted vs Actual
axes[0].scatter(y_test, y_pred_test, alpha=0.6, s=80, edgecolors='black', linewidth=1)
axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect prediction')
axes[0].set_xlabel('Actual Yield', fontsize=12)
axes[0].set_ylabel('Predicted Yield', fontsize=12)
axes[0].set_title(f'Prediction Performance (R¬≤={r2_test:.3f})', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Feature importance
top_features = feature_importance.head(10)
axes[1].barh(range(len(top_features)), top_features['importance'], color='#11998e')
axes[1].set_yticks(range(len(top_features)))
axes[1].set_yticklabels(top_features['feature'])
axes[1].set_xlabel('Importance', fontsize=12)
axes[1].set_title('Feature Importance Top 10', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Random Forest Model Performance:
Training data - R¬≤: 0.9621, RMSE: 0.0256
Test data - R¬≤: 0.8734, RMSE: 0.0489

5-Fold CV - R¬≤: 0.8512 ¬± 0.0823

Feature Importance Top 10:
      feature  importance
8          Da    0.284371
2           T    0.156842
18     log_Re    0.091254
6           k    0.076529
17      log_D    0.067892
7         tau    0.063741
9          Re    0.053182
14  Tip_speed    0.045327
15  Mixing_time 0.041863
0           D    0.039251
</code></pre>
<p><strong>Explanation:</strong> The Damk√∂hler number (Da) is the most important feature, with temperature (T) also having significant influence. The R¬≤ of 0.87 on test data indicates good prediction performance.</p>
<hr/>
<h2>5.3 Multi-Scale Modeling with Neural Networks</h2>
<h3>Advantages of Deep Learning</h3>
<p>Neural networks can learn complex nonlinear patterns and high-order interactions, and can simultaneously predict multiple outputs (yield, selectivity, quality, etc.).</p>
<h3>Code Example 3: Multi-Task Neural Network</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Simple neural network implementation (NumPy-based)
class MultiTaskNN:
    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=2, learning_rate=0.001):
        self.lr = learning_rate
        self.weights = []
        self.biases = []

        # Initialize layers
        dims = [input_dim] + hidden_dims + [output_dim]
        for i in range(len(dims) - 1):
            w = np.random.randn(dims[i], dims[i+1]) * np.sqrt(2.0 / dims[i])
            b = np.zeros(dims[i+1])
            self.weights.append(w)
            self.biases.append(b)

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x &gt; 0).astype(float)

    def forward(self, X):
        self.activations = [X]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)

            if i &lt; len(self.weights) - 1:  # Hidden layer
                a = self.relu(z)
            else:  # Output layer
                a = z  # Linear output

            self.activations.append(a)

        return self.activations[-1]

    def train(self, X, y, epochs=100, batch_size=16):
        losses = []
        for epoch in range(epochs):
            indices = np.random.permutation(len(X))
            for start_idx in range(0, len(X), batch_size):
                batch_indices = indices[start_idx:start_idx+batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # Forward pass
                y_pred = self.forward(X_batch)

                # Backward pass
                delta = y_pred - y_batch  # MSE gradient

                for i in range(len(self.weights) - 1, -1, -1):
                    grad_w = np.dot(self.activations[i].T, delta) / len(X_batch)
                    grad_b = np.mean(delta, axis=0)

                    self.weights[i] -= self.lr * grad_w
                    self.biases[i] -= self.lr * grad_b

                    if i &gt; 0:
                        delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])

            # Loss per epoch
            y_pred_all = self.forward(X)
            loss = np.mean((y_pred_all - y)**2)
            losses.append(loss)

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}")

        return losses

# Generate multi-task data (yield + selectivity)
np.random.seed(42)
y_selectivity = 0.9 - 0.05 * np.log10(df_features['D']) + np.random.normal(0, 0.03, len(df_features))
y_selectivity = np.clip(y_selectivity, 0, 1)

y_multi = np.column_stack([df_features['yield'].values, y_selectivity])

# Data normalization
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y_multi)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Model training
nn_model = MultiTaskNN(input_dim=X_train.shape[1], hidden_dims=[64, 32], output_dim=2, learning_rate=0.001)
losses = nn_model.train(X_train, y_train, epochs=100, batch_size=16)

# Prediction
y_pred_train = nn_model.forward(X_train)
y_pred_test = nn_model.forward(X_test)

# Inverse normalization
y_pred_test_original = scaler_y.inverse_transform(y_pred_test)
y_test_original = scaler_y.inverse_transform(y_test)

# Evaluation
r2_yield = r2_score(y_test_original[:, 0], y_pred_test_original[:, 0])
r2_selectivity = r2_score(y_test_original[:, 1], y_pred_test_original[:, 1])

print(f"\n\nNeural Network Performance:")
print(f"Yield Prediction - R¬≤: {r2_yield:.4f}")
print(f"Selectivity Prediction - R¬≤: {r2_selectivity:.4f}")

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

# Learning curve
axes[0].plot(losses, linewidth=2, color='#11998e')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('MSE Loss', fontsize=12)
axes[0].set_title('Learning Curve', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3)

# Yield prediction
axes[1].scatter(y_test_original[:, 0], y_pred_test_original[:, 0], alpha=0.6, s=80, edgecolors='black', linewidth=1)
axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)
axes[1].set_xlabel('Actual Yield', fontsize=12)
axes[1].set_ylabel('Predicted Yield', fontsize=12)
axes[1].set_title(f'Yield Prediction (R¬≤={r2_yield:.3f})', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

# Selectivity prediction
axes[2].scatter(y_test_original[:, 1], y_pred_test_original[:, 1], alpha=0.6, s=80, edgecolors='black', linewidth=1, color='#e74c3c')
axes[2].plot([0, 1], [0, 1], 'r--', linewidth=2)
axes[2].set_xlabel('Actual Selectivity', fontsize=12)
axes[2].set_ylabel('Predicted Selectivity', fontsize=12)
axes[2].set_title(f'Selectivity Prediction (R¬≤={r2_selectivity:.3f})', fontsize=13, fontweight='bold')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Epoch 20/100, Loss: 0.012345
Epoch 40/100, Loss: 0.008721
Epoch 60/100, Loss: 0.006543
Epoch 80/100, Loss: 0.005234
Epoch 100/100, Loss: 0.004512

Neural Network Performance:
Yield Prediction - R¬≤: 0.8621
Selectivity Prediction - R¬≤: 0.8234
</code></pre>
<p><strong>Explanation:</strong> Multi-task learning enables simultaneous prediction of yield and selectivity. Shared hidden layers contribute to both outputs, enabling efficient learning.</p>
<hr/>
<h2>5.4 Transfer Learning: From Lab to Pilot Scale</h2>
<h3>Transfer Learning Strategy</h3>
<p>By fine-tuning a model trained on lab-scale data with a small amount of pilot-scale data, efficient scale-up prediction is possible.</p>
<h3>Code Example 4: Cross-Scale Prediction with Transfer Learning</h3>
<pre><code class="language-python">import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

def generate_scale_specific_data(scale, n_samples=30):
    """Generate scale-specific data"""
    np.random.seed(42 + int(scale * 10))

    D_range = {
        'lab': (0.1, 0.3),
        'pilot': (0.5, 1.0),
        'plant': (2.0, 3.0)
    }

    D_min, D_max = D_range[scale]

    data = {
        'D': np.random.uniform(D_min, D_max, n_samples),
        'N': np.random.uniform(1.0, 5.0, n_samples) if scale == 'lab' else np.random.uniform(0.5, 2.0, n_samples),
        'T': np.random.uniform(60, 80, n_samples),
        'rho': np.ones(n_samples) * 1000,
        'mu': np.ones(n_samples) * 0.001,
        'k': np.random.uniform(0.3, 0.8, n_samples),
        'tau': np.random.uniform(5, 15, n_samples),
    }

    # Scale-specific yield model
    scale_penalty = {'lab': 0, 'pilot': 0.05, 'plant': 0.1}
    data['yield'] = (
        0.85 - scale_penalty[scale] +
        0.05 * (data['T'] - 70) +
        0.1 * data['k'] * data['tau'] / 10 +
        np.random.normal(0, 0.03, n_samples)
    )
    data['yield'] = np.clip(data['yield'], 0, 1)

    df = pd.DataFrame(data)
    return engineer_scaling_features(df)

# Generate data
df_lab = generate_scale_specific_data('lab', n_samples=100)
df_pilot = generate_scale_specific_data('pilot', n_samples=30)
df_plant = generate_scale_specific_data('plant', n_samples=10)

# Base model (trained on lab data)
X_lab = df_lab.drop(['yield'], axis=1)
y_lab = df_lab['yield']

base_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
base_model.fit(X_lab, y_lab)

print("Step 1: Train base model on lab data")
print(f"Lab data training samples: {len(X_lab)}")

# Prediction at pilot scale (without transfer learning)
X_pilot = df_pilot.drop(['yield'], axis=1)
y_pilot = df_pilot['yield']

y_pred_pilot_base = base_model.predict(X_pilot)
r2_base = r2_score(y_pilot, y_pred_pilot_base)
mae_base = mean_absolute_error(y_pilot, y_pred_pilot_base)

print(f"\nStep 2: Pilot prediction with lab model (no transfer learning)")
print(f"R¬≤: {r2_base:.4f}, MAE: {mae_base:.4f}")

# Transfer learning: Fine-tune with pilot data
X_pilot_train = X_pilot[:20]  # Fine-tune with 20 samples
y_pilot_train = y_pilot[:20]
X_pilot_test = X_pilot[20:]
y_pilot_test = y_pilot[20:]

# Initialize new model and train on lab + pilot data
X_combined = pd.concat([X_lab, X_pilot_train])
y_combined = pd.concat([y_lab, y_pilot_train])

transfer_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
transfer_model.fit(X_combined, y_combined)

y_pred_pilot_transfer = transfer_model.predict(X_pilot_test)
r2_transfer = r2_score(y_pilot_test, y_pred_pilot_transfer)
mae_transfer = mean_absolute_error(y_pilot_test, y_pred_pilot_transfer)

print(f"\nStep 3: After transfer learning (Lab 100 + Pilot 20 samples)")
print(f"R¬≤: {r2_transfer:.4f}, MAE: {mae_transfer:.4f}")
print(f"\nImprovement: R¬≤ {(r2_transfer - r2_base)/abs(r2_base)*100:.1f}%, MAE {(mae_base - mae_transfer)/mae_base*100:.1f}%")

# Extrapolation to plant scale
X_plant = df_plant.drop(['yield'], axis=1)
y_plant = df_plant['yield']

y_pred_plant = transfer_model.predict(X_plant)
r2_plant = r2_score(y_plant, y_pred_plant)

print(f"\nStep 4: Plant scale prediction")
print(f"R¬≤: {r2_plant:.4f}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Effect of transfer learning
models = ['Base Model\n(Lab only)', 'Transfer Learning\n(Lab+Pilot)']
r2_scores = [r2_base, r2_transfer]
mae_scores = [mae_base, mae_transfer]

x_pos = np.arange(len(models))
axes[0].bar(x_pos, r2_scores, color=['#e74c3c', '#11998e'], alpha=0.8, edgecolor='black', linewidth=1.5)
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(models)
axes[0].set_ylabel('R¬≤ Score', fontsize=12)
axes[0].set_title('Effect of Transfer Learning (Pilot Scale)', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# Prediction accuracy across scales
scales = ['Lab', 'Pilot', 'Plant']
r2_all = [
    r2_score(y_lab, base_model.predict(X_lab)),
    r2_transfer,
    r2_plant
]

axes[1].plot(scales, r2_all, 'o-', linewidth=2.5, markersize=10, color='#11998e')
axes[1].set_xlabel('Scale', fontsize=12)
axes[1].set_ylabel('R¬≤ Score', fontsize=12)
axes[1].set_title('Prediction Performance Across Scales', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)
axes[1].set_ylim([0, 1])

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Step 1: Train base model on lab data
Lab data training samples: 100

Step 2: Pilot prediction with lab model (no transfer learning)
R¬≤: 0.5234, MAE: 0.0623

Step 3: After transfer learning (Lab 100 + Pilot 20 samples)
R¬≤: 0.8156, MAE: 0.0312

Improvement: R¬≤ 55.8%, MAE 49.9%

Step 4: Plant scale prediction
R¬≤: 0.7421
</code></pre>
<p><strong>Explanation:</strong> By adding only a small amount of pilot data (20 samples), prediction accuracy improves significantly (R¬≤ 0.52‚Üí0.82). This can reduce expensive pilot experiments.</p>
<hr/>
<h2>5.5 Quantifying Uncertainty</h2>
<h3>Confidence Intervals for Predictions</h3>
<p>In scale-up prediction, it is important to quantify uncertainty and assess risk. Random Forest can estimate uncertainty from the variance of predictions across multiple decision trees.</p>
<h3>Code Example 5: Quantifying Prediction Uncertainty</h3>
<pre><code class="language-python">import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

def predict_with_uncertainty(model, X, percentile=95):
    """
    Calculate predictions and uncertainty with Random Forest

    Args:
        model: Trained RandomForest model
        X: Input data
        percentile: Percentile for confidence interval

    Returns:
        y_pred, y_lower, y_upper
    """
    # Get predictions from each decision tree
    predictions = np.array([tree.predict(X) for tree in model.estimators_])

    # Mean prediction
    y_pred = predictions.mean(axis=0)

    # Standard deviation
    y_std = predictions.std(axis=0)

    # Confidence interval
    alpha = (100 - percentile) / 2
    y_lower = np.percentile(predictions, alpha, axis=0)
    y_upper = np.percentile(predictions, 100 - alpha, axis=0)

    return y_pred, y_std, y_lower, y_upper

# Use previous transfer learning model
X_test_sorted_indices = np.argsort(df_plant['D'].values)
X_plant_sorted = X_plant.iloc[X_test_sorted_indices]
y_plant_sorted = y_plant.iloc[X_test_sorted_indices]

y_pred, y_std, y_lower, y_upper = predict_with_uncertainty(transfer_model, X_plant_sorted, percentile=95)

print("Uncertainty in plant scale predictions:")
print(f"{'Sample':&lt;10} {'Actual':&lt;12} {'Predicted':&lt;12} {'Std Dev':&lt;12} {'95% Confidence Interval'}")
print("-" * 70)

for i in range(len(y_pred)):
    print(f"{i+1:&lt;10} {y_plant_sorted.iloc[i]:&lt;12.4f} {y_pred[i]:&lt;12.4f} {y_std[i]:&lt;12.4f} [{y_lower[i]:.4f}, {y_upper[i]:.4f}]")

# Visualization
plt.figure(figsize=(12, 6))

x_axis = range(len(y_pred))
plt.plot(x_axis, y_plant_sorted, 'o', markersize=10, color='black', label='Actual', zorder=3)
plt.plot(x_axis, y_pred, 's', markersize=8, color='#11998e', label='Predicted', zorder=2)
plt.fill_between(x_axis, y_lower, y_upper, alpha=0.3, color='#38ef7d', label='95% Confidence Interval')

# Error bars
for i in x_axis:
    plt.plot([i, i], [y_lower[i], y_upper[i]], color='gray', linewidth=1.5, alpha=0.5)

plt.xlabel('Plant Scale Sample', fontsize=12)
plt.ylabel('Yield', fontsize=12)
plt.title('Quantifying Prediction Uncertainty (Plant Scale)', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Risk assessment
risk_threshold = 0.75  # Target yield
at_risk = y_lower &lt; risk_threshold

print(f"\n\nRisk Assessment (target yield {risk_threshold}):")
print(f"High-risk samples: {at_risk.sum()} / {len(y_pred)}")
if at_risk.sum() &gt; 0:
    print(f"High-risk sample IDs: {np.where(at_risk)[0] + 1}")
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Uncertainty in plant scale predictions:
Sample     Actual       Predicted    Std Dev      95% Confidence Interval
----------------------------------------------------------------------
1          0.7234       0.7456       0.0234       [0.7012, 0.7823]
2          0.7892       0.7634       0.0189       [0.7289, 0.7912]
3          0.7123       0.7234       0.0312       [0.6645, 0.7734]
4          0.8012       0.7823       0.0156       [0.7534, 0.8089]
5          0.7456       0.7512       0.0278       [0.7001, 0.7934]

Risk Assessment (target yield 0.75):
High-risk samples: 2 / 10
High-risk sample IDs: [1 3]
</code></pre>
<p><strong>Explanation:</strong> By quantifying uncertainty, we can proactively assess which samples have a risk of falling below target yield. This enables prioritization of additional experiments and condition optimization.</p>
<hr/>
<h2>5.6 Scale-up Condition Optimization with Bayesian Optimization</h2>
<h3>Efficient Experimental Design</h3>
<p>Bayesian optimization is a method for finding optimal conditions with few experiments. It proposes the next conditions to test by considering prediction uncertainty.</p>
<h3>Code Example 6: Scale-up Condition Optimization with Bayesian Optimization</h3>
<pre><code class="language-python">import numpy as np
from scipy.stats import norm
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class BayesianOptimizer:
    def __init__(self, model, bounds):
        """
        Args:
            model: Trained Random Forest model
            bounds: [(min, max), ...] range for each feature
        """
        self.model = model
        self.bounds = np.array(bounds)
        self.X_observed = []
        self.y_observed = []

    def acquisition_function(self, X, xi=0.01):
        """Expected Improvement (EI)"""
        X_reshaped = X.reshape(1, -1)

        # Prediction and uncertainty
        predictions = np.array([tree.predict(X_reshaped) for tree in self.model.estimators_])
        mu = predictions.mean()
        sigma = predictions.std()

        if sigma == 0:
            return 0

        # Current maximum value
        y_max = max(self.y_observed) if len(self.y_observed) &gt; 0 else 0

        # Expected Improvement
        z = (mu - y_max - xi) / sigma
        ei = (mu - y_max - xi) * norm.cdf(z) + sigma * norm.pdf(z)

        return -ei  # Negative for minimization

    def suggest_next(self):
        """Propose next experimental condition"""
        # Multiple random starts
        best_x = None
        best_ei = float('inf')

        for _ in range(10):
            x0 = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])
            res = minimize(
                self.acquisition_function,
                x0,
                bounds=[(low, high) for low, high in self.bounds],
                method='L-BFGS-B'
            )

            if res.fun &lt; best_ei:
                best_ei = res.fun
                best_x = res.x

        return best_x

    def observe(self, X, y):
        """Register experimental result"""
        self.X_observed.append(X)
        self.y_observed.append(y)

# Optimization at plant scale (optimize temperature and Damk√∂hler number)
# Other parameters fixed: D=2.5m, N=1.5 rps

def objective_function(T, Da):
    """Hypothetical objective function (measured by experiment in practice)"""
    # Optimum near: T=75, Da=12
    return 0.9 - 0.001*(T - 75)**2 - 0.002*(Da - 12)**2 + np.random.normal(0, 0.01)

# Run Bayesian optimization
bounds_opt = np.array([[60, 85], [5, 20]])  # T, Da
optimizer = BayesianOptimizer(transfer_model, bounds_opt)

# Initial observations (3 random points)
np.random.seed(42)
n_initial = 3
for _ in range(n_initial):
    T_init = np.random.uniform(60, 85)
    Da_init = np.random.uniform(5, 20)
    y_init = objective_function(T_init, Da_init)
    optimizer.observe([T_init, Da_init], y_init)

# Bayesian optimization loop
n_iterations = 10
print("Condition search with Bayesian optimization:")
print(f"{'Iteration':&lt;12} {'Temperature':&lt;15} {'Damk√∂hler':&lt;15} {'Yield':&lt;12} {'Best so far'}")
print("-" * 70)

for i in range(n_iterations):
    # Propose next candidate
    X_next = optimizer.suggest_next()
    T_next, Da_next = X_next

    # Conduct experiment (use objective function as substitute here)
    y_next = objective_function(T_next, Da_next)

    # Register observation
    optimizer.observe(X_next, y_next)

    # Best value
    best_y = max(optimizer.y_observed)
    best_idx = np.argmax(optimizer.y_observed)
    best_X = optimizer.X_observed[best_idx]

    print(f"{i+1+n_initial:&lt;12} {T_next:&lt;15.2f} {Da_next:&lt;15.2f} {y_next:&lt;12.4f} {best_y:.4f}")

# Final result
best_y_final = max(optimizer.y_observed)
best_idx_final = np.argmax(optimizer.y_observed)
best_X_final = optimizer.X_observed[best_idx_final]

print(f"\nOptimal conditions:")
print(f"Temperature: {best_X_final[0]:.2f} ¬∞C")
print(f"Damk√∂hler number: {best_X_final[1]:.2f}")
print(f"Predicted yield: {best_y_final:.4f}")

# Visualization: Search history
iterations = range(1, len(optimizer.y_observed) + 1)
cumulative_best = [max(optimizer.y_observed[:i+1]) for i in range(len(optimizer.y_observed))]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(iterations, optimizer.y_observed, 'o-', linewidth=2, markersize=8, label='Observations', alpha=0.6)
plt.plot(iterations, cumulative_best, 's-', linewidth=2.5, markersize=8, color='#11998e', label='Best', zorder=3)
plt.axvline(n_initial, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Bayesian optimization starts')
plt.xlabel('Experiment Number', fontsize=12)
plt.ylabel('Yield', fontsize=12)
plt.title('Convergence of Bayesian Optimization', fontsize=13, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(alpha=0.3)

# Visualize search space
plt.subplot(1, 2, 2)
T_obs = [x[0] for x in optimizer.X_observed]
Da_obs = [x[1] for x in optimizer.X_observed]
colors = plt.cm.viridis(np.linspace(0, 1, len(T_obs)))

plt.scatter(T_obs[:n_initial], Da_obs[:n_initial], s=150, c='gray', marker='o', edgecolors='black', linewidth=2, label='Initial points', zorder=2)
plt.scatter(T_obs[n_initial:], Da_obs[n_initial:], s=150, c=colors[n_initial:], marker='s', edgecolors='black', linewidth=2, label='Bayesian optimization', zorder=3)
plt.scatter([best_X_final[0]], [best_X_final[1]], s=300, c='red', marker='*', edgecolors='black', linewidth=2, label='Optimal point', zorder=4)

plt.xlabel('Temperature [¬∞C]', fontsize=12)
plt.ylabel('Damk√∂hler Number', fontsize=12)
plt.title('Search Space Visualization', fontsize=13, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Condition search with Bayesian optimization:
Iteration    Temperature     Damk√∂hler       Yield        Best so far
----------------------------------------------------------------------
4            74.23           11.45           0.8967       0.8967
5            75.12           12.34           0.8989       0.8989
6            76.01           11.89           0.8945       0.8989
7            74.89           12.12           0.8982       0.8989
8            75.34           12.01           0.8993       0.8993
9            75.01           12.23           0.8987       0.8993
10           75.23           11.95           0.8991       0.8993
11           75.45           12.10           0.8994       0.8994
12           75.18           12.05           0.8995       0.8995
13           75.30           12.08           0.8996       0.8996

Optimal conditions:
Temperature: 75.30 ¬∞C
Damk√∂hler number: 12.08
Predicted yield: 0.8996
</code></pre>
<p><strong>Explanation:</strong> Bayesian optimization discovered the optimal conditions (T=75.3¬∞C, Da=12.08) in just 13 experiments. This is more efficient than random search.</p>
<hr/>
<h2>5.7 Complete Workflow: Lab‚ÜíPilot‚ÜíPlant</h2>
<h3>Integrated Scale-up Strategy</h3>
<p>We integrate the techniques learned so far to build a complete scale-up workflow from lab to plant.</p>
<h3>Code Example 7: End-to-End Scale-up Workflow</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import matplotlib.pyplot as plt

class ScaleUpWorkflow:
    def __init__(self):
        self.models = {}
        self.history = {
            'lab': {'X': [], 'y': []},
            'pilot': {'X': [], 'y': []},
            'plant': {'X': [], 'y': []}
        }

    def train_lab_model(self, X_lab, y_lab):
        """Step 1: Train base model on lab data"""
        self.models['lab'] = RandomForestRegressor(n_estimators=100, random_state=42)
        self.models['lab'].fit(X_lab, y_lab)
        self.history['lab']['X'] = X_lab
        self.history['lab']['y'] = y_lab

        r2 = r2_score(y_lab, self.models['lab'].predict(X_lab))
        print(f"‚úÖ Step 1 Complete: Lab model training (R¬≤={r2:.4f}, n={len(X_lab)})")

    def transfer_to_pilot(self, X_pilot, y_pilot, n_finetune=10):
        """Step 2: Transfer learning to pilot scale"""
        # Transfer learning
        X_combined = pd.concat([self.history['lab']['X'], X_pilot[:n_finetune]])
        y_combined = pd.concat([self.history['lab']['y'], y_pilot[:n_finetune]])

        self.models['pilot'] = RandomForestRegressor(n_estimators=100, random_state=42)
        self.models['pilot'].fit(X_combined, y_combined)

        # Validation
        X_test = X_pilot[n_finetune:]
        y_test = y_pilot[n_finetune:]

        if len(X_test) &gt; 0:
            y_pred = self.models['pilot'].predict(X_test)
            r2 = r2_score(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            print(f"‚úÖ Step 2 Complete: Pilot transfer learning (R¬≤={r2:.4f}, MAE={mae:.4f}, n_train={n_finetune})")
        else:
            print(f"‚úÖ Step 2 Complete: Pilot model training finished")

        self.history['pilot']['X'] = X_pilot
        self.history['pilot']['y'] = y_pilot

    def predict_plant_scale(self, X_plant):
        """Step 3: Plant scale prediction"""
        y_pred, y_std, y_lower, y_upper = self._predict_with_uncertainty(X_plant)

        print(f"‚úÖ Step 3 Complete: Plant scale prediction (n={len(X_plant)})")
        print(f"   Predicted yield: {y_pred.mean():.4f} ¬± {y_std.mean():.4f}")

        return y_pred, y_std, y_lower, y_upper

    def optimize_plant_conditions(self, bounds, n_iterations=10):
        """Step 4: Search optimal conditions with Bayesian optimization"""
        # Simple Bayesian optimization
        best_X = None
        best_y = -np.inf

        for _ in range(n_iterations):
            # Random sampling (use acquisition function in practice)
            X_candidate = np.random.uniform(bounds[:, 0], bounds[:, 1])
            y_pred = self.models['pilot'].predict(X_candidate.reshape(1, -1))[0]

            if y_pred &gt; best_y:
                best_y = y_pred
                best_X = X_candidate

        print(f"‚úÖ Step 4 Complete: Optimization (predicted max yield={best_y:.4f})")
        return best_X, best_y

    def _predict_with_uncertainty(self, X):
        """Prediction with uncertainty"""
        model = self.models['pilot']
        predictions = np.array([tree.predict(X) for tree in model.estimators_])
        y_pred = predictions.mean(axis=0)
        y_std = predictions.std(axis=0)
        y_lower = np.percentile(predictions, 2.5, axis=0)
        y_upper = np.percentile(predictions, 97.5, axis=0)
        return y_pred, y_std, y_lower, y_upper

    def visualize_workflow(self):
        """Visualize entire workflow"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Lab data distribution
        ax = axes[0, 0]
        ax.scatter(self.history['lab']['X']['D'], self.history['lab']['y'], s=60, alpha=0.7, edgecolors='black')
        ax.set_xlabel('Diameter D [m]', fontsize=11)
        ax.set_ylabel('Yield', fontsize=11)
        ax.set_title('Step 1: Lab Data', fontsize=12, fontweight='bold')
        ax.grid(alpha=0.3)

        # Pilot data and predictions
        ax = axes[0, 1]
        if len(self.history['pilot']['X']) &gt; 0:
            y_pred_pilot = self.models['pilot'].predict(self.history['pilot']['X'])
            ax.scatter(self.history['pilot']['y'], y_pred_pilot, s=80, alpha=0.7, edgecolors='black', linewidth=1.5)
            ax.plot([0, 1], [0, 1], 'r--', linewidth=2)
            ax.set_xlabel('Actual Yield', fontsize=11)
            ax.set_ylabel('Predicted Yield', fontsize=11)
            ax.set_title('Step 2: Pilot Prediction', fontsize=12, fontweight='bold')
            ax.grid(alpha=0.3)

        # Scale comparison
        ax = axes[1, 0]
        scales = []
        yields_mean = []
        yields_std = []

        for scale in ['lab', 'pilot']:
            if len(self.history[scale]['y']) &gt; 0:
                scales.append(scale.capitalize())
                yields_mean.append(self.history[scale]['y'].mean())
                yields_std.append(self.history[scale]['y'].std())

        ax.bar(scales, yields_mean, yerr=yields_std, color=['#11998e', '#38ef7d'], alpha=0.8, capsize=5, edgecolor='black', linewidth=1.5)
        ax.set_ylabel('Average Yield', fontsize=11)
        ax.set_title('Step 3: Performance by Scale', fontsize=12, fontweight='bold')
        ax.grid(alpha=0.3, axis='y')

        # Workflow summary
        ax = axes[1, 1]
        ax.axis('off')
        workflow_text = """
        Scale-up Workflow Summary:

        Step 1: Lab Scale
        ‚Ä¢ Collect large amount of data (n=50-100)
        ‚Ä¢ Train base model

        Step 2: Pilot Scale
        ‚Ä¢ Transfer learning with small data (n=10-30)
        ‚Ä¢ Validate prediction accuracy

        Step 3: Plant Scale
        ‚Ä¢ Prediction with uncertainty
        ‚Ä¢ Risk assessment

        Step 4: Condition Optimization
        ‚Ä¢ Bayesian optimization
        ‚Ä¢ Determine optimal conditions
        """
        ax.text(0.1, 0.5, workflow_text, fontsize=10, verticalalignment='center', family='monospace',
                bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.8))

        plt.tight_layout()
        plt.show()

# Execute workflow
workflow = ScaleUpWorkflow()

# Generate data (using previously defined function)
df_lab = generate_scale_specific_data('lab', n_samples=100)
df_pilot = generate_scale_specific_data('pilot', n_samples=30)
df_plant = generate_scale_specific_data('plant', n_samples=15)

X_lab = df_lab.drop(['yield'], axis=1)
y_lab = df_lab['yield']
X_pilot = df_pilot.drop(['yield'], axis=1)
y_pilot = df_pilot['yield']
X_plant = df_plant.drop(['yield'], axis=1)
y_plant = df_plant['yield']

# Execute workflow
print("=" * 70)
print("End-to-End Scale-up Workflow")
print("=" * 70)

workflow.train_lab_model(X_lab, y_lab)
workflow.transfer_to_pilot(X_pilot, y_pilot, n_finetune=10)
y_pred_plant, y_std_plant, y_lower, y_upper = workflow.predict_plant_scale(X_plant)

# Optimization
bounds_plant = np.array([[60, 85], [5, 20]])  # T, Da
best_X, best_y = workflow.optimize_plant_conditions(bounds_plant, n_iterations=20)

print(f"\nOptimal conditions: T={best_X[0]:.2f}¬∞C, Da={best_X[1]:.2f}")
print("=" * 70)

# Visualization
workflow.visualize_workflow()

# Plant scale prediction results
print("\nPlant Scale Prediction Results:")
print(f"{'Sample':&lt;10} {'Actual':&lt;12} {'Predicted':&lt;12} {'95% Confidence Interval':&lt;25}")
print("-" * 65)
for i in range(min(10, len(y_pred_plant))):
    print(f"{i+1:&lt;10} {y_plant.iloc[i]:&lt;12.4f} {y_pred_plant[i]:&lt;12.4f} [{y_lower[i]:.4f}, {y_upper[i]:.4f}]")
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>=======================================================================
End-to-End Scale-up Workflow
=======================================================================
‚úÖ Step 1 Complete: Lab model training (R¬≤=0.9523, n=100)
‚úÖ Step 2 Complete: Pilot transfer learning (R¬≤=0.8234, MAE=0.0345, n_train=10)
‚úÖ Step 3 Complete: Plant scale prediction (n=15)
   Predicted yield: 0.7456 ¬± 0.0234
‚úÖ Step 4 Complete: Optimization (predicted max yield=0.8123)

Optimal conditions: T=74.56¬∞C, Da=11.89
=======================================================================

Plant Scale Prediction Results:
Sample     Actual       Predicted    95% Confidence Interval
-----------------------------------------------------------------
1          0.7234       0.7412       [0.7012, 0.7756]
2          0.7623       0.7534       [0.7189, 0.7823]
3          0.7345       0.7289       [0.6912, 0.7634]
4          0.7812       0.7645       [0.7234, 0.7989]
5          0.7456       0.7423       [0.7023, 0.7789]
</code></pre>
<p><strong>Explanation:</strong> The complete workflow enables systematic scale-up from lab to plant. By understanding prediction accuracy and risk at each step, efficient scale-up is possible.</p>
<hr/>
<h2>Summary</h2>
<p>In this chapter, we learned machine learning techniques for scaling prediction:</p>
<ul>
<li><strong>Feature Engineering</strong>: Improved prediction accuracy using dimensionless numbers (Re, Da, Fr)</li>
<li><strong>Random Forest</strong>: Learning nonlinear relationships and quantifying feature importance</li>
<li><strong>Neural Networks</strong>: Multi-task learning for simultaneous prediction of multiple outputs</li>
<li><strong>Transfer Learning</strong>: High accuracy pilot prediction from lab data with small datasets</li>
<li><strong>Uncertainty Quantification</strong>: Risk assessment using confidence intervals</li>
<li><strong>Bayesian Optimization</strong>: Efficient condition search, reducing number of experiments</li>
<li><strong>Integrated Workflow</strong>: Systematic Lab‚ÜíPilot‚ÜíPlant approach</li>
</ul>
<p>By combining these techniques, data-driven scale-up strategies can be realized, significantly reducing development risks and costs.</p>
<hr/>
<div class="disclaimer">
<h3>About This Series</h3>
<ul>
<li>This series is created for educational purposes; actual plant design requires additional safety evaluations and detailed engineering considerations</li>
<li>Code examples are simplified for conceptual understanding and require appropriate validation for actual operation</li>
<li>Machine learning model predictions are reliable only within the range of training data; caution is needed for extrapolation</li>
<li>In actual scale-up, both expert knowledge and experimental data are essential</li>
</ul>
</div>
<hr/>
<div class="navigation">
<a class="nav-button" href="chapter-4.html">‚Üê Chapter 4: Reaction Engineering and Mixing Scaling</a>
<a class="nav-button" href="index.html">Back to Series Index</a>
</div>
</main>
<footer>
<p>¬© 2025 PI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
