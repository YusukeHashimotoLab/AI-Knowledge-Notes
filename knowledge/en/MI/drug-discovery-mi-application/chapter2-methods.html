<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Drug Discovery-Specific MI Methods - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            securityLevel: 'strict'
        });
    </script>
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true
            }
        };
    </script>
<script async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/drug-discovery-mi-application/index.html">Drug Discovery MI Application</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Drug Discovery-Specific MI Methods</h1>
<p class="subtitle">From Molecular Descriptors to Generative Models</p>
<div class="meta">
<span class="meta-item">üìñ 35-40 min</span>
<span class="meta-item">üìä Intermediate</span>
<span class="meta-item">üíª 5 Examples</span>
<span class="meta-item">üìù 5 Exercises</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 2: Drug Discovery-Specific MI Methods</h1><p class="chapter-description">This chapter covers Drug Discovery. You will learn 4 types of molecular representations (SMILES, QSAR principles, and ADMET 5 items specifically.</p>


<p><strong>From Molecular Representation to Generative Models - Technical Foundation of AI-Driven Drug Discovery</strong></p>
<h2>2.1 Molecular Representation and Descriptors</h2>
<p>The first step in drug discovery MI is converting chemical structures into formats that computers can understand. This choice of "molecular representation" significantly affects model performance.</p>
<h3>2.1.1 SMILES Representation</h3>
<p><strong>SMILES (Simplified Molecular Input Line Entry System)</strong> is the most widely used format for representing molecular structures as strings.</p>
<p><strong>Basic Rules:</strong></p>
<pre><code>Atoms: C (carbon), N (nitrogen), O (oxygen), S (sulfur), etc.
Bonds: Single bond (omitted), Double bond (=), Triple bond (#)
Ring structures: Marked with numbers (e.g., C1CCCCC1 = cyclohexane)
Branches: Expressed with parentheses (e.g., CC(C)C = isobutane)
Aromatic: Lowercase letters (e.g., c1ccccc1 = benzene)
</code></pre>
<p><strong>Examples:</strong></p>
<table>
<thead>
<tr>
<th>Compound</th>
<th>SMILES</th>
<th>Structural Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ethanol</td>
<td><code>CCO</code></td>
<td>2 carbons + hydroxyl group</td>
</tr>
<tr>
<td>Aspirin</td>
<td><code>CC(=O)OC1=CC=CC=C1C(=O)O</code></td>
<td>Ester + carboxylic acid</td>
</tr>
<tr>
<td>Caffeine</td>
<td><code>CN1C=NC2=C1C(=O)N(C(=O)N2C)C</code></td>
<td>Purine scaffold + methyl groups</td>
</tr>
<tr>
<td>Ibuprofen</td>
<td><code>CC(C)Cc1ccc(cc1)[C@@H](C)C(=O)O</code></td>
<td>Aromatic ring + chiral center</td>
</tr>
<tr>
<td>Penicillin G</td>
<td><code>CC1(C)S[C@@H]2[C@H](NC(=O)Cc3ccccc3)C(=O)N2[C@H]1C(=O)O</code></td>
<td>Œ≤-lactam ring + thiazolidine ring</td>
</tr>
</tbody>
</table>
<p><strong>Advantages:</strong>
- ‚úÖ Compact (complex molecules in dozens of characters)
- ‚úÖ Human readable and writable
- ‚úÖ Suitable for database searches
- ‚úÖ Uniqueness (Canonical SMILES)</p>
<p><strong>Disadvantages:</strong>
- ‚ùå No 3D structural information (conformations lost)
- ‚ùå Difficulty distinguishing tautomers
- ‚ùå Existence of invalid SMILES (syntax errors)</p>
<h3>2.1.2 Molecular Fingerprints</h3>
<p>Molecular fingerprints represent molecules as bit vectors (sequences of 0/1). Widely used for similarity searches and QSAR.</p>
<h4>ECFP (Extended Connectivity Fingerprints)</h4>
<p><strong>Principle:</strong>
1. Start from each atom
2. Hash neighborhood structure of radius R (typically 2-4)
3. Convert to bit vector (length 1024-4096)</p>
<p><strong>Example: ECFP4 (radius 2, 4-step neighborhood)</strong></p>
<pre><code>Molecule: CCO (ethanol)

Atom 1 (C):
  - Radius 0: C
  - Radius 1: C-C
  - Radius 2: C-C-O

Atom 2 (C):
  - Radius 0: C
  - Radius 1: C-C, C-O
  - Radius 2: C-C-O, C-O

Atom 3 (O):
  - Radius 0: O
  - Radius 1: O-C
  - Radius 2: O-C-C

‚Üí Hash these to determine bit positions
‚Üí Set corresponding bits to 1
</code></pre>
<p><strong>Types:</strong></p>
<table>
<thead>
<tr>
<th>Fingerprint</th>
<th>Size</th>
<th>Features</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>ECFP4</td>
<td>1024-4096 bit</td>
<td>Radius 2 environment</td>
<td>Similarity search, QSAR</td>
</tr>
<tr>
<td>ECFP6</td>
<td>1024-4096 bit</td>
<td>Radius 3 environment</td>
<td>More precise structure recognition</td>
</tr>
<tr>
<td>MACCS keys</td>
<td>166 bit</td>
<td>Fixed substructures</td>
<td>Fast search, diversity analysis</td>
</tr>
<tr>
<td>RDKit Fingerprint</td>
<td>2048 bit</td>
<td>Path-based</td>
<td>General-purpose QSAR</td>
</tr>
<tr>
<td>Morgan Fingerprint</td>
<td>Variable</td>
<td>ECFP implementation</td>
<td>RDKit standard</td>
</tr>
</tbody>
</table>
<p><strong>Tanimoto Coefficient for Similarity:</strong></p>
<pre><code>Tanimoto Coefficient = (A ‚à© B) / (A ‚à™ B)

A, B: Fingerprint bit vectors of two molecules
‚à©: Bit AND (number of bits that are 1 in both)
‚à™: Bit OR (number of bits that are 1 in at least one)

Range: 0 (completely different) ~ 1 (perfect match)
</code></pre>
<p><strong>Practical Thresholds:</strong>
- Tanimoto &gt; 0.85: Very similar (same compound class)
- 0.70-0.85: Similar (possible similar pharmacological activity)
- 0.50-0.70: Somewhat similar
- &lt; 0.50: Different</p>
<h4>MACCS keys</h4>
<p><strong>Features:</strong>
- Represents presence/absence of 166 fixed substructures
- Examples: benzene ring, carboxylic acid, amino group, halogens, etc.</p>
<p><strong>Advantages:</strong>
- Interpretable (know which substructures are present)
- Fast computation
- Chemically meaningful similarity</p>
<p><strong>Disadvantages:</strong>
- Low information content (only 166 bits)
- Difficult to handle novel scaffolds</p>
<h3>2.1.3 3D Descriptors</h3>
<p>3D descriptors quantify the three-dimensional structure of molecules.</p>
<p><strong>Major 3D Descriptors:</strong></p>
<ol>
<li>
<p><strong>Molecular Surface Area</strong>
   - TPSA (Topological Polar Surface Area): Polar surface area
   - Prediction: Oral absorption (favorable when TPSA &lt; 140 √Ö¬≤)</p>
</li>
<li>
<p><strong>Volume and Shape</strong>
   - Molecular Volume
   - Sphericity
   - Aspect Ratio</p>
</li>
<li>
<p><strong>Charge Distribution</strong>
   - Partial Charges: Gasteiger, MMFF
   - Dipole Moment
   - Quadrupole Moment</p>
</li>
<li>
<p><strong>Pharmacophore</strong>
   - Hydrogen bond donor/acceptor positions
   - Hydrophobic regions
   - Positive/negative charge centers
   - Aromatic ring orientations</p>
</li>
</ol>
<p><strong>Example: Descriptors Used in Lipinski's Rule of Five</strong></p>
<pre><code>Molecular Weight (MW): &lt; 500 Da
LogP (lipophilicity): &lt; 5
Hydrogen Bond Donors (HBD): &lt; 5
Hydrogen Bond Acceptors (HBA): &lt; 10

Compounds satisfying these tend to have high oral absorption
</code></pre>
<h3>2.1.4 Graph Representation (For Graph Neural Networks)</h3>
<p>Molecules are represented as mathematical graphs.</p>
<p><strong>Definition:</strong></p>
<pre><code>G = (V, E)

V: Vertices = Atoms
E: Edges = Bonds

Each vertex v has feature vector h_v
Each edge e has feature vector h_e
</code></pre>
<p><strong>Atom (Vertex) Features:</strong>
- Atomic number (C=6, N=7, O=8, etc.)
- Atom type (C, N, O, S, F, Cl, Br, I)
- Hybridization (sp, sp2, sp3)
- Formal Charge
- Aromaticity (Aromatic or not)
- Hydrogen Count
- Number of lone pairs
- Chirality (R/S)</p>
<p><strong>Bond (Edge) Features:</strong>
- Bond order (Single=1, Double=2, Triple=3, Aromatic=1.5)
- Bond type (Covalent, Ionic, etc.)
- Part of ring or not
- Stereochemistry (E/Z)</p>
<p><strong>Graph Advantages:</strong>
- Direct representation of molecular structure
- Rotation and translation invariance
- Learnable with Graph Neural Networks</p>
<hr/>
<h2>2.2 QSAR (Quantitative Structure-Activity Relationship)</h2>
<h3>2.2.1 Basic Principles of QSAR</h3>
<p><strong>QSAR (Quantitative Structure-Activity Relationship)</strong> expresses the quantitative relationship between molecular structure and biological activity using mathematical equations.</p>
<p><strong>Basic Hypothesis:</strong></p>
<blockquote>
<p>Similar molecular structures show similar biological activity (Similar Property Principle)</p>
</blockquote>
<p><strong>General QSAR Equation:</strong></p>
<pre><code>Activity = f(Descriptors)

Activity: Biological activity (IC50, EC50, Ki, etc.)
Descriptors: Molecular descriptors (MW, LogP, TPSA, etc.)
f: Mathematical function (linear regression, Random Forest, NN, etc.)
</code></pre>
<p><strong>Historical QSAR Equation (Hansch-Fujita Equation, 1962):</strong></p>
<pre><code>log(1/C) = a * logP + b * œÉ + c * Es + d

C: Biological activity concentration (lower = higher activity)
logP: Partition coefficient (lipophilicity)
œÉ: Hammett constant (electronic effect)
Es: Steric parameter
a, b, c, d: Regression coefficients
</code></pre>
<h3>2.2.2 QSAR Workflow</h3>
<div class="mermaid">
flowchart TD
    A["Compound Library<br/>SMILES + Activity Data"] --&gt; B["Calculate Molecular Descriptors<br/>MW, LogP, ECFP, etc."]
    B --&gt; C["Data Split<br/>Train 80% / Test 20%"]
    C --&gt; D["Model Training<br/>RF, SVM, NN"]
    D --&gt; E["Performance Evaluation<br/>R^2, MAE, ROC-AUC"]
    E --&gt; F{Performance OK?}
    F --&gt;|No| G["Hyperparameter<br/>Tuning"]
    G --&gt; D
    F --&gt;|Yes| H["Predict New Compounds<br/>Virtual Screening"]
    H --&gt; I["Experimental Validation<br/>Top Candidates Only"]

    style A fill:#e3f2fd
    style E fill:#fff3e0
    style H fill:#e8f5e9
    style I fill:#ffebee
</div>
<h3>2.2.3 Types of QSAR Models</h3>
<h4>Classification Models (Predicting Active/Inactive)</h4>
<p><strong>Purpose:</strong> Predict whether a compound is active or inactive</p>
<p><strong>Evaluation Metrics:</strong></p>
<pre><code>ROC-AUC: 0.5 (random) ~ 1.0 (perfect)
Target: &gt; 0.80

Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 Score = 2 * (Precision * Recall) / (Precision + Recall)

TP: True Positive (correctly predict active)
FP: False Positive (incorrectly predict inactive as active)
FN: False Negative (incorrectly predict active as inactive)
</code></pre>
<p><strong>Typical Problem Setting:</strong>
- IC50 &lt; 1 ŒºM ‚Üí Active (1)
- IC50 ‚â• 1 ŒºM ‚Üí Inactive (0)</p>
<h4>Regression Models (Predicting Activity Values)</h4>
<p><strong>Purpose:</strong> Predict continuous values like IC50, Ki, EC50</p>
<p><strong>Evaluation Metrics:</strong></p>
<pre><code>R¬≤ (Coefficient of determination): 0 (no correlation) ~ 1 (perfect)
Target: &gt; 0.70

MAE (Mean Absolute Error) = Œ£|y_pred - y_true| / n
RMSE (Root Mean Square Error) = ‚àö(Œ£(y_pred - y_true)¬≤ / n)

y_pred: Predicted value
y_true: True value
n: Number of samples
</code></pre>
<p><strong>Log Transformation:</strong>
In most cases, activity values (IC50, etc.) are log-transformed before regression</p>
<pre><code>pIC50 = -log10(IC50[M])

Example:
IC50 = 1 nM = 10^-9 M ‚Üí pIC50 = 9.0
IC50 = 100 nM = 10^-7 M ‚Üí pIC50 = 7.0

Range: typically 4-10 (10 ŒºM ~ 0.1 nM)
</code></pre>
<h3>2.2.4 Comparison of Machine Learning Methods</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
<th>Speed</th>
<th>Interpretability</th>
<th>Data Size</th>
<th>Recommended Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Regression</td>
<td>‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>100+</td>
<td>Baseline, linear relationships</td>
</tr>
<tr>
<td>Random Forest</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê‚≠ê</td>
<td>1K-100K</td>
<td>Medium data, non-linear</td>
</tr>
<tr>
<td>SVM (RBF kernel)</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>1K-10K</td>
<td>Small-medium, high-dimensional</td>
</tr>
<tr>
<td>Neural Network</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>‚≠ê</td>
<td>10K+</td>
<td>Large data, complex relationships</td>
</tr>
<tr>
<td>LightGBM/XGBoost</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>1K-1M</td>
<td>Kaggle winners, fast</td>
</tr>
<tr>
<td>Graph Neural Network</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>‚≠ê‚≠ê</td>
<td>10K+</td>
<td>Molecular graphs, SOTA</td>
</tr>
</tbody>
</table>
<p><strong>Real-World Performance (ChEMBL Data):</strong></p>
<pre><code>Task: Kinase inhibition activity prediction (classification)

Linear Regression: ROC-AUC = 0.72
Random Forest:      ROC-AUC = 0.87
SVM (RBF):          ROC-AUC = 0.85
Neural Network:     ROC-AUC = 0.89
LightGBM:           ROC-AUC = 0.90
GNN (MPNN):         ROC-AUC = 0.92

Training data: 10,000 compounds
Test data: 2,000 compounds
</code></pre>
<h3>2.2.5 QSAR Applicability Domain</h3>
<p><strong>Problem:</strong> Predictions for compounds very different from training data are unreliable</p>
<p><strong>Applicability Domain Definition:</strong></p>
<pre><code>AD = {x | included in chemical space of training data}

Determination methods:
1. Tanimoto distance: Similarity to nearest neighbor &gt; 0.3
2. Leverage: h_i &lt; 3p/n (h: hat matrix, p: number of features, n: sample size)
3. Standardized distance: d_i &lt; 3œÉ (œÉ: standard deviation)
</code></pre>
<p><strong>Compounds Outside AD:</strong>
- Low prediction reliability
- Experimental validation required
- Consider model retraining</p>
<p><strong>Example:</strong></p>
<pre><code>Training data: Kinase inhibitors (mainly ATP-competitive)
Novel compound: Allosteric inhibitor (different binding site)

‚Üí Likely outside AD
‚Üí Prediction accuracy not guaranteed
</code></pre>
<hr/>
<h2>2.3 ADMET Prediction</h2>
<h3>2.3.1 Importance of ADMET</h3>
<p><strong>30% of clinical trial failures are due to ADMET issues</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>Main Failure Causes</th>
<th>ADMET-Related</th>
</tr>
</thead>
<tbody>
<tr>
<td>Phase I</td>
<td>Toxicity (hepatotoxicity, cardiotoxicity)</td>
<td>30%</td>
</tr>
<tr>
<td>Phase II</td>
<td>Lack of efficacy, poor PK</td>
<td>20%</td>
</tr>
<tr>
<td>Phase III</td>
<td>Long-term toxicity, side effects</td>
<td>15%</td>
</tr>
</tbody>
</table>
<p><strong>Failure Reduction Through ADMET Prediction:</strong>
- Early ADMET evaluation (during lead discovery stage)
- Eliminate problematic compounds
- Reduce development costs by $100M-500M</p>
<h3>2.3.2 Absorption</h3>
<h4>Caco-2 Permeability</h4>
<p><strong>Caco-2 Cells:</strong> Colon cancer cell line, model of intestinal epithelial cells</p>
<p><strong>Measurement:</strong> Rate of permeation through cell layer (Papp: apparent permeability coefficient)</p>
<pre><code>Papp [cm/s]

Papp &gt; 10^-6 cm/s: High permeability (good absorption)
10^-7 &lt; Papp &lt; 10^-6: Moderate
Papp &lt; 10^-7 cm/s: Low permeability (poor absorption)
</code></pre>
<p><strong>Prediction Model:</strong></p>
<pre><code class="language-python"># Simple prediction equation (R¬≤ = 0.75)
log Papp = 0.5 * logP - 0.01 * TPSA + 0.3 * HBD - 2.5

logP: Partition coefficient (lipophilicity)
TPSA: Topological polar surface area
HBD: Number of hydrogen bond donors
</code></pre>
<p><strong>Machine Learning Prediction Accuracy:</strong>
- Random Forest: R¬≤ = 0.80-0.85
- Neural Network: R¬≤ = 0.82-0.88
- Graph NN: R¬≤ = 0.85-0.90</p>
<h4>Oral Bioavailability (F%)</h4>
<p><strong>Definition:</strong> Fraction of administered dose that reaches systemic circulation</p>
<pre><code>F% = (AUC_oral / AUC_iv) * (Dose_iv / Dose_oral) * 100

AUC: Area under the plasma concentration-time curve
</code></pre>
<p><strong>Target Values:</strong>
- F% &gt; 30%: Acceptable range
- F% &gt; 50%: Good
- F% &gt; 70%: Excellent</p>
<p><strong>Predictive Factors:</strong>
1. Solubility
2. Permeability
3. First-pass metabolism (hepatic first-pass metabolism)
4. P-glycoprotein efflux</p>
<p><strong>BCS Classification (Biopharmaceutics Classification System):</strong></p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Solubility</th>
<th>Permeability</th>
<th>Example</th>
<th>F%</th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>High</td>
<td>High</td>
<td>Metoprolol</td>
<td>&gt; 80%</td>
</tr>
<tr>
<td>II</td>
<td>Low</td>
<td>High</td>
<td>Ibuprofen</td>
<td>50-80%</td>
</tr>
<tr>
<td>III</td>
<td>High</td>
<td>Low</td>
<td>Atenolol</td>
<td>30-50%</td>
</tr>
<tr>
<td>IV</td>
<td>Low</td>
<td>Low</td>
<td>Tacrolimus</td>
<td>&lt; 30%</td>
</tr>
</tbody>
</table>
<h3>2.3.3 Distribution</h3>
<h4>Plasma Protein Binding</h4>
<p><strong>Definition:</strong> Percentage of drug bound to plasma proteins (albumin, Œ±1-acid glycoprotein)</p>
<pre><code>Binding rate = (Bound / Total) * 100%

Bound: Bound drug concentration
Total: Total drug concentration
</code></pre>
<p><strong>Clinical Significance:</strong>
- High binding (&gt; 90%): Low free drug (active form)
- Risk of drug-drug interactions (binding site competition)
- Affects volume of distribution</p>
<p><strong>Prediction Models:</strong>
- Random Forest: R¬≤ = 0.65-0.75
- Deep Learning: R¬≤ = 0.70-0.80</p>
<h4>Blood-Brain Barrier (BBB) Permeability</h4>
<p><strong>LogBB (Brain/Blood Ratio):</strong></p>
<pre><code>LogBB = log10(C_brain / C_blood)

LogBB &gt; 0: Concentrates in brain (favorable for CNS drugs)
LogBB &lt; -1: Does not penetrate brain (avoid CNS side effects)
</code></pre>
<p><strong>Predictive Factors:</strong>
- Molecular weight (&lt; 400 Da favorable)
- TPSA (&lt; 60 √Ö¬≤ favorable)
- LogP (2-5 optimal)
- Number of hydrogen bonds (fewer is better)</p>
<p><strong>Machine Learning Prediction:</strong>
- Random Forest: R¬≤ = 0.70-0.80
- Neural Network: R¬≤ = 0.75-0.85</p>
<h3>2.3.4 Metabolism</h3>
<h4>CYP450 Inhibition</h4>
<p><strong>CYP450 (Cytochrome P450):</strong> Major metabolic enzyme in liver</p>
<p><strong>Major Isoforms:</strong></p>
<table>
<thead>
<tr>
<th>CYP</th>
<th>Percentage of Substrate Drugs</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>CYP3A4</td>
<td>50%</td>
<td>Statins, immunosuppressants</td>
</tr>
<tr>
<td>CYP2D6</td>
<td>25%</td>
<td>Œ≤-blockers, antidepressants</td>
</tr>
<tr>
<td>CYP2C9</td>
<td>15%</td>
<td>Warfarin, NSAIDs</td>
</tr>
<tr>
<td>CYP2C19</td>
<td>10%</td>
<td>Proton pump inhibitors</td>
</tr>
</tbody>
</table>
<p><strong>Problem with Inhibition:</strong>
- Drug-drug interactions (DDI)
- Increased blood concentration of co-administered drugs ‚Üí toxicity risk</p>
<p><strong>Prediction (Classification Problem):</strong></p>
<pre><code>Inhibitor: IC50 &lt; 10 ŒºM
Non-inhibitor: IC50 ‚â• 10 ŒºM

Prediction accuracy:
Random Forest: ROC-AUC = 0.85-0.90
Neural Network: ROC-AUC = 0.87-0.92
</code></pre>
<h4>Metabolic Stability</h4>
<p><strong>Measurement:</strong> Percentage remaining after incubation with liver microsomes</p>
<pre><code>t1/2 (half-life) [min]

t1/2 &gt; 60 min: Stable (slow metabolism)
30 &lt; t1/2 &lt; 60: Moderate
t1/2 &lt; 30 min: Unstable (fast metabolism)
</code></pre>
<p><strong>Prediction:</strong>
- Random Forest: R¬≤ = 0.55-0.65 (somewhat difficult)
- Graph NN: R¬≤ = 0.60-0.70</p>
<h3>2.3.5 Excretion</h3>
<h4>Renal Clearance</h4>
<p><strong>Definition:</strong> Rate of drug removal by kidneys</p>
<pre><code>CL_renal [mL/min/kg]

CL_renal &gt; 10: High clearance (rapidly excreted)
1 &lt; CL_renal &lt; 10: Moderate
CL_renal &lt; 1: Low clearance
</code></pre>
<p><strong>Influencing Factors:</strong>
- Molecular weight (smaller = easier excretion)
- Polarity (higher = easier excretion)
- Renal transporter substrate</p>
<h4>Half-Life (t1/2)</h4>
<p><strong>Definition:</strong> Time for plasma concentration to decrease by half</p>
<pre><code>t1/2 = 0.693 / (CL / Vd)

CL: Clearance (systemic)
Vd: Volume of distribution
</code></pre>
<p><strong>Clinical Significance:</strong>
- t1/2 &lt; 2 h: Frequent dosing required (3-4 times/day)
- 2 &lt; t1/2 &lt; 8 h: Twice daily dosing
- t1/2 &gt; 8 h: Once daily dosing possible</p>
<p><strong>Prediction Accuracy:</strong>
- Random Forest: R¬≤ = 0.55-0.65 (difficult)
- Complex combination of pharmacokinetic parameters</p>
<h3>2.3.6 Toxicity</h3>
<h4>hERG Inhibition (Cardiotoxicity)</h4>
<p><strong>hERG (human Ether-√†-go-go-Related Gene):</strong> Cardiac potassium channel</p>
<p><strong>Result of Inhibition:</strong> QT prolongation ‚Üí fatal arrhythmia (Torsades de pointes)</p>
<p><strong>Risk Assessment:</strong></p>
<pre><code>IC50 &lt; 1 ŒºM: High risk (development termination)
1 &lt; IC50 &lt; 10 ŒºM: Medium risk (careful evaluation needed)
IC50 &gt; 10 ŒºM: Low risk (safe)
</code></pre>
<p><strong>Prediction Accuracy (Highest Accuracy in ADMET Predictions):</strong>
- Random Forest: ROC-AUC = 0.85-0.90
- Deep Learning: ROC-AUC = 0.90-0.95
- Graph NN: ROC-AUC = 0.92-0.97</p>
<p><strong>Structural Alerts (Structures Prone to hERG Inhibition):</strong>
- Basic nitrogen atom (pKa &gt; 7)
- Hydrophobic aromatic rings
- Flexible linker</p>
<h4>Hepatotoxicity</h4>
<p><strong>DILI (Drug-Induced Liver Injury):</strong> Drug-induced liver damage</p>
<p><strong>Mechanisms:</strong>
1. Formation of reactive metabolites
2. Mitochondrial toxicity
3. Bile acid transport inhibition</p>
<p><strong>Prediction (Classification Problem):</strong></p>
<pre><code>Hepatotoxic drug classification:

Random Forest: ROC-AUC = 0.75-0.85
Neural Network: ROC-AUC = 0.78-0.88

Challenge: Data shortage (many cases discovered post-approval)
</code></pre>
<h4>Mutagenicity (Ames Test)</h4>
<p><strong>Ames Test:</strong> Mutagenicity test using bacteria</p>
<p><strong>Prediction:</strong></p>
<pre><code>Positive: Mutagenic (carcinogenicity risk)
Negative: Non-mutagenic

Random Forest: ROC-AUC = 0.80-0.90
Deep Learning: ROC-AUC = 0.85-0.93
</code></pre>
<p><strong>Structural Alerts:</strong>
- Nitro group (-NO2)
- Azo group (-N=N-)
- Epoxide
- Alkylating agents</p>
<hr/>
<h2>2.4 Molecular Generative Models</h2>
<h3>2.4.1 Need for Generative Models</h3>
<p><strong>Traditional Virtual Screening:</strong>
- Select from existing compound libraries (10^6-10^9 compounds)
- Limitation: Cannot discover compounds not in library</p>
<p><strong>Generative Model Approach:</strong>
- Generate novel molecules directly
- Freely explore chemical space (10^60 molecules)
- Design molecules with desired properties</p>
<p><strong>Types of Generative Models:</strong></p>
<div class="mermaid">
flowchart TD
    A[Molecular Generative Models] --&gt; B["VAE<br/>Variational Autoencoder"]
    A --&gt; C["GAN<br/>Generative Adversarial Network"]
    A --&gt; D["Transformer<br/>GPT-like"]
    A --&gt; E["Reinforcement Learning<br/>RL"]
    A --&gt; F["Graph Generation<br/>GNN-based"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fff9c4
    style F fill:#fce4ec
</div>
<h3>2.4.2 VAE (Variational Autoencoder)</h3>
<p><strong>Architecture:</strong></p>
<pre><code>SMILES ‚Üí Encoder ‚Üí Latent space (z) ‚Üí Decoder ‚Üí SMILES'

Encoder: Compresses SMILES string to low-dimensional vector (z)
Latent space: Continuous chemical space (typically 50-500 dimensions)
Decoder: Reconstructs SMILES string from vector (z)
</code></pre>
<p><strong>Training Objective:</strong></p>
<pre><code>Loss = Reconstruction Loss + KL Divergence

Reconstruction Loss: Error in reconstructing input SMILES
KL Divergence: Regularization of latent space (approach normal distribution)
</code></pre>
<p><strong>Molecule Generation Workflow:</strong>
1. Train VAE on known molecules (training data)
2. Sample in latent space (z ~ N(0, I))
3. Generate novel SMILES with Decoder
4. Validity check (parseable by RDKit?)
5. Property prediction (QSAR models, ADMET models)
6. Select favorable molecules</p>
<p><strong>Advantages:</strong>
- ‚úÖ Optimization possible in continuous latent space
- ‚úÖ Explore around known molecules (generate similar molecules)
- ‚úÖ Generate intermediate molecules via interpolation</p>
<p><strong>Disadvantages:</strong>
- ‚ùå High rate of invalid SMILES generation (30-50%)
- ‚ùå Difficult to generate novel scaffolds (depends on training data)</p>
<p><strong>Example: ChemVAE (G√≥mez-Bombarelli et al., 2018)</strong></p>
<pre><code>Training data: ZINC 250K compounds
Latent space: 196 dimensions
Valid SMILES generation rate: 68%
Application: Property optimization using penalty method (LogP, QED)
</code></pre>
<h3>2.4.3 GAN (Generative Adversarial Network)</h3>
<p><strong>Two Networks:</strong></p>
<pre><code>Generator: Noise ‚Üí Molecular SMILES
Discriminator: SMILES ‚Üí Real or Fake?

Adversarial learning:
- Generator: Generate molecules to fool Discriminator
- Discriminator: Distinguish Real (training data) from Fake (generated)
</code></pre>
<p><strong>Training Process:</strong></p>
<pre><code>1. Generator: Random noise ‚Üí SMILES generation
2. Discriminator: Real/Fake classification
3. Generator: Update with Discriminator's gradient (make Fake "Real-like")
4. Discriminator: Improve classification accuracy
5. Repeat ‚Üí Generator produces molecules similar to training data
</code></pre>
<p><strong>Advantages:</strong>
- ‚úÖ Diverse molecule generation (with mode collapse countermeasures)
- ‚úÖ High-quality molecules (filtering effect by Discriminator)</p>
<p><strong>Disadvantages:</strong>
- ‚ùå Training instability (mode collapse, gradient vanishing)
- ‚ùå Difficult to evaluate (quantifying generation quality)</p>
<p><strong>Example: ORGANIC (Guimaraes et al., 2017)</strong></p>
<pre><code>Generator: LSTM (sequential SMILES generation)
Discriminator: CNN (SMILES string classification)
Application: Combined with reinforcement learning for property optimization
</code></pre>
<p><strong>Insilico Medicine's Chemistry42:</strong>
- GAN-based
- Discovered IPF treatment candidate in 18 months
- Technology: WGAN-GP (Wasserstein GAN with Gradient Penalty)</p>
<h3>2.4.4 Transformer (GPT-like Models)</h3>
<p><strong>Principle:</strong>
- Treat SMILES like natural language
- Capture context with Attention mechanism
- Large-scale pre-training (10^6-10^7 molecules)</p>
<p><strong>Architecture:</strong></p>
<pre><code>SMILES: C C ( = O ) O [EOS]
‚Üì
Tokenization: [C] [C] [(] [=] [O] [)] [O] [EOS]
‚Üì
Transformer Encoder/Decoder
‚Üì
Next token prediction: P(token_i+1 | token_1, ..., token_i)
</code></pre>
<p><strong>Generation Method:</strong>
1. Start from start token ([SOS])
2. Probabilistically sample next token
3. Repeat until [EOS]
4. Validate as SMILES</p>
<p><strong>Advantages:</strong>
- ‚úÖ High valid SMILES generation rate (&gt; 90%)
- ‚úÖ Benefits of large-scale pre-training (transfer learning)
- ‚úÖ Controllable generation (conditional generation)</p>
<p><strong>Disadvantages:</strong>
- ‚ùå Computational cost (large models)
- ‚ùå Difficult to control novelty</p>
<p><strong>Examples:</strong>
1. <strong>ChemBERTa (HuggingFace, 2020)</strong>
   - Pre-training: 10 million SMILES (PubChem)
   - Fine-tuning: High-accuracy QSAR with 100 samples</p>
<ol start="2">
<li>
<p><strong>MolGPT (Bagal et al., 2021)</strong>
   - GPT-2 architecture
   - Conditional generation (specify property values)</p>
</li>
<li>
<p><strong>SMILES-BERT (Wang et al., 2019)</strong>
   - Masked Language Model (MLM task)
   - High accuracy with small data via transfer learning</p>
</li>
</ol>
<h3>2.4.5 Reinforcement Learning (RL)</h3>
<p><strong>Setup:</strong></p>
<pre><code>Agent: Molecular generative model
Environment: Chemical space
Action: Select next token (SMILES generation)
State: Current SMILES prefix
Reward: Properties of generated molecule (QED, LogP, ADMET, etc.)
</code></pre>
<p><strong>RL + Generative Model Flow:</strong></p>
<div class="mermaid">
flowchart LR
    A["Agent<br/>Generative Model"] --&gt;|Action| B[SMILES Generation]
    B --&gt;|State| C["Property Prediction<br/>QSAR, ADMET"]
    C --&gt;|Reward| D[Reward Calculation]
    D --&gt;|Update| A

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#e8f5e9
</div>
<p><strong>Reward Function Design:</strong></p>
<pre><code>Reward = w1 * QED + w2 * LogP_score + w3 * SA_score + w4 * ADMET_score

QED: Quantitative Estimate of Druglikeness
LogP_score: Lipophilicity score (2-5 optimal)
SA_score: Synthetic Accessibility
ADMET_score: ADMET model prediction

w1, w2, w3, w4: Weights (multi-objective optimization)
</code></pre>
<p><strong>Learning Algorithms:</strong>
1. <strong>Policy Gradient (REINFORCE)</strong>
   ```
   ‚àáJ(Œ∏) = E[‚àálog œÄ(a|s) * R]</p>
<p>œÄ: Policy (generation probability)
   R: Cumulative reward
   ```</p>
<ol start="2">
<li><strong>Proximal Policy Optimization (PPO)</strong>
   - More stable learning
   - Clip gradients to prevent large updates</li>
</ol>
<p><strong>Advantages:</strong>
- ‚úÖ Explicit optimization (control via reward function)
- ‚úÖ Multi-objective optimization possible
- ‚úÖ Balance exploration and exploitation</p>
<p><strong>Disadvantages:</strong>
- ‚ùå Difficult to design reward function
- ‚ùå Time-consuming training</p>
<p><strong>Example: ReLeaSE (Popova et al., 2018)</strong></p>
<pre><code>Technology: RL + LSTM
Objective: Optimize LogP, QED
Result: Generated molecules with properties superior to known drugs
</code></pre>
<h3>2.4.6 Graph Generative Models</h3>
<p><strong>Principle:</strong> Directly generate molecular graphs</p>
<p><strong>Generation Process:</strong></p>
<pre><code>1. Initial state: Empty graph
2. Add atom: Which atom type to add (C, N, O, etc.)
3. Add bond: Between which atoms to add bonds
4. Repeat: Until desired size is reached
</code></pre>
<p><strong>Major Methods:</strong></p>
<ol>
<li>
<p><strong>GraphRNN (You et al., 2018)</strong>
   - Sequential graph generation with RNN</p>
</li>
<li>
<p><strong>Junction Tree VAE (Jin et al., 2018)</strong>
   - Generate molecular scaffold (Junction Tree)
   - 100% valid SMILES generation rate (theoretically)</p>
</li>
<li>
<p><strong>MoFlow (Zang &amp; Wang, 2020)</strong>
   - Based on Normalizing Flows
   - Exact probability density through invertible transformations</p>
</li>
</ol>
<p><strong>Advantages:</strong>
- ‚úÖ Generate only valid molecules (incorporate chemical constraints of graphs)
- ‚úÖ Can consider 3D structure</p>
<p><strong>Disadvantages:</strong>
- ‚ùå High computational cost
- ‚ùå Complex implementation</p>
<hr/>
<h2>2.5 Major Databases and Tools</h2>
<h3>2.5.1 ChEMBL</h3>
<p><strong>Overview:</strong>
- Bioactivity database (European Bioinformatics Institute, EBI)
- 2+ million compounds
- 15+ million bioactivity data points
- 14,000+ targets</p>
<p><strong>Data Structure:</strong></p>
<pre><code>Compound: ChEMBL ID, SMILES, InChI, molecular weight, etc.
Assay: Assay type, target, cell line, etc.
Activity: IC50, Ki, EC50, Kd, etc.
Target: Protein, gene, cell, etc.
</code></pre>
<p><strong>API Access:</strong></p>
<pre><code class="language-python">from chembl_webresource_client.new_client import new_client

# Target search
target = new_client.target
kinases = target.filter(target_type='PROTEIN KINASE')

# Get compound activity data
activity = new_client.activity
egfr_data = activity.filter(target_chembl_id='CHEMBL203', pchembl_value__gte=6)
# pchembl_value ‚â• 6 ‚Üí IC50 ‚â§ 1 ŒºM
</code></pre>
<p><strong>Use Cases:</strong>
- <strong>ChEMBL</strong>: Bioactivity data, QSAR training
- <strong>PubChem</strong>: Chemical structures, literature search
- <strong>DrugBank</strong>: Approved drugs, clinical information
- <strong>BindingDB</strong>: Protein-ligand binding affinity</p>
<h3>2.5.2 PubChem</h3>
<p><strong>Overview:</strong>
- Chemical information database (NIH, USA)
- 100+ million compounds
- Bioactivity assay data (PubChem BioAssay)</p>
<p><strong>Features:</strong>
- Free access
- REST API, FTP provided
- 2D/3D structure data
- Literature links (PubMed)</p>
<p><strong>Applications:</strong>
- Building compound libraries
- Structure search (similarity, substructure)
- Property data collection</p>
<h3>2.5.3 DrugBank</h3>
<p><strong>Overview:</strong>
- Approved and clinical trial drug database (Canada)
- 14,000+ drugs
- Detailed pharmacokinetic, pharmacodynamic data</p>
<p><strong>Information:</strong>
- ADMET properties
- Drug-drug interactions
- Target information
- Clinical trial status</p>
<p><strong>Applications:</strong>
- Drug repurposing (finding new indications for approved drugs)
- ADMET training data
- Benchmarking</p>
<h3>2.5.4 BindingDB</h3>
<p><strong>Overview:</strong>
- Protein-ligand binding affinity database
- 2.5+ million binding data points
- 9,000+ proteins</p>
<p><strong>Data Types:</strong>
- Ki (inhibition constant)
- Kd (dissociation constant)
- IC50 (50% inhibitory concentration)
- EC50 (50% effective concentration)</p>
<p><strong>Applications:</strong>
- Docking validation
- Binding affinity prediction model training</p>
<h3>2.5.5 RDKit</h3>
<p><strong>Overview:</strong>
- Open-source cheminformatics library
- Python, C++, Java support
- Standard tool for drug discovery MI</p>
<p><strong>Main Functions:</strong>
1. <strong>Molecular I/O</strong>: Read/write SMILES, MOL, SDF
2. <strong>Descriptor calculation</strong>: 200+ physicochemical properties
3. <strong>Molecular fingerprints</strong>: ECFP, MACCS, RDKit FP
4. <strong>Substructure search</strong>: SMARTS, MCS (maximum common substructure)
5. <strong>2D drawing</strong>: Molecular structure visualization
6. <strong>3D structure generation</strong>: ETKDG (distance geometry method)</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem

# Read SMILES
mol = Chem.MolFromSmiles('CC(=O)OC1=CC=CC=C1C(=O)O')  # Aspirin

# Calculate descriptors
mw = Descriptors.MolWt(mol)  # 180.16
logp = Descriptors.MolLogP(mol)  # 1.19

# Molecular fingerprint
fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)

# Similarity calculation
mol2 = Chem.MolFromSmiles('CC(C)Cc1ccc(cc1)C(C)C(=O)O')  # Ibuprofen
similarity = DataStructs.TanimotoSimilarity(fp, fp2)  # 0.31
</code></pre>
<hr/>
<h2>2.6 Overall Drug Discovery MI Workflow</h2>
<div class="mermaid">
flowchart TB
    A["Target Identification<br/>Disease-related Protein"] --&gt; B["Compound Library Building<br/>ChEMBL, PubChem, In-house"]
    B --&gt; C["Virtual Screening<br/>QSAR, Docking"]
    C --&gt; D["Hit Compounds<br/>Top 0.1-1%"]
    D --&gt; E["ADMET Prediction<br/>in silico Evaluation"]
    E --&gt; F{ADMET OK?}
    F --&gt;|No| G["Structure Optimization<br/>Scaffold Hopping"]
    G --&gt; E
    F --&gt;|Yes| H["in vitro Validation<br/>Experimental Confirmation"]
    H --&gt; I{Activity OK?}
    I --&gt;|No| J["Active Learning<br/>Model Update"]
    J --&gt; C
    I --&gt;|Yes| K[Lead Compound]
    K --&gt; L["Lead Optimization<br/>Multi-objective Optimization"]
    L --&gt; M[Preclinical Studies]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style K fill:#e8f5e9
    style M fill:#fce4ec
</div>
<p><strong>Time and Cost for Each Stage:</strong></p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Traditional</th>
<th>AI-Enabled</th>
<th>Reduction Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Target Identification</td>
<td>1-2 years</td>
<td>0.5-1 year</td>
<td>50%</td>
</tr>
<tr>
<td>Lead Discovery</td>
<td>2-3 years</td>
<td>0.5-1 year</td>
<td>75%</td>
</tr>
<tr>
<td>Lead Optimization</td>
<td>2-3 years</td>
<td>1-1.5 years</td>
<td>50%</td>
</tr>
<tr>
<td>Preclinical</td>
<td>1-2 years</td>
<td>0.5-1 year</td>
<td>50%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>6-10 years</strong></td>
<td><strong>2.5-4.5 years</strong></td>
<td><strong>60-70%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Cost Reduction:</strong>
- Virtual Screening: $500M ‚Üí $50M (90% reduction)
- ADMET failure reduction: $200M ‚Üí $50M (75% reduction)
- Experimental number reduction: 1,000 experiments ‚Üí 100 experiments (90% reduction)</p>
<hr/>
<h2>Learning Objectives Check</h2>
<p>Upon completing this chapter, you will be able to explain:</p>
<h3>Basic Understanding (Remember &amp; Understand)</h3>
<ul>
<li>‚úÖ Explain 4 types of molecular representations (SMILES, molecular fingerprints, 3D descriptors, Graph)</li>
<li>‚úÖ Understand QSAR principles and 5-step workflow</li>
<li>‚úÖ Explain ADMET 5 items specifically</li>
<li>‚úÖ Understand differences among 4 types of molecular generative models (VAE, GAN, Transformer, RL)</li>
<li>‚úÖ Grasp features of major databases (ChEMBL, PubChem, DrugBank, BindingDB)</li>
</ul>
<h3>Practical Skills (Apply &amp; Analyze)</h3>
<ul>
<li>‚úÖ Calculate Lipinski's Rule of Five and evaluate druglikeness</li>
<li>‚úÖ Calculate molecular similarity using Tanimoto coefficient</li>
<li>‚úÖ Interpret QSAR model performance metrics (R¬≤, ROC-AUC)</li>
<li>‚úÖ Judge development feasibility from ADMET prediction results</li>
<li>‚úÖ Appropriately select machine learning methods (RF, SVM, NN, GNN)</li>
</ul>
<h3>Application Ability (Evaluate &amp; Create)</h3>
<ul>
<li>‚úÖ Design workflow suitable for new drug discovery projects</li>
<li>‚úÖ Evaluate QSAR model Applicability Domain</li>
<li>‚úÖ Judge application scenarios for molecular generative models</li>
<li>‚úÖ Build datasets using ChEMBL API</li>
<li>‚úÖ Optimize overall drug discovery MI flow</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Easy (Basic Confirmation)</h3>
<p><strong>Q1</strong>: What compound does the following SMILES string represent?</p>
<pre><code>CCO
</code></pre>
<p>a) Methanol
b) Ethanol
c) Propanol
d) Butanol</p>
<details>
<summary>View Answer</summary>

**Correct Answer**: b) Ethanol

**Explanation**:
- `C`: Carbon (methyl group)
- `C`: Carbon (methylene group)
- `O`: Oxygen (hydroxyl group)

Structure: CH3-CH2-OH = Ethanol

**Other Options:**
- a) Methanol: `CO`
- c) Propanol: `CCCO`
- d) Butanol: `CCCCO`
</details>
<p><strong>Q2</strong>: In Lipinski's Rule of Five, what is the upper limit for molecular weight?</p>
<details>
<summary>View Answer</summary>

**Correct Answer**: 500 Da

**Lipinski's Rule of Five (Recap):**
1. Molecular Weight (MW): **&lt; 500 Da**
2. LogP (lipophilicity): &lt; 5
3. Hydrogen Bond Donors (HBD): &lt; 5
4. Hydrogen Bond Acceptors (HBA): &lt; 10

Compounds satisfying these tend to have high oral absorption.
</details>
<p><strong>Q3</strong>: A compound has an hERG inhibition IC50 of 0.5 ŒºM. How is this classified in risk assessment?
a) Low risk
b) Medium risk
c) High risk</p>
<details>
<summary>View Answer</summary>

**Correct Answer**: c) High risk

**hERG Risk Assessment Criteria:**
- IC50 &lt; 1 ŒºM: **High risk** (consider development termination)
- 1 &lt; IC50 &lt; 10 ŒºM: Medium risk (careful evaluation needed)
- IC50 &gt; 10 ŒºM: Low risk (safe)

IC50 = 0.5 ŒºM &lt; 1 ŒºM ‚Üí High risk

hERG inhibition can cause fatal arrhythmia (Torsades de pointes),
making it one of the most important toxicity evaluation items in drug discovery.
</details>
<h3>Medium (Application)</h3>
<p><strong>Q4</strong>: When comparing ECFP4 fingerprints (2048 bits) of two molecules, the following results were obtained. Calculate the Tanimoto coefficient.</p>
<pre><code>Molecule A: Number of positions with bit 1 = 250
Molecule B: Number of positions with bit 1 = 280
Number of positions with bit 1 in both = 120
</code></pre>
<details>
<summary>View Answer</summary>

**Correct Answer**: Tanimoto = 0.293

**Calculation:**

<pre><code>Tanimoto = (A ‚à© B) / (A ‚à™ B)

A ‚à© B = 120 (number of bits that are 1 in both)
A ‚à™ B = 250 + 280 - 120 = 410

Tanimoto = 120 / 410 = 0.293
</code></pre>


**Interpretation:**
Tanimoto = 0.293 &lt; 0.50 ‚Üí Structurally different molecules

Similarity guideline:
- &gt; 0.85: Very similar
- 0.70-0.85: Similar
- 0.50-0.70: Somewhat similar
- **&lt; 0.50: Different** (current case)
</details>
<p><strong>Q5</strong>: A QSAR model was built and the following performance was obtained. Is this model practical?</p>
<pre><code>Training data: R¬≤ = 0.92, MAE = 0.3
Test data: R¬≤ = 0.58, MAE = 1.2
</code></pre>
<details>
<summary>View Answer</summary>

**Correct Answer**: Not practical (overfitting)

**Analysis:**
- Training data: R¬≤ = 0.92 (excellent)
- Test data: R¬≤ = 0.58 (insufficient)
- **Difference**: 0.92 - 0.58 = 0.34 (too large)

**Problem: Overfitting**
- Model over-fitted to training data
- Low generalization performance on unknown data
- Test R¬≤ &lt; 0.70 is not practical

**Countermeasures:**
1. Regularization (L1/L2, Dropout)
2. Add training data
3. Feature reduction (use only important descriptors)
4. Simpler model (limit depth of Random Forest trees, etc.)
5. Hyperparameter tuning with cross-validation
</details>
<p><strong>Q6</strong>: A compound's Caco-2 permeability is Papp = 5 √ó 10^-7 cm/s. Evaluate this compound's oral absorption.</p>
<details>
<summary>View Answer</summary>

**Correct Answer**: Low to moderate permeability (somewhat poor absorption)

**Caco-2 Permeability Criteria:**
- Papp &gt; 10^-6 cm/s: **High permeability** (good absorption)
- 10^-7 &lt; Papp &lt; 10^-6 cm/s: **Moderate** (absorption possible but not optimal)
- Papp &lt; 10^-7 cm/s: Low permeability (poor absorption)

**Current Case:**
Papp = 5 √ó 10^-7 cm/s

10^-7 &lt; 5 √ó 10^-7 &lt; 10^-6 ‚Üí Moderate

**Improvement Strategies:**
1. Adjust lipophilicity (optimize LogP)
2. Reduce TPSA (make polar surface area smaller)
3. Reduce number of hydrogen bonds
4. Formulation technology (nanoparticles, liposomes)

However, oral drugs can still be developed with this value (e.g., atenolol).
</details>
<h3>Hard (Advanced)</h3>
<p><strong>Q7</strong>: When generating 10,000 novel molecules with ChemVAE (latent space 196 dimensions), 6,800 were valid SMILES. Of the generated molecules, 40% had a Tanimoto coefficient &gt; 0.95 with the most similar molecule in the training data (ZINC 250K). How do you evaluate this result? What improvement strategies are available?</p>
<details>
<summary>View Answer</summary>

**Evaluation:**

**Positive Aspects:**
- Valid SMILES generation rate = 6,800 / 10,000 = 68%
  ‚Üí Typical ChemVAE performance (matches paper value of 68%)
  ‚Üí Technically successful

**Negative Aspects:**
- Tanimoto &gt; 0.95 = 40% = 4,000 molecules
  ‚Üí Very similar to training data (low novelty)
  ‚Üí Too many "nearly copied" molecules

**Conclusion:**
Insufficient novelty. Close to regenerating known compounds, limited drug discovery value.

**Improvement Strategies:**

1. **Change Latent Space Sampling Strategy**
   ```python
   # Current: Sample from standard normal distribution
   z = np.random.randn(196)

   # Improvement: Sample from regions far from training data
   z = np.random.randn(196) * 2.0  # Increase variance
   ```

2. **Add Penalty Term**
   ```
   Loss = Reconstruction + KL + Œª * Novelty Penalty

   Novelty Penalty = -log(1 - max_tanimoto)
   (Higher penalty for higher maximum similarity to training data)
   ```

3. **Use Conditional VAE**
   - Provide desired properties (LogP, MW, etc.) as conditions
   - Explore regions far from training data in property space

4. **Combine with Reinforcement Learning**
   - Optimize VAE-generated molecules with RL
   - Add novelty term to reward function
   ```
   Reward = Activity + Œª1 * Novelty + Œª2 * Druglikeness
   ```

5. **Consider Junction Tree VAE**
   - Superior at generating novel scaffolds
   - 100% valid SMILES generation rate

**Practical Example:**
Insilico Medicine discovered IPF treatment candidates by combining GAN and RL,
generating molecules with novel scaffolds different from training data.
</details>
<p><strong>Q8</strong>: For the following drug discovery project, which machine learning method should be selected? Explain with reasoning.</p>
<pre><code>Project: Development of EGFR (epidermal growth factor receptor) kinase inhibitor
Data: EGFR activity data from ChEMBL, 15,000 compounds
Task: IC50 prediction (regression)
Goal: R¬≤ &gt; 0.80, prediction time &lt; 1 sec/compound
Additional requirement: Want model interpretability (which structures contribute to activity)
</code></pre>
<details>
<summary>View Answer</summary>

**Recommended Method: Random Forest**

**Reasoning:**

1. **Fit with Data Size**
   - 15,000 compounds = medium-scale data
   - Random Forest optimal performance at 1K-100K
   - Neural Network excels at 10K+, but 15K is borderline
   - Random Forest more stable (less prone to overfitting)

2. **Achieving Performance Goal**
   - ChEMBL EGFR activity prediction benchmark:
     - Random Forest: R¬≤ = 0.82-0.88 (meets goal R¬≤ &gt; 0.80)
     - SVM: R¬≤ = 0.78-0.85 (somewhat unstable)
     - Neural Network: R¬≤ = 0.85-0.90 (high but overkill)
     - LightGBM: R¬≤ = 0.85-0.92 (best performance but lower interpretability)

3. **Prediction Speed**
   - Random Forest: &lt; 0.1 sec/compound (10x faster than goal)
   - Neural Network: 0.5-2 sec/compound (without GPU)
   - Practical even for 1 million compound Virtual Screening

4. **Interpretability (Most Important Requirement)**
   - **Feature Importance**:
     ```python
     importances = model.feature_importances_
     # Rank which descriptors are important for prediction
     # Example: LogP, TPSA, number of aromatic rings, etc.
     ```
   - **SHAP (SHapley Additive exPlanations)**:
     ```python
     import shap
     explainer = shap.TreeExplainer(model)
     shap_values = explainer.shap_values(X_test)
     # Visualize contribution of each feature for each compound
     ```
   - This provides insights like "hydrophobic sites fitting ATP binding pocket are important"

**Implementation Example:**

<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - shap&gt;=0.42.0

"""
Example: Q8: For the following drug discovery project, which machine 

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: 1-5 minutes
Dependencies: None
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import shap

# Model training
rf = RandomForestRegressor(
    n_estimators=500,  # Number of trees (more = more stable)
    max_depth=20,      # Depth limit (prevent overfitting)
    min_samples_leaf=5,
    n_jobs=-1          # Parallelization
)

rf.fit(X_train, y_train)

# Performance evaluation
cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-validation R¬≤: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

# Feature importance
importances = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 10 important features:")
print(importances.head(10))

# SHAP interpretation
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test[:100])  # Sample 100 compounds

shap.summary_plot(shap_values, X_test[:100], feature_names=feature_names)
# ‚Üí Visualize which features contribute to high/low activity
</code></pre>


**Alternative Options (Depending on Situation):**

- **If data increases to 100K+**: LightGBM
  - Higher accuracy (R¬≤ &gt; 0.90 possible)
  - Fast (2-5x faster than Random Forest)
  - Feature Importance also available

- **If interpretability is top priority**: Linear Regression with feature selection
  - Coefficients directly interpretable
  - However, performance around R¬≤ = 0.70 (doesn't meet goal)

- **If highest accuracy needed**: Graph Neural Network (MPNN)
  - R¬≤ = 0.90-0.95
  - However, low interpretability, long training time (hours-days)
  - Partial interpretation possible with Attention weights

**Conclusion:**
Random Forest optimal for balance of performance, speed, and interpretability.
</details>
<hr/>
<h2>Next Steps</h2>
<p>In Chapter 2, you understood drug discovery-specific MI methods. In Chapter 3, you'll implement these methods in Python and actually run them. Through 30 code examples using RDKit and ChEMBL, you'll acquire practical skills.</p>
<p><strong><a href="./chapter3-hands-on.html">Chapter 3: Implementing Drug Discovery MI in Python - RDKit &amp; ChEMBL Practice ‚Üí</a></strong></p>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>G√≥mez-Bombarelli, R., et al. (2018). "Automatic chemical design using a data-driven continuous representation of molecules." <em>ACS Central Science</em>, 4(2), 268-276.</p>
</li>
<li>
<p>Guimaraes, G. L., et al. (2017). "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models." <em>arXiv:1705.10843</em>.</p>
</li>
<li>
<p>Jin, W., Barzilay, R., &amp; Jaakkola, T. (2018). "Junction tree variational autoencoder for molecular graph generation." <em>ICML 2018</em>.</p>
</li>
<li>
<p>Lipinski, C. A., et al. (2001). "Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings." <em>Advanced Drug Delivery Reviews</em>, 46(1-3), 3-26.</p>
</li>
<li>
<p>Popova, M., et al. (2018). "Deep reinforcement learning for de novo drug design." <em>Science Advances</em>, 4(7), eaap7885.</p>
</li>
<li>
<p>Rogers, D., &amp; Hahn, M. (2010). "Extended-connectivity fingerprints." <em>Journal of Chemical Information and Modeling</em>, 50(5), 742-754.</p>
</li>
<li>
<p>Wang, S., et al. (2019). "SMILES-BERT: Large scale unsupervised pre-training for molecular property prediction." <em>BCB 2019</em>.</p>
</li>
<li>
<p>Zhavoronkov, A., et al. (2019). "Deep learning enables rapid identification of potent DDR1 kinase inhibitors." <em>Nature Biotechnology</em>, 37(9), 1038-1040.</p>
</li>
<li>
<p>Gaulton, A., et al. (2017). "The ChEMBL database in 2017." <em>Nucleic Acids Research</em>, 45(D1), D945-D954.</p>
</li>
<li>
<p>Landrum, G. (2023). RDKit: Open-source cheminformatics. https://www.rdkit.org</p>
</li>
</ol>
<hr/>
<p><a href="./index.html">Return to Series Table of Contents</a> | <a href="./chapter1-background.html">Return to Chapter 1</a> | <a href="./chapter3-hands-on.html">Proceed to Chapter 3 ‚Üí</a></p>
<div class="navigation">
<a class="nav-button" href="chapter1-background.html">‚Üê Chapter 1</a>
<a class="nav-button" href="index.html">Series Table of Contents</a>
<a class="nav-button" href="chapter3-hands-on.html">Chapter 3 ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, functionality, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>The author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content shall follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>AI Terakoya Materials Informatics Educational Content</strong></p>
<p>¬© 2025 AI Terakoya. Licensed under CC BY 4.0</p>
</footer>
</body>
</html>