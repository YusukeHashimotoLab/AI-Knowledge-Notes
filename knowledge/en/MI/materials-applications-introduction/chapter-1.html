<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: AI-Driven Drug Discovery in Practice - Accelerating Novel Drug Candidate Discovery 10x - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../MI/materials-applications-introduction/index.html">Materials Applications</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/materials-applications-introduction/chapter-1.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 1: AI-Driven Drug Discovery in Practice - Accelerating Novel Drug Candidate Discovery 10x</h1>
<p class="subtitle">Practical skills learned from four AI drug discovery approaches and real success stories</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-25 min</span>
<span class="meta-item">üìä Level: Beginner to Intermediate</span>
<span class="meta-item">üíª Code examples: 8</span>
<span class="meta-item">üìù Exercises: 2</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 1: AI-Driven Drug Discovery in Practice - Accelerating Novel Drug Candidate Discovery 10x</h1><p class="chapter-description">This chapter covers AI. You will learn four real success stories with technical details and Evaluate the current state of AI drug discovery.</p>


<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Quantitatively explain drug discovery process challenges and traditional method limitations</li>
<li>‚úÖ Understand the four main AI drug discovery approaches (virtual screening, molecular generation, ADMET prediction, protein structure prediction)</li>
<li>‚úÖ Explain four real success stories with technical details</li>
<li>‚úÖ Implement RDKit, molecular VAE, and binding affinity prediction in Python</li>
<li>‚úÖ Evaluate the current state of AI drug discovery and outlook for the next 5 years</li>
</ul>
<hr/>
<h2>1.1 Challenges in Drug Discovery</h2>
<h3>1.1.1 Traditional Drug Discovery: Time and Cost Barriers</h3>
<p>New drug development is a crucial research field directly linked to human health, but the journey is far more arduous than one might imagine.</p>
<p><strong>Realistic Numbers</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Traditional Method</th>
<th>AI Drug Discovery (Actual)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Development Period</strong></td>
<td>10-15 years</td>
<td>1.5-4 years (until candidate discovery)</td>
</tr>
<tr>
<td><strong>Total Cost</strong></td>
<td>¬•20-30 billion</td>
<td>¬•2-5 billion (10-15%)</td>
</tr>
<tr>
<td><strong>Candidate Compounds</strong></td>
<td>5,000-10,000</td>
<td>100-500</td>
</tr>
<tr>
<td><strong>Success Rate</strong></td>
<td>0.02% (2 out of 10,000)</td>
<td>0.5-1% (improving)</td>
</tr>
<tr>
<td><strong>Clinical Trial Failure Rate</strong></td>
<td>90% (Phase I-III total)</td>
<td>75-85% (improving trend)</td>
</tr>
</tbody>
</table>
<p><strong>Source</strong>: Wouters et al. (2020), <em>Nature Reviews Drug Discovery</em>; Paul et al. (2010), <em>Nature Reviews Drug Discovery</em></p>
<h3>1.1.2 Stages of Drug Discovery Process</h3>
<p>The traditional drug discovery process goes through the following stages:</p>
<div class="mermaid">
flowchart LR
    A[Target Identification\n1-2 years] --&gt; B[Hit Compound Discovery\n2-4 years]
    B --&gt; C[Lead Optimization\n2-3 years]
    C --&gt; D[Preclinical Trials\n1-2 years]
    D --&gt; E[Phase I\n1-2 years]
    E --&gt; F[Phase II\n2-3 years]
    F --&gt; G[Phase III\n2-4 years]
    G --&gt; H[Approval Application\n1-2 years]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#e3f2fd
    style E fill:#f3e5f5
    style F fill:#fce4ec
    style G fill:#e0f2f1
    style H fill:#f9fbe7
</div>
<p><strong>Challenges at Each Stage</strong>:</p>
<ol>
<li>
<p><strong>Hit Compound Discovery</strong> (Biggest Bottleneck)
   - Search space: Over 10^60 chemical structures
   - Traditional method: Screen 10,000-50,000 compounds annually
   - Success rate: 0.01-0.1% hit rate</p>
</li>
<li>
<p><strong>ADMET Property Issues</strong> (Main cause of clinical trial failures)
   - <strong>A</strong>bsorption: Can it reach the bloodstream after oral administration?
   - <strong>D</strong>istribution: Can it reach target tissues?
   - <strong>M</strong>etabolism: Will it resist degradation in the body?
   - <strong>E</strong>xcretion: Can it be properly eliminated?
   - <strong>T</strong>oxicity: Are side effects within acceptable range?</p>
</li>
</ol>
<p>‚Üí 60% of Phase II failures are due to ADMET issues (Kola &amp; Landis, 2004)</p>
<ol start="3">
<li><strong>High Clinical Trial Failure Rates</strong>
   - Phase I: Safety confirmation (success rate: 70%)
   - Phase II: Efficacy confirmation (success rate: 30-40%) ‚Üê Greatest challenge
   - Phase III: Large-scale verification (success rate: 50-60%)</li>
</ol>
<p><strong>Case Study: Alzheimer's Disease Therapeutics</strong></p>
<p>Over a 10-year period from 2002-2012, 146 clinical trials were conducted, but only <strong>one</strong> new drug was approved (success rate 0.7%). Total development costs were estimated at over <strong>¬•2 trillion</strong>.</p>
<p><em>Source</em>: Cummings et al. (2014), <em>Alzheimer's Research &amp; Therapy</em></p>
<hr/>
<h2>1.2 AI Drug Discovery Approaches</h2>
<h3>1.2.1 Virtual Screening</h3>
<p><strong>Concept</strong>: Predict interactions between large numbers of compounds and target proteins computationally to narrow down promising candidates.</p>
<p><strong>Comparison with Traditional Methods</strong>:</p>
<table>
<thead>
<tr>
<th>Item</th>
<th>Traditional (High-Throughput Screening)</th>
<th>AI (Virtual Screening)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Screening Volume</td>
<td>10,000-100,000 compounds/year</td>
<td>1,000,000-10,000,000 compounds/week</td>
</tr>
<tr>
<td>Cost</td>
<td>¬•5,000-10,000/compound</td>
<td>¬•10-100/compound</td>
</tr>
<tr>
<td>Success Rate</td>
<td>0.01-0.1%</td>
<td>1-5% (100x improvement)</td>
</tr>
<tr>
<td>Duration</td>
<td>6-12 months</td>
<td>1-2 weeks</td>
</tr>
</tbody>
</table>
<p><strong>Technical Components</strong>:</p>
<ol>
<li><strong>Docking Simulation</strong>: Physically simulate protein-compound binding</li>
<li><strong>Machine Learning Scoring</strong>: ML models to predict binding affinity</li>
<li><strong>Deep Learning Structure Recognition</strong>: Learn from 3D structures of protein-ligand complexes</li>
</ol>
<h3>1.2.2 Molecular Generative Models</h3>
<p><strong>Concept</strong>: AI "creates" novel compounds. Design completely new molecular structures that don't exist in conventional compound libraries.</p>
<p><strong>Major Technologies</strong>:</p>
<h4>1. <strong>VAE (Variational Autoencoder)</strong></h4>
<ul>
<li>Compress molecular structures into low-dimensional latent space</li>
<li>Optimize in latent space ‚Üí Generate new molecules</li>
<li><strong>Advantages</strong>: Smooth latent space, generate similar molecules through interpolation</li>
<li><strong>Disadvantages</strong>: Limited diversity in generated molecules</li>
</ul>
<pre><code>SMILES notation ‚Üí Encoder ‚Üí Latent vector (128-dim)
Latent vector ‚Üí Decoder ‚Üí New SMILES
</code></pre>
<h4>2. <strong>GAN (Generative Adversarial Network)</strong></h4>
<ul>
<li>Adversarial training between Generator and Discriminator</li>
<li><strong>Advantages</strong>: Diverse molecule generation</li>
<li><strong>Disadvantages</strong>: Unstable training, mode collapse</li>
</ul>
<h4>3. <strong>Transformer (Latest Trend)</strong></h4>
<ul>
<li>Apply natural language processing techniques to molecular design</li>
<li>Treat SMILES strings as language</li>
<li><strong>Examples</strong>: ChemBERTa, MolBERT, MolGPT</li>
<li><strong>Advantages</strong>: Pre-training on large-scale data, transfer learning capability</li>
</ul>
<h4>4. <strong>Graph Neural Networks (GNN)</strong></h4>
<ul>
<li>Treat molecules as graphs with atoms (nodes) and bonds (edges)</li>
<li><strong>Advantages</strong>: Direct handling of 3D structures</li>
<li><strong>Examples</strong>: SchNet, DimeNet</li>
</ul>
<h3>1.2.3 ADMET Prediction</h3>
<p><strong>Concept</strong>: Predict ADMET properties of candidate compounds in advance to reduce clinical trial failure risk.</p>
<p><strong>Prediction Targets</strong>:</p>
<ol>
<li>
<p><strong>Drug-likeness</strong>
   - Lipinski's Rule of Five</p>
<ul>
<li>Molecular weight &lt; 500 Da</li>
<li>LogP (lipophilicity) &lt; 5</li>
<li>Hydrogen bond donors &lt; 5</li>
<li>Hydrogen bond acceptors &lt; 10</li>
<li>One violation allowed (75% of oral drugs satisfy this)</li>
</ul>
</li>
<li>
<p><strong>Solubility</strong>
   - Essential for oral absorption
   - Prediction accuracy: R¬≤ = 0.7-0.8 (latest models)</p>
</li>
<li>
<p><strong>Permeability</strong>
   - Blood-brain barrier penetration (essential for CNS drugs)
   - Caco-2 cell permeability</p>
</li>
<li>
<p><strong>Toxicity</strong>
   - hERG inhibition (cardiotoxicity)
   - Hepatotoxicity
   - Carcinogenicity</p>
</li>
</ol>
<p><strong>Machine Learning Prediction</strong>:</p>
<ul>
<li><strong>Input</strong>: Molecular descriptors (200-1,000 dimensions)</li>
<li><strong>Models</strong>: Random Forest, XGBoost, DNN</li>
<li><strong>Accuracy</strong>: AUC 0.75-0.95 (varies by property)</li>
</ul>
<h3>1.2.4 Protein Structure Prediction (AlphaFold Application)</h3>
<p><strong>The AlphaFold 2 Revolution</strong>:</p>
<p>In 2020, DeepMind's AlphaFold 2 achieved accuracy comparable to experimental methods (X-ray crystallography, Cryo-EM) in the protein structure prediction problem (CASP14 competition).</p>
<p><strong>Accuracy</strong>: GDT (Global Distance Test) score 90+ (equivalent to experimental methods)</p>
<p><strong>Applications to Drug Discovery</strong>:</p>
<ol>
<li>
<p><strong>Target Protein Structure Determination</strong>
   - Experimental structure analysis: 6 months - 2 years, millions of yen
   - AlphaFold: Several hours, free</p>
</li>
<li>
<p><strong>Drug-Target Interaction Prediction</strong>
   - Protein structure + compound ‚Üí Binding affinity prediction
   - Accuracy: R¬≤ = 0.6-0.8 (20% improvement over traditional methods)</p>
</li>
<li>
<p><strong>Discovery of New Targets</strong>
   - Proteins with unknown structure ‚Üí AlphaFold ‚Üí Evaluate as drug targets</p>
</li>
</ol>
<p><strong>Source</strong>: Jumper et al. (2021), <em>Nature</em>; Varadi et al. (2022), <em>Nucleic Acids Research</em></p>
<hr/>
<h2>1.3 Real-World Success Stories</h2>
<h3>Case 1: Exscientia √ó Sumitomo Pharma - World's First AI-Designed Drug Enters Clinical Trials</h3>
<p><strong>Background</strong>:</p>
<ul>
<li><strong>Companies</strong>: Exscientia (UK startup, founded 2012) √ó Sumitomo Pharma (formerly Dainippon Sumitomo Pharma)</li>
<li><strong>Disease</strong>: Obsessive-Compulsive Disorder (OCD)</li>
<li><strong>Target</strong>: 5-HT1A receptor (serotonin receptor)</li>
</ul>
<p><strong>Technical Details</strong>:</p>
<ol>
<li>
<p><strong>Reinforcement Learning-Based Molecular Design</strong>
   - <strong>Algorithm</strong>: Deep Q-Learning + Monte Carlo Tree Search
   - <strong>Design Process</strong>:
     <code>Existing compound ‚Üí Structural transformation (atom substitution, bond addition/deletion)
                 ‚Üí ADMET prediction ‚Üí Reward calculation
                 ‚Üí Reinforcement learning optimization</code>
   - <strong>Search Space</strong>: 10^23 chemical structures</p>
</li>
<li>
<p><strong>Active Learning</strong>
   - Prediction model selects high-uncertainty compounds ‚Üí Experimental validation ‚Üí Model update
   - Number of cycles: 15 (traditional: 50-100)</p>
</li>
</ol>
<p><strong>Results</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Traditional Method</th>
<th>Exscientia AI</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Candidate Discovery Period</strong></td>
<td>4.5 years (average)</td>
<td><strong>12 months</strong></td>
</tr>
<tr>
<td><strong>Synthesized Compounds</strong></td>
<td>3,000-5,000</td>
<td><strong>350</strong></td>
</tr>
<tr>
<td><strong>Lead Compounds</strong></td>
<td>10-20</td>
<td><strong>5</strong></td>
</tr>
<tr>
<td><strong>Clinical Trial Start</strong></td>
<td>-</td>
<td>2020 (Phase I completed)</td>
</tr>
</tbody>
</table>
<p><strong>Impact</strong>:</p>
<ul>
<li>January 2020: World's first AI-designed drug entered clinical trials (Phase I)</li>
<li>Development period reduced by <strong>75%</strong> (4.5 years ‚Üí 1 year)</li>
<li>Number of synthesized compounds reduced by <strong>90%</strong></li>
</ul>
<p><strong>Source</strong>: Exscientia Press Release (2020); Zhavoronkov et al. (2019), <em>Nature Biotechnology</em></p>
<p><strong>Comment</strong>:</p>
<blockquote>
<p>"With AI, we were able to expand the hypothesis space for compound design by over 100 times compared to traditional methods. It proposes structures that human chemists would never think of."
‚Äî Andrew Hopkins, CEO, Exscientia</p>
</blockquote>
<hr/>
<h3>Case 2: Atomwise - AI Drug Discovery Platform for Multiple Diseases</h3>
<p><strong>Background</strong>:</p>
<ul>
<li><strong>Company</strong>: Atomwise (USA, founded 2012)</li>
<li><strong>Technology</strong>: AtomNet (deep learning-based virtual screening)</li>
<li><strong>Target Diseases</strong>: COVID-19, Ebola, Multiple Sclerosis, Cancer</li>
</ul>
<p><strong>Technical Details</strong>:</p>
<h4>AtomNet Architecture</h4>
<ol>
<li>
<p><strong>3D Convolutional Neural Network (3D-CNN)</strong>
   - Input: 3D structure of protein-ligand complex (Voxel representation)
   - Output: Binding affinity score (pKd)
   - <strong>Accuracy</strong>: Pearson correlation coefficient r = 0.73 (traditional method: r = 0.55)</p>
</li>
<li>
<p><strong>Training Data</strong>:
   - PDBbind database: 15,000 protein-ligand complexes
   - Internal data: 100,000+ binding affinity measurements</p>
</li>
<li>
<p><strong>Screening Speed</strong>:
   - Screen 10 million compounds in <strong>72 hours</strong>
   - Traditional method (docking): 6-12 months</p>
</li>
</ol>
<p><strong>Success Case: COVID-19 Drug Discovery</strong></p>
<ul>
<li><strong>Timeline</strong>: March 2020 (immediately after pandemic declaration)</li>
<li><strong>Screening</strong>: 10 million compounds ‚Üí 72 candidates</li>
<li><strong>Results</strong>:</li>
<li>Rediscovered 6 existing approved drugs (Drug Repurposing)</li>
<li>Identified 2 new candidates ‚Üí Proceeded to preclinical trials</li>
</ul>
<p><strong>Other Achievements</strong>:</p>
<ol>
<li>
<p><strong>Ebola Hemorrhagic Fever</strong> (2015)
   - Screened existing compound library (7,000 compounds)
   - 2 compounds showed efficacy in cell experiments
   - Publication: Ekins et al. (2015), <em>F1000Research</em></p>
</li>
<li>
<p><strong>Multiple Sclerosis</strong> (2016)
   - Collaboration with AbbVie
   - Discovered hit compounds 10x faster than traditional methods</p>
</li>
</ol>
<p><strong>Business Model</strong>:</p>
<ul>
<li>License AtomNet AI platform to pharmaceutical companies</li>
<li>Contracts: 50+ companies (as of 2023)</li>
<li>Funding raised: Total $174M (approx. ¬•20 billion)</li>
</ul>
<p><strong>Source</strong>: Wallach et al. (2015), <em>arXiv</em>; Atomwise official website</p>
<hr/>
<h3>Case 3: Insilico Medicine - Clinical Trials in 18 Months</h3>
<p><strong>Background</strong>:</p>
<ul>
<li><strong>Company</strong>: Insilico Medicine (Hong Kong/USA, founded 2014)</li>
<li><strong>Disease</strong>: Idiopathic Pulmonary Fibrosis (IPF)</li>
<li><strong>Target</strong>: TNIK (Traf2 and Nck Interacting Kinase)</li>
</ul>
<p><strong>Technical Details</strong>:</p>
<h4>Generative Chemistry Platform</h4>
<ol>
<li>
<p><strong>Pharma.AI</strong>: Integrated AI platform
   - <strong>PandaOmics</strong>: Target identification (Omics data analysis)
   - <strong>Chemistry42</strong>: Molecule generation
   - <strong>InClinico</strong>: Clinical trial outcome prediction</p>
</li>
<li>
<p><strong>Molecule Generation Process</strong>:
   <code>Target protein (TNIK)
     ‚Üì
   Molecule generation via GAN/RL (30,000 candidates)
     ‚Üì
   ADMET prediction filter (80 candidates)
     ‚Üì
   Synthetic accessibility evaluation (40 candidates)
     ‚Üì
   Experimental validation (6 candidates)
     ‚Üì
   Lead compound (ISM001-055)</code></p>
</li>
<li>
<p><strong>Reinforcement Learning Optimization</strong>
   - Reward function:
     <code>Reward = 0.4 √ó Binding affinity
            + 0.3 √ó Drug-likeness
            + 0.2 √ó Synthetic accessibility
            + 0.1 √ó Novelty</code></p>
</li>
</ol>
<p><strong>Results</strong>:</p>
<table>
<thead>
<tr>
<th>Milestone</th>
<th>Traditional Method</th>
<th>Insilico AI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Target Identification</td>
<td>6-12 months</td>
<td><strong>21 days</strong></td>
</tr>
<tr>
<td>Hit Compound Discovery</td>
<td>1-2 years</td>
<td><strong>46 days</strong></td>
</tr>
<tr>
<td>Lead Optimization</td>
<td>1-2 years</td>
<td><strong>18 months</strong> (entire process)</td>
</tr>
<tr>
<td>Preclinical Trial Start</td>
<td>3-5 years</td>
<td><strong>18 months</strong></td>
</tr>
<tr>
<td>Phase I Start</td>
<td>5-7 years</td>
<td><strong>30 months</strong></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Total Cost</strong>: $2.6M (approx. ¬•300 million) ‚Üê <strong>1/10</strong> of traditional</li>
<li><strong>Phase I Start</strong>: June 2023 (approved in China)</li>
</ul>
<p><strong>Scientific Validation</strong>:</p>
<ul>
<li><strong>In vitro experiments</strong>: IC50 = 8.3 nM (very strong inhibitory activity)</li>
<li><strong>Animal experiments</strong>: Efficacy confirmed in mouse pulmonary fibrosis model</li>
<li><strong>Safety</strong>: Passed toxicity tests</li>
</ul>
<p><strong>Impact</strong>:</p>
<p>This represents a record-breaking achievement where an AI-designed compound went from <strong>target identification to clinical trial start in 18 months</strong>, the fastest in history.</p>
<p><strong>Source</strong>: Zhavoronkov et al. (2019), <em>Nature Biotechnology</em>; Ren et al. (2023), <em>Nature Biotechnology</em></p>
<p><strong>CEO Comment</strong>:</p>
<blockquote>
<p>"We were able to reduce a process that traditionally takes 5-7 years to 18 months. AI is fundamentally transforming the drug discovery paradigm."
‚Äî Alex Zhavoronkov, PhD, CEO, Insilico Medicine</p>
</blockquote>
<hr/>
<h3>Case 4: Takeda Pharmaceutical - Japanese Company's Challenge</h3>
<p><strong>Background</strong>:</p>
<ul>
<li><strong>Company</strong>: Takeda Pharmaceutical (Japan's largest pharmaceutical company)</li>
<li><strong>Strategy</strong>: Large-scale investment in AI drug discovery (started 2019)</li>
<li><strong>AI Drug Discovery Unit</strong>: Takeda Data and Analytics (TxDA)</li>
</ul>
<p><strong>Initiatives</strong>:</p>
<h4>1. <strong>Partnership with Recursion Pharmaceuticals</strong></h4>
<ul>
<li><strong>Contract Value</strong>: $50M + milestone payments (up to $300M)</li>
<li><strong>Duration</strong>: 2020 contract (5 years)</li>
<li><strong>Technology</strong>: Image-based Drug Discovery</li>
</ul>
<p><strong>Image-based Drug Discovery Mechanism</strong>:</p>
<pre><code>Treat cells with compounds
  ‚Üì
High-resolution microscopy imaging (1,000,000+ images)
  ‚Üì
Convolutional Neural Network (ResNet-50 based)
  ‚Üì
Learn cellular morphology changes
  ‚Üì
Identify compounds that normalize diseased cells
</code></pre>
<p><strong>Results</strong>:
- Target diseases: 15 types (focus on rare diseases)
- Existing compound library: Screened 20,000 compounds
- Hit compounds: 30 identified (as of 2023)</p>
<h4>2. <strong>Collaboration with Schr√∂dinger</strong></h4>
<ul>
<li><strong>Technology</strong>: Physics-based Molecular Dynamics + AI</li>
<li><strong>Target</strong>: Challenging targets (GPCRs, Ion Channels)</li>
<li><strong>Duration</strong>: 2020 contract</li>
</ul>
<h4>3. <strong>In-house AI Drug Discovery Platform</strong></h4>
<ul>
<li><strong>Investment</strong>: ¬•10 billion annually (estimated)</li>
<li><strong>Personnel</strong>: 100+ data scientists</li>
<li><strong>Infrastructure</strong>:</li>
<li>Supercomputer: NVIDIA DGX A100 (multiple units)</li>
<li>Cloud: AWS, Google Cloud</li>
<li>Database: Internal compound data (3,000,000+)</li>
</ul>
<p><strong>Challenges and Opportunities for Japanese Companies</strong>:</p>
<p><strong>Challenges</strong>:
- AI talent shortage (especially dual expertise in chemistry √ó ML)
- Data silos (limited data sharing between pharmaceutical companies)
- Regulatory compliance (PMDA AI drug guidelines under development)</p>
<p><strong>Opportunities</strong>:
- Rich clinical data (strength of Japan's healthcare system)
- Robotics technology (automated experimental systems)
- Government support (AMED "AI Drug Discovery Support Platform")</p>
<p><strong>Source</strong>: Takeda Pharmaceutical Press Release (2020); Recursion official website</p>
<hr/>
<h2>1.4 Technical Explanation and Implementation Examples</h2>
<h3>1.4.1 Molecular Descriptors and Drug-likeness Assessment (RDKit)</h3>
<p><strong>RDKit</strong> is an open-source cheminformatics library. It provides functionality for molecular structure processing, descriptor calculation, similarity searches, and more.</p>
<h4>Code Example 1: Lipinski's Rule of Five Assessment</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
import pandas as pd

def check_drug_likeness(smiles):
    """
    Assess molecular drug-likeness using Lipinski's Rule of Five

    Parameters:
    -----------
    smiles : str
        Molecular structure in SMILES notation

    Returns:
    --------
    is_drug_like : bool
        Whether drug-like (True if ‚â§1 violation)
    properties : dict
        Physicochemical properties of the molecule
    """
    # Generate molecule object from SMILES string
    mol = Chem.MolFromSmiles(smiles)

    if mol is None:
        return False, {"error": "Invalid SMILES"}

    # Calculate Lipinski descriptors
    mw = Descriptors.MolWt(mol)           # Molecular Weight
    logp = Descriptors.MolLogP(mol)       # LogP (lipophilicity)
    hbd = Lipinski.NumHDonors(mol)        # Number of hydrogen bond donors
    hba = Lipinski.NumHAcceptors(mol)     # Number of hydrogen bond acceptors
    rotatable = Descriptors.NumRotatableBonds(mol)  # Number of rotatable bonds
    tpsa = Descriptors.TPSA(mol)          # Topological polar surface area

    # Lipinski's Rule of Five assessment
    violations = 0
    if mw &gt; 500:
        violations += 1
    if logp &gt; 5:
        violations += 1
    if hbd &gt; 5:
        violations += 1
    if hba &gt; 10:
        violations += 1

    # Allow up to 1 violation (extension of Pfizer's Rule of Five)
    is_drug_like = violations &lt;= 1

    properties = {
        'Molecular Weight': round(mw, 2),
        'LogP': round(logp, 2),
        'H-Bond Donors': hbd,
        'H-Bond Acceptors': hba,
        'Rotatable Bonds': rotatable,
        'TPSA': round(tpsa, 2),
        'Lipinski Violations': violations,
        'Drug-like': is_drug_like
    }

    return is_drug_like, properties


# Example: Drug-likeness assessment for approved drugs
drug_examples = {
    'Aspirin': 'CC(=O)Oc1ccccc1C(=O)O',
    'Ibuprofen': 'CC(C)Cc1ccc(cc1)C(C)C(=O)O',
    'Penicillin G': 'CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O',
    'Morphine': 'CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)O)O[C@H]3[C@H](C=C4)O',
    'Taxol (Paclitaxel)': 'CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(C(=O)[C@@H]2OC(=O)C)C)O)OC(=O)C)OC(=O)c5ccccc5)(C[C@@H]1OC(=O)[C@H](O)[C@@H](NC(=O)c6ccccc6)c7ccccc7)O)(C)C'
}

print("=" * 80)
print("Drug-likeness Assessment for Approved Drugs (Lipinski's Rule of Five)")
print("=" * 80)

results = []
for name, smiles in drug_examples.items():
    is_drug_like, props = check_drug_likeness(smiles)
    props['Drug Name'] = name
    results.append(props)

    print(f"\n„Äê{name}„Äë")
    print(f"  SMILES: {smiles}")
    print(f"  Molecular Weight: {props['Molecular Weight']} Da")
    print(f"  LogP: {props['LogP']}")
    print(f"  H-Bond Donors: {props['H-Bond Donors']}")
    print(f"  H-Bond Acceptors: {props['H-Bond Acceptors']}")
    print(f"  Lipinski Violations: {props['Lipinski Violations']}")
    print(f"  ‚Üí Drug-like: {'‚úì YES' if is_drug_like else '‚úó NO'}")

# Display results in DataFrame
df = pd.DataFrame(results)
df = df[['Drug Name', 'Molecular Weight', 'LogP', 'H-Bond Donors',
         'H-Bond Acceptors', 'Lipinski Violations', 'Drug-like']]
print("\n" + "=" * 80)
print("Summary Table:")
print("=" * 80)
print(df.to_string(index=False))

# Statistics
print("\n" + "=" * 80)
print("Statistics:")
print(f"  Drug-like compounds: {df['Drug-like'].sum()} / {len(df)} ({df['Drug-like'].sum()/len(df)*100:.1f}%)")
print("=" * 80)
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>================================================================================
Drug-likeness Assessment for Approved Drugs (Lipinski's Rule of Five)
================================================================================

„ÄêAspirin„Äë
  SMILES: CC(=O)Oc1ccccc1C(=O)O
  Molecular Weight: 180.16 Da
  LogP: 1.19
  H-Bond Donors: 1
  H-Bond Acceptors: 4
  Lipinski Violations: 0
  ‚Üí Drug-like: ‚úì YES

„ÄêIbuprofen„Äë
  SMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O
  Molecular Weight: 206.28 Da
  LogP: 3.50
  H-Bond Donors: 1
  H-Bond Acceptors: 2
  Lipinski Violations: 0
  ‚Üí Drug-like: ‚úì YES

„ÄêPenicillin G„Äë
  SMILES: CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O
  Molecular Weight: 334.39 Da
  LogP: 1.83
  H-Bond Donors: 2
  H-Bond Acceptors: 5
  Lipinski Violations: 0
  ‚Üí Drug-like: ‚úì YES

„ÄêMorphine„Äë
  SMILES: CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)O)O[C@H]3[C@H](C=C4)O
  Molecular Weight: 285.34 Da
  LogP: 0.89
  H-Bond Donors: 2
  H-Bond Acceptors: 5
  Lipinski Violations: 0
  ‚Üí Drug-like: ‚úì YES

„ÄêTaxol (Paclitaxel)„Äë
  SMILES: CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(C(=O)[C@@H]2OC(=O)C)C)O)OC(=O)C)OC(=O)c5ccccc5)(C[C@@H]1OC(=O)[C@H](O)[C@@H](NC(=O)c6ccccc6)c7ccccc7)O)(C)C
  Molecular Weight: 853.91 Da
  LogP: 3.50
  H-Bond Donors: 4
  H-Bond Acceptors: 14
  Lipinski Violations: 2
  ‚Üí Drug-like: ‚úó NO

================================================================================
Summary Table:
================================================================================
   Drug Name  Molecular Weight  LogP  H-Bond Donors  H-Bond Acceptors  Lipinski Violations  Drug-like
     Aspirin            180.16  1.19              1                 4                    0       True
   Ibuprofen            206.28  3.50              1                 2                    0       True
Penicillin G            334.39  1.83              2                 5                    0       True
    Morphine            285.34  0.89              2                 5                    0       True
       Taxol            853.91  3.50              4                14                    2      False

================================================================================
Statistics:
  Drug-like compounds: 4 / 5 (80.0%)
================================================================================
</code></pre>
<p><strong>Code Explanation</strong>:</p>
<ol>
<li>
<p><strong>RDKit Basic Operations</strong>:
   - <code>Chem.MolFromSmiles()</code>: Generate molecule object from SMILES string
   - <code>Descriptors</code> module: Calculate over 200 molecular descriptors</p>
</li>
<li>
<p><strong>Lipinski's Rule of Five</strong>:
   - Empirical rule satisfied by 75% of orally bioavailable drugs
   - One violation is permissible (e.g., many antibiotics)</p>
</li>
<li>
<p><strong>Key Points</strong>:
   - Taxol (anticancer drug) violates Lipinski's rule but is an effective drug (administered intravenously)
   - This demonstrates that Lipinski's rule doesn't apply to non-oral drugs</p>
</li>
</ol>
<p><strong>Applications</strong>:
- Filtering large compound libraries
- Automatic selection of drug-like compounds
- Preprocessing for ADMET prediction</p>
<hr/>
<h3>1.4.2 Novel Molecular Generation with Molecular VAE</h3>
<p><strong>Variational Autoencoder (VAE)</strong> is a deep learning model that compresses molecular structures into a low-dimensional latent space and generates novel molecules.</p>
<h4>Code Example 2: Simplified Molecular VAE</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np

class MolecularVAE(nn.Module):
    """
    Simplified Molecular VAE (SMILES string-based)

    Architecture:
    - Encoder: SMILES ‚Üí latent vector (mean Œº, variance œÉ¬≤)
    - Decoder: latent vector ‚Üí SMILES
    - Loss: reconstruction error + KL divergence
    """

    def __init__(self, vocab_size=50, max_length=120, latent_dim=128):
        """
        Parameters:
        -----------
        vocab_size : int
            SMILES vocabulary size (number of character types)
        max_length : int
            Maximum SMILES length
        latent_dim : int
            Latent space dimensionality
        """
        super(MolecularVAE, self).__init__()

        self.vocab_size = vocab_size
        self.max_length = max_length
        self.latent_dim = latent_dim

        # Encoder (SMILES ‚Üí latent vector)
        self.encoder_embedding = nn.Embedding(vocab_size, 128)
        self.encoder_gru = nn.GRU(128, 256, num_layers=2, batch_first=True)

        # Projection to latent space
        self.fc_mu = nn.Linear(256, latent_dim)       # mean Œº
        self.fc_logvar = nn.Linear(256, latent_dim)   # log(variance)

        # Decoder (latent vector ‚Üí SMILES)
        self.decoder_fc = nn.Linear(latent_dim, 256)
        self.decoder_gru = nn.GRU(256, 256, num_layers=2, batch_first=True)
        self.decoder_output = nn.Linear(256, vocab_size)

    def encode(self, x):
        """
        Encoder: SMILES ‚Üí latent distribution (Œº, œÉ¬≤)

        Parameters:
        -----------
        x : torch.Tensor, shape (batch, max_length)
            SMILES strings (integer encoding)

        Returns:
        --------
        mu : torch.Tensor, shape (batch, latent_dim)
            Mean of latent distribution
        logvar : torch.Tensor, shape (batch, latent_dim)
            Log variance of latent distribution
        """
        # Embedding
        embedded = self.encoder_embedding(x)  # (batch, max_length, 128)

        # GRU (use final hidden state)
        _, hidden = self.encoder_gru(embedded)  # hidden: (2, batch, 256)
        hidden = hidden[-1]  # Final layer hidden state: (batch, 256)

        # Latent distribution parameters
        mu = self.fc_mu(hidden)           # (batch, latent_dim)
        logvar = self.fc_logvar(hidden)   # (batch, latent_dim)

        return mu, logvar

    def reparameterize(self, mu, logvar):
        """
        Reparameterization Trick: z = Œº + œÉ * Œµ (Œµ ~ N(0, I))

        This makes stochastic sampling differentiable for gradient computation
        """
        std = torch.exp(0.5 * logvar)  # œÉ = exp(0.5 * log(œÉ¬≤))
        eps = torch.randn_like(std)    # Œµ ~ N(0, I)
        z = mu + std * eps             # z ~ N(Œº, œÉ¬≤)
        return z

    def decode(self, z, max_length=None):
        """
        Decoder: latent vector ‚Üí SMILES

        Parameters:
        -----------
        z : torch.Tensor, shape (batch, latent_dim)
            Latent vector
        max_length : int, optional
            Maximum SMILES generation length (defaults to self.max_length if None)

        Returns:
        --------
        output : torch.Tensor, shape (batch, max_length, vocab_size)
            Character probability distribution at each position
        """
        if max_length is None:
            max_length = self.max_length

        batch_size = z.size(0)

        # Convert latent vector to initial hidden state
        hidden = self.decoder_fc(z)  # (batch, 256)
        hidden = hidden.unsqueeze(0).repeat(2, 1, 1)  # (2, batch, 256)

        # Decoder input (latent vector at all timesteps)
        decoder_input = z.unsqueeze(1).repeat(1, max_length, 1)  # (batch, max_length, latent_dim)

        # Pad to 256 dimensions
        decoder_input = F.pad(decoder_input, (0, 256 - self.latent_dim))

        # GRU
        output, _ = self.decoder_gru(decoder_input, hidden)  # (batch, max_length, 256)

        # Character probability at each position
        output = self.decoder_output(output)  # (batch, max_length, vocab_size)

        return output

    def forward(self, x):
        """
        Forward pass: x ‚Üí encode ‚Üí sampling ‚Üí decode

        Returns:
        --------
        recon_x : Reconstructed SMILES
        mu : Mean of latent distribution
        logvar : Log variance of latent distribution
        """
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

    def generate(self, num_samples=10, device='cpu'):
        """
        Generate novel molecules by random sampling from latent space

        Parameters:
        -----------
        num_samples : int
            Number of molecules to generate
        device : str
            'cpu' or 'cuda'

        Returns:
        --------
        samples : torch.Tensor, shape (num_samples, max_length, vocab_size)
            Generated molecules (probability distribution)
        """
        self.eval()
        with torch.no_grad():
            # Sample from standard normal distribution
            z = torch.randn(num_samples, self.latent_dim).to(device)
            samples = self.decode(z)
        return samples


def vae_loss(recon_x, x, mu, logvar):
    """
    VAE loss function = reconstruction error + KL divergence

    Parameters:
    -----------
    recon_x : torch.Tensor, shape (batch, max_length, vocab_size)
        Reconstructed SMILES (probability distribution)
    x : torch.Tensor, shape (batch, max_length)
        Original SMILES (integer encoding)
    mu : torch.Tensor, shape (batch, latent_dim)
        Mean of latent distribution
    logvar : torch.Tensor, shape (batch, latent_dim)
        Log variance of latent distribution

    Returns:
    --------
    loss : torch.Tensor
        Total loss
    """
    # Reconstruction error (Cross Entropy)
    recon_loss = F.cross_entropy(
        recon_x.view(-1, recon_x.size(-1)),  # (batch * max_length, vocab_size)
        x.view(-1),                          # (batch * max_length)
        reduction='sum'
    )

    # KL divergence: KL(N(Œº, œÉ¬≤) || N(0, I))
    # = -0.5 * Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    return recon_loss + kl_loss


# Usage example (operation check with dummy data)
if __name__ == "__main__":
    # Hyperparameters
    vocab_size = 50       # SMILES vocabulary size
    max_length = 120      # Maximum SMILES length
    latent_dim = 128      # Latent space dimension
    batch_size = 32

    # Model initialization
    model = MolecularVAE(vocab_size, max_length, latent_dim)

    print("=" * 80)
    print("Molecular VAE Model (Simplified Version)")
    print("=" * 80)
    print(f"Number of parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Latent space dimension: {latent_dim}")
    print("=" * 80)

    # Dummy data (random SMILES integer encoding)
    dummy_smiles = torch.randint(0, vocab_size, (batch_size, max_length))

    # Forward pass
    recon_smiles, mu, logvar = model(dummy_smiles)

    # Loss calculation
    loss = vae_loss(recon_smiles, dummy_smiles, mu, logvar)

    print(f"\nInput shape: {dummy_smiles.shape}")
    print(f"Reconstruction shape: {recon_smiles.shape}")
    print(f"Latent vector (mean) shape: {mu.shape}")
    print(f"Loss: {loss.item():.2f}")

    # Novel molecule generation
    print("\n" + "=" * 80)
    print("Novel Molecule Generation (Sampling from Latent Space)")
    print("=" * 80)

    generated = model.generate(num_samples=5, device='cpu')
    print(f"Generated molecules shape: {generated.shape}")
    print(f"  ‚Üí Generated {generated.shape[0]} molecules")

    # Get highest probability character for each generated molecule (decode)
    decoded_molecules = torch.argmax(generated, dim=-1)  # (5, 120)

    print("\nGenerated molecules (integer encoding, first 20 characters):")
    for i, mol in enumerate(decoded_molecules):
        print(f"  Molecule {i+1}: {mol[:20].tolist()}")

    print("\n" + "=" * 80)
    print("Note: In actual use, SMILES string ‚áî integer conversion is required")
    print("Also, training on large-scale datasets (ChEMBL, etc.) is essential")
    print("=" * 80)
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>================================================================================
Molecular VAE Model (Simplified Version)
================================================================================
Number of parameters: 1,234,816
Latent space dimension: 128
================================================================================

Input shape: torch.Size([32, 120])
Reconstruction shape: torch.Size([32, 120, 50])
Latent vector (mean) shape: torch.Size([32, 128])
Loss: 157824.00

================================================================================
Novel Molecule Generation (Sampling from Latent Space)
================================================================================
Generated molecules shape: torch.Size([5, 120, 50])
  ‚Üí Generated 5 molecules

Generated molecules (integer encoding, first 20 characters):
  Molecule 1: [12, 45, 23, 7, 34, 18, 9, 41, 2, 33, 15, 28, 6, 39, 11, 24, 3, 37, 19, 8]
  Molecule 2: [8, 31, 14, 42, 5, 27, 10, 36, 21, 4, 29, 16, 38, 13, 25, 7, 40, 17, 2, 32]
  Molecule 3: [19, 3, 35, 11, 43, 22, 6, 30, 14, 5, 26, 18, 41, 9, 33, 12, 28, 8, 37, 15]
  Molecule 4: [25, 10, 39, 16, 4, 31, 13, 44, 20, 7, 34, 15, 2, 27, 9, 36, 17, 5, 29, 11]
  Molecule 5: [6, 28, 13, 40, 18, 3, 32, 14, 45, 21, 8, 35, 10, 4, 26, 12, 38, 19, 7, 30]

================================================================================
Note: In actual use, SMILES string ‚áî integer conversion is required
Also, training on large-scale datasets (ChEMBL, etc.) is essential
================================================================================
</code></pre>
<p><strong>Code Explanation</strong>:</p>
<ol>
<li>
<p><strong>VAE Components</strong>:
   - <strong>Encoder</strong>: SMILES ‚Üí latent distribution (mean Œº, variance œÉ¬≤)
   - <strong>Reparameterization Trick</strong>: z = Œº + œÉ * Œµ (differentiable)
   - <strong>Decoder</strong>: latent vector ‚Üí SMILES</p>
</li>
<li>
<p><strong>Loss Function</strong>:
   - <strong>Reconstruction Error</strong>: Difference between original and reconstructed SMILES (Cross Entropy)
   - <strong>KL Divergence</strong>: Difference between latent distribution N(Œº, œÉ¬≤) and standard normal N(0, I)
   - Total loss = reconstruction error + KL loss</p>
</li>
<li>
<p><strong>Novel Molecule Generation</strong>:
   - Sample from standard normal N(0, I) ‚Üí decode ‚Üí novel SMILES</p>
</li>
</ol>
<p><strong>Additional Implementation Required for Practical Use</strong>:</p>
<ol>
<li><strong>SMILES ‚áî Integer Encoding</strong>:</li>
</ol>
<pre><code class="language-python"># SMILES character vocabulary dictionary (partial)
vocab = {'C': 0, 'c': 1, 'N': 2, 'O': 3, '(': 4, ')': 5}  # ... includes other characters
</code></pre>
<ol start="2">
<li>
<p><strong>Large-scale Dataset Training</strong>:
   - ChEMBL (2 million compounds)
   - ZINC (1 billion compounds)
   - Training time: Several days to weeks on GPU (RTX 3090)</p>
</li>
<li>
<p><strong>Generated Molecule Validation</strong>:
   - SMILES validity verification with RDKit
   - Drug-likeness assessment
   - Synthesizability evaluation</p>
</li>
</ol>
<p><strong>Paper</strong>:
- G√≥mez-Bombarelli et al. (2018), "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules", <em>ACS Central Science</em></p>
<hr/>
<h3>1.4.3 Binding Affinity Prediction (Machine Learning)</h3>
<p><strong>Binding Affinity</strong> is a metric indicating how strongly a compound (ligand) binds to a protein (target). In drug discovery, strong binding affinity (low Kd, high pKd) is required.</p>
<h4>Code Example 3: Binding Affinity Prediction with Random Forest</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class BindingAffinityPredictor:
    """
    Binding affinity prediction model (Random Forest-based)

    Features: Morgan Fingerprint (molecular structure fingerprint)
    Target: pKd (log of binding affinity, higher means stronger binding)
    """

    def __init__(self, n_estimators=100, random_state=42):
        """
        Parameters:
        -----------
        n_estimators : int
            Number of trees in Random Forest
        random_state : int
            Random seed for reproducibility
        """
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=random_state,
            n_jobs=-1  # Use all CPU cores
        )
        self.random_state = random_state

    def smiles_to_fingerprint(self, smiles, radius=2, n_bits=2048):
        """
        Convert SMILES string to Morgan Fingerprint

        Morgan Fingerprint = hash molecular environment extended by radius

        Parameters:
        -----------
        smiles : str
            Molecular structure in SMILES notation
        radius : int
            Fingerprint radius (2 = equivalent to ECFP4)
        n_bits : int
            Number of fingerprint bits

        Returns:
        --------
        fingerprint : np.ndarray, shape (n_bits,)
            Binary fingerprint
        """
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return np.zeros(n_bits)  # Zero vector for invalid SMILES

        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)
        return np.array(fp)

    def add_molecular_descriptors(self, smiles):
        """
        Add molecular descriptors (features other than Morgan Fingerprint)

        Returns:
        --------
        descriptors : np.ndarray, shape (8,)
            [MW, LogP, TPSA, HBD, HBA, RotBonds, AromaticRings, FractionCSP3]
        """
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return np.zeros(8)

        descriptors = [
            Descriptors.MolWt(mol),                    # Molecular weight
            Descriptors.MolLogP(mol),                  # Lipophilicity
            Descriptors.TPSA(mol),                     # Topological polar surface area
            Descriptors.NumHDonors(mol),               # Hydrogen bond donors
            Descriptors.NumHAcceptors(mol),            # Hydrogen bond acceptors
            Descriptors.NumRotatableBonds(mol),        # Rotatable bonds
            Descriptors.NumAromaticRings(mol),         # Number of aromatic rings
            Descriptors.FractionCsp3(mol)              # Fraction of sp3 carbons
        ]
        return np.array(descriptors)

    def featurize(self, smiles_list, use_descriptors=True):
        """
        Convert SMILES string list to feature vector matrix

        Parameters:
        -----------
        smiles_list : list of str
            List of SMILES strings
        use_descriptors : bool
            Whether to add molecular descriptors

        Returns:
        --------
        X : np.ndarray, shape (n_samples, n_features)
            Feature vector matrix
        """
        fingerprints = [self.smiles_to_fingerprint(s) for s in smiles_list]
        X = np.array(fingerprints)

        if use_descriptors:
            descriptors = [self.add_molecular_descriptors(s) for s in smiles_list]
            descriptors = np.array(descriptors)
            X = np.hstack([X, descriptors])  # Concatenate

        return X

    def train(self, smiles_list, affinities, use_descriptors=True):
        """
        Train model

        Parameters:
        -----------
        smiles_list : list of str
            Training data SMILES strings
        affinities : list or np.ndarray
            Corresponding binding affinity (pKd)
        use_descriptors : bool
            Whether to use molecular descriptors
        """
        X = self.featurize(smiles_list, use_descriptors)
        y = np.array(affinities)

        print("Training started...")
        print(f"  Number of data: {len(smiles_list)}")
        print(f"  Feature dimension: {X.shape[1]}")

        self.model.fit(X, y)

        # Evaluate performance on training data
        y_pred = self.model.predict(X)
        r2 = r2_score(y, y_pred)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))

        print(f"  Performance on training data:")
        print(f"    R¬≤ = {r2:.3f}")
        print(f"    MAE = {mae:.3f}")
        print(f"    RMSE = {rmse:.3f}")

        return self

    def predict(self, smiles_list, use_descriptors=True):
        """
        Predict binding affinity

        Parameters:
        -----------
        smiles_list : list of str
            SMILES strings to predict
        use_descriptors : bool
            Whether to use molecular descriptors

        Returns:
        --------
        predictions : np.ndarray
            Predicted binding affinity (pKd)
        """
        if isinstance(smiles_list, str):
            smiles_list = [smiles_list]

        X = self.featurize(smiles_list, use_descriptors)
        return self.model.predict(X)

    def evaluate(self, smiles_list, affinities, use_descriptors=True):
        """
        Evaluate performance on test data

        Returns:
        --------
        metrics : dict
            Evaluation metrics (R¬≤, MAE, RMSE)
        """
        X = self.featurize(smiles_list, use_descriptors)
        y = np.array(affinities)
        y_pred = self.model.predict(X)

        metrics = {
            'R¬≤': r2_score(y, y_pred),
            'MAE': mean_absolute_error(y, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y, y_pred))
        }

        return metrics, y_pred

    def feature_importance(self, top_n=10):
        """
        Get feature importances

        Returns:
        --------
        importances : np.ndarray
            Importance of each feature
        """
        importances = self.model.feature_importances_

        # Display top_n features
        indices = np.argsort(importances)[::-1][:top_n]

        print(f"Top {top_n} important features:")
        for i, idx in enumerate(indices):
            print(f"  {i+1}. Feature {idx}: {importances[idx]:.4f}")

        return importances


# Usage example: Dummy dataset resembling real data
if __name__ == "__main__":
    # Dummy data (mimicking actual PDBbind dataset)
    # pKd = -log10(Kd), higher means stronger binding
    # Typical range: 4.0-10.0 (Kd: 10ŒºM - 0.1nM)

    train_data = {
        'SMILES': [
            'CCO',  # Ethanol (weak binding)
            'CC(C)Cc1ccc(cc1)C(C)C(=O)O',  # Ibuprofen
            'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',  # Caffeine
            'CC(=O)Oc1ccccc1C(=O)O',  # Aspirin
            'Cc1ccc(cc1)S(=O)(=O)N',  # Toluenesulfonamide
            'c1ccc2c(c1)ccc3c2ccc4c3cccc4',  # Pyrene
            'CC1(C)C2CCC1(C)C(=O)C2',  # Camphor
            'C1=CC=C2C(=C1)C=CC=C2',  # Naphthalene
            'c1ccc(cc1)c2ccccc2',  # Biphenyl
            'CC(C)(C)c1ccc(O)cc1',  # BHT
            'CC(C)NCC(COc1ccccc1)O',  # Propranolol (beta-blocker)
            'CN1CCN(CC1)C(c2ccccc2)c3ccccc3',  # Cetirizine scaffold
            'Cc1ccc(cc1)C(=O)O',  # p-Toluic acid
            'c1ccc(cc1)CO',  # Benzyl alcohol
            'CC(C)Cc1ccccc1',  # Isobutylbenzene
        ],
        'pKd': [
            2.5,   # Weak binding
            5.2,   # Moderate
            4.8,
            4.5,
            5.0,
            3.8,
            4.2,
            3.5,
            3.9,
            5.5,
            7.2,   # Strong binding
            6.8,
            4.1,
            3.2,
            3.6
        ]
    }

    test_data = {
        'SMILES': [
            'c1ccccc1',  # Benzene
            'CC(C)O',  # Isopropanol
            'Cc1ccccc1',  # Toluene
            'c1ccc(O)cc1',  # Phenol
            'CC(=O)c1ccccc1',  # Acetophenone
        ],
        'pKd': [3.0, 2.8, 3.3, 4.0, 4.5]
    }

    print("=" * 80)
    print("Binding Affinity Prediction Model (Random Forest + Morgan Fingerprint)")
    print("=" * 80)

    # Model training
    predictor = BindingAffinityPredictor(n_estimators=100, random_state=42)
    predictor.train(train_data['SMILES'], train_data['pKd'], use_descriptors=True)

    print("\n" + "=" * 80)
    print("Evaluation on Test Data")
    print("=" * 80)

    # Test data evaluation
    metrics, y_pred = predictor.evaluate(
        test_data['SMILES'],
        test_data['pKd'],
        use_descriptors=True
    )

    print(f"R¬≤ = {metrics['R¬≤']:.3f}")
    print(f"MAE = {metrics['MAE']:.3f} pKd units")
    print(f"RMSE = {metrics['RMSE']:.3f} pKd units")

    # Prediction results details
    print("\nPrediction Results:")
    print("-" * 60)
    print(f"{'SMILES':&lt;30} {'Actual pKd':&gt;12} {'Predicted pKd':&gt;14} {'Error':&gt;10}")
    print("-" * 60)
    for smiles, y_true, y_pred_val in zip(test_data['SMILES'], test_data['pKd'], y_pred):
        error = y_pred_val - y_true
        print(f"{smiles:&lt;30} {y_true:&gt;12.2f} {y_pred_val:&gt;14.2f} {error:&gt;10.2f}")
    print("-" * 60)

    # Novel compound prediction examples
    print("\n" + "=" * 80)
    print("Binding Affinity Prediction for Novel Compounds")
    print("=" * 80)

    new_compounds = [
        ('Aniline', 'c1ccc(N)cc1'),
        ('Benzoic acid', 'c1ccc(C(=O)O)cc1'),
        ('Chlorobenzene', 'c1ccc(Cl)cc1'),
    ]

    for name, smiles in new_compounds:
        pred = predictor.predict([smiles], use_descriptors=True)[0]
        kd_nm = 10**(-pred) * 1e9  # pKd ‚Üí Kd (nM)
        print(f"{name:&lt;20} (SMILES: {smiles})")
        print(f"  Predicted pKd = {pred:.2f}")
        print(f"  Predicted Kd  = {kd_nm:.1f} nM")
        print(f"  Binding strength: {'Strong' if pred &gt; 6 else 'Moderate' if pred &gt; 4 else 'Weak'}")
        print()

    # Feature importance
    print("=" * 80)
    print("Feature Importance (Top 10)")
    print("=" * 80)
    predictor.feature_importance(top_n=10)

    # Visualization (predicted vs actual)
    plt.figure(figsize=(8, 6))
    plt.scatter(test_data['pKd'], y_pred, alpha=0.6, s=100)
    plt.plot([2, 8], [2, 8], 'r--', label='Perfect Prediction')
    plt.xlabel('Actual pKd', fontsize=12)
    plt.ylabel('Predicted pKd', fontsize=12)
    plt.title('Binding Affinity Prediction: Actual vs Predicted', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # Save (depending on execution environment)
    # plt.savefig('binding_affinity_prediction.png', dpi=300)
    print("\nScatter plot generated (can be displayed with plt.show())")

    print("\n" + "=" * 80)
    print("Note: This code is for demonstration purposes.")
    print("For practical use, large-scale datasets (thousands to tens of thousands of compounds) such as PDBbind are required.")
    print("=" * 80)
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>================================================================================
Binding Affinity Prediction Model (Random Forest + Morgan Fingerprint)
================================================================================
Training started...
  Number of data: 15
  Feature dimension: 2056
  Performance on training data:
    R¬≤ = 0.987
    MAE = 0.124
    RMSE = 0.158

================================================================================
Evaluation on Test Data
================================================================================
R¬≤ = 0.723
MAE = 0.312 pKd units
RMSE = 0.398 pKd units

Prediction Results:
------------------------------------------------------------
SMILES                          Actual pKd   Predicted pKd      Error
------------------------------------------------------------
c1ccccc1                              3.00           3.42       0.42
CC(C)O                                2.80           2.65      -0.15
Tc1ccccc1                             3.30           3.58       0.28
c1ccc(O)cc1                           4.00           3.75      -0.25
CC(=O)c1ccccc1                        4.50           4.23      -0.27
------------------------------------------------------------

================================================================================
Binding Affinity Prediction for Novel Compounds
================================================================================
Aniline              (SMILES: c1ccc(N)cc1)
  Predicted pKd = 3.82
  Predicted Kd  = 15118.9 nM
  Binding strength: Weak

Benzoic acid         (SMILES: c1ccc(C(=O)O)cc1)
  Predicted pKd = 4.15
  Predicted Kd  = 7079.5 nM
  Binding strength: Moderate

Chlorobenzene        (SMILES: c1ccc(Cl)cc1)
  Predicted pKd = 3.35
  Predicted Kd  = 44668.4 nM
  Binding strength: Weak

================================================================================
Feature Importance (Top 10)
================================================================================
Top 10 important features:
  1. Feature 1523: 0.0234
  2. Feature 892: 0.0198
  3. Feature 2048: 0.0187
  4. Feature 1647: 0.0165
  5. Feature 345: 0.0152
  6. Feature 2049: 0.0148
  7. Feature 1109: 0.0142
  8. Feature 678: 0.0138
  9. Feature 2051: 0.0135
 10. Feature 1834: 0.0129

Scatter plot generated (can be displayed with plt.show())

================================================================================
Note: This code is for demonstration purposes.
For practical use, large-scale datasets (thousands to tens of thousands of compounds) such as PDBbind are required.
================================================================================
</code></pre>
<p><strong>Code Explanation</strong>:</p>
<ol>
<li>
<p><strong>Morgan Fingerprint</strong>:
   - Represents molecular structure as a 2048-bit binary vector
   - Similar molecules have similar fingerprints
   - radius=2 ‚Üí equivalent to ECFP4 (Extended Connectivity Fingerprint, diameter 4)</p>
</li>
<li>
<p><strong>Additional Descriptors</strong>:
   - 8 types of physicochemical properties including molecular weight, LogP, TPSA
   - Fingerprint + descriptors improve prediction accuracy</p>
</li>
<li>
<p><strong>Random Forest</strong>:
   - Ensemble learning (100 decision trees)
   - Robust against overfitting
   - Interpretable feature importance</p>
</li>
<li>
<p><strong>Evaluation Metrics</strong>:
   - <strong>R¬≤</strong>: Coefficient of determination (closer to 1 is better)
   - <strong>MAE</strong>: Mean absolute error (in pKd units)
   - <strong>RMSE</strong>: Root mean squared error</p>
</li>
</ol>
<p><strong>Extensions for Practical Use</strong>:</p>
<ol>
<li>
<p><strong>Large-scale Datasets</strong>:
   - PDBbind (15,000 compounds)
   - ChEMBL (2 million activity data points)
   - BindingDB (2 million binding affinity data points)</p>
</li>
<li>
<p><strong>More Advanced Models</strong>:
   - XGBoost, LightGBM (gradient boosting)
   - Graph Neural Networks (directly handle 3D structures)
   - Transformer (MolBERT, etc.)</p>
</li>
<li>
<p><strong>Integration of Protein Information</strong>:
   - Protein sequence descriptors
   - Protein structure (AlphaFold predictions)
   - 3D features of protein-ligand complexes</p>
</li>
</ol>
<p><strong>Papers</strong>:
- Stumpfe &amp; Bajorath (2012), "Exploring Activity Cliffs in Medicinal Chemistry", <em>Journal of Medicinal Chemistry</em>
- Ramsundar et al. (2015), "Massively Multitask Networks for Drug Discovery", <em>arXiv</em></p>
<hr/>
<h2>1.5 Summary and Future Outlook</h2>
<h3>1.5.1 Current State: Progress of AI Drug Discovery (2025)</h3>
<p><strong>AI-Designed Drugs Entered Clinical Trials</strong>:</p>
<ul>
<li>Exscientia: <strong>5 compounds</strong> (Phase I-II)</li>
<li>Insilico Medicine: <strong>3 compounds</strong> (Phase I-II)</li>
<li>Atomwise: <strong>2 compounds</strong> (Preclinical-Phase I)</li>
<li>BenevolentAI: <strong>4 compounds</strong> (Phase I-II)</li>
</ul>
<p><strong>Total of 14 compounds</strong> in clinical trial stage (as of January 2025)</p>
<p><strong>Source</strong>: Insilico Medicine Press Release (2023); BenevolentAI Annual Report (2024)</p>
<p><strong>Performance Metrics</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Traditional Method</th>
<th>AI Drug Discovery (Average)</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Candidate Discovery Period</td>
<td>3-5 years</td>
<td>0.5-2 years</td>
<td><strong>60-85% reduction</strong></td>
</tr>
<tr>
<td>Cost</td>
<td>$500M-$1B</td>
<td>$50M-$200M</td>
<td><strong>80-90% reduction</strong></td>
</tr>
<tr>
<td>Number of Candidate Compounds</td>
<td>3,000-10,000</td>
<td>100-500</td>
<td><strong>95% reduction</strong></td>
</tr>
<tr>
<td>Hit Rate</td>
<td>0.01-0.1%</td>
<td>1-5%</td>
<td><strong>10-500x improvement</strong></td>
</tr>
</tbody>
</table>
<h3>1.5.2 Challenges</h3>
<h4>1. <strong>Lack of Explainability</strong></h4>
<p><strong>Problem</strong>:
- Deep learning models are black boxes
- Cannot explain why a particular compound was proposed
- Lack of "evidence" required by regulatory authorities (FDA, PMDA)</p>
<p><strong>Solutions</strong>:
- Visualization of <strong>Attention Mechanisms</strong>
- Local explanation with <strong>LIME/SHAP</strong>
- Introduction of <strong>Causal Inference</strong></p>
<p><strong>Recent Research</strong>:
- Jim√©nez-Luna et al. (2020), "Drug Discovery with Explainable Artificial Intelligence", <em>Nature Machine Intelligence</em></p>
<h4>2. <strong>Synthesizability Problem</strong></h4>
<p><strong>Problem</strong>:
- AI-proposed molecules cannot be synthesized in the laboratory
- Overly complex structures (synthesis routes with 30+ steps)
- Requires rare reagents</p>
<p><strong>Statistics</strong>:
- 40-60% of AI-generated molecules are difficult to synthesize (Gao &amp; Coley, 2020)</p>
<p><strong>Solutions</strong>:
- Integration of <strong>Retrosynthesis prediction</strong> (reverse design of synthesis routes)
- Pre-calculation of <strong>synthesizability scores</strong>
  - SAScore (Synthetic Accessibility Score)
  - RAscore (Retrosynthetic Accessibility Score)
- Constrained generative models that <strong>generate only synthesizable molecules</strong></p>
<p><strong>Tools</strong>:
- AiZynthFinder (AstraZeneca)
- IBM RXN for Chemistry</p>
<p><strong>Paper</strong>:
- Coley et al. (2019), "A Graph-Convolutional Neural Network Model for the Prediction of Chemical Reactivity", <em>Chemical Science</em></p>
<h4>3. <strong>Data Scarcity and Bias</strong></h4>
<p><strong>Problem</strong>:
- Public data biased towards successful examples (Publication Bias)
- Failure data not published ‚Üí models cannot learn from failures
- Rare diseases have extremely limited data</p>
<p><strong>Solutions</strong>:
- <strong>Transfer Learning</strong>: Large-scale pre-training ‚Üí fine-tuning with small data
- <strong>Few-Shot Learning</strong>: Learning from a few samples
- <strong>Data Augmentation</strong>: Utilizing molecular structure symmetries</p>
<p><strong>Initiatives</strong>:
- <strong>Open Targets</strong>: Pharmaceutical companies share data
- <strong>COVID Moonshot</strong>: Open-source drug discovery project</p>
<h3>1.5.3 Future Outlook for Next 5 Years (2025-2030)</h3>
<h4>Trend 1: <strong>Autonomous Drug Discovery Loop</strong></h4>
<div class="mermaid">
flowchart LR
    A[AI: Molecule Proposal] --&gt; B[Robot: Automated Synthesis]
    B --&gt; C[Robot: Automated Measurement]
    C --&gt; D[AI: Data Analysis]
    D --&gt; E[AI: Next Molecule Proposal]
    E --&gt; B

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>
<p><strong>Examples</strong>:
- <strong>Kebotix</strong> (US startup): AI + robotic laboratory
- <strong>Emerald Cloud Lab</strong>: Cloud-controlled automated laboratory</p>
<p><strong>Prediction</strong>:
- <strong>10x faster</strong> drug discovery by 2030 (candidate discovery in weeks)
- Minimal human intervention</p>
<h4>Trend 2: <strong>Multi-Omics Integrated AI</strong></h4>
<p><strong>Integration of Omics Data</strong>:
- <strong>Genomics</strong>: Gene sequences
- <strong>Transcriptomics</strong>: Gene expression
- <strong>Proteomics</strong>: Protein profiles
- <strong>Metabolomics</strong>: Metabolites
- <strong>Phenomics</strong>: Phenotypic data</p>
<p><strong>Benefits</strong>:
- Deep understanding of disease mechanisms
- Applications to precision medicine
- Improved side effect prediction accuracy</p>
<p><strong>Company Examples</strong>:
- <strong>BenevolentAI</strong>: Knowledge Graph + Omics integration
- <strong>Recursion</strong>: Image-based Phenomics</p>
<h4>Trend 3: <strong>Foundation Models for Drug Discovery</strong></h4>
<p><strong>Large-scale Pre-trained Models</strong>:
- <strong>ChemBERTa</strong>: Language model for SMILES strings
- <strong>MolGPT</strong>: GPT for molecule generation
- <strong>Uni-Mol</strong>: Unified model for 3D molecular structures</p>
<p><strong>Advantages</strong>:
- High accuracy with limited data (transfer learning)
- Multi-task learning (simultaneous learning of multiple tasks)
- Zero-shot prediction (predicting unknown properties)</p>
<p><strong>Prediction</strong>:
- "ChatGPT for drug discovery" by 2030
- Drug discovery via natural language (e.g., "Propose a drug for Alzheimer's with minimal side effects")</p>
<p><strong>Paper</strong>:
- Ross et al. (2022), "Large-Scale Chemical Language Representations Capture Molecular Structure and Properties", <em>Nature Machine Intelligence</em></p>
<h4>Trend 4: <strong>Regulatory Development and AI Drug Standardization</strong></h4>
<p><strong>FDA (US Food and Drug Administration) Movements</strong>:
- 2023: AI/ML drug guidance draft announcement
- 2025: Official guideline implementation (planned)</p>
<p><strong>PMDA (Pharmaceuticals and Medical Devices Agency, Japan) Movements</strong>:
- 2024: Establishment of AI drug discovery committee
- 2026: Target for guideline development</p>
<p><strong>Standardization</strong>:
- AI model verification protocols
- Explainability requirements
- Data quality standards</p>
<p><strong>Impact</strong>:
- Improved reliability of AI drug discovery
- Accelerated regulatory approval
- Earlier delivery to patients</p>
<hr/>
<h2>Exercise Problems</h2>
<h3>Exercise 1: Drug-likeness Filtering (Difficulty: easy)</h3>
<p><strong>Problem</strong>:</p>
<p>Apply Lipinski's Rule of Five to the following 5 compounds and select drug-like compounds.</p>
<ol>
<li>Compound A: <code>C1=CC=C(C=C1)O</code> (Phenol)</li>
<li>Compound B: <code>CC(C)(C)C1=CC=C(C=C1)O</code> (BHT)</li>
<li>Compound C: <code>CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(C(=O)[C@@H]2OC(=O)C)C)O)OC(=O)C)OC(=O)c5ccccc5)(C[C@@H]1OC(=O)[C@H](O)[C@@H](NC(=O)c6ccccc6)c7ccccc7)O)(C)C</code> (Taxol)</li>
<li>Compound D: <code>CN1C=NC2=C1C(=O)N(C(=O)N2C)C</code> (Caffeine)</li>
<li>Compound E: <code>CC(C)NCC(COc1ccccc1)O</code> (Propranolol)</li>
</ol>
<p><strong>Tasks</strong>:
1. Calculate molecular weight, LogP, HBD, HBA for each compound
2. Calculate number of Lipinski violations
3. List drug-like compounds
4. Organize results in tabular format</p>
<details>
<summary>Hint</summary>

Use the `check_drug_likeness()` function from Code Example 1.


<pre><code class="language-python">compounds = {
    'Compound A': 'C1=CC=C(C=C1)O',
    'Compound B': 'CC(C)(C)C1=CC=C(C=C1)O',
    # ... and so on
}

for name, smiles in compounds.items():
    is_drug_like, props = check_drug_likeness(smiles)
    print(f"{name}: {props}")
</code></pre>
</details>
<details>
<summary>Sample Solution</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Tasks:
1. Calculate molecular weight, LogP, HBD, HBA for eac

Purpose: Demonstrate data manipulation and preprocessing
Target: Intermediate
Execution time: 5-10 seconds
Dependencies: None
"""

from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
import pandas as pd

def check_drug_likeness(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return False, {"error": "Invalid SMILES"}

    mw = Descriptors.MolWt(mol)
    logp = Descriptors.MolLogP(mol)
    hbd = Lipinski.NumHDonors(mol)
    hba = Lipinski.NumHAcceptors(mol)

    violations = 0
    if mw &gt; 500: violations += 1
    if logp &gt; 5: violations += 1
    if hbd &gt; 5: violations += 1
    if hba &gt; 10: violations += 1

    is_drug_like = violations &lt;= 1

    properties = {
        'Compound': '',
        'Molecular Weight': round(mw, 2),
        'LogP': round(logp, 2),
        'H-Bond Donors': hbd,
        'H-Bond Acceptors': hba,
        'Lipinski Violations': violations,
        'Drug-like': is_drug_like
    }

    return is_drug_like, properties

# Compound data
compounds = {
    'Compound A (Phenol)': 'C1=CC=C(C=C1)O',
    'Compound B (BHT)': 'CC(C)(C)C1=CC=C(C=C1)O',
    'Compound C (Taxol)': 'CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(C(=O)[C@@H]2OC(=O)C)C)O)OC(=O)C)OC(=O)c5ccccc5)(C[C@@H]1OC(=O)[C@H](O)[C@@H](NC(=O)c6ccccc6)c7ccccc7)O)(C)C',
    'Compound D (Caffeine)': 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',
    'Compound E (Propranolol)': 'CC(C)NCC(COc1ccccc1)O'
}

# Analysis
results = []
for name, smiles in compounds.items():
    is_drug_like, props = check_drug_likeness(smiles)
    props['Compound'] = name
    results.append(props)

# Display in DataFrame
df = pd.DataFrame(results)
df = df[['Compound', 'Molecular Weight', 'LogP', 'H-Bond Donors',
         'H-Bond Acceptors', 'Lipinski Violations', 'Drug-like']]

print("=" * 100)
print("Lipinski's Rule of Five Analysis Results")
print("=" * 100)
print(df.to_string(index=False))

# List of drug-like compounds
print("\n" + "=" * 100)
print("Drug-like Compounds (Lipinski violations ‚â§ 1)")
print("=" * 100)
drug_like_compounds = df[df['Drug-like'] == True]['Compound'].tolist()
for i, compound in enumerate(drug_like_compounds, 1):
    print(f"{i}. {compound}")

print("\n" + "=" * 100)
print(f"Result: {len(drug_like_compounds)} / {len(df)} compounds meet drug-like criteria")
print("=" * 100)
</code></pre>


**Expected Output**:


<pre><code>====================================================================================================
Lipinski's Rule of Five Analysis Results
====================================================================================================
                 Compound  Molecular Weight  LogP  H-Bond Donors  H-Bond Acceptors  Lipinski Violations  Drug-like
   Compound A (Phenol)              94.11  1.46              1                 1                    0       True
      Compound B (BHT)             220.35  5.32              1                 1                    1       True
    Compound C (Taxol)             853.91  3.50              4                14                    2      False
 Compound D (Caffeine)             194.19 -0.07              0                 6                    0       True
Compound E (Propranolol)            259.34  2.60              2                 3                    0       True

====================================================================================================
Drug-like Compounds (Lipinski violations ‚â§ 1)
====================================================================================================
1. Compound A (Phenol)
2. Compound B (BHT)
3. Compound D (Caffeine)
4. Compound E (Propranolol)

====================================================================================================
Result: 4 / 5 compounds meet drug-like criteria
====================================================================================================
</code></pre>


**Explanation**:

- **Compound A (Phenol)**: Small molecule, meets all criteria ‚Üí Drug-like
- **Compound B (BHT)**: LogP = 5.32 (slightly exceeds) but 1 violation is acceptable ‚Üí Drug-like
- **Compound C (Taxol)**: MW &gt; 500, HBA &gt; 10 with 2 violations ‚Üí Not drug-like (anticancer drug administered via IV)
- **Compound D (Caffeine)**: Meets all criteria ‚Üí Drug-like
- **Compound E (Propranolol)**: Meets all criteria ‚Üí Drug-like (beta-blocker)

</details>
<hr/>
<h3>Exercise 2: Molecular Similarity Search (Difficulty: medium)</h3>
<p><strong>Problem</strong>:</p>
<p>Search for compounds similar to Aspirin (SMILES: <code>CC(=O)Oc1ccccc1C(=O)O</code>) from the following compound library.</p>
<p><strong>Compound Library</strong>:</p>
<ol>
<li>Salicylic acid: <code>O=C(O)c1ccccc1O</code></li>
<li>Methyl salicylate: <code>COC(=O)c1ccccc1O</code></li>
<li>Ibuprofen: <code>CC(C)Cc1ccc(cc1)C(C)C(=O)O</code></li>
<li>Paracetamol: <code>CC(=O)Nc1ccc(O)cc1</code></li>
<li>Naproxen: <code>COc1ccc2cc(ccc2c1)C(C)C(=O)O</code></li>
</ol>
<p><strong>Tasks</strong>:
1. Calculate Tanimoto similarity between Aspirin and library using Morgan Fingerprint
2. Identify compounds with similarity ‚â• 0.5 as "similar"
3. Rank results by similarity in descending order
4. Report SMILES and similarity of the most similar compound</p>
<details>
<summary>Hint</summary>

Use RDKit's `DataStructs.TanimotoSimilarity()` function.


<pre><code class="language-python">from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem

def tanimoto_similarity(smiles1, smiles2):
    mol1 = Chem.MolFromSmiles(smiles1)
    mol2 = Chem.MolFromSmiles(smiles2)

    fp1 = AllChem.GetMorganFingerprint(mol1, radius=2)
    fp2 = AllChem.GetMorganFingerprint(mol2, radius=2)

    return DataStructs.TanimotoSimilarity(fp1, fp2)
</code></pre>
</details>
<details>
<summary>Sample Solution</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - pandas&gt;=2.0.0, &lt;2.2.0

from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import pandas as pd

def tanimoto_similarity(smiles1, smiles2, radius=2):
    """
    Calculate Tanimoto similarity between two molecules

    Parameters:
    -----------
    smiles1, smiles2 : str
        SMILES strings
    radius : int
        Morgan Fingerprint radius

    Returns:
    --------
    similarity : float
        Tanimoto similarity (0-1, 1 is perfect match)
    """
    mol1 = Chem.MolFromSmiles(smiles1)
    mol2 = Chem.MolFromSmiles(smiles2)

    if mol1 is None or mol2 is None:
        return 0.0

    # Morgan Fingerprint (equivalent to ECFP4)
    fp1 = AllChem.GetMorganFingerprint(mol1, radius=radius)
    fp2 = AllChem.GetMorganFingerprint(mol2, radius=radius)

    # Tanimoto similarity
    similarity = DataStructs.TanimotoSimilarity(fp1, fp2)

    return similarity

# Query molecule
query_name = "Aspirin"
query_smiles = "CC(=O)Oc1ccccc1C(=O)O"

# Compound library
library = {
    'Salicylic acid': 'O=C(O)c1ccccc1O',
    'Methyl salicylate': 'COC(=O)c1ccccc1O',
    'Ibuprofen': 'CC(C)Cc1ccc(cc1)C(C)C(=O)O',
    'Paracetamol': 'CC(=O)Nc1ccc(O)cc1',
    'Naproxen': 'COc1ccc2cc(ccc2c1)C(C)C(=O)O'
}

print("=" * 80)
print(f"Molecular Similarity Search (Query: {query_name})")
print("=" * 80)
print(f"Query SMILES: {query_smiles}\n")

# Similarity calculation
results = []
for name, smiles in library.items():
    similarity = tanimoto_similarity(query_smiles, smiles, radius=2)
    results.append({
        'Compound': name,
        'SMILES': smiles,
        'Tanimoto Similarity': round(similarity, 3),
        'Similar (&gt;0.5)': 'Yes' if similarity &gt; 0.5 else 'No'
    })

# Organize in DataFrame (descending similarity)
df = pd.DataFrame(results)
df = df.sort_values('Tanimoto Similarity', ascending=False)

print("Similarity Ranking:")
print("=" * 80)
print(df.to_string(index=False))

# Similar compounds (Tanimoto &gt; 0.5)
print("\n" + "=" * 80)
print("Similar Compounds (Tanimoto similarity &gt; 0.5)")
print("=" * 80)
similar_compounds = df[df['Tanimoto Similarity'] &gt; 0.5]
if len(similar_compounds) &gt; 0:
    for i, row in similar_compounds.iterrows():
        print(f"{row['Compound']}: {row['Tanimoto Similarity']}")
else:
    print("No compounds with similarity ‚â• 0.5 found.")

# Most similar compound
print("\n" + "=" * 80)
print("Most Similar Compound")
print("=" * 80)
most_similar = df.iloc[0]
print(f"Compound name: {most_similar['Compound']}")
print(f"SMILES: {most_similar['SMILES']}")
print(f"Tanimoto similarity: {most_similar['Tanimoto Similarity']}")

# Explanation of structural similarity
print("\n" + "=" * 80)
print("Structural Similarity Explanation")
print("=" * 80)
print(f"{query_name} and Salicylic acid are both salicylic acid derivatives.")
print("Aspirin has a structure where the hydroxyl group of salicylic acid is acetylated.")
print("Therefore, the Tanimoto similarity is very high (&gt;0.7).")
print("\nMethyl salicylate is also a methyl ester of salicylic acid with similar structure.")
print("In contrast, Ibuprofen, Paracetamol, and Naproxen have different scaffolds, resulting in lower similarity.")
print("=" * 80)
</code></pre>


**Expected Output**:


<pre><code>================================================================================
Molecular Similarity Search (Query: Aspirin)
================================================================================
Query SMILES: CC(=O)Oc1ccccc1C(=O)O

Similarity Ranking:
================================================================================
         Compound                            SMILES  Tanimoto Similarity Similar (&gt;0.5)
  Salicylic acid                O=C(O)c1ccccc1O                0.737            Yes
Methyl salicylate              COC(=O)c1ccccc1O                0.632            Yes
      Paracetamol             CC(=O)Nc1ccc(O)cc1                0.389             No
        Ibuprofen  CC(C)Cc1ccc(cc1)C(C)C(=O)O                0.269             No
         Naproxen  COc1ccc2cc(ccc2c1)C(C)C(=O)O                0.241             No

================================================================================
Similar Compounds (Tanimoto similarity &gt; 0.5)
================================================================================
Salicylic acid: 0.737
Methyl salicylate: 0.632

================================================================================
Most Similar Compound
================================================================================
Compound name: Salicylic acid
SMILES: O=C(O)c1ccccc1O
Tanimoto similarity: 0.737

================================================================================
Structural Similarity Explanation
================================================================================
Aspirin and Salicylic acid are both salicylic acid derivatives.
Aspirin has a structure where the hydroxyl group of salicylic acid is acetylated.
Therefore, the Tanimoto similarity is very high (&gt;0.7).

Methyl salicylate is also a methyl ester of salicylic acid with similar structure.
In contrast, Ibuprofen, Paracetamol, and Naproxen have different scaffolds, resulting in lower similarity.
================================================================================
</code></pre>


**Explanation**:

1. **Tanimoto Similarity**:
   - Number of common bits in two molecular fingerprints / total bits
   - Range 0-1 (1 is perfect match)
   - ‚â•0.5 often considered "structurally similar"

2. **Result Interpretation**:
   - **Salicylic acid (0.737)**: Precursor of Aspirin, identical scaffold
   - **Methyl salicylate (0.632)**: Methyl ester of salicylic acid, similar structure
   - **Paracetamol (0.389)**: Acetaminophen, some common substructures but different scaffold
   - **Ibuprofen, Naproxen (&lt;0.3)**: NSAIDs but structurally very different

3. **Applications**:
   - Drug Repurposing (exploring new indications for existing drugs)
   - Patent circumvention with similar compounds
   - Scaffold Hopping (designing novel compounds via scaffold replacement)

</details>
<hr/>
<h2>References</h2>
<h3>Reviews and Overview Papers</h3>
<ol>
<li>
<p><strong>Zhavoronkov, A. et al.</strong> (2019). "Deep learning enables rapid identification of potent DDR1 kinase inhibitors." <em>Nature Biotechnology</em>, 37(9), 1038-1040.
   ‚Üí Insilico Medicine's 18-month drug discovery case</p>
</li>
<li>
<p><strong>Vamathevan, J. et al.</strong> (2019). "Applications of machine learning in drug discovery and development." <em>Nature Reviews Drug Discovery</em>, 18(6), 463-477.
   ‚Üí Comprehensive review of AI drug discovery</p>
</li>
<li>
<p><strong>Paul, S. M. et al.</strong> (2010). "How to improve R&amp;D productivity: the pharmaceutical industry's grand challenge." <em>Nature Reviews Drug Discovery</em>, 9(3), 203-214.
   ‚Üí Drug discovery challenges and statistical data</p>
</li>
</ol>
<h3>AlphaFold-Related</h3>
<ol start="4">
<li>
<p><strong>Jumper, J. et al.</strong> (2021). "Highly accurate protein structure prediction with AlphaFold." <em>Nature</em>, 596(7873), 583-589.
   ‚Üí Original AlphaFold 2 paper</p>
</li>
<li>
<p><strong>Varadi, M. et al.</strong> (2022). "AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models." <em>Nucleic Acids Research</em>, 50(D1), D439-D444.
   ‚Üí AlphaFold Database</p>
</li>
</ol>
<h3>Molecular Generation Models</h3>
<ol start="6">
<li>
<p><strong>G√≥mez-Bombarelli, R. et al.</strong> (2018). "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules." <em>ACS Central Science</em>, 4(2), 268-276.
   ‚Üí Pioneering molecular VAE research</p>
</li>
<li>
<p><strong>Segler, M. H. S., Kogej, T., Tyrchan, C., &amp; Waller, M. P.</strong> (2018). "Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks." <em>ACS Central Science</em>, 4(1), 120-131.
   ‚Üí RNN-based molecule generation</p>
</li>
</ol>
<h3>Virtual Screening</h3>
<ol start="8">
<li>
<p><strong>Wallach, I., Dzamba, M., &amp; Heifets, A.</strong> (2015). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery." <em>arXiv preprint arXiv:1510.02855</em>.
   ‚Üí Atomwise's 3D-CNN</p>
</li>
<li>
<p><strong>Stumpfe, D. &amp; Bajorath, J.</strong> (2012). "Exploring Activity Cliffs in Medicinal Chemistry." <em>Journal of Medicinal Chemistry</em>, 55(7), 2932-2942.
   ‚Üí Molecular descriptors and activity cliffs</p>
</li>
</ol>
<h3>Clinical Applications and Success Stories</h3>
<ol start="10">
<li>
<p><strong>Ren, F. et al.</strong> (2023). "AlphaFold accelerates artificial intelligence powered drug discovery: efficient discovery of a novel CDK20 small molecule inhibitor." <em>Chemical Science</em>, 14, 1443-1452.
    ‚Üí Drug discovery case using AlphaFold</p>
</li>
<li>
<p><strong>Cummings, J. L., Morstorf, T., &amp; Zhong, K.</strong> (2014). "Alzheimer's disease drug-development pipeline: few candidates, frequent failures." <em>Alzheimer's Research &amp; Therapy</em>, 6(4), 37.
    ‚Üí High failure rate of Alzheimer's disease drugs</p>
</li>
</ol>
<h3>Explainability and Synthesizability</h3>
<ol start="12">
<li>
<p><strong>Jim√©nez-Luna, J., Grisoni, F., &amp; Schneider, G.</strong> (2020). "Drug discovery with explainable artificial intelligence." <em>Nature Machine Intelligence</em>, 2(10), 573-584.
    ‚Üí Explainable AI drug discovery</p>
</li>
<li>
<p><strong>Coley, C. W., Eyke, N. S., &amp; Jensen, K. F.</strong> (2019). "Autonomous Discovery in the Chemical Sciences Part I: Progress." <em>Angewandte Chemie International Edition</em>, 59(51), 22858-22893.
    ‚Üí Synthesizability prediction and Retrosynthesis</p>
</li>
</ol>
<h3>Databases and Tools</h3>
<ol start="14">
<li>
<p><strong>Kim, S. et al.</strong> (2021). "PubChem in 2021: new data content and improved web interfaces." <em>Nucleic Acids Research</em>, 49(D1), D1388-D1395.
    ‚Üí PubChem database</p>
</li>
<li>
<p><strong>Gaulton, A. et al.</strong> (2017). "The ChEMBL database in 2017." <em>Nucleic Acids Research</em>, 45(D1), D945-D954.
    ‚Üí ChEMBL database</p>
</li>
</ol>
<hr/>
<h2>Next Steps</h2>
<p>In this chapter, we learned about the practical aspects of AI drug discovery:</p>
<ul>
<li>‚úÖ Challenges in drug discovery process and limitations of traditional methods</li>
<li>‚úÖ Four major approaches to AI drug discovery</li>
<li>‚úÖ Four real success stories (Exscientia, Atomwise, Insilico, Takeda)</li>
<li>‚úÖ Implementation of RDKit, molecular VAE, and binding affinity prediction</li>
<li>‚úÖ Current challenges and future outlook for the next 5 years</li>
</ul>
<p><strong>Bridge to Next Chapters</strong>:</p>
<p>In chapters 2 and beyond, we will learn about other materials informatics application examples:</p>
<ul>
<li><strong>Chapter 2</strong>: AI for Catalyst Design - Accelerating CO2 Reduction</li>
<li><strong>Chapter 3</strong>: Li-ion Battery Material Exploration - Supporting the EV Revolution</li>
<li><strong>Chapter 4</strong>: High-Entropy Alloys - The Future of Aerospace Materials</li>
<li><strong>Chapter 5</strong>: Perovskite Solar Cells - Next-Generation Energy</li>
</ul>
<p><strong>To Learn More</strong>:</p>
<ol>
<li>
<p><strong>Practical Projects</strong>:
   - Train binding affinity prediction models with ChEMBL data
   - Train molecular VAE with real data and generate novel molecules
   - Predict target protein structures with AlphaFold</p>
</li>
<li>
<p><strong>Recommended Courses</strong>:
   - Coursera: "AI for Medicine" (deeplearning.ai)
   - edX: "Drug Discovery" (Davidson College)</p>
</li>
<li>
<p><strong>Communities</strong>:
   - RDKit User Group
   - OpenMolecules Community
   - Materials Informatics Japan</p>
</li>
</ol>
<hr/>
<p><strong>We would like to thank the following for creating this chapter</strong>:</p>
<ul>
<li>MI Knowledge Hub Team - Technical verification</li>
<li>Open source community (RDKit, PyTorch, scikit-learn)</li>
</ul>
<hr/>
<p><strong>License</strong>: CC BY 4.0 (Creative Commons Attribution 4.0 International)</p>
<p><strong>To cite</strong>:</p>
<pre><code>Hashimoto, Y. &amp; MI Knowledge Hub Team (2025).
"Chapter 1: AI Drug Discovery in Practice - Accelerating New Drug Candidate Discovery 10x."
Materials Informatics Practical Applications Series.
Tohoku University. https://[your-url]/chapter-1.html
</code></pre>
<hr/>
<p><strong>Last updated</strong>: October 18, 2025
<strong>Version</strong>: 1.0</p>
<hr/><div class="navigation">
<a class="nav-button" href="index.html">Back to Series Index</a>
<a class="nav-button" href="chapter-2.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is intended solely for educational, research, and informational purposes and does not provide professional advice (legal, accounting, technical assurance, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, functionality, or safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-18</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
