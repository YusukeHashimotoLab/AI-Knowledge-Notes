<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Theory of Bayesian Optimization - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/bayesian-optimization-introduction/index.html">Bayesian Optimization</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 2: Theory of Bayesian Optimization</h1>
            <p class="subtitle">Optimizing exploration with Gaussian Process and Acquisition Functions</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
                <span class="meta-item">üíª Code Examples: 10</span>
                <span class="meta-item">üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: Theory of Bayesian Optimization</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Understand the roles of Gaussian Process and Acquisition Functions (EI/UCB/PI) through visual diagrams. Learn how to balance exploration and exploitation.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Supplement:</strong> EI "digs deeper into promising areas", UCB "verifies uncertain regions". Switch between them depending on the situation.</p>





<p><strong>Optimizing exploration with Gaussian Process and Acquisition Functions</strong></p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the basic principles of Gaussian Process regression</li>
<li>‚úÖ Explain the role and construction method of surrogate models</li>
<li>‚úÖ Implement the three main Acquisition Functions (EI, PI, UCB)</li>
<li>‚úÖ Express the exploration-exploitation trade-off mathematically</li>
<li>‚úÖ Understand uncertainty quantification and its importance</li>
</ul>
<p><strong>Reading Time</strong>: 25-30 minutes
<strong>Code Examples</strong>: 10
<strong>Exercises</strong>: 3</p>
<hr />
<h2>2.1 What is a Surrogate Model</h2>
<h3>Why Surrogate Models are Necessary</h3>
<p>In materials exploration, evaluating the true objective function (e.g., ionic conductivity, catalyst activity) requires <strong>experiments</strong>. However, experiments are:</p>
<ul>
<li><strong>Time-consuming</strong>: Several hours to days per sample</li>
<li><strong>Expensive</strong>: Material costs, equipment costs, labor costs</li>
<li><strong>Limited in number</strong>: Budget and time constraints</li>
</ul>
<p>Therefore, we construct a <strong>model that estimates the objective function from a small number of experimental results</strong>. This is called a <strong>Surrogate Model</strong>.</p>
<h3>Role of Surrogate Models</h3>
<div class="mermaid">
flowchart LR
    A[Small experimental data\nExample: 10-20 points] --> B[Surrogate model construction\nGaussian Process regression]
    B --> C[Prediction in unknown regions\nMean + Uncertainty]
    C --> D[Acquisition Function calculation\nPropose next experimental point]
    D --> E[Execute experiment\nAcquire new data]
    E --> B

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>

<p><strong>Requirements for Surrogate Models</strong>:
1. <strong>Function with small data</strong>: Predictable with about 10-20 points
2. <strong>Quantify uncertainty</strong>: Evaluate prediction reliability
3. <strong>Fast</strong>: Instant prediction even for thousands of points
4. <strong>Flexible</strong>: Handle complex function shapes</p>
<p><strong>Gaussian Process Regression</strong> is a powerful method that satisfies these requirements.</p>
<hr />
<h2>2.2 Fundamentals of Gaussian Process Regression</h2>
<h3>What is a Gaussian Process</h3>
<p><strong>Gaussian Process (GP)</strong> is a method for defining a probability distribution over functions.</p>
<p><strong>Definition</strong>:</p>
<blockquote>
<p>A Gaussian Process is a stochastic process where the function values at any finite set of points <strong>follow a multivariate Gaussian distribution</strong>.</p>
</blockquote>
<p><strong>Intuitive Understanding</strong>:
- Consider a <strong>distribution of infinite functions</strong>, not just one function
- Update the function distribution based on observed data
- Predicted values at each point are represented by <strong>mean and variance</strong></p>
<h3>Mathematical Definition of Gaussian Process</h3>
<p>A Gaussian Process is completely defined by a <strong>mean function</strong> $m(x)$ and a <strong>kernel function (covariance function)</strong> $k(x, x')$:</p>
<p>$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$</p>
<p><strong>Mean Function</strong> $m(x)$:
- Usually assumed to be $m(x) = 0$ (learned from data)</p>
<p><strong>Kernel Function</strong> $k(x, x')$:
- Represents the "similarity" between two points
- Assumes that closer inputs lead to similar outputs</p>
<h3>Representative Kernel Functions</h3>
<p><strong>1. RBF (Radial Basis Function) Kernel</strong></p>
<p>$$
k(x, x') = \sigma^2 \exp\left(-\frac{||x - x'||^2}{2\ell^2}\right)
$$</p>
<ul>
<li>$\sigma^2$: Variance (output scale)</li>
<li>$\ell$: Length scale (how smooth)</li>
</ul>
<p><strong>Characteristics</strong>:
- Most common
- Infinitely differentiable (smooth function)
- Suitable for materials property prediction</p>
<p><strong>Code Example 1: RBF Kernel Visualization</strong></p>
<pre><code class="language-python"># RBF Kernel Visualization
import numpy as np
import matplotlib.pyplot as plt

def rbf_kernel(x1, x2, sigma=1.0, length_scale=1.0):
    &quot;&quot;&quot;
    RBF Kernel Function

    Parameters:
    -----------
    x1, x2 : array
        Input points
    sigma : float
        Standard deviation (output scale)
    length_scale : float
        Length scale (input correlation distance)

    Returns:
    --------
    float : Kernel value (similarity)
    &quot;&quot;&quot;
    distance = np.abs(x1 - x2)
    return sigma**2 * np.exp(-0.5 * (distance / length_scale)**2)

# Visualize kernel with different length scales
x_ref = 0.5  # Reference point
x_range = np.linspace(0, 1, 100)

plt.figure(figsize=(12, 4))

# Left: Effect of length scale
plt.subplot(1, 3, 1)
for length_scale in [0.05, 0.1, 0.2, 0.5]:
    k_values = [rbf_kernel(x_ref, x, sigma=1.0,
                           length_scale=length_scale)
                for x in x_range]
    plt.plot(x_range, k_values,
             label=f'$\\ell$ = {length_scale}', linewidth=2)
plt.axvline(x_ref, color='black', linestyle='--', alpha=0.5)
plt.xlabel('x', fontsize=12)
plt.ylabel('k(0.5, x)', fontsize=12)
plt.title('Effect of Length Scale', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Center: Multiple reference points
plt.subplot(1, 3, 2)
for x_ref_temp in [0.2, 0.5, 0.8]:
    k_values = [rbf_kernel(x_ref_temp, x, sigma=1.0,
                           length_scale=0.1)
                for x in x_range]
    plt.plot(x_range, k_values,
             label=f'x_ref = {x_ref_temp}', linewidth=2)
plt.xlabel('x', fontsize=12)
plt.ylabel('k(x_ref, x)', fontsize=12)
plt.title('Effect of Reference Point', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Right: Kernel matrix visualization
plt.subplot(1, 3, 3)
x_grid = np.linspace(0, 1, 50)
K = np.zeros((len(x_grid), len(x_grid)))
for i, x1 in enumerate(x_grid):
    for j, x2 in enumerate(x_grid):
        K[i, j] = rbf_kernel(x1, x2, sigma=1.0, length_scale=0.1)

plt.imshow(K, cmap='viridis', origin='lower', extent=[0, 1, 0, 1])
plt.colorbar(label='Kernel Value')
plt.xlabel('x', fontsize=12)
plt.ylabel(&quot;x'&quot;, fontsize=12)
plt.title('Kernel Matrix', fontsize=14)

plt.tight_layout()
plt.savefig('rbf_kernel_visualization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(&quot;RBF Kernel Characteristics:&quot;)
print(&quot;  - Small length scale ‚Üí Local correlation (sharp peak)&quot;)
print(&quot;  - Large length scale ‚Üí Wide-range correlation (smooth curve)&quot;)
print(&quot;  - Maximum kernel value on diagonal (x = x')&quot;)
</code></pre>
<p><strong>Important Points</strong>:
- <strong>Length Scale $\ell$</strong>: Controls function smoothness
  - Small $\ell$ ‚Üí Allows steep changes
  - Large $\ell$ ‚Üí Assumes smooth function
- <strong>Meaning in Materials Science</strong>: Assumes that similar compositions or conditions lead to similar properties</p>
<hr />
<h3>Prediction Formulas for Gaussian Process Regression</h3>
<p>Given observed data $\mathcal{D} = {(x_1, y_1), \ldots, (x_n, y_n)}$, the prediction at a new point $x_*$ is:</p>
<p><strong>Predictive Mean</strong>:
$$
\mu(x_*) = k_* K^{-1} \mathbf{y}
$$</p>
<p><strong>Predictive Variance</strong>:
$$
\sigma^2(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_*
$$</p>
<p>Where:
- $K$: Kernel matrix between observed points $K_{ij} = k(x_i, x_j)$
- $k_*$: Kernel vector between new point and observed points
- $\mathbf{y}$: Vector of observed values</p>
<p><strong>Predictive Distribution</strong>:
$$
f(x_*) | \mathcal{D} \sim \mathcal{N}(\mu(x_*), \sigma^2(x_*))
$$</p>
<p><strong>Code Example 2: Gaussian Process Regression Implementation and Visualization</strong></p>
<pre><code class="language-python"># Gaussian Process Regression Implementation
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist

class GaussianProcessRegressor:
    &quot;&quot;&quot;
    Simple Gaussian Process Regression Implementation

    Parameters:
    -----------
    kernel : str
        Kernel type (only 'rbf' supported)
    sigma : float
        Kernel standard deviation
    length_scale : float
        Kernel length scale
    noise : float
        Observation noise standard deviation
    &quot;&quot;&quot;

    def __init__(self, kernel='rbf', sigma=1.0,
                 length_scale=0.1, noise=0.01):
        self.kernel = kernel
        self.sigma = sigma
        self.length_scale = length_scale
        self.noise = noise
        self.X_train = None
        self.y_train = None
        self.K_inv = None

    def _kernel(self, X1, X2):
        &quot;&quot;&quot;Compute kernel matrix&quot;&quot;&quot;
        if self.kernel == 'rbf':
            dists = cdist(X1, X2, 'sqeuclidean')
            K = self.sigma**2 * np.exp(-0.5 * dists /
                                        self.length_scale**2)
            return K
        else:
            raise ValueError(f&quot;Unknown kernel: {self.kernel}&quot;)

    def fit(self, X_train, y_train):
        &quot;&quot;&quot;
        Train Gaussian Process model

        Parameters:
        -----------
        X_train : array (n_samples, n_features)
            Training input
        y_train : array (n_samples,)
            Training output
        &quot;&quot;&quot;
        self.X_train = X_train
        self.y_train = y_train

        # Compute kernel matrix (add noise term)
        K = self._kernel(X_train, X_train)
        K += self.noise**2 * np.eye(len(X_train))

        # Compute inverse matrix (used in prediction)
        self.K_inv = np.linalg.inv(K)

    def predict(self, X_test, return_std=False):
        &quot;&quot;&quot;
        Perform prediction

        Parameters:
        -----------
        X_test : array (n_test, n_features)
            Test input
        return_std : bool
            Whether to return standard deviation

        Returns:
        --------
        mean : array (n_test,)
            Predictive mean
        std : array (n_test,) (if return_std=True)
            Predictive standard deviation
        &quot;&quot;&quot;
        # k_* = k(X_test, X_train)
        k_star = self._kernel(X_test, self.X_train)

        # Predictive mean: Œº(x_*) = k_* K^{-1} y
        mean = k_star @ self.K_inv @ self.y_train

        if return_std:
            # k(x_*, x_*)
            k_starstar = self._kernel(X_test, X_test)

            # Predictive variance: œÉ¬≤(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_*
            variance = np.diag(k_starstar) - np.sum(
                (k_star @ self.K_inv) * k_star, axis=1
            )
            std = np.sqrt(np.maximum(variance, 0))  # Prevent numerical errors
            return mean, std
        else:
            return mean

# Demonstration: Ionic conductivity prediction for materials
np.random.seed(42)

# True function (assumed unknown)
def true_function(x):
    &quot;&quot;&quot;Ionic conductivity of Li-ion battery electrolyte (hypothetical)&quot;&quot;&quot;
    return (
        np.sin(3 * x) * np.exp(-x) +
        0.7 * np.exp(-((x - 0.5) / 0.2)**2)
    )

# Observed data (small number of experimental results)
n_observations = 8
X_train = np.random.uniform(0, 1, n_observations).reshape(-1, 1)
y_train = true_function(X_train).ravel() + np.random.normal(0, 0.05,
                                                             n_observations)

# Test data (points to predict)
X_test = np.linspace(0, 1, 200).reshape(-1, 1)
y_true = true_function(X_test).ravel()

# Train Gaussian Process regression model
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15, noise=0.05)
gp.fit(X_train, y_train)

# Prediction
y_pred, y_std = gp.predict(X_test, return_std=True)

# Visualization
plt.figure(figsize=(12, 6))

# True function
plt.plot(X_test, y_true, 'k--', linewidth=2, label='True Function')

# Observed data
plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data (Experimental Results)')

# Predictive mean
plt.plot(X_test, y_pred, 'b-', linewidth=2, label='Predictive Mean')

# Uncertainty (95% confidence interval)
plt.fill_between(
    X_test.ravel(),
    y_pred - 1.96 * y_std,
    y_pred + 1.96 * y_std,
    alpha=0.3,
    color='blue',
    label='95% Confidence Interval'
)

plt.xlabel('Composition Parameter x', fontsize=12)
plt.ylabel('Ionic Conductivity (mS/cm)', fontsize=12)
plt.title('Material Property Prediction using Gaussian Process Regression', fontsize=14)
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gp_regression_demo.png', dpi=150, bbox_inches='tight')
plt.show()

print(&quot;Gaussian Process Regression Results:&quot;)
print(f&quot;  Number of observations: {n_observations}&quot;)
print(f&quot;  Number of prediction points: {len(X_test)}&quot;)
print(f&quot;  RMSE: {np.sqrt(np.mean((y_pred - y_true)**2)):.4f}&quot;)
print(&quot;\nCharacteristics:&quot;)
print(&quot;  - Near observation points: Small uncertainty (narrow confidence interval)&quot;)
print(&quot;  - Unexplored regions: Large uncertainty (wide confidence interval)&quot;)
print(&quot;  - This uncertainty information is exploited by Acquisition Functions&quot;)
</code></pre>
<p><strong>Important Observations</strong>:
1. <strong>Near observation points</strong>: High prediction accuracy (low uncertainty)
2. <strong>Unexplored regions</strong>: High uncertainty
3. <strong>More data</strong>: Improved prediction accuracy
4. <strong>Quantifying uncertainty</strong>: Key to Bayesian Optimization</p>
<hr />
<h2>2.3 Acquisition Function: How to Select the Next Experimental Point</h2>
<h3>Role of Acquisition Functions</h3>
<p>The <strong>Acquisition Function</strong> mathematically determines "where to experiment next".</p>
<p><strong>Design Philosophy</strong>:
- <strong>Explore high predicted value locations</strong> (Exploitation)
- <strong>Explore high uncertainty locations</strong> (Exploration)
- <strong>Balance these two aspects</strong> through optimization</p>
<h3>Acquisition Function Workflow</h3>
<div class="mermaid">
flowchart TB
    A[Gaussian Process Model] --> B[Predictive Mean Œº(x)]
    A --> C[Predictive Standard Deviation œÉ(x)]
    B --> D[Acquisition Function Œ±(x)]
    C --> D
    D --> E[Maximize Acquisition Function]
    E --> F[Next Experimental Point x_next]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style F fill:#e8f5e9
</div>

<hr />
<h3>Main Acquisition Functions</h3>
<h4>1. Expected Improvement (EI)</h4>
<p><strong>Definition</strong>:
Maximize the expected improvement over the current best value $f_{\text{best}}$</p>
<p>$$
\text{EI}(x) = \mathbb{E}[\max(0, f(x) - f_{\text{best}})]
$$</p>
<p><strong>Analytical Solution</strong>:
$$
\text{EI}(x) = \begin{cases}
(\mu(x) - f_{\text{best}}) \Phi(Z) + \sigma(x) \phi(Z) &amp; \text{if } \sigma(x) &gt; 0 \
0 &amp; \text{if } \sigma(x) = 0
\end{cases}
$$</p>
<p>Where:
$$
Z = \frac{\mu(x) - f_{\text{best}}}{\sigma(x)}
$$
- $\Phi$: Cumulative distribution function of standard normal distribution
- $\phi$: Probability density function of standard normal distribution</p>
<p><strong>Characteristics</strong>:
- <strong>Well-balanced</strong>: Automatically adjusts exploration and exploitation
- <strong>Most common</strong>: Widely used in materials science
- <strong>Analytical</strong>: Fast computation</p>
<p><strong>Code Example 3: Expected Improvement Implementation</strong></p>
<pre><code class="language-python"># Expected Improvement Implementation
from scipy.stats import norm

def expected_improvement(X, gp, f_best, xi=0.01):
    &quot;&quot;&quot;
    Expected Improvement Acquisition Function

    Parameters:
    -----------
    X : array (n_samples, n_features)
        Evaluation points
    gp : GaussianProcessRegressor
        Trained Gaussian Process model
    f_best : float
        Current best value
    xi : float
        Exploration strength (exploration parameter)

    Returns:
    --------
    ei : array (n_samples,)
        EI values
    &quot;&quot;&quot;
    mu, sigma = gp.predict(X, return_std=True)

    # Improvement
    improvement = mu - f_best - xi

    # Standardization
    Z = improvement / (sigma + 1e-9)  # Avoid division by zero

    # Expected Improvement
    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)

    # EI = 0 when œÉ = 0
    ei[sigma == 0.0] = 0.0

    return ei

# Demonstration
np.random.seed(42)

# Observed data
X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

# Gaussian Process model
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.2, noise=0.01)
gp.fit(X_train, y_train)

# Test points
X_test = np.linspace(0, 1, 500).reshape(-1, 1)

# Prediction
y_pred, y_std = gp.predict(X_test, return_std=True)

# Current best value
f_best = np.max(y_train)

# Compute EI
ei = expected_improvement(X_test, gp, f_best, xi=0.01)

# Propose next experimental point
next_idx = np.argmax(ei)
next_x = X_test[next_idx]

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# Top: Gaussian Process prediction
ax1 = axes[0]
ax1.plot(X_test, true_function(X_test), 'k--',
         linewidth=2, label='True Function')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data')
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='Predictive Mean')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                 label='95% Confidence Interval')
ax1.axhline(f_best, color='green', linestyle=':',
            linewidth=2, label=f'Current Best = {f_best:.3f}')
ax1.axvline(next_x, color='orange', linestyle='--',
            linewidth=2, label=f'Proposed Point = {next_x[0]:.3f}')
ax1.set_ylabel('Objective Function', fontsize=12)
ax1.set_title('Gaussian Process Regression Prediction', fontsize=14)
ax1.legend(loc='best')
ax1.grid(True, alpha=0.3)

# Bottom: Expected Improvement
ax2 = axes[1]
ax2.plot(X_test, ei, 'r-', linewidth=2, label='Expected Improvement')
ax2.axvline(next_x, color='orange', linestyle='--',
            linewidth=2, label=f'Max EI Point = {next_x[0]:.3f}')
ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
ax2.set_xlabel('Parameter x', fontsize=12)
ax2.set_ylabel('EI(x)', fontsize=12)
ax2.set_title('Expected Improvement Acquisition Function', fontsize=14)
ax2.legend(loc='best')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('expected_improvement_demo.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(f&quot;Proposal by Expected Improvement:&quot;)
print(f&quot;  Next experimental point: x = {next_x[0]:.3f}&quot;)
print(f&quot;  EI value: {np.max(ei):.4f}&quot;)
print(f&quot;  Predictive mean: {y_pred[next_idx]:.3f}&quot;)
print(f&quot;  Predictive standard deviation: {y_std[next_idx]:.3f}&quot;)
</code></pre>
<p><strong>EI Interpretation</strong>:
- Locations with <strong>high mean values</strong> ‚Üí Exploitation
- Locations with <strong>high uncertainty</strong> ‚Üí Exploration
- <strong>Considers both</strong> for balance</p>
<hr />
<h4>2. Upper Confidence Bound (UCB)</h4>
<p><strong>Definition</strong>:
Maximize the "optimistic estimate" by adding uncertainty to the predictive mean</p>
<p>$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$</p>
<ul>
<li>$\kappa$: Parameter controlling exploration strength (typically $\kappa = 2$)</li>
</ul>
<p><strong>Characteristics</strong>:
- <strong>Simple</strong>: Easy to implement
- <strong>Intuitive</strong>: Principle of Optimism in the Face of Uncertainty
- <strong>Adjustable</strong>: Control exploration level with $\kappa$</p>
<p><strong>Influence of $\kappa$</strong>:
- Large $\kappa$ ‚Üí Exploration-focused (take risks)
- Small $\kappa$ ‚Üí Exploitation-focused (safe strategy)</p>
<p><strong>Code Example 4: Upper Confidence Bound Implementation</strong></p>
<pre><code class="language-python"># Upper Confidence Bound Implementation
def upper_confidence_bound(X, gp, kappa=2.0):
    &quot;&quot;&quot;
    Upper Confidence Bound Acquisition Function

    Parameters:
    -----------
    X : array (n_samples, n_features)
        Evaluation points
    gp : GaussianProcessRegressor
        Trained Gaussian Process model
    kappa : float
        Exploration strength (typically 2.0)

    Returns:
    --------
    ucb : array (n_samples,)
        UCB values
    &quot;&quot;&quot;
    mu, sigma = gp.predict(X, return_std=True)
    ucb = mu + kappa * sigma
    return ucb

# Demonstration: Comparison with different Œ∫ values
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

kappa_values = [0.5, 2.0, 5.0]

for i, kappa in enumerate(kappa_values):
    ax = axes[i]

    # Compute UCB
    ucb = upper_confidence_bound(X_test, gp, kappa=kappa)

    # Next experimental point
    next_idx = np.argmax(ucb)
    next_x = X_test[next_idx]

    # Predictive mean and confidence interval
    ax.plot(X_test, y_pred, 'b-', linewidth=2, label='Predictive Mean Œº(x)')
    ax.fill_between(X_test.ravel(),
                    y_pred - 1.96 * y_std,
                    y_pred + 1.96 * y_std,
                    alpha=0.2, color='blue',
                    label='95% Confidence Interval')

    # UCB
    ax.plot(X_test, ucb, 'r-', linewidth=2,
            label=f'UCB(x) (Œ∫={kappa})')

    # Observed data
    ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
               edgecolors='black', label='Observed Data')

    # Proposed point
    ax.axvline(next_x, color='orange', linestyle='--',
               linewidth=2, label=f'Proposed Point = {next_x[0]:.3f}')

    ax.set_ylabel('Objective Function', fontsize=12)
    ax.set_title(f'UCB with Œ∫ = {kappa}', fontsize=14)
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)

    if i == 2:
        ax.set_xlabel('Parameter x', fontsize=12)

plt.tight_layout()
plt.savefig('ucb_kappa_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print(&quot;Influence of Œ∫:&quot;)
print(&quot;  Œ∫ = 0.5: Exploitation-focused (explore near observed data)&quot;)
print(&quot;  Œ∫ = 2.0: Balanced (standard setting)&quot;)
print(&quot;  Œ∫ = 5.0: Exploration-focused (actively explore unknown regions)&quot;)
</code></pre>
<hr />
<h4>3. Probability of Improvement (PI)</h4>
<p><strong>Definition</strong>:
Maximize the probability of exceeding the current best value</p>
<p>$$
\text{PI}(x) = P(f(x) &gt; f_{\text{best}}) = \Phi\left(\frac{\mu(x) - f_{\text{best}}}{\sigma(x)}\right)
$$</p>
<p><strong>Characteristics</strong>:
- <strong>Simplest</strong>: Easy to interpret
- <strong>Conservative</strong>: Does not expect large improvements
- <strong>Practical</strong>: Strategy of accumulating small improvements</p>
<p><strong>Code Example 5: Probability of Improvement Implementation</strong></p>
<pre><code class="language-python"># Probability of Improvement Implementation
def probability_of_improvement(X, gp, f_best, xi=0.01):
    &quot;&quot;&quot;
    Probability of Improvement Acquisition Function

    Parameters:
    -----------
    X : array (n_samples, n_features)
        Evaluation points
    gp : GaussianProcessRegressor
        Trained Gaussian Process model
    f_best : float
        Current best value
    xi : float
        Exploration strength

    Returns:
    --------
    pi : array (n_samples,)
        PI values
    &quot;&quot;&quot;
    mu, sigma = gp.predict(X, return_std=True)

    # Improvement
    improvement = mu - f_best - xi

    # Standardization
    Z = improvement / (sigma + 1e-9)

    # Probability of Improvement
    pi = norm.cdf(Z)

    return pi

# Compute PI
pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)

# Visualization
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(X_test, pi, 'g-', linewidth=2, label='PI(x)')
plt.axvline(X_test[np.argmax(pi)], color='orange',
            linestyle='--', linewidth=2,
            label=f'Max PI Point = {X_test[np.argmax(pi)][0]:.3f}')
plt.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
plt.xlabel('Parameter x', fontsize=12)
plt.ylabel('PI(x)', fontsize=12)
plt.title('Probability of Improvement', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Comparison: EI vs PI vs UCB
plt.subplot(1, 2, 2)
ei_normalized = ei / np.max(ei)
pi_normalized = pi / np.max(pi)
ucb_normalized = upper_confidence_bound(X_test, gp, kappa=2.0)
ucb_normalized = (ucb_normalized - np.min(ucb_normalized)) / \
                 (np.max(ucb_normalized) - np.min(ucb_normalized))

plt.plot(X_test, ei_normalized, 'r-', linewidth=2, label='EI (Normalized)')
plt.plot(X_test, pi_normalized, 'g-', linewidth=2, label='PI (Normalized)')
plt.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (Normalized)')
plt.scatter(X_train, [0.5]*len(X_train), c='red', s=100,
            zorder=10, edgecolors='black', label='Observed Data')
plt.xlabel('Parameter x', fontsize=12)
plt.ylabel('Acquisition Function Value (Normalized)', fontsize=12)
plt.title('Comparison of Acquisition Functions', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('acquisition_functions_comparison.png', dpi=150,
            bbox_inches='tight')
plt.show()
</code></pre>
<hr />
<h3>Comparison Table of Acquisition Functions</h3>
<table>
<thead>
<tr>
<th>Acquisition Function</th>
<th>Feature</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Recommended Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EI</strong></td>
<td>Expected improvement value</td>
<td>Well-balanced, proven track record</td>
<td>Somewhat complex</td>
<td>General optimization</td>
</tr>
<tr>
<td><strong>UCB</strong></td>
<td>Optimistic estimation</td>
<td>Simple, adjustable</td>
<td>Requires Œ∫ tuning</td>
<td>Control exploration level</td>
</tr>
<tr>
<td><strong>PI</strong></td>
<td>Probability of improvement</td>
<td>Very simple</td>
<td>Conservative</td>
<td>Safe exploration</td>
</tr>
</tbody>
</table>
<p><strong>Recommendations for Materials Science</strong>:
- <strong>Beginners</strong>: EI (well-balanced, excellent by default)
- <strong>Exploration-focused</strong>: UCB (Œ∫ = 2-5)
- <strong>Safe strategy</strong>: PI (ensure small improvements)</p>
<hr />
<h2>2.4 Exploration-Exploitation Trade-off</h2>
<h3>Mathematical Formulation</h3>
<p>The Acquisition Function can be decomposed into two terms:</p>
<p>$$
\alpha(x) = \underbrace{\mu(x)}_{\text{Exploitation}} + \underbrace{\lambda \sigma(x)}_{\text{Exploration}}
$$</p>
<ul>
<li><strong>Exploitation Term</strong> $\mu(x)$: Locations with high predictive mean</li>
<li><strong>Exploration Term</strong> $\lambda \sigma(x)$: Locations with high uncertainty</li>
</ul>
<h3>Visualization of the Trade-off</h3>
<div class="mermaid">
flowchart LR
    subgraph Exploitation
    A1[Known good regions]
    A2[High predictive value Œº(x)]
    A3[Low uncertainty œÉ(x)]
    A1 --> A2
    A1 --> A3
    end

    subgraph Exploration
    B1[Unknown regions]
    B2[Unknown predictive value Œº(x)]
    B3[High uncertainty œÉ(x)]
    B1 --> B2
    B1 --> B3
    end

    subgraph Optimal Balance
    C1[Acquisition Function]
    C2[Œº(x) + ŒªœÉ(x)]
    C3[Next experimental point]
    C1 --> C2
    C2 --> C3
    end

    A2 --> C1
    A3 --> C1
    B2 --> C1
    B3 --> C1

    style A1 fill:#fff3e0
    style B1 fill:#e3f2fd
    style C3 fill:#e8f5e9
</div>

<p><strong>Code Example 6: Visualizing Exploration-Exploitation Balance</strong></p>
<pre><code class="language-python"># Decomposing exploration and exploitation
def decompose_acquisition(X, gp, f_best, xi=0.01):
    &quot;&quot;&quot;
    Decompose Acquisition Function into exploration and exploitation terms

    Returns:
    --------
    exploitation : Exploitation term (based on predictive mean)
    exploration : Exploration term (based on uncertainty)
    &quot;&quot;&quot;
    mu, sigma = gp.predict(X, return_std=True)

    # Exploitation term (larger for higher mean)
    exploitation = mu

    # Exploration term (larger for higher uncertainty)
    exploration = sigma

    return exploitation, exploration

# Decomposition
exploitation, exploration = decompose_acquisition(X_test, gp, f_best)

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(12, 14))

# 1. Gaussian Process prediction
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='Predictive Mean Œº(x)')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                 label='95% Confidence Interval')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data')
ax1.set_ylabel('Objective Function', fontsize=12)
ax1.set_title('Gaussian Process Prediction', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. Exploitation term
ax2 = axes[1]
ax2.plot(X_test, exploitation, 'g-', linewidth=2,
         label='Exploitation Term (Predictive Mean)')
ax2.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', alpha=0.5)
ax2.set_ylabel('Exploitation Term', fontsize=12)
ax2.set_title('Exploitation: Emphasize Known Good Regions', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Exploration term
ax3 = axes[2]
ax3.plot(X_test, exploration, 'orange', linewidth=2,
         label='Exploration Term (Uncertainty)')
ax3.scatter(X_train, [0]*len(X_train), c='red', s=100,
            zorder=10, edgecolors='black', alpha=0.5,
            label='Observed Data Locations')
ax3.set_ylabel('Exploration Term', fontsize=12)
ax3.set_title('Exploration: Emphasize Unknown Regions', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Integration (EI)
ax4 = axes[3]
ei_values = expected_improvement(X_test, gp, f_best, xi=0.01)
ax4.plot(X_test, ei_values, 'r-', linewidth=2,
         label='Expected Improvement')
next_x = X_test[np.argmax(ei_values)]
ax4.axvline(next_x, color='purple', linestyle='--',
            linewidth=2, label=f'Proposed Point = {next_x[0]:.3f}')
ax4.fill_between(X_test.ravel(), 0, ei_values,
                 alpha=0.3, color='red')
ax4.set_xlabel('Parameter x', fontsize=12)
ax4.set_ylabel('EI(x)', fontsize=12)
ax4.set_title('Integration: Optimize Balance of Both', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('exploitation_exploration_tradeoff.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(&quot;Exploration-Exploitation Trade-off:&quot;)
print(f&quot;  Proposed point x = {next_x[0]:.3f}&quot;)
print(f&quot;    Predictive mean (exploitation): {y_pred[np.argmax(ei_values)]:.3f}&quot;)
print(f&quot;    Uncertainty (exploration): {y_std[np.argmax(ei_values)]:.3f}&quot;)
print(f&quot;    EI value: {np.max(ei_values):.4f}&quot;)
</code></pre>
<hr />
<h2>2.5 Constrained and Multi-objective Optimization</h2>
<h3>Constrained Bayesian Optimization</h3>
<p>In actual materials development, <strong>constraints</strong> exist:</p>
<p><strong>Example: Li-ion Battery Electrolyte</strong>
- Maximize ionic conductivity (objective function)
- Viscosity &lt; 10 cP (constraint 1)
- Flash point &gt; 100¬∞C (constraint 2)
- Cost &lt; $50/kg (constraint 3)</p>
<p><strong>Mathematical Formulation</strong>:
$$
\begin{align}
\max_{x} \quad &amp; f(x) \
\text{s.t.} \quad &amp; g_i(x) \leq 0, \quad i = 1, \ldots, m \
&amp; h_j(x) = 0, \quad j = 1, \ldots, p
\end{align}
$$</p>
<p><strong>Approach</strong>:
1. <strong>Model constraint functions with Gaussian Process</strong>
2. <strong>Incorporate probability of satisfying constraints into Acquisition Function</strong></p>
<p><strong>Code Example 7: Constrained Bayesian Optimization Demo</strong></p>
<pre><code class="language-python"># Constrained Bayesian Optimization
def constrained_expected_improvement(X, gp_obj, gp_constraint,
                                     f_best, constraint_threshold=0):
    &quot;&quot;&quot;
    Constrained Expected Improvement

    Parameters:
    -----------
    gp_obj : Gaussian Process (objective function)
    gp_constraint : Gaussian Process (constraint function)
    constraint_threshold : Constraint threshold (‚â§ 0 is feasible)
    &quot;&quot;&quot;
    # EI for objective function
    ei = expected_improvement(X, gp_obj, f_best, xi=0.01)

    # Probability of satisfying constraints
    mu_c, sigma_c = gp_constraint.predict(X, return_std=True)
    prob_feasible = norm.cdf((constraint_threshold - mu_c) /
                             (sigma_c + 1e-9))

    # Constrained EI = EI √ó constraint satisfaction probability
    cei = ei * prob_feasible

    return cei

# Demo: Define constraint function
def constraint_function(x):
    &quot;&quot;&quot;Constraint function (e.g., viscosity upper limit)&quot;&quot;&quot;
    return 0.5 - x  # x &lt; 0.5 is feasible region

# Constraint data
y_constraint = constraint_function(X_train).ravel()

# Gaussian Process for constraint function
gp_constraint = GaussianProcessRegressor(sigma=0.5, length_scale=0.2,
                                         noise=0.01)
gp_constraint.fit(X_train, y_constraint)

# Compute constrained EI
cei = constrained_expected_improvement(X_test, gp, gp_constraint,
                                       f_best, constraint_threshold=0)

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

# Top: Objective function
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='Objective Function Prediction')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data')
ax1.set_ylabel('Objective Function', fontsize=12)
ax1.set_title('Objective Function (Ionic Conductivity)', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# Middle: Constraint function
ax2 = axes[1]
mu_c, sigma_c = gp_constraint.predict(X_test, return_std=True)
ax2.plot(X_test, mu_c, 'g-', linewidth=2, label='Constraint Function Prediction')
ax2.fill_between(X_test.ravel(), mu_c - 1.96 * sigma_c,
                 mu_c + 1.96 * sigma_c, alpha=0.3, color='green')
ax2.axhline(0, color='red', linestyle='--', linewidth=2,
            label='Constraint Boundary (‚â§ 0 is feasible)')
ax2.axhspan(-10, 0, alpha=0.2, color='green',
            label='Feasible Region')
ax2.scatter(X_train, y_constraint, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data')
ax2.set_ylabel('Constraint Function', fontsize=12)
ax2.set_title('Constraint Function (Viscosity Limit)', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

# Bottom: Constrained EI
ax3 = axes[2]
ei_unconstrained = expected_improvement(X_test, gp, f_best, xi=0.01)
ax3.plot(X_test, ei_unconstrained, 'r--', linewidth=2,
         label='EI (Unconstrained)', alpha=0.5)
ax3.plot(X_test, cei, 'r-', linewidth=2, label='Constrained EI')
next_x = X_test[np.argmax(cei)]
ax3.axvline(next_x, color='purple', linestyle='--', linewidth=2,
            label=f'Proposed Point = {next_x[0]:.3f}')
ax3.fill_between(X_test.ravel(), 0, cei, alpha=0.3, color='red')
ax3.set_xlabel('Parameter x', fontsize=12)
ax3.set_ylabel('Acquisition Function', fontsize=12)
ax3.set_title('Constrained Expected Improvement', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('constrained_bayesian_optimization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(&quot;Constrained Optimization Results:&quot;)
print(f&quot;  Proposed point: x = {next_x[0]:.3f}&quot;)
print(f&quot;  Unconstrained EI maximum: x = {X_test[np.argmax(ei_unconstrained)][0]:.3f}&quot;)
print(f&quot;  ‚Üí Proposed point changed due to constraints&quot;)
</code></pre>
<hr />
<h3>Multi-objective Optimization</h3>
<p>In materials development, we often want to <strong>optimize multiple properties simultaneously</strong>.</p>
<p><strong>Example: Thermoelectric Materials</strong>
- Maximize Seebeck coefficient
- Minimize electrical resistivity
- Minimize thermal conductivity</p>
<p><strong>Pareto Front</strong>:
- When trade-offs exist, there is no single optimal solution
- Find <strong>a set of Pareto optimal solutions</strong></p>
<p><strong>Approaches</strong>:
1. <strong>Scalarization</strong>: Weighted sum $f(x) = w_1 f_1(x) + w_2 f_2(x)$
2. <strong>ParEGO</strong>: Repeat scalarization with random weights
3. <strong>EHVI</strong>: Expected Hypervolume Improvement</p>
<p><strong>Code Example 8: Multi-objective Optimization Visualization</strong></p>
<pre><code class="language-python"># Multi-objective Optimization Demo
def objective1(x):
    &quot;&quot;&quot;Objective 1: Ionic conductivity (maximize)&quot;&quot;&quot;
    return true_function(x)

def objective2(x):
    &quot;&quot;&quot;Objective 2: Viscosity (minimize)&quot;&quot;&quot;
    return 0.5 + 0.3 * np.sin(5 * x)

# Compute Pareto optimal solutions
x_grid = np.linspace(0, 1, 1000)
obj1_values = objective1(x_grid)
obj2_values = objective2(x_grid)

# Determine Pareto optimality
def is_pareto_optimal(costs):
    &quot;&quot;&quot;
    Determine Pareto optimal solutions

    Parameters:
    -----------
    costs : array (n_samples, n_objectives)
        Cost of each point (as minimization problem)

    Returns:
    --------
    pareto_mask : array (n_samples,)
        True for Pareto optimal
    &quot;&quot;&quot;
    is_pareto = np.ones(len(costs), dtype=bool)
    for i, c in enumerate(costs):
        if is_pareto[i]:
            # Check if dominated by other points
            is_pareto[is_pareto] = np.any(
                costs[is_pareto] &lt; c, axis=1
            )
            is_pareto[i] = True
    return is_pareto

# Cost matrix (as minimization problem)
costs = np.column_stack([
    -obj1_values,  # Maximize ‚Üí Minimize
    obj2_values    # Minimize
])

# Pareto optimal solutions
pareto_mask = is_pareto_optimal(costs)
pareto_x = x_grid[pareto_mask]
pareto_obj1 = obj1_values[pareto_mask]
pareto_obj2 = obj2_values[pareto_mask]

# Visualization
fig = plt.figure(figsize=(14, 6))

# Left: Parameter space
ax1 = plt.subplot(1, 2, 1)
ax1.plot(x_grid, obj1_values, 'b-', linewidth=2,
         label='Objective 1 (Ionic Conductivity)')
ax1.plot(x_grid, obj2_values, 'r-', linewidth=2,
         label='Objective 2 (Viscosity)')
ax1.scatter(pareto_x, pareto_obj1, c='blue', s=50, alpha=0.6,
            label='Pareto Optimal (Obj 1)')
ax1.scatter(pareto_x, pareto_obj2, c='red', s=50, alpha=0.6,
            label='Pareto Optimal (Obj 2)')
ax1.set_xlabel('Parameter x', fontsize=12)
ax1.set_ylabel('Objective Function Value', fontsize=12)
ax1.set_title('Parameter Space', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right: Objective space (Pareto front)
ax2 = plt.subplot(1, 2, 2)
ax2.scatter(obj1_values, obj2_values, c='lightgray', s=20,
            alpha=0.5, label='All Exploration Points')
ax2.scatter(pareto_obj1, pareto_obj2, c='red', s=100,
            edgecolors='black', zorder=10,
            label='Pareto Front')
ax2.plot(pareto_obj1, pareto_obj2, 'r--', linewidth=2, alpha=0.5)
ax2.set_xlabel('Objective 1 (Ionic Conductivity) ‚Üí Maximize', fontsize=12)
ax2.set_ylabel('Objective 2 (Viscosity) ‚Üí Minimize', fontsize=12)
ax2.set_title('Objective Space and Pareto Front', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('multi_objective_optimization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(f&quot;Number of Pareto optimal solutions: {np.sum(pareto_mask)}&quot;)
print(&quot;Trade-off examples:&quot;)
print(f&quot;  High conductivity: Obj 1 = {np.max(pareto_obj1):.3f}, &quot;
      f&quot;Obj 2 = {pareto_obj2[np.argmax(pareto_obj1)]:.3f}&quot;)
print(f&quot;  Low viscosity: Obj 1 = {pareto_obj1[np.argmin(pareto_obj2)]:.3f}, &quot;
      f&quot;Obj 2 = {np.min(pareto_obj2):.3f}&quot;)
</code></pre>
<hr />
<h2>2.6 Column: Practical Kernel Selection</h2>
<h3>Types and Characteristics of Kernels</h3>
<p>Besides RBF, various kernels exist:</p>
<p><strong>Mat√©rn Kernel</strong>:
$$
k(x, x') = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right)
$$</p>
<ul>
<li>$\nu$: Smoothness parameter</li>
<li>$\nu = 1/2$: Exponential kernel (rough function)</li>
<li>$\nu = 3/2, 5/2$: Moderate smoothness</li>
<li>$\nu \to \infty$: RBF kernel (very smooth)</li>
</ul>
<p><strong>Selection Guidelines for Materials Science</strong>:
- <strong>DFT calculation results</strong>: RBF (smooth)
- <strong>Experimental data</strong>: Mat√©rn 3/2 or 5/2 (considering noise)
- <strong>Composition optimization</strong>: RBF
- <strong>Process conditions</strong>: Mat√©rn (steep changes exist)</p>
<p><strong>Periodic phenomena</strong>: Periodic kernel
$$
k(x, x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi|x - x'|/p)}{\ell^2}\right)
$$
- Crystal structure (with periodicity)
- Temperature cycles</p>
<hr />
<h2>2.7 Troubleshooting</h2>
<h3>Common Problems and Solutions</h3>
<p><strong>Problem 1: Acquisition Function Always Proposes the Same Location</strong></p>
<p><strong>Causes</strong>:
- Length scale too large ‚Üí Overall too smooth
- Noise parameter too small ‚Üí Over-trusting observation points</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Adjust length scale
gp = GaussianProcessRegressor(length_scale=0.05, noise=0.1)

# Or, automatically tune hyperparameters
from sklearn.gaussian_process import GaussianProcessRegressor as SKGP
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

kernel = RBF(length_scale=0.1) + WhiteKernel(noise_level=0.1)
gp = SKGP(kernel=kernel, n_restarts_optimizer=10)
gp.fit(X_train, y_train)
</code></pre>
<p><strong>Problem 2: Unstable Predictions (Abnormally Wide Confidence Intervals)</strong></p>
<p><strong>Causes</strong>:
- Too little data
- Kernel matrix numerically unstable</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Add regularization term
K = kernel_matrix + 1e-6 * np.eye(n_samples)  # Add jitter

# Or use Cholesky decomposition (improved numerical stability)
from scipy.linalg import cho_solve, cho_factor

L = cho_factor(K, lower=True)
alpha = cho_solve(L, y_train)
</code></pre>
<p><strong>Problem 3: Slow Computation (Large-scale Data)</strong></p>
<p><strong>Cause</strong>:
- Gaussian Process computational complexity: $O(n^3)$ (n = number of data)</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># 1. Sparse Gaussian Process
# Use inducing points

# 2. Approximation methods
# - Sparse GP
# - Local GP (domain partitioning)

# 3. Library utilization
# GPyTorch (GPU acceleration)
# GPflow (TensorFlow backend)
</code></pre>
<hr />
<h2>2.8 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Role of Surrogate Models</strong>
   - Estimate objective functions from small experimental data
   - Gaussian Process regression is most common
   - Capable of quantifying uncertainty</p>
</li>
<li>
<p><strong>Gaussian Process Regression</strong>
   - Define similarity between points with kernel functions
   - RBF kernel is standard in materials science
   - Compute predictive mean and predictive variance</p>
</li>
<li>
<p><strong>Acquisition Functions</strong>
   - Mathematical criteria for determining next experimental point
   - EI (Expected Improvement): Balanced approach
   - UCB (Upper Confidence Bound): Adjustable
   - PI (Probability of Improvement): Simple</p>
</li>
<li>
<p><strong>Exploration and Exploitation</strong>
   - Exploitation: Exploit known good regions
   - Exploration: Explore unknown regions
   - Acquisition Function automatically balances both</p>
</li>
<li>
<p><strong>Advanced Topics</strong>
   - Constrained optimization: Important in practice
   - Multi-objective optimization: Visualizing trade-offs</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>‚úÖ Gaussian Process regression can <strong>quantify uncertainty</strong></li>
<li>‚úÖ Acquisition Functions <strong>mathematically optimize exploration and exploitation</strong></li>
<li>‚úÖ EI is the <strong>most common with proven track record</strong></li>
<li>‚úÖ Kernel selection <strong>determines model performance</strong></li>
<li>‚úÖ <strong>Extensions to constrained and multi-objective</strong> are possible</li>
</ul>
<h3>Next Chapter</h3>
<p>In Chapter 3, we will learn implementation using Python libraries:
- How to use scikit-optimize (skopt)
- BoTorch (PyTorch version) implementation
- Materials exploration with real data
- Hyperparameter tuning</p>
<p><strong><a href="./chapter-3.html">Chapter 3: Python Practice ‚Üí</a></strong></p>
<hr />
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: Easy)</h3>
<p>Investigate the effect of the RBF kernel length scale $\ell$ on Gaussian Process predictions.</p>
<p><strong>Tasks</strong>:
1. Generate 5 observation data points
2. Train Gaussian Process with $\ell = 0.05, 0.1, 0.2, 0.5$
3. Plot predictive mean and confidence intervals
4. Explain the effect of length scale</p>
<details>
<summary>Hint</summary>

- Change the `length_scale` parameter of `GaussianProcessRegressor`
- Get standard deviation with `predict(return_std=True)`
- Small length scale ‚Üí Fits locally
- Large length scale ‚Üí Smooth curve

</details>

<details>
<summary>Solution Example</summary>


<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Observation data
np.random.seed(42)
X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

# Test data
X_test = np.linspace(0, 1, 200).reshape(-1, 1)
y_true = true_function(X_test).ravel()

# Train with different length scales
length_scales = [0.05, 0.1, 0.2, 0.5]
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for i, ls in enumerate(length_scales):
    ax = axes[i]

    # Gaussian Process model
    gp = GaussianProcessRegressor(sigma=1.0, length_scale=ls,
                                   noise=0.01)
    gp.fit(X_train, y_train)

    # Prediction
    y_pred, y_std = gp.predict(X_test, return_std=True)

    # Plot
    ax.plot(X_test, y_true, 'k--', linewidth=2, label='True Function')
    ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
               edgecolors='black', label='Observed Data')
    ax.plot(X_test, y_pred, 'b-', linewidth=2, label='Predictive Mean')
    ax.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                    y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                    label='95% Confidence Interval')
    ax.set_title(f'Length Scale = {ls}', fontsize=14)
    ax.set_xlabel('x', fontsize=12)
    ax.set_ylabel('y', fontsize=12)
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('length_scale_effect.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(&quot;Effect of Length Scale:&quot;)
print(&quot;  Small (0.05): Fits observation points closely, unstable between points&quot;)
print(&quot;  Medium (0.1-0.2): Well-balanced&quot;)
print(&quot;  Large (0.5): Smooth but deviates from observation points&quot;)
</code></pre>


**Explanation**:
- **$\ell$ = 0.05**: Tends to overfit, unstable between observation points
- **$\ell$ = 0.1-0.2**: Moderate smoothness, practical
- **$\ell$ = 0.5**: Underfits, too smooth

**Implications for Materials Science**:
- Experimental data: $\ell$ = 0.1-0.3 is common
- DFT calculations: Smoother ($\ell$ = 0.3-0.5)
- Determine optimal value with cross-validation

</details>

<hr />
<h3>Exercise 2 (Difficulty: Medium)</h3>
<p>Implement three Acquisition Functions (EI, UCB, PI) and compare them with the same data.</p>
<p><strong>Tasks</strong>:
1. Use the same observation data
2. Propose next experimental points with each Acquisition Function
3. Visualize differences in proposed points
4. Explain characteristics of each</p>
<details>
<summary>Hint</summary>

- Evaluate the same Gaussian Process model with three Acquisition Functions
- Use `np.argmax()` to get the position of maximum value
- Use $\kappa = 2.0$ for UCB

</details>

<details>
<summary>Solution Example</summary>


<pre><code class="language-python"># Observed data
np.random.seed(42)
X_train = np.array([0.15, 0.4, 0.6, 0.85]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

# Gaussian Process model
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15,
                               noise=0.01)
gp.fit(X_train, y_train)

# Current best value
f_best = np.max(y_train)

# Test points
X_test = np.linspace(0, 1, 500).reshape(-1, 1)
y_pred, y_std = gp.predict(X_test, return_std=True)

# Calculate Acquisition Functions
ei = expected_improvement(X_test, gp, f_best, xi=0.01)
ucb = upper_confidence_bound(X_test, gp, kappa=2.0)
pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)

# Proposed points
next_x_ei = X_test[np.argmax(ei)]
next_x_ucb = X_test[np.argmax(ucb)]
next_x_pi = X_test[np.argmax(pi)]

# Visualization
fig, axes = plt.subplots(4, 1, figsize=(12, 14))

# 1. Gaussian Process prediction
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='Predicted Mean')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='Observed Data')
ax1.axhline(f_best, color='green', linestyle=':', linewidth=2,
            label=f'Best Value = {f_best:.3f}')
ax1.set_ylabel('Objective Function', fontsize=12)
ax1.set_title('Gaussian Process Prediction', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. Expected Improvement
ax2 = axes[1]
ax2.plot(X_test, ei, 'r-', linewidth=2, label='EI')
ax2.axvline(next_x_ei, color='red', linestyle='--', linewidth=2,
            label=f'Proposed Point = {next_x_ei[0]:.3f}')
ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
ax2.set_ylabel('EI(x)', fontsize=12)
ax2.set_title('Expected Improvement', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Upper Confidence Bound
ax3 = axes[2]
# Normalize UCB (for easier comparison)
ucb_normalized = (ucb - np.min(ucb)) / (np.max(ucb) - np.min(ucb))
ax3.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (Normalized)')
ax3.axvline(next_x_ucb, color='blue', linestyle='--', linewidth=2,
            label=f'Proposed Point = {next_x_ucb[0]:.3f}')
ax3.fill_between(X_test.ravel(), 0, ucb_normalized, alpha=0.3,
                 color='blue')
ax3.set_ylabel('UCB(x)', fontsize=12)
ax3.set_title('Upper Confidence Bound (Œ∫=2.0)', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Probability of Improvement
ax4 = axes[3]
ax4.plot(X_test, pi, 'g-', linewidth=2, label='PI')
ax4.axvline(next_x_pi, color='green', linestyle='--', linewidth=2,
            label=f'Proposed Point = {next_x_pi[0]:.3f}')
ax4.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
ax4.set_xlabel('Parameter x', fontsize=12)
ax4.set_ylabel('PI(x)', fontsize=12)
ax4.set_title('Probability of Improvement', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('acquisition_functions_detailed_comparison.png',
            dpi=150, bbox_inches='tight')
plt.show()

# Results summary
print(&quot;Proposed Points by Acquisition Function:&quot;)
print(f&quot;  EI:  x = {next_x_ei[0]:.3f}&quot;)
print(f&quot;  UCB: x = {next_x_ucb[0]:.3f}&quot;)
print(f&quot;  PI:  x = {next_x_pi[0]:.3f}&quot;)

print(&quot;\nCharacteristics:&quot;)
print(&quot;  EI: Balanced approach, maximizes expected improvement&quot;)
print(&quot;  UCB: Exploration-focused, favors high uncertainty regions&quot;)
print(&quot;  PI: Conservative, favors even small improvements&quot;)
</code></pre>


**Expected Output**:

<pre><code>Proposed Points by Acquisition Function:
  EI:  x = 0.722
  UCB: x = 0.108
  PI:  x = 0.752

Characteristics:
  EI: Balanced approach, maximizes expected improvement
  UCB: Exploration-focused, favors high uncertainty regions
  PI: Conservative, favors even small improvements
</code></pre>


**Detailed Explanation**:
- **EI**: Proposes points between unexplored regions and regions with good predictions
- **UCB**: Explores the left edge with sparse data (uncertainty-focused)
- **PI**: Proposes locations where predicted mean is likely to exceed best value

**Practical Selection**:
- General optimization ‚Üí EI
- Initial exploration phase ‚Üí UCB (with larger Œ∫)
- Convergence phase ‚Üí PI or EI

</details>

<hr />
<h3>Problem 3 (Difficulty: Hard)</h3>
<p>Implement constrained Bayesian Optimization and compare it with the unconstrained case.</p>
<p><strong>Background</strong>:
Li-ion battery electrolyte optimization
- Objective: Maximize ionic conductivity
- Constraint: Viscosity &lt; 10 cP</p>
<p><strong>Tasks</strong>:
1. Define objective function and constraint function
2. Run unconstrained Bayesian Optimization (10 iterations)
3. Run constrained Bayesian Optimization (10 iterations)
4. Compare exploration trajectories
5. Evaluate final solutions found</p>
<details>
<summary>Hint</summary>

**Approach**:
1. Initial random sampling (3 points)
2. Build two Gaussian Process models (for objective function and constraint function)
3. Sequential sampling in loop
4. Use constrained EI

**Functions to use**:
- `constrained_expected_improvement()`

</details>

<details>
<summary>Solution Example</summary>


<pre><code class="language-python"># Define objective function and constraint function
def objective_conductivity(x):
    &quot;&quot;&quot;Ionic conductivity (to maximize)&quot;&quot;&quot;
    return true_function(x)

def constraint_viscosity(x):
    &quot;&quot;&quot;Viscosity constraint (‚â§ 10 cP normalized to 0)&quot;&quot;&quot;
    viscosity = 15 - 10 * x  # Viscosity model
    return viscosity - 10  # ‚â§ 0 is feasible (10 cP or below)

# Bayesian Optimization simulation
def run_bayesian_optimization(n_iterations=10,
                               use_constraint=False):
    &quot;&quot;&quot;
    Run Bayesian Optimization

    Parameters:
    -----------
    n_iterations : int
        Number of optimization iterations
    use_constraint : bool
        Whether to use constraints

    Returns:
    --------
    X_sampled : Experimental points
    y_sampled : Objective function values
    c_sampled : Constraint function values (only when constrained)
    &quot;&quot;&quot;
    # Initial random sampling
    np.random.seed(42)
    X_sampled = np.random.uniform(0, 1, 3).reshape(-1, 1)
    y_sampled = objective_conductivity(X_sampled).ravel()
    c_sampled = constraint_viscosity(X_sampled).ravel()

    # Sequential sampling
    for i in range(n_iterations - 3):
        # Gaussian Process model (objective function)
        gp_obj = GaussianProcessRegressor(sigma=1.0,
                                           length_scale=0.15,
                                           noise=0.01)
        gp_obj.fit(X_sampled, y_sampled)

        # Candidate points
        X_candidate = np.linspace(0, 1, 1000).reshape(-1, 1)

        if use_constraint:
            # Gaussian Process model (constraint function)
            gp_constraint = GaussianProcessRegressor(sigma=0.5,
                                                     length_scale=0.2,
                                                     noise=0.01)
            gp_constraint.fit(X_sampled, c_sampled)

            # Constrained EI
            f_best = np.max(y_sampled)
            acq = constrained_expected_improvement(
                X_candidate, gp_obj, gp_constraint, f_best,
                constraint_threshold=0
            )
        else:
            # Unconstrained EI
            f_best = np.max(y_sampled)
            acq = expected_improvement(X_candidate, gp_obj,
                                       f_best, xi=0.01)

        # Next experimental point
        next_x = X_candidate[np.argmax(acq)]

        # Run experiment
        next_y = objective_conductivity(next_x).ravel()[0]
        next_c = constraint_viscosity(next_x).ravel()[0]

        # Add to data
        X_sampled = np.vstack([X_sampled, next_x])
        y_sampled = np.append(y_sampled, next_y)
        c_sampled = np.append(c_sampled, next_c)

    return X_sampled, y_sampled, c_sampled

# Run two scenarios
X_unconst, y_unconst, c_unconst = run_bayesian_optimization(
    n_iterations=10, use_constraint=False
)
X_const, y_const, c_const = run_bayesian_optimization(
    n_iterations=10, use_constraint=True
)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Top left: Objective function
ax1 = axes[0, 0]
x_fine = np.linspace(0, 1, 500)
y_fine = objective_conductivity(x_fine)
ax1.plot(x_fine, y_fine, 'k-', linewidth=2, label='True Function')
ax1.scatter(X_unconst, y_unconst, c='blue', s=100, alpha=0.6,
            label='Unconstrained', marker='o')
ax1.scatter(X_const, y_const, c='red', s=100, alpha=0.6,
            label='Constrained', marker='^')
ax1.set_xlabel('Parameter x', fontsize=12)
ax1.set_ylabel('Ionic Conductivity', fontsize=12)
ax1.set_title('Objective Function Exploration', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

# Top right: Constraint function
ax2 = axes[0, 1]
c_fine = constraint_viscosity(x_fine)
ax2.plot(x_fine, c_fine, 'k-', linewidth=2, label='Constraint Function')
ax2.axhline(0, color='red', linestyle='--', linewidth=2,
            label='Constraint Boundary (‚â§ 0 is feasible)')
ax2.axhspan(-20, 0, alpha=0.2, color='green',
            label='Feasible Region')
ax2.scatter(X_unconst, c_unconst, c='blue', s=100, alpha=0.6,
            label='Unconstrained', marker='o')
ax2.scatter(X_const, c_const, c='red', s=100, alpha=0.6,
            label='Constrained', marker='^')
ax2.set_xlabel('Parameter x', fontsize=12)
ax2.set_ylabel('Constraint Value', fontsize=12)
ax2.set_title('Constraint Satisfaction', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

# Bottom left: Best value progression
ax3 = axes[1, 0]
best_unconst = np.maximum.accumulate(y_unconst)
best_const = np.maximum.accumulate(y_const)
ax3.plot(range(1, 11), best_unconst, 'o-', color='blue',
         linewidth=2, markersize=8, label='Unconstrained')
ax3.plot(range(1, 11), best_const, '^-', color='red',
         linewidth=2, markersize=8, label='Constrained')
ax3.set_xlabel('Number of Experiments', fontsize=12)
ax3.set_ylabel('Best Value So Far', fontsize=12)
ax3.set_title('Best Value Progression', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

# Bottom right: Constraint satisfaction progression
ax4 = axes[1, 1]
# Number of samples satisfying constraints
feasible_unconst = np.cumsum(c_unconst &lt;= 0)
feasible_const = np.cumsum(c_const &lt;= 0)
ax4.plot(range(1, 11), feasible_unconst, 'o-', color='blue',
         linewidth=2, markersize=8, label='Unconstrained')
ax4.plot(range(1, 11), feasible_const, '^-', color='red',
         linewidth=2, markersize=8, label='Constrained')
ax4.set_xlabel('Number of Experiments', fontsize=12)
ax4.set_ylabel('Cumulative Feasible Solutions', fontsize=12)
ax4.set_title('Constraint Satisfaction Progression', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('constrained_bo_comparison.png', dpi=150,
            bbox_inches='tight')
plt.show()

# Results summary
print(&quot;Optimization Results Comparison:&quot;)
print(&quot;\nUnconstrained Bayesian Optimization:&quot;)
print(f&quot;  Best Value: {np.max(y_unconst):.4f}&quot;)
print(f&quot;  Corresponding x: {X_unconst[np.argmax(y_unconst)][0]:.3f}&quot;)
print(f&quot;  Constraint Value: {c_unconst[np.argmax(y_unconst)]:.4f}&quot;)
print(f&quot;  Constraint Satisfied: {'Yes' if c_unconst[np.argmax(y_unconst)] &lt;= 0 else 'No'}&quot;)
print(f&quot;  Number of Feasible Solutions: {np.sum(c_unconst &lt;= 0)}/10&quot;)

print(&quot;\nConstrained Bayesian Optimization:&quot;)
# Find best solution among those satisfying constraints
feasible_indices = np.where(c_const &lt;= 0)[0]
if len(feasible_indices) &gt; 0:
    best_feasible_idx = feasible_indices[np.argmax(y_const[feasible_indices])]
    print(f&quot;  Best Value: {y_const[best_feasible_idx]:.4f}&quot;)
    print(f&quot;  Corresponding x: {X_const[best_feasible_idx][0]:.3f}&quot;)
    print(f&quot;  Constraint Value: {c_const[best_feasible_idx]:.4f}&quot;)
    print(f&quot;  Constraint Satisfied: Yes&quot;)
else:
    print(&quot;  No Feasible Solutions&quot;)
print(f&quot;  Number of Feasible Solutions: {np.sum(c_const &lt;= 0)}/10&quot;)

print(&quot;\nInsights:&quot;)
print(&quot;  - Constrained approach concentrates exploration in feasible region&quot;)
print(&quot;  - Unconstrained finds higher objective values but may violate constraints&quot;)
print(&quot;  - In practice, constraint-aware optimization is essential&quot;)
</code></pre>


**Expected Output**:

<pre><code>Optimization Results Comparison:

Unconstrained Bayesian Optimization:
  Best Value: 0.8234
  Corresponding x: 0.312
  Constraint Value: 1.876
  Constraint Satisfied: No
  Number of Feasible Solutions: 4/10

Constrained Bayesian Optimization:
  Best Value: 0.7456
  Corresponding x: 0.523
  Constraint Value: -0.234
  Constraint Satisfied: Yes
  Number of Feasible Solutions: 8/10

Insights:
  - Constrained approach concentrates exploration in feasible region
  - Unconstrained finds higher objective values but may violate constraints
  - In practice, constraint-aware optimization is essential
</code></pre>


**Key Insights**:
1. **Unconstrained**: Finds higher objective values but infeasible
2. **Constrained**: Slightly lower objective values but feasible
3. **Practical Use**: Solutions violating constraints are meaningless (materials cannot be used)
4. **Efficiency**: Constrained approach focuses on feasible region with less waste

</details>

<hr />
<h2>References</h2>
<ol>
<li>
<p>Rasmussen, C. E. &amp; Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.
   <a href="http://gaussianprocess.org/gpml/">Online Version</a></p>
</li>
<li>
<p>Brochu, E. et al. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." <em>arXiv:1012.2599</em>.
   <a href="https://arxiv.org/abs/1012.2599">arXiv:1012.2599</a></p>
</li>
<li>
<p>Mockus, J. (1974). "On Bayesian Methods for Seeking the Extremum." <em>Optimization Techniques IFIP Technical Conference</em>, 400-404.</p>
</li>
<li>
<p>Srinivas, N. et al. (2010). "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design." <em>ICML 2010</em>.
   <a href="https://arxiv.org/abs/0912.3995">arXiv:0912.3995</a></p>
</li>
<li>
<p>Gelbart, M. A. et al. (2014). "Bayesian Optimization with Unknown Constraints." <em>UAI 2014</em>.</p>
</li>
<li>
<p>Mochihashi, D. & Oba, S. (2019). Gaussian Processes and Machine Learning. Kodansha. ISBN: 978-4061529267</p>
</li>
</ol>
<hr />
<h2>Navigation</h2>
<h3>Previous Chapter</h3>
<p><strong><a href="./chapter-1.html">‚Üê Chapter 1: Why Optimization is Essential for Materials Discovery</a></strong></p>
<h3>Next Chapter</h3>
<p><strong><a href="./chapter-3.html">Chapter 3: Python Practice ‚Üí</a></strong></p>
<h3>Series Table of Contents</h3>
<p><strong><a href="./index.html">‚Üê Return to Series Table of Contents</a></strong></p>
<hr />
<h2>Author Information</h2>
<p><strong>Author</strong>: AI Terakoya Content Team
<strong>Date Created</strong>: 2025-10-17
<strong>Version</strong>: 1.0</p>
<p><strong>Update History</strong>:
- 2025-10-17: v1.0 Initial Release</p>
<p><strong>Feedback</strong>:
- GitHub Issues: <a href="https://github.com/your-repo/AI_Homepage/issues">AI_Homepage/issues</a>
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr />
<p><strong>Learn implementation in the Next Chapter!</strong></p><div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Return to Series Table of Contents</a>
    <a href="chapter-3-enhancements.html" class="nav-button">Next Chapter ‚Üí</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is intended for educational, research, and informational purposes only and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are governed by the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Date Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
