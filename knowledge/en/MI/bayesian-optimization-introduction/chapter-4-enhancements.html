<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/bayesian-optimization-introduction/index.html">Bayesian Optimization</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4-enhancements</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/bayesian-optimization-introduction/chapter-4-enhancements.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter</h1>
<p class="subtitle"></p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 0</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 4 Quality Enhancements</h1><p class="chapter-description">This chapter covers Chapter 4 Quality Enhancements. You will learn essential concepts and techniques.</p>


<p>This file contains enhancements to be integrated into chapter-4.md</p>
<h2>Code Reproducibility Section (add after section 4.1)</h2>
<h3>Ensuring Code Reproducibility</h3>
<p><strong>Environment Setup</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

# Chapter 4: Active Learning Strategies
# Required Library Versions
"""
Python: 3.8+
numpy: 1.21.0
scikit-learn: 1.0.0
scipy: 1.7.0
matplotlib: 3.5.0
"""

import numpy as np
import random
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern

# Ensure reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Recommended kernel configuration (for Active Learning)
kernel_default = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \
                 Matern(length_scale=0.2, length_scale_bounds=(1e-2, 1e0), nu=2.5)

print("Environment setup complete (for Active Learning)")
</code></pre>
<hr/>
<h2>Practical Pitfalls Section (add after section 4.2)</h2>
<h3>4.3 Practical Pitfalls and Solutions</h3>
<h4>Pitfall 1: Bias in Uncertainty Sampling</h4>
<p><strong>Problem</strong>: Uncertainty sampling becomes too concentrated at the edges of the search space</p>
<p><strong>Symptoms</strong>:
- Sampling concentrated near boundaries
- Insufficient information in interior regions
- Uneven prediction accuracy</p>
<p><strong>Solution</strong>: Combination with epsilon-greedy method</p>
<pre><code class="language-python">def epsilon_greedy_uncertainty_sampling(gp, X_candidate, epsilon=0.1):
    """
    Uncertainty sampling with epsilon-greedy strategy

    Parameters:
    -----------
    gp : GaussianProcessRegressor
        Trained GP model
    X_candidate : array (n_candidates, n_features)
        Candidate points
    epsilon : float
        Probability of random search (0~1)

    Returns:
    --------
    next_x : array
        Next sampling point
    """
    if np.random.rand() &lt; epsilon:
        # Random sampling with epsilon probability
        next_idx = np.random.randint(len(X_candidate))
        print(f"  random search (Œµ={epsilon})")
    else:
        # Uncertainty sampling with (1-epsilon) probability
        _, sigma = gp.predict(X_candidate, return_std=True)
        next_idx = np.argmax(sigma)
        print(f"  uncertainty sampling (œÉ={sigma[next_idx]:.4f})")

    next_x = X_candidate[next_idx]
    return next_x, next_idx

# Usage example
np.random.seed(42)
X_train = np.array([[0.1], [0.5], [0.9]])
y_train = np.sin(5 * X_train).ravel()

kernel = ConstantKernel(1.0) * RBF(length_scale=0.15)
gp = GaussianProcessRegressor(kernel=kernel)
gp.fit(X_train, y_train)

X_candidate = np.linspace(0, 1, 100).reshape(-1, 1)

# Epsilon-greedy uncertainty sampling
for i in range(5):
    print(f"\nIteration {i+1}:")
    next_x, idx = epsilon_greedy_uncertainty_sampling(
        gp, X_candidate, epsilon=0.2  # 20% random
    )
    print(f"  Selected point: x={next_x[0]:.3f}")
</code></pre>
<hr/>
<h4>Pitfall 2: Computational Cost of Diversity Sampling</h4>
<p><strong>Problem</strong>: Distance calculations are slow with large-scale data</p>
<p><strong>Symptoms</strong>:
- Time-consuming sampling
- High memory usage
- Does not scale</p>
<p><strong>Solution</strong>: Approximation using k-means clustering</p>
<pre><code class="language-python">from sklearn.cluster import KMeans

def fast_diversity_sampling(X_sampled, X_candidate, n_clusters=10):
    """
    Fast diversity sampling using k-means clustering

    Parameters:
    -----------
    X_sampled : array (n_sampled, n_features)
        Existing samples
    X_candidate : array (n_candidates, n_features)
        Candidate points
    n_clusters : int
        Number of clusters

    Returns:
    --------
    next_x : array
        Next sampling point
    """
    # Cluster candidate points
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X_candidate)

    # Select the farthest candidate point from each cluster center
    cluster_centers = kmeans.cluster_centers_
    distances_from_sampled = np.min(
        np.linalg.norm(
            cluster_centers[:, np.newaxis, :] -
            X_sampled[np.newaxis, :, :],
            axis=2
        ),
        axis=1
    )

    # Select the representative point of the farthest cluster
    farthest_cluster = np.argmax(distances_from_sampled)
    cluster_mask = (kmeans.labels_ == farthest_cluster)
    candidates_in_cluster = X_candidate[cluster_mask]

    # Select the point closest to the cluster center within the cluster
    distances_to_center = np.linalg.norm(
        candidates_in_cluster - cluster_centers[farthest_cluster],
        axis=1
    )
    next_idx_in_cluster = np.argmin(distances_to_center)
    next_x = candidates_in_cluster[next_idx_in_cluster]

    return next_x

# Benchmark
import time

n_sampled = 100
n_candidates = 10000
X_sampled = np.random.rand(n_sampled, 4)
X_candidate = np.random.rand(n_candidates, 4)

# Traditional method (full distance calculation)
start = time.time()
from scipy.spatial.distance import cdist
distances = cdist(X_candidate, X_sampled)
min_distances = np.min(distances, axis=1)
next_idx_naive = np.argmax(min_distances)
time_naive = time.time() - start

# k-means approximation method
start = time.time()
next_x_fast = fast_diversity_sampling(X_sampled, X_candidate, n_clusters=20)
time_fast = time.time() - start

print(f"Traditional method: {time_naive:.4f} seconds")
print(f"k-means method: {time_fast:.4f} seconds")
print(f"Speedup ratio: {time_naive/time_fast:.1f}x")
</code></pre>
<hr/>
<h4>Pitfall 3: Handling Experimental Failures in Closed-Loop Systems</h4>
<p><strong>Problem</strong>: Experimental failures are not considered</p>
<p><strong>Symptoms</strong>:
- Loop stops due to experimental failures
- Cannot exploit failure data
- Low robustness</p>
<p><strong>Solution</strong>: Active Learning considering failures</p>
<pre><code class="language-python">class RobustClosedLoopOptimizer:
    """
    Closed-loop optimization handling experimental failures
    """

    def __init__(self, objective_function, total_budget=50, failure_rate=0.1):
        """
        Parameters:
        -----------
        objective_function : callable
            Objective function (experiment simulator)
        total_budget : int
            Total experiment budget
        failure_rate : float
            Experimental failure rate (0~1)
        """
        self.objective_function = objective_function
        self.total_budget = total_budget
        self.failure_rate = failure_rate

        self.X_sampled = []
        self.y_observed = []
        self.failures = []

    def execute_experiment(self, x):
        """
        Execute experiment (with possibility of failure)

        Returns:
        --------
        success : bool
            Experiment success flag
        result : float or None
            Measured value on success, None on failure
        """
        # Failure simulation
        if np.random.rand() &lt; self.failure_rate:
            print(f"  Experiment failed: x={x}")
            return False, None

        # Evaluate objective function on success
        y = self.objective_function(x)
        return True, y

    def run(self):
        """Execute closed-loop optimization"""
        # Initialization
        X_init = np.random.uniform(0, 1, (5, 1))
        for x in X_init:
            success, y = self.execute_experiment(x)
            if success:
                self.X_sampled.append(x)
                self.y_observed.append(y)
                self.failures.append(False)
            else:
                self.failures.append(True)

        # Main loop
        experiments_done = len(X_init)

        while len(self.y_observed) &lt; self.total_budget:
            if experiments_done &gt;= self.total_budget * 1.5:
                print("Experiment budget exceeded (many failures)")
                break

            # Train GP model
            if len(self.y_observed) &lt; 3:
                # Random sampling when data is insufficient
                next_x = np.random.uniform(0, 1, (1, 1))
                print(f"Insufficient data: random sampling")
            else:
                kernel = ConstantKernel(1.0) * RBF(length_scale=0.15)
                gp = GaussianProcessRegressor(kernel=kernel)
                X_array = np.array(self.X_sampled)
                y_array = np.array(self.y_observed)
                gp.fit(X_array, y_array)

                # Maximize EI
                X_candidate = np.linspace(0, 1, 500).reshape(-1, 1)
                mu, sigma = gp.predict(X_candidate, return_std=True)
                f_best = np.max(y_array)

                from scipy.stats import norm
                improvement = mu - f_best - 0.01
                Z = improvement / (sigma + 1e-9)
                ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)

                next_idx = np.argmax(ei)
                next_x = X_candidate[next_idx:next_idx+1]

            # Execute experiment
            success, y = self.execute_experiment(next_x)
            experiments_done += 1

            if success:
                self.X_sampled.append(next_x)
                self.y_observed.append(y)
                self.failures.append(False)
                print(f"Success {len(self.y_observed)}/{self.total_budget}: "
                      f"x={next_x[0][0]:.3f}, y={y:.3f}")
            else:
                self.failures.append(True)
                print(f"Failed: Retrying")

        # Results summary
        success_rate = len(self.y_observed) / experiments_done
        print(f"\nFinal results:")
        print(f"  Total experiments: {experiments_done}")
        print(f"  Successful experiments: {len(self.y_observed)}")
        print(f"  Success rate: {success_rate:.1%}")
        print(f"  Best value: {np.max(self.y_observed):.4f}")

# Usage example
def noisy_objective(x):
    """Noisy objective function"""
    return np.sin(5 * x[0]) * np.exp(-x[0]) + 0.1 * np.random.randn()

np.random.seed(42)
optimizer = RobustClosedLoopOptimizer(
    objective_function=noisy_objective,
    total_budget=20,
    failure_rate=0.2  # 20% failure rate
)
optimizer.run()
</code></pre>
<hr/>
<h2>End-of-Chapter Checklist (add before "Exercises")</h2>
<h3>4.7 End-of-Chapter Checklist</h3>
<h4>‚úÖ Understanding Active Learning</h4>
<ul>
<li>[ ] Can explain the difference between Active Learning and Bayesian Optimization</li>
<li>[ ] Understand the three main strategies (uncertainty, diversity, model change)</li>
<li>[ ] Can explain the advantages and disadvantages of each strategy</li>
<li>[ ] Can select strategies according to the problem</li>
<li>[ ] Know how to combine strategies</li>
</ul>
<p><strong>Selection Guide</strong>:</p>
<pre><code>Understanding search space            ‚Üí Diversity sampling
Improving prediction accuracy         ‚Üí Uncertainty sampling
Improving model generalization        ‚Üí Expected model change
Finding optimal solutions             ‚Üí Bayesian Optimization (EI/UCB)
Discovering diverse candidate materials ‚Üí Combination of diversity + uncertainty
</code></pre>
<hr/>
<h4>‚úÖ Uncertainty Sampling</h4>
<ul>
<li>[ ] Understand the meaning of prediction standard deviation œÉ</li>
<li>[ ] Know how to identify regions with high uncertainty</li>
<li>[ ] Can implement combination with epsilon-greedy method</li>
<li>[ ] Understand application to classification problems (margin, entropy)</li>
<li>[ ] Know the limitations of uncertainty sampling</li>
</ul>
<p><strong>Implementation Check</strong>:</p>
<pre><code class="language-python"># Can you complete this code?
def uncertainty_sampling(gp, X_candidate):
    """
    Select the point with maximum uncertainty

    Returns:
    --------
    next_x : array
        Next sampling point
    uncertainty : float
        Uncertainty at that point
    """
    # Your implementation
    _, sigma = gp.predict(X_candidate, return_std=True)
    next_idx = np.argmax(sigma)
    next_x = X_candidate[next_idx]
    uncertainty = sigma[next_idx]

    return next_x, uncertainty

# Correct!
</code></pre>
<hr/>
<h4>‚úÖ Diversity Sampling</h4>
<ul>
<li>[ ] Understand the concept of MaxMin distance</li>
<li>[ ] Can implement approximation using k-means clustering</li>
<li>[ ] Know the basics of Determinantal Point Process (DPP)</li>
<li>[ ] Can evaluate search space coverage</li>
<li>[ ] Know speedup methods for large-scale data</li>
</ul>
<p><strong>Diversity Evaluation Metrics</strong>:</p>
<pre><code class="language-python">def evaluate_diversity(X_sampled, bounds):
    """
    Evaluate sampling diversity

    Returns:
    --------
    coverage_score : float
        Search space coverage (0~1)
    """
    # Divide search space into 10 parts and calculate coverage
    n_dims = X_sampled.shape[1]
    n_bins = 10

    coverage_count = 0
    total_bins = n_bins ** n_dims

    # Simplified version: coverage per dimension
    for dim in range(n_dims):
        hist, _ = np.histogram(
            X_sampled[:, dim],
            bins=n_bins,
            range=(bounds[dim, 0], bounds[dim, 1])
        )
        coverage_count += np.sum(hist &gt; 0)

    coverage_score = coverage_count / (n_bins * n_dims)
    return coverage_score

# Usage example
bounds = np.array([[0, 1], [0, 1], [0, 1], [0, 1]])
X_sampled = np.random.rand(20, 4)
coverage = evaluate_diversity(X_sampled, bounds)
print(f"Coverage: {coverage:.1%}")
</code></pre>
<hr/>
<h4>‚úÖ Closed-Loop Optimization</h4>
<ul>
<li>[ ] Understand the components of a closed-loop system</li>
<li>[ ] Know how to integrate AI engine, experimental equipment, and data management</li>
<li>[ ] Can implement methods for handling experimental failures</li>
<li>[ ] Can design real-time monitoring</li>
<li>[ ] Understand the role of human researchers</li>
</ul>
<p><strong>System Design Checklist</strong>:</p>
<pre><code>‚ñ° Definition of objective function and evaluation method
‚ñ° Explicit constraints
‚ñ° Initial sampling strategy
‚ñ° Selection of acquisition function
‚ñ° Determination of batch size
‚ñ° Retry logic for experimental failures
‚ñ° Anomaly detection and human notification
‚ñ° Automatic data saving and backup
‚ñ° Progress visualization
‚ñ° Setting termination conditions
</code></pre>
<hr/>
<h4>‚úÖ Understanding Real-World Applications</h4>
<ul>
<li>[ ] Can explain the achievements of Berkeley A-Lab</li>
<li>[ ] Understand the approach of RoboRXN</li>
<li>[ ] Know the features of Materials Acceleration Platform</li>
<li>[ ] Can evaluate ROI of industrial applications</li>
<li>[ ] Can analyze success factors and challenges</li>
</ul>
<p><strong>ROI Calculation Template</strong>:</p>
<pre><code>Traditional method:
  Number of experiments: ________ times
  Experiment time: ________ hours/time
  Labor cost: ________ $/hour
  Total cost: ________ $
  Development period: ________ months

AI-driven method (closed-loop):
  Number of experiments: ________ times (__% reduction)
  Experiment time: ________ hours/time (automated)
  Labor cost: ________ $/hour (monitoring only)
  System construction: ________ $ (initial investment)
  Total cost: ________ $
  Development period: ________ months (__% reduction)

Payback period: ________ months
</code></pre>
<hr/>
<h4>‚úÖ Human-AI Collaboration</h4>
<ul>
<li>[ ] Understand human intuition and AI strengths</li>
<li>[ ] Can design hybrid approaches</li>
<li>[ ] Can determine when humans should intervene</li>
<li>[ ] Can build decision support systems</li>
<li>[ ] Can design feedback loops</li>
</ul>
<p><strong>Collaboration Protocol</strong>:</p>
<pre><code>Phase 1: Problem formulation (human-led)
  ‚Üí Define objective function, constraints, and search space
  ‚Üí AI checks feasibility

Phase 2: Initial exploration (AI-led)
  ‚Üí AI explores data-efficiently
  ‚Üí Human validates anomalies

Phase 3: Refinement (hybrid)
  ‚Üí AI proposes
  ‚Üí Human evaluates physical validity
  ‚Üí Collaborative decision-making

Phase 4: Implementation (human-led)
  ‚Üí Human selects final candidates
  ‚Üí AI quantifies uncertainty
</code></pre>
<hr/>
<h3>‚úÖ Understanding Career Paths</h3>
<ul>
<li>[ ] Understand the academic researcher path</li>
<li>[ ] Know the industry R&amp;D engineer path</li>
<li>[ ] Can consider the autonomous experimentation specialist path</li>
<li>[ ] Can identify skills to learn next</li>
<li>[ ] Have clarified your own career goals</li>
</ul>
<p><strong>Next Steps Selection Guide</strong>:</p>
<pre><code>Theory research orientation
‚Üí GNN Beginner + Reinforcement Learning Beginner
‚Üí Paper writing, conference presentations

Implementation/application orientation
‚Üí Robotics Experiment Automation Beginner
‚Üí Original projects, portfolio creation

Industrial application orientation
‚Üí Deep dive into industrial case studies
‚Üí Internships, practical experience

System building orientation
‚Üí Closed-loop system construction
‚Üí API design, hardware integration
</code></pre>
<hr/>
<h3>Pass Criteria</h3>
<p>If you have achieved the following, you have completed the series:</p>
<ol>
<li><strong>Theoretical Understanding</strong>: Clear 80% or more of each checklist item</li>
<li><strong>Implementation Skills</strong>: Can solve all exercises</li>
<li><strong>Application Ability</strong>: Can formulate new materials exploration problems</li>
<li><strong>Career</strong>: Next steps are clear</li>
</ol>
<p><strong>Final Confirmation Questions</strong>:
1. Can you implement and compare the performance of three Active Learning strategies?
2. Can you design a closed-loop optimization system?
3. Can you extract learnings from real-world application success stories?
4. Can you explain the next steps toward your career goals?</p>
<p>If all are YES, congratulations!
You have completed the Bayesian Optimization &amp; Active Learning Beginner series!</p>
<p><strong>To the Next Series</strong>:
- Robotics Experiment Automation Beginner
- Reinforcement Learning Beginner (Materials Science Specialized)
- GNN Beginner</p>
<p><strong>Continuous Learning</strong>:
- Paper reading (1 per week)
- Open source contributions
- Community participation
- Application to real projects</p>
<p>We wish you success!</p>
<div class="navigation">
<a class="nav-button" href="chapter-3.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Return to Series Table of Contents</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (such as legal, accounting, or technical guarantees).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, data, tools, or libraries provided by third parties.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
