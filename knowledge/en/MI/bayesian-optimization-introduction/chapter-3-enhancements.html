<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/bayesian-optimization-introduction/index.html">Bayesian Optimization</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3-enhancements</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter</h1>
<p class="subtitle"></p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 min</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 0</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 3 Quality Enhancements</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Starting with minimal implementations in scikit-optimize and BoTorch, we'll learn the essentials of parameter configuration. We'll also show entry points for extending to constrained and multi-objective problems.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Stabilize through noise estimation and scale adjustment. Design iteration counts for "quality over quantity".</p>
<p>This file contains enhancements to be integrated into chapter-3.md</p>
<h2>Code Reproducibility Section (add after section 3.1)</h2>
<h3>Ensuring Code Reproducibility</h3>
<p><strong>Importance of Environment Configuration</strong>:</p>
<p>All code examples have been tested in the following environment:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - torch&gt;=2.0.0, &lt;2.3.0

# Required library versions
&quot;&quot;&quot;
Python: 3.8+
numpy: 1.21.0
scikit-learn: 1.0.0
scikit-optimize: 0.9.0
torch: 1.12.0
gpytorch: 1.8.0
botorch: 0.7.0
matplotlib: 3.5.0
pandas: 1.3.0
scipy: 1.7.0
&quot;&quot;&quot;

# Configuration for reproducibility
import numpy as np
import torch
import random

# Fix random seeds
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# GPyTorch kernel configuration (recommended)
from gpytorch.kernels import RBF, MaternKernel, ScaleKernel

# RBF kernel (most common)
kernel_rbf = ScaleKernel(RBF(
    lengthscale_prior=None,  # Data-driven optimization
    ard_num_dims=None  # Automatic Relevance Determination
))

# Matern kernel (adjustable smoothness)
kernel_matern = ScaleKernel(MaternKernel(
    nu=2.5,  # Smoothness parameter (1.5, 2.5, or inf (equivalent to RBF))
    ard_num_dims=None
))

print(&quot;Environment setup complete&quot;)
print(f&quot;NumPy version: {np.__version__}&quot;)
print(f&quot;PyTorch version: {torch.__version__}&quot;)
</code></pre>
<p><strong>Installation Steps</strong>:</p>
<pre><code class="language-bash"># Create virtual environment (recommended)
python -m venv bo_env
source bo_env/bin/activate  # Linux/Mac
# bo_env\Scripts\activate  # Windows

# Install required packages
pip install numpy==1.21.0 scikit-learn==1.0.0 scikit-optimize==0.9.0
pip install torch==1.12.0 gpytorch==1.8.0 botorch==0.7.0
pip install matplotlib==3.5.0 pandas==1.3.0 scipy==1.7.0

# Optional: Materials Project API
pip install mp-api==0.30.0

# Verify installation
python -c "import botorch; print(f'BoTorch {botorch.__version__} installed')"
</code></pre>
<hr/>
<h2>Practical Pitfalls Section (add after section 3.7)</h2>
<h3>3.8 Practical Pitfalls and Solutions</h3>
<h4>Pitfall 1: Inappropriate Kernel Selection</h4>
<p><strong>Problem</strong>: Kernel selection doesn't match the nature of the objective function</p>
<p><strong>Symptoms</strong>:
- Low prediction accuracy
- Poor exploration efficiency
- Easy to fall into local optima</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Kernel selection guide
from gpytorch.kernels import RBF, MaternKernel, PeriodicKernel

def select_kernel(problem_characteristics):
    """
    Kernel selection based on problem characteristics

    Parameters:
    -----------
    problem_characteristics : dict
        Dictionary describing problem characteristics
        - 'smoothness': 'smooth' | 'rough'
        - 'periodicity': True | False
        - 'dimensionality': int

    Returns:
    --------
    kernel : gpytorch.kernels.Kernel
        Recommended kernel
    """
    if problem_characteristics.get('periodicity'):
        # If periodic behavior exists
        return PeriodicKernel()

    elif problem_characteristics.get('smoothness') == 'smooth':
        # Smooth functions (material properties, etc.)
        return RBF()

    elif problem_characteristics.get('smoothness') == 'rough':
        # Noisy or discontinuous
        return MaternKernel(nu=1.5)

    else:
        # Default: Matern 5/2 (high versatility)
        return MaternKernel(nu=2.5)

# Usage example
problem_specs = {
    'smoothness': 'smooth',
    'periodicity': False,
    'dimensionality': 4
}

recommended_kernel = select_kernel(problem_specs)
print(f"Recommended kernel: {recommended_kernel}")
</code></pre>
<p><strong>Kernel Comparison Experiment</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, Matern

# Test function
def test_function(x):
    &quot;&quot;&quot;Complex function with noise&quot;&quot;&quot;
    return np.sin(5*x) + 0.5*np.cos(15*x) + 0.1*np.random.randn(len(x))

# Generate data
np.random.seed(42)
X_train = np.random.uniform(0, 1, 20).reshape(-1, 1)
y_train = test_function(X_train.ravel())

X_test = np.linspace(0, 1, 200).reshape(-1, 1)
y_true = test_function(X_test.ravel())

# Compare different kernels
kernels = {
    'RBF': RBF(length_scale=0.1),
    'Matern 1.5': Matern(length_scale=0.1, nu=1.5),
    'Matern 2.5': Matern(length_scale=0.1, nu=2.5)
}

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for ax, (name, kernel) in zip(axes, kernels.items()):
    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)
    gp.fit(X_train, y_train)
    y_pred, y_std = gp.predict(X_test, return_std=True)

    ax.scatter(X_train, y_train, c='red', label='Training data')
    ax.plot(X_test, y_pred, 'b-', label='Prediction')
    ax.fill_between(X_test.ravel(), y_pred - 2*y_std, y_pred + 2*y_std,
                     alpha=0.3, color='blue')
    ax.set_title(f'Kernel: {name}')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('kernel_comparison.png', dpi=150)
plt.show()

print(&quot;Conclusions:&quot;)
print(&quot;  RBF: Optimal for smooth functions&quot;)
print(&quot;  Matern 1.5: High noise resistance&quot;)
print(&quot;  Matern 2.5: Well-balanced (recommended)&quot;)
</code></pre>
<hr/>
<h4>Pitfall 2: Failed Initialization Strategy</h4>
<p><strong>Problem</strong>: Initial sampling doesn't adequately cover the search space</p>
<p><strong>Symptoms</strong>:
- Biased exploration
- Missing important regions
- Slow convergence</p>
<p><strong>Solution</strong>: Latin Hypercube Sampling (LHS)</p>
<pre><code class="language-python">from scipy.stats.qmc import LatinHypercube

def initialize_with_lhs(n_samples, bounds, seed=42):
    """
    Generate initial points using Latin Hypercube Sampling

    Parameters:
    -----------
    n_samples : int
        Number of samples
    bounds : array (n_dims, 2)
        [lower, upper] bounds for each dimension
    seed : int
        Random seed

    Returns:
    --------
    X_init : array (n_samples, n_dims)
        Initial sampling points
    """
    bounds = np.array(bounds)
    n_dims = len(bounds)

    # LHS sampler
    sampler = LatinHypercube(d=n_dims, seed=seed)
    X_unit = sampler.random(n=n_samples)

    # Scaling
    X_init = bounds[:, 0] + (bounds[:, 1] - bounds[:, 0]) * X_unit

    return X_init

# Usage example: Li-ion battery composition initialization
bounds_composition = [
    [0.1, 0.5],  # Li
    [0.1, 0.4],  # Ni
    [0.1, 0.3],  # Co
    [0.0, 0.5]   # Mn
]

X_init_lhs = initialize_with_lhs(
    n_samples=20,
    bounds=bounds_composition,
    seed=42
)

# Normalize composition
X_init_lhs = X_init_lhs / X_init_lhs.sum(axis=1, keepdims=True)

print("LHS initialization complete")
print(f"Initial sample count: {len(X_init_lhs)}")
print(f"Coverage range for each dimension:")
for i, dim_name in enumerate(['Li', 'Ni', 'Co', 'Mn']):
    print(f"  {dim_name}: [{X_init_lhs[:, i].min():.3f}, "
          f"{X_init_lhs[:, i].max():.3f}]")

# Visualization comparing with random sampling
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Random sampling
np.random.seed(42)
X_random = np.random.uniform(0, 1, (20, 2))

axes[0].scatter(X_random[:, 0], X_random[:, 1], s=100)
axes[0].set_title('Random Sampling')
axes[0].set_xlabel('Dimension 1')
axes[0].set_ylabel('Dimension 2')
axes[0].grid(True, alpha=0.3)

# LHS
axes[1].scatter(X_init_lhs[:, 0], X_init_lhs[:, 1], s=100, c='red')
axes[1].set_title('Latin Hypercube Sampling (LHS)')
axes[1].set_xlabel('Dimension 1 (Li)')
axes[1].set_ylabel('Dimension 2 (Ni)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('lhs_vs_random.png', dpi=150)
plt.show()
</code></pre>
<hr/>
<h4>Pitfall 3: Inadequate Handling of Noisy Observations</h4>
<p><strong>Problem</strong>: Not accounting for experimental noise</p>
<p><strong>Symptoms</strong>:
- Results not reproducible under same conditions
- Model overfitting
- Unstable optimal points</p>
<p><strong>Solution</strong>: Explicitly model noise</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

import torch
from botorch.models import SingleTaskGP
from gpytorch.mlls import ExactMarginalLogLikelihood

def fit_gp_with_noise(X, y, noise_variance=0.01):
    &quot;&quot;&quot;
    Train Gaussian Process with noise consideration

    Parameters:
    -----------
    X : Tensor (n, d)
        Input data
    y : Tensor (n, 1)
        Observations (including noise)
    noise_variance : float
        Observation noise variance (set from prior knowledge)

    Returns:
    --------
    gp_model : SingleTaskGP
        Trained GP model
    &quot;&quot;&quot;
    # Build GP with noise variance
    gp_model = SingleTaskGP(X, y, train_Yvar=torch.full_like(y, noise_variance))

    # Maximize likelihood
    mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)
    from botorch.fit import fit_gpytorch_model
    fit_gpytorch_model(mll)

    return gp_model

# Usage example: Experimental data with noise
np.random.seed(42)
X_obs = np.random.rand(15, 4)
X_obs = X_obs / X_obs.sum(axis=1, keepdims=True)

# True capacity + experimental noise
y_true = 200 + 150 * X_obs[:, 0] + 50 * X_obs[:, 1]
noise = np.random.randn(15) * 10  # Experimental noise œÉ=10 mAh/g
y_obs = y_true + noise

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_obs, dtype=torch.float64)
y_tensor = torch.tensor(y_obs, dtype=torch.float64).unsqueeze(-1)

# Train GP with noise consideration
gp_noisy = fit_gp_with_noise(X_tensor, y_tensor, noise_variance=100.0)

print(&quot;GP training with noise consideration complete&quot;)
print(f&quot;Observation noise standard deviation: 10 mAh/g&quot;)
print(f&quot;Modeled noise variance: 100.0 (mAh/g)¬≤&quot;)
</code></pre>
<p><strong>Noise Level Estimation</strong>:</p>
<pre><code class="language-python">def estimate_noise_level(X, y, n_replicates=3):
    """
    Estimate noise level from replicate experiments

    Parameters:
    -----------
    X : array (n, d)
        Experimental conditions
    y : array (n,)
        Observations
    n_replicates : int
        Number of replicate experiments per condition

    Returns:
    --------
    noise_std : float
        Estimated noise standard deviation
    """
    # Extract replicate experiments with identical conditions
    unique_X, indices = np.unique(X, axis=0, return_inverse=True)

    variances = []
    for i in range(len(unique_X)):
        replicates = y[indices == i]
        if len(replicates) >= 2:
            variances.append(np.var(replicates, ddof=1))

    if len(variances) == 0:
        print("Warning: No replicate experiments found. Using default value")
        return 1.0

    noise_std = np.sqrt(np.mean(variances))
    return noise_std

# Usage example
noise_std_estimated = estimate_noise_level(X_obs, y_obs)
print(f"Estimated noise standard deviation: {noise_std_estimated:.2f} mAh/g")
</code></pre>
<hr/>
<h4>Pitfall 4: Inadequate Constraint Handling</h4>
<p><strong>Problem</strong>: Not properly handling constraint violations</p>
<p><strong>Symptoms</strong>:
- Proposing infeasible materials
- Optimization doesn't converge
- Many wasted experiments</p>
<p><strong>Solution</strong>: Constrained Acquisition Function</p>
<pre><code class="language-python">from botorch.acquisition import ConstrainedExpectedImprovement

def constrained_bayesian_optimization_example():
    """
    Implementation example of constrained Bayesian Optimization

    Constraints:
    1. Sum of composition = 1.0 (¬±2%)
    2. Co content < 0.3 (cost constraint)
    3. Stability: formation energy < -1.5 eV/atom
    """
    # Initial data
    n_initial = 10
    X_init = initialize_with_lhs(n_initial, bounds_composition, seed=42)
    X_init = X_init / X_init.sum(axis=1, keepdims=True)  # Normalize

    # Evaluate objective function and constraints
    y_capacity = []
    constraints_satisfied = []

    for x in X_init:
        # Capacity prediction (objective function)
        capacity = 200 + 150*x[0] + 50*x[1]
        y_capacity.append(capacity)

        # Constraint check
        co_constraint = x[2] < 0.3  # Co < 0.3
        stability = -2.0 - 0.5*x[0] - 0.3*x[1]
        stability_constraint = stability < -1.5  # Stable

        all_satisfied = co_constraint and stability_constraint
        constraints_satisfied.append(1.0 if all_satisfied else 0.0)

    X_tensor = torch.tensor(X_init, dtype=torch.float64)
    y_tensor = torch.tensor(y_capacity, dtype=torch.float64).unsqueeze(-1)
    c_tensor = torch.tensor(constraints_satisfied, dtype=torch.float64).unsqueeze(-1)

    # Gaussian Process model (objective function)
    gp_objective = SingleTaskGP(X_tensor, y_tensor)
    mll_obj = ExactMarginalLogLikelihood(gp_objective.likelihood, gp_objective)
    from botorch.fit import fit_gpytorch_model
    fit_gpytorch_model(mll_obj)

    # Gaussian Process model (constraints)
    gp_constraint = SingleTaskGP(X_tensor, c_tensor)
    mll_con = ExactMarginalLogLikelihood(gp_constraint.likelihood, gp_constraint)
    fit_gpytorch_model(mll_con)

    # Constrained EI Acquisition Function
    best_f = y_tensor.max()
    acq_func = ConstrainedExpectedImprovement(
        model=gp_objective,
        best_f=best_f,
        objective_index=0,
        constraints={0: [None, 0.5]}  # Constraint satisfaction probability > 0.5
    )

    print("Constrained Bayesian Optimization setup complete")
    print(f"Initial feasible solutions: {sum(constraints_satisfied)}/{n_initial}")

    return gp_objective, gp_constraint, acq_func

# Execute
gp_obj, gp_con, acq = constrained_bayesian_optimization_example()
</code></pre>
<hr/>
<h2>End-of-Chapter Checklist (add before "Exercises")</h2>
<h3>3.9 End-of-Chapter Checklist</h3>
<h4>‚úÖ Understanding Gaussian Processes</h4>
<ul>
<li>[ ] Can explain the basic concepts of Gaussian Processes</li>
<li>[ ] Understand the role of Kernel Functions</li>
<li>[ ] Know the meaning of predictive mean and uncertainty</li>
<li>[ ] Can select appropriate kernels</li>
<li>[ ] Can explain the influence of hyperparameters</li>
</ul>
<p><strong>Verification Question</strong>:</p>
<pre><code>Q: What is the difference between RBF and Matern kernels?
A: RBF is infinitely differentiable (very smooth), Matern allows
   adjustable smoothness via parameter ŒΩ. Matern (ŒΩ=2.5) is
   recommended when noise is present.
</code></pre>
<hr/>
<h4>‚úÖ Acquisition Function Selection</h4>
<ul>
<li>[ ] Understand the mechanism of Expected Improvement (EI)</li>
<li>[ ] Can explain the exploration-exploitation balance of Upper Confidence Bound (UCB)</li>
<li>[ ] Know the characteristics of Probability of Improvement (PI)</li>
<li>[ ] Understand application scenarios for Knowledge Gradient (KG)</li>
<li>[ ] Can select Acquisition Function based on the problem</li>
</ul>
<p><strong>Selection Guide</strong>:</p>
<pre><code>General optimization      ‚Üí EI (well-balanced)
Exploration-focused early ‚Üí UCB (Œ∫=2~3)
Safety-focused           ‚Üí PI (conservative)
Batch optimization       ‚Üí q-EI, q-KG
Multi-objective          ‚Üí EHVI (Hypervolume)
</code></pre>
<hr/>
<h4>‚úÖ Multi-Objective Optimization</h4>
<ul>
<li>[ ] Can explain the definition of Pareto optimality</li>
<li>[ ] Understand the meaning of Pareto frontier</li>
<li>[ ] Know the mechanism of Expected Hypervolume Improvement (EHVI)</li>
<li>[ ] Can quantitatively evaluate trade-offs</li>
<li>[ ] Can implement multi-objective optimization</li>
</ul>
<p><strong>Implementation Check</strong>:</p>
<pre><code class="language-python"># Can you implement the following?
def is_pareto_optimal(objectives):
    """
    Function to determine Pareto optimal solutions
    objectives: (n_points, n_objectives)
    """
    # Your implementation
    pass

# See Exercises 3 for the solution
</code></pre>
<hr/>
<h4>‚úÖ Batch Bayesian Optimization</h4>
<ul>
<li>[ ] Can explain the advantages of batch optimization</li>
<li>[ ] Understand the mechanism of q-EI Acquisition Function</li>
<li>[ ] Know the Kriging Believer method</li>
<li>[ ] Can develop strategies for efficient parallel experiments</li>
<li>[ ] Understand batch size selection criteria</li>
</ul>
<p><strong>Batch Size Selection</strong>:</p>
<pre><code>Number of experimental devices: n units
‚Üí Batch size: n (maximum exploitation)

With computational cost constraints
‚Üí Batch size: 3~5 (practical)

Early exploration phase
‚Üí Batch size: larger (diversity-focused)

Convergence phase
‚Üí Batch size: smaller (refinement)
</code></pre>
<hr/>
<h4>‚úÖ Constraint Handling</h4>
<ul>
<li>[ ] Can distinguish types of constraints (equality, inequality)</li>
<li>[ ] Understand the concept of feasible region</li>
<li>[ ] Can implement constrained Acquisition Functions</li>
<li>[ ] Can calculate feasibility probability</li>
<li>[ ] Know strategies for gradual constraint relaxation</li>
</ul>
<p><strong>Constraint Handling Checklist</strong>:</p>
<pre><code>‚ñ° Handle composition constraints (sum=1.0) with normalization
‚ñ° Set boundary constraints with bounds parameter
‚ñ° Express nonlinear constraints with penalty functions
‚ñ° Prepare strategies when no feasible solution is found
‚ñ° Visualize constraint satisfaction probability
</code></pre>
<hr/>
<h4>‚úÖ Implementation Skills (GPyTorch/BoTorch)</h4>
<ul>
<li>[ ] Can construct SingleTaskGP models</li>
<li>[ ] Can appropriately select and configure kernels</li>
<li>[ ] Can optimize Acquisition Functions</li>
<li>[ ] Can implement batch optimization</li>
<li>[ ] Can perform modeling with noise consideration</li>
</ul>
<p><strong>Code Implementation Verification</strong>:</p>
<pre><code class="language-python"># Can you understand this code?
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_model
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.acquisition import ExpectedImprovement
from botorch.optim import optimize_acqf

# Build GP model
gp = SingleTaskGP(X_train, y_train)
mll = ExactMarginalLogLikelihood(gp.likelihood, gp)
fit_gpytorch_model(mll)

# Maximize EI
EI = ExpectedImprovement(gp, best_f=y_train.max())
candidate, acq_value = optimize_acqf(
    EI, bounds=bounds, q=1, num_restarts=10
)

# Can you explain the meaning of each line?
</code></pre>
<hr/>
<h4>‚úÖ Integration with Experimental Design</h4>
<ul>
<li>[ ] Can exploit real data sources like Materials Project</li>
<li>[ ] Can integrate ML models with Bayesian Optimization</li>
<li>[ ] Can develop experimental plans</li>
<li>[ ] Can visualize and interpret results</li>
<li>[ ] Can evaluate ROI</li>
</ul>
<p><strong>Experimental Planning Template</strong>:</p>
<pre><code>1. Objective Setting
   - Property to optimize: ________
   - Constraints: ________
   - Experimental budget: ________ trials

2. Initialization
   - Initial sample count: ________
   - Sampling method: LHS / Random
   - Expected experiment duration: ________

3. Optimization Strategy
   - Acquisition Function: ________
   - Kernel: ________
   - Batch size: ________

4. Termination Criteria
   - Maximum experiments: ________
   - Target performance: ________
   - Improvement rate threshold: ________
</code></pre>
<hr/>
<h4>‚úÖ Troubleshooting</h4>
<ul>
<li>[ ] Know methods to escape local optima</li>
<li>[ ] Understand strategies for handling constraint violations</li>
<li>[ ] Know techniques for reducing computation time</li>
<li>[ ] Can implement noise handling strategies</li>
<li>[ ] Know debugging methods</li>
</ul>
<p><strong>Common Errors and Solutions</strong>:</p>
<pre><code>Error: "RuntimeError: cholesky_cpu: U(i,i) is zero"
‚Üí Cause: Numerical instability
‚Üí Solution: Add jitter to GP model
   gp = SingleTaskGP(X, y, covar_module=...).double()
   gp.likelihood.noise = 1e-4

Error: "All points violate constraints"
‚Üí Cause: Constraints too strict
‚Üí Solution: Gradual constraint relaxation, initial LHS sampling

Warning: "Optimization failed to converge"
‚Üí Cause: Acquisition Function optimization failure
‚Üí Solution: Increase num_restarts, increase raw_samples
</code></pre>
<hr/>
<h3>Passing Criteria</h3>
<p>If you clear 80% or more of the checklist items in each section
and understand the implementation verification code, you are ready to proceed to the next chapter.</p>
<p><strong>Final Verification Questions</strong>:
1. Can you formulate an optimization problem for Li-ion battery cathode materials?
2. Can you implement a 3-objective (capacity, voltage, stability) optimization?
3. Can you find 10 Pareto optimal solutions within 50 experiments?</p>
<p>If all answers are YES, proceed to Chapter 4 "Active Learning and Experimental Integration"!</p>
<div class="navigation">
<a class="nav-button" href="chapter-2.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Return to Series Table of Contents</a>
<a class="nav-button" href="chapter-4-enhancements.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is intended solely for educational, research, and informational purposes and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, data, tools, libraries, etc. provided by third parties.</li>
<li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
