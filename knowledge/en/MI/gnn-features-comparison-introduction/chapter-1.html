<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Chapter 1: Fundamentals of GNN Structure-Based Features - Graph Representation and Differences from Composition-Based Approaches" name="description"/>
<title>Chapter 1: Fundamentals of GNN Structure-Based Features | Introduction to GNN Features Comparison</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- MathJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- Mermaid -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
<!-- Prism.js for syntax highlighting -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="./index.html">Introduction to GNN Features Comparison</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/gnn-features-comparison-introduction/chapter-1.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="container">
<h1>Chapter 1: Fundamentals of GNN Structure-Based Features</h1>
<div class="meta">
<span>üìñ Reading Time: 25-30 min</span>
<span>üìä Level: Introductory</span>
<span>üíª Code Examples: 8</span>
</div>
</div>
</header>
<main class="container">

<p class="chapter-description">This chapter covers the fundamentals of Fundamentals of GNN Structure, which based features. You will learn three steps of message passing and differences between CGCNN.</p>
<p><strong>Structural Information Captured by Graph Representations: A World Invisible to Composition-Based Features</strong></p>
<h2 id="intro">1.1 Limitations of Composition-Based Features</h2>
<p>In materials science AI prediction, <strong>composition-based features</strong> have long been the mainstream approach. Methods like Magpie (Ward et al., 2016) and Matminer calculate statistical features from chemical composition (e.g., Fe‚ÇÇO‚ÇÉ).</p>
<h3>1.1.1 Examples of Composition-Based Features</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: 1.1.1 Examples of Composition-Based Features

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

# Google Colab environment setup
!pip install matminer pymatgen scikit-learn

import numpy as np
from matminer.featurizers.composition import ElementProperty
from pymatgen.core.composition import Composition

# Example 1: Computing Magpie features
magpie = ElementProperty.from_preset("magpie")

# Features for Fe‚ÇÇO‚ÇÉ (iron oxide)
comp = Composition("Fe2O3")
features = magpie.featurize(comp)

print(f"Feature dimensionality: {len(features)}")
print(f"First 10 features: {features[:10]}")
# Output example: Feature dimensionality: 132
# Output example: First 10 features: [55.845, 15.999, 39.998, ...]
</code></pre>
<p>Composition-based features include the following 132 dimensions:</p>
<ul>
<li>Average atomic weight, density</li>
<li>Statistics of electronegativity, ionic radius (mean, variance, max, min)</li>
<li>Statistics of electron configuration</li>
</ul>
<h3>1.1.2 Critical Limitation: Missing Structural Information</h3>
<p>Composition-based features do not consider <strong>spatial arrangement of atoms</strong> at all. This causes the following problems:</p>
<div class="mermaid">
graph LR
    A[C: Diamond] -.Same composition.-&gt; B[C: Graphite]
    A --&gt; C[Hardness: 10,000 HV]
    B --&gt; D[Hardness: 2-3 HV]

    E[SiO‚ÇÇ: Œ±-quartz] -.Same composition.-&gt; F[SiO‚ÇÇ: Œ≤-cristobalite]
    E --&gt; G[Density: 2.65 g/cm¬≥]
    F --&gt; H[Density: 2.33 g/cm¬≥]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style E fill:#fff3e0
    style F fill:#fff3e0
</div>
<p><strong>Concrete Examples</strong>:</p>
<ol>
<li><strong>Diamond vs Graphite</strong> (both are C):
                <ul>
<li>Diamond: sp¬≥ hybridization, tetrahedral structure, hardness 10,000 HV</li>
<li>Graphite: sp¬≤ hybridization, layered structure, hardness 2-3 HV</li>
<li>Composition-based features are completely identical!</li>
</ul>
</li>
<li><strong>Polymorphs of SiO‚ÇÇ</strong> (Œ±-quartz vs Œ≤-cristobalite):
                <ul>
<li>Œ±-quartz: density 2.65 g/cm¬≥, hexagonal</li>
<li>Œ≤-cristobalite: density 2.33 g/cm¬≥, cubic</li>
<li>Same composition but completely different properties</li>
</ul>
</li>
</ol>
<pre><code class="language-python"># Example 2: Demonstrating limitations of composition-based features
from pymatgen.core import Structure, Lattice

# Diamond structure (sp¬≥)
diamond_lattice = Lattice.cubic(3.567)
diamond = Structure(diamond_lattice, ["C", "C"],
                    [[0, 0, 0], [0.25, 0.25, 0.25]])

# Graphite structure (sp¬≤, simplified version)
graphite_lattice = Lattice.hexagonal(2.46, 6.71)
graphite = Structure(graphite_lattice, ["C", "C"],
                     [[0, 0, 0], [1/3, 2/3, 0.5]])

# Computing composition-based features
comp_diamond = diamond.composition
comp_graphite = graphite.composition

features_diamond = magpie.featurize(comp_diamond)
features_graphite = magpie.featurize(comp_graphite)

print(f"Diamond and graphite features are identical: {np.allclose(features_diamond, features_graphite)}")
# Output: True (indistinguishable with composition-based approach)

print(f"Actual densities:")
print(f"  Diamond: {diamond.density:.2f} g/cm¬≥")  # 3.51
print(f"  Graphite: {graphite.density:.2f} g/cm¬≥")  # 2.26
print(f"Density difference: {abs(diamond.density - graphite.density)/diamond.density * 100:.1f}%")
# Output: Density difference: 35.6% (cannot be captured by composition-based features)
</code></pre>
<h2 id="graph">1.2 Graph Representation: A Mathematical Language to Describe Structure</h2>
<h3>1.2.1 Fundamentals of Graph Theory</h3>
<p>A graph \( G = (V, E) \) consists of:</p>
<ul>
<li><strong>Vertex set \( V \)</strong>: Set of atoms</li>
<li><strong>Edge set \( E \)</strong>: Bonds between atoms (chemical bonds or spatial proximity)</li>
</ul>
<p>Each vertex \( v_i \in V \) is assigned <strong>node features \( \mathbf{x}_i \in \mathbb{R}^d \)</strong>:</p>
<ul>
<li>Atomic number, atomic weight</li>
<li>Electronegativity, ionic radius</li>
<li>Number of valence electrons</li>
</ul>
<p>Each edge \( e_{ij} \in E \) is assigned <strong>edge features \( \mathbf{e}_{ij} \in \mathbb{R}^k \)</strong>:</p>
<ul>
<li>Interatomic distance \( r_{ij} \)</li>
<li>Bond type (single bond, double bond, etc.)</li>
<li>Bond angle</li>
</ul>
<div class="mermaid">
graph TD
    subgraph "Molecule: H‚ÇÇO"
        O[O<br/>Atomic number=8<br/>Electronegativity=3.44]
        H1[H<br/>Atomic number=1<br/>Electronegativity=2.20]
        H2[H<br/>Atomic number=1<br/>Electronegativity=2.20]

        O ---|"Distance=0.96√Ö<br/>Single bond"| H1
        O ---|"Distance=0.96√Ö<br/>Single bond"| H2
    end

    style O fill:#e8f5e9
    style H1 fill:#e3f2fd
    style H2 fill:#e3f2fd
</div>
<h3>1.2.2 Graph Data Structure in PyTorch Geometric</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: 1.2.2 Graph Data Structure in PyTorch Geometric

Purpose: Demonstrate data visualization techniques
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

# Example 3: Graph representation of H‚ÇÇO in PyTorch Geometric
!pip install torch-geometric torch-scatter torch-sparse

import torch
from torch_geometric.data import Data

# Graph representation of H‚ÇÇO molecule
# Node features: [atomic number, electronegativity]
node_features = torch.tensor([
    [8, 3.44],   # O
    [1, 2.20],   # H1
    [1, 2.20]    # H2
], dtype=torch.float)

# Edge list (bidirectional for undirected graph)
edge_index = torch.tensor([
    [0, 1, 1, 0, 0, 2, 2, 0],  # source nodes
    [1, 0, 0, 1, 2, 0, 0, 2]   # target nodes
], dtype=torch.long)

# Edge features: [interatomic distance (√Ö)]
edge_attr = torch.tensor([
    [0.96], [0.96],  # O-H1
    [0.96], [0.96],  # H1-O (bidirectional)
    [0.96], [0.96],  # O-H2
    [0.96], [0.96]   # H2-O (bidirectional)
], dtype=torch.float)

# PyTorch Geometric Data object
data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)

print(f"Number of nodes: {data.num_nodes}")        # 3
print(f"Number of edges: {data.num_edges}")        # 8 (bidirectional)
print(f"Node feature dimensions: {data.num_node_features}")  # 2
print(f"Edge feature dimensions: {data.num_edge_features}")  # 1
print(f"\nData structure:")
print(data)
</code></pre>
<h3>1.2.3 Composition-Based vs Graph-Based: Information Comparison</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Composition-Based (Magpie)</th>
<th>Graph-Based (GNN)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Information Source</strong></td>
<td>Chemical composition only</td>
<td>Composition + atomic arrangement</td>
</tr>
<tr>
<td><strong>Feature Dimensionality</strong></td>
<td>Fixed (132 dimensions)</td>
<td>Variable (depends on number of nodes)</td>
</tr>
<tr>
<td><strong>Polymorph Distinction</strong></td>
<td>‚ùå Impossible</td>
<td>‚úÖ Possible</td>
</tr>
<tr>
<td><strong>Local Environment</strong></td>
<td>‚ùå Not considered</td>
<td>‚úÖ Considered via message passing</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Low (seconds)</td>
<td>Medium-High (minutes, GPU recommended)</td>
</tr>
<tr>
<td><strong>Data Requirements</strong></td>
<td>Low (100-1000 samples)</td>
<td>Medium-High (1000-10000 samples)</td>
</tr>
</tbody>
</table>
<h2 id="gnn-basics">1.3 Basic Principles of GNN (Graph Neural Networks)</h2>
<h3>1.3.1 Concept of Message Passing</h3>
<p>The core of GNNs is <strong>message passing</strong>. Each atom (node) aggregates information from neighboring atoms and updates its own representation.</p>
<p><strong>Mathematical Formulation</strong>:</p>
<p>\[
        \mathbf{h}_i^{(k+1)} = \text{UPDATE}^{(k)} \left( \mathbf{h}_i^{(k)}, \text{AGGREGATE}^{(k)} \left( \{ \mathbf{h}_j^{(k)} : j \in \mathcal{N}(i) \} \right) \right)
        \]</p>
<p>Where:</p>
<ul>
<li>\( \mathbf{h}_i^{(k)} \): Hidden representation of node \( i \) at layer \( k \)</li>
<li>\( \mathcal{N}(i) \): Set of neighbor nodes of node \( i \)</li>
<li>\( \text{AGGREGATE} \): Aggregation function for neighbor information (SUM, MEAN, MAX, etc.)</li>
<li>\( \text{UPDATE} \): Update function for node representation (MLP, GRU, etc.)</li>
</ul>
<div class="mermaid">
graph LR
    subgraph "Layer k"
        A1[h‚ÇÅ‚ÅΩ·µè‚Åæ]
        B1[h‚ÇÇ‚ÅΩ·µè‚Åæ]
        C1[h‚ÇÉ‚ÅΩ·µè‚Åæ]
        D1[h‚ÇÑ‚ÅΩ·µè‚Åæ]
    end

    subgraph "Message Passing"
        B1 --&gt; M[AGGREGATE]
        C1 --&gt; M
        D1 --&gt; M
        A1 --&gt; U[UPDATE]
        M --&gt; U
    end

    subgraph "Layer k+1"
        A2[h‚ÇÅ‚ÅΩ·µè‚Å∫¬π‚Åæ]
    end

    U --&gt; A2

    style M fill:#fff3e0
    style U fill:#e8f5e9
    style A2 fill:#e3f2fd
</div>
<h3>1.3.2 Implementation of a Simple GNN</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

# Example 4: Minimal implementation of message passing
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class SimpleGCNConv(MessagePassing):
    """Simple Graph Convolutional Network layer"""
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr='add')  # "add" aggregation
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        # Step 1: Add self-loops (consider messages from self)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linear transformation
        x = self.lin(x)

        # Step 3: Normalization by degree
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4: Message passing
        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j, norm):
        # Normalized message
        return norm.view(-1, 1) * x_j

# Test
conv = SimpleGCNConv(in_channels=2, out_channels=8)
h0 = data.x  # Node features of H‚ÇÇO (3 nodes, 2 dimensions)

h1 = conv(h0, data.edge_index)
print(f"Input shape: {h0.shape}")  # torch.Size([3, 2])
print(f"Output shape: {h1.shape}")  # torch.Size([3, 8])
print(f"Layer 1 output (first node):\n{h1[0]}")
</code></pre>
<h3>1.3.3 CGCNN vs MPNN: Architectural Differences</h3>
<p><strong>CGCNN (Crystal Graph Convolutional Neural Networks)</strong> is specialized for crystalline materials:</p>
<ul>
<li><strong>Target</strong>: Crystal structures (with periodic boundary conditions)</li>
<li><strong>Edge Features</strong>: Emphasizes interatomic distances</li>
<li><strong>Aggregation</strong>: Soft attention via edge gating mechanism</li>
<li><strong>Applications</strong>: Materials Project, OQMD</li>
</ul>
<p><strong>MPNN (Message Passing Neural Networks)</strong> is a general framework:</p>
<ul>
<li><strong>Target</strong>: Molecules, proteins, crystals - all types</li>
<li><strong>Message Function</strong>: Customizable</li>
<li><strong>Aggregation</strong>: Choose from SUM, MEAN, MAX, etc.</li>
<li><strong>Applications</strong>: QM9, ZINC, ChEMBL</li>
</ul>
<table>
<thead>
<tr>
<th>Feature</th>
<th>CGCNN</th>
<th>MPNN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Paper</strong></td>
<td>Xie &amp; Grossman (2018)</td>
<td>Gilmer et al. (2017)</td>
</tr>
<tr>
<td><strong>Main Target</strong></td>
<td>Crystalline materials</td>
<td>Both molecules and crystals</td>
</tr>
<tr>
<td><strong>Edge Processing</strong></td>
<td>Gating mechanism</td>
<td>General message function</td>
</tr>
<tr>
<td><strong>Aggregation Method</strong></td>
<td>Weighted SUM</td>
<td>SUM/MEAN/MAX</td>
</tr>
<tr>
<td><strong>Periodic Boundary Conditions</strong></td>
<td>‚úÖ Considered</td>
<td>‚ùå Not supported by default</td>
</tr>
</tbody>
</table>
<h2 id="example">1.4 Example: Distinguishing Diamond and Graphite</h2>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

# Example 5: Distinguishing diamond and graphite with GNN
from torch_geometric.data import Data
import numpy as np

def structure_to_graph(structure, cutoff=5.0):
    """Create PyTorch Geometric graph from pymatgen structure"""
    # Node features: atomic number
    x = torch.tensor([[site.specie.Z] for site in structure], dtype=torch.float)

    # Create edge list (neighbors within cutoff radius)
    edges = []
    edge_attrs = []

    for i, site_i in enumerate(structure):
        for j, site_j in enumerate(structure):
            if i != j:
                dist = structure.get_distance(i, j)
                if dist &lt;= cutoff:
                    edges.append([i, j])
                    edge_attrs.append([dist])

    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)

    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)

# Convert diamond and graphite to graphs
graph_diamond = structure_to_graph(diamond, cutoff=2.0)
graph_graphite = structure_to_graph(graphite, cutoff=2.0)

print("Diamond graph:")
print(f"  Number of nodes: {graph_diamond.num_nodes}")
print(f"  Number of edges: {graph_diamond.num_edges}")
print(f"  Average degree: {graph_diamond.num_edges / graph_diamond.num_nodes:.2f}")

print("\nGraphite graph:")
print(f"  Number of nodes: {graph_graphite.num_nodes}")
print(f"  Number of edges: {graph_graphite.num_edges}")
print(f"  Average degree: {graph_graphite.num_edges / graph_graphite.num_nodes:.2f}")

# Output example:
# Diamond: Average degree 4.00 (sp¬≥, tetrahedral)
# Graphite: Average degree 3.00 (sp¬≤, planar trigonal)
# ‚Üí Distinguishable from graph structure!
</code></pre>
<h2 id="pytorch-geometric">1.5 Fundamentals of PyTorch Geometric</h2>
<h3>1.5.1 Details of Data Object</h3>
<pre><code class="language-python"># Example 6: Attributes of PyTorch Geometric Data object
from torch_geometric.data import Data

# More complex example: CO‚ÇÇ molecule
# O=C=O (linear structure)

node_features = torch.tensor([
    [8, 3.44, 6],   # O1: atomic number, electronegativity, valence electrons
    [6, 2.55, 4],   # C:  atomic number, electronegativity, valence electrons
    [8, 3.44, 6]    # O2: atomic number, electronegativity, valence electrons
], dtype=torch.float)

edge_index = torch.tensor([
    [0, 1, 1, 0, 1, 2, 2, 1],
    [1, 0, 0, 1, 2, 1, 1, 2]
], dtype=torch.long)

edge_attr = torch.tensor([
    [1.16, 2],  # O1-C: distance (√Ö), bond order (double bond)
    [1.16, 2],  # C-O1
    [1.16, 2],  # O1-C (reverse direction)
    [1.16, 2],  # C-O1 (reverse direction)
    [1.16, 2],  # C-O2
    [1.16, 2],  # O2-C
    [1.16, 2],  # C-O2 (reverse direction)
    [1.16, 2]   # O2-C (reverse direction)
], dtype=torch.float)

# Target value: dipole moment (Debye)
y = torch.tensor([[0.0]], dtype=torch.float)  # CO‚ÇÇ is symmetric, so 0

data_co2 = Data(x=node_features, edge_index=edge_index,
                edge_attr=edge_attr, y=y)

print("CO‚ÇÇ molecule graph data:")
print(f"  Node feature shape: {data_co2.x.shape}")        # [3, 3]
print(f"  Edge index shape: {data_co2.edge_index.shape}")  # [2, 8]
print(f"  Edge feature shape: {data_co2.edge_attr.shape}")     # [8, 2]
print(f"  Target value: {data_co2.y.item():.2f} Debye")
print(f"  Directed graph: {data_co2.is_directed()}")  # True
</code></pre>
<h3>1.5.2 DataLoader and Batch Processing</h3>
<pre><code class="language-python"># Example 7: Implementing batch processing
from torch_geometric.loader import DataLoader
from torch_geometric.data import Batch

# Create multiple molecule data
molecules = [data, data_co2]  # H‚ÇÇO and CO‚ÇÇ

# Create DataLoader
loader = DataLoader(molecules, batch_size=2, shuffle=True)

for batch in loader:
    print("Batch data:")
    print(f"  Batch size: {batch.num_graphs}")
    print(f"  Total number of nodes: {batch.num_nodes}")
    print(f"  Total number of edges: {batch.num_edges}")
    print(f"  Node feature shape: {batch.x.shape}")
    print(f"  Batch vector: {batch.batch}")
    # Batch vector example: tensor([0, 0, 0, 1, 1, 1])
    # ‚Üí First 3 nodes belong to molecule 0, next 3 nodes to molecule 1
    break

# Advantages of batch processing
print("\n‚úÖ Advantages of batch processing:")
print("  1. Speedup through GPU parallelization")
print("  2. Improved memory efficiency")
print("  3. Application of mini-batch gradient descent")
</code></pre>
<h3>1.5.3 Graph Pooling: Molecular-Level Representation</h3>
<pre><code class="language-python"># Example 8: Global pooling
from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool

# Create graph-level representation from node features
batch = Batch.from_data_list(molecules)

# Mean pooling
graph_emb_mean = global_mean_pool(batch.x, batch.batch)
print(f"Mean pooling output shape: {graph_emb_mean.shape}")  # [2, feature_dim]

# Max pooling
graph_emb_max = global_max_pool(batch.x, batch.batch)
print(f"Max pooling output shape: {graph_emb_max.shape}")

# Sum pooling
graph_emb_sum = global_add_pool(batch.x, batch.batch)
print(f"Sum pooling output shape: {graph_emb_sum.shape}")

print("\nWhen to use each pooling method:")
print("  - Mean pooling: Invariant to molecule size (recommended)")
print("  - Max pooling: Emphasizes most important atoms")
print("  - Sum pooling: Depends on molecule size (suitable for additive properties)")
</code></pre>
<h2 id="summary">1.6 Summary</h2>
<p>In this chapter, we learned the fundamentals of GNN structure-based features:</p>
<ol>
<li><strong>Limitations of composition-based features</strong>: Cannot distinguish diamond and graphite</li>
<li><strong>Strengths of graph representation</strong>: Mathematically describes atomic arrangement</li>
<li><strong>Message passing</strong>: Core computational process of GNNs</li>
<li><strong>CGCNN vs MPNN</strong>: Crystal-specific vs general framework</li>
<li><strong>PyTorch Geometric</strong>: Implementation foundation for graph deep learning</li>
</ol>
<p>In the next chapter, we will learn the detailed implementation of CGCNN and crystal property prediction using Materials Project.</p>
<hr/>
<h2 id="exercises">Exercises</h2>
<h3>Easy (Basic Confirmation)</h3>
<details>
<summary><strong>Q1</strong>: What information cannot be captured by composition-based features?</summary>
<p><strong>Answer</strong>: Spatial arrangement of atoms (structural information)</p>
<p><strong>Explanation</strong>:</p>
<p>Composition-based features (Magpie, etc.) calculate statistical features from chemical composition (Fe‚ÇÇO‚ÇÉ, etc.), but do not consider the following at all:</p>
<ul>
<li>3D coordinates of atoms</li>
<li>Bond lengths, bond angles</li>
<li>Crystal structure (fcc, bcc, etc.)</li>
<li>Local environment (coordination number, coordination polyhedra)</li>
</ul>
<p>Therefore, they cannot distinguish diamond and graphite (both are pure carbon).</p>
</details>
<details>
<summary><strong>Q2</strong>: In PyTorch Geometric's Data object, which attribute stores node features?</summary>
<p><strong>Answer</strong>: <code>x</code></p>
<p><strong>Explanation</strong>:</p>
<p>Main attributes of PyTorch Geometric Data object:</p>
<ul>
<li><code>data.x</code>: Node features (shape: [num_nodes, num_features])</li>
<li><code>data.edge_index</code>: Edge list (shape: [2, num_edges])</li>
<li><code>data.edge_attr</code>: Edge features (shape: [num_edges, num_edge_features])</li>
<li><code>data.y</code>: Target value (property to predict)</li>
</ul>
</details>
<details>
<summary><strong>Q3</strong>: What are the three main steps of message passing?</summary>
<p><strong>Answer</strong>: Message (message generation), Aggregate (aggregation), Update (update)</p>
<p><strong>Explanation</strong>:</p>
<ol>
<li><strong>Message</strong>: Generate messages on each edge
                    <ul><li>Example: \( m_{ij} = \text{MLP}(\mathbf{h}_j, \mathbf{e}_{ij}) \)</li></ul>
</li>
<li><strong>Aggregate</strong>: Aggregate messages from neighbors
                    <ul><li>Example: \( m_i = \sum_{j \in \mathcal{N}(i)} m_{ij} \)</li></ul>
</li>
<li><strong>Update</strong>: Update node representation
                    <ul><li>Example: \( \mathbf{h}_i^{(k+1)} = \text{GRU}(\mathbf{h}_i^{(k)}, m_i) \)</li></ul>
</li>
</ol>
</details>
<h3>Medium (Application)</h3>
<details>
<summary><strong>Q4</strong>: Explain the difference in graph structure between diamond and graphite using average degree.</summary>
<p><strong>Answer</strong>: Diamond (average degree ‚âà 4.0) vs Graphite (average degree ‚âà 3.0)</p>
<p><strong>Explanation</strong>:</p>
<ul>
<li><strong>Diamond</strong>:
                    <ul>
<li>sp¬≥ hybrid orbitals</li>
<li>Each carbon atom bonds with 4 neighboring atoms</li>
<li>Tetrahedral structure</li>
<li>Average degree ‚âà 4.0</li>
</ul>
</li>
<li><strong>Graphite</strong>:
                    <ul>
<li>sp¬≤ hybrid orbitals</li>
<li>Each carbon atom bonds with 3 neighboring atoms</li>
<li>Planar trigonal structure (layered)</li>
<li>Average degree ‚âà 3.0</li>
</ul>
</li>
</ul>
<p>GNNs can learn this difference in degree and distinguish between the two.</p>
</details>
<details>
<summary><strong>Q5</strong>: Give two reasons why CGCNN is suitable for crystalline materials compared to MPNN.</summary>
<p><strong>Answer</strong>: (1) Consideration of periodic boundary conditions, (2) Edge gating mechanism</p>
<p><strong>Explanation</strong>:</p>
<ol>
<li><strong>Periodic Boundary Conditions</strong>:
                    <ul>
<li>Crystals are periodic structures that repeat infinitely</li>
<li>CGCNN considers atoms within the unit cell and periodically repeated neighboring atoms</li>
<li>MPNN is non-periodic by default (designed for molecules)</li>
</ul>
</li>
<li><strong>Edge Gating Mechanism</strong>:
                    <ul>
<li>Weighting according to interatomic distance</li>
<li>Suppresses messages from distant atoms</li>
<li>Appropriately models local environment of crystals</li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>Q6</strong>: Create a graph for the CO‚ÇÇ molecule (O=C=O) in PyTorch Geometric. Use only atomic number for node features.</summary>
<p><strong>Solution Example</strong>:</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - torch&gt;=2.0.0, &lt;2.3.0

"""
Example: Solution Example:

Purpose: Demonstrate core concepts and implementation patterns
Target: Advanced
Execution time: ~5 seconds
Dependencies: None
"""

import torch
from torch_geometric.data import Data

# Node features: atomic number
x = torch.tensor([[8], [6], [8]], dtype=torch.float)  # O, C, O

# Edge list (undirected graph)
edge_index = torch.tensor([
    [0, 1, 1, 2],  # O-C, C-O, C-O, O-C
    [1, 0, 2, 1]
], dtype=torch.long)

data_co2 = Data(x=x, edge_index=edge_index)
print(data_co2)
</code></pre>
<p><strong>Explanation</strong>:</p>
<ul>
<li>Node 0: O (atomic number 8)</li>
<li>Node 1: C (atomic number 6)</li>
<li>Node 2: O (atomic number 8)</li>
<li>Edges: O-C, C-O (bidirectional for undirected graph)</li>
</ul>
</details>
<h3>Hard (Advanced)</h3>
<details>
<summary><strong>Q7</strong>: Compare the data efficiency of composition-based features and GNN features. Which is more advantageous with small data (100 samples) and large data (10,000 samples)? Explain with reasons.</summary>
<p><strong>Answer</strong>:</p>
<p><strong>Small Data (100 samples)</strong>: Composition-based features are advantageous</p>
<ul>
<li><strong>Reason 1</strong>: Fixed dimensionality (132 dimensions) is less prone to overfitting</li>
<li><strong>Reason 2</strong>: Domain knowledge (electronegativity, ionic radius, etc.) is incorporated</li>
<li><strong>Reason 3</strong>: High accuracy even with linear models (Ridge, Lasso)</li>
</ul>
<p><strong>Large Data (10,000 samples)</strong>: GNN features are advantageous</p>
<ul>
<li><strong>Reason 1</strong>: Utilizes structural information, achieving higher accuracy than composition-based</li>
<li><strong>Reason 2</strong>: Can fully leverage the expressive power of deep learning</li>
<li><strong>Reason 3</strong>: Further accuracy improvement with transfer learning (pre-trained models)</li>
</ul>
<p><strong>Quantitative Comparison (Materials Project Data)</strong>:</p>
<table>
<thead>
<tr>
<th>Data Size</th>
<th>Composition-Based (MAE)</th>
<th>GNN (MAE)</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 samples</td>
<td>0.25 eV/atom</td>
<td>0.35 eV/atom (overfitting)</td>
</tr>
<tr>
<td>1,000 samples</td>
<td>0.18 eV/atom</td>
<td>0.15 eV/atom</td>
</tr>
<tr>
<td>10,000 samples</td>
<td>0.15 eV/atom</td>
<td>0.08 eV/atom ‚≠ê</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary><strong>Q8</strong>: Compare the characteristics of aggregation functions in message passing (SUM, MEAN, MAX) and explain situations where each is appropriate.</summary>
<p><strong>Answer</strong>:</p>
<table>
<thead>
<tr>
<th>Aggregation Function</th>
<th>Formula</th>
<th>Characteristics</th>
<th>Appropriate Situations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SUM</strong></td>
<td>\( \sum_{j \in \mathcal{N}(i)} m_{ij} \)</td>
<td>Additive, degree-dependent</td>
<td>Stoichiometric properties (mass, charge)</td>
</tr>
<tr>
<td><strong>MEAN</strong></td>
<td>\( \frac{1}{|\mathcal{N}(i)|} \sum_{j} m_{ij} \)</td>
<td>Normalized, degree-invariant</td>
<td>Average local environment properties (electronegativity)</td>
</tr>
<tr>
<td><strong>MAX</strong></td>
<td>\( \max_{j \in \mathcal{N}(i)} m_{ij} \)</td>
<td>Max value extraction</td>
<td>Emphasize most important features (active site detection)</td>
</tr>
</tbody>
</table>
<p><strong>Practical Recommendations</strong>:</p>
<ul>
<li><strong>MEAN</strong>: Recommended by default (degree-invariant, stable learning)</li>
<li><strong>SUM</strong>: When predicting additive properties</li>
<li><strong>MAX</strong>: Local anomaly detection, identifying catalytic active sites</li>
</ul>
</details>
<details>
<summary><strong>Q9</strong>: Discuss the impact of cutoff radius selection on prediction accuracy in graph representations of crystalline materials. Explain the problems when cutoff radius is too short and too long.</summary>
<p><strong>Answer</strong>:</p>
<p><strong>When cutoff radius is too short (e.g., 2√Ö)</strong>:</p>
<ul>
<li><strong>Problem 1</strong>: Considers only first neighbors, ignores long-range interactions</li>
<li><strong>Problem 2</strong>: Few edges, slow information propagation</li>
<li><strong>Problem 3</strong>: Requires multi-layer GNN (increased computational cost)</li>
<li><strong>Example</strong>: Cannot capture Coulomb interactions in ionic crystals (long-range)</li>
</ul>
<p><strong>When cutoff radius is too long (e.g., 10√Ö)</strong>:</p>
<ul>
<li><strong>Problem 1</strong>: Explosive increase in edges (O(N¬≤))</li>
<li><strong>Problem 2</strong>: Memory exhaustion, increased computation time</li>
<li><strong>Problem 3</strong>: Increased noise (overestimating influence of distant atoms)</li>
<li><strong>Problem 4</strong>: Increased risk of overfitting</li>
</ul>
<p><strong>Selecting Optimal Cutoff Radius</strong>:</p>
<table>
<thead>
<tr>
<th>Material Type</th>
<th>Recommended Cutoff Radius</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Covalent crystals (Si, Diamond)</td>
<td>4-5√Ö</td>
<td>Consider up to second neighbors</td>
</tr>
<tr>
<td>Ionic crystals (NaCl, MgO)</td>
<td>6-8√Ö</td>
<td>Long-range Coulomb interactions</td>
</tr>
<tr>
<td>Metals (Fe, Cu)</td>
<td>5-6√Ö</td>
<td>Consider up to third neighbors</td>
</tr>
<tr>
<td>Molecular crystals</td>
<td>8-10√Ö</td>
<td>Intermolecular interactions (van der Waals forces)</td>
</tr>
</tbody>
</table>
<p><strong>Experimental Optimization</strong>:</p>
<pre><code class="language-python"># Evaluate impact of cutoff radius
cutoffs = [3.0, 4.0, 5.0, 6.0, 8.0, 10.0]
for cutoff in cutoffs:
    graph = structure_to_graph(structure, cutoff=cutoff)
    print(f"Cutoff {cutoff}√Ö: {graph.num_edges} edges")
    # Train and evaluate accuracy
</code></pre>
</details>
<hr/>
<h2 id="objectives">Learning Objectives Checklist</h2>
<p>Upon completing this chapter, you should be able to explain the following:</p>
<h3>Basic Understanding</h3>
<ul>
<li>‚úÖ List three limitations of composition-based features with concrete examples</li>
<li>‚úÖ Explain the mathematical definition of graph representation (\( G = (V, E) \))</li>
<li>‚úÖ Understand the three steps of message passing</li>
<li>‚úÖ Explain the differences between CGCNN and MPNN</li>
</ul>
<h3>Practical Skills</h3>
<ul>
<li>‚úÖ Create Data objects in PyTorch Geometric</li>
<li>‚úÖ Construct graphs from pymatgen structures</li>
<li>‚úÖ Distinguish diamond and graphite by graph structure</li>
<li>‚úÖ Implement batch processing with DataLoader</li>
</ul>
<h3>Application Ability</h3>
<ul>
<li>‚úÖ Choose between composition-based and GNN for new materials problems</li>
<li>‚úÖ Experimentally determine optimal cutoff radius</li>
<li>‚úÖ Infer material properties from graph structure characteristics (average degree, density)</li>
</ul>
<hr/>
<h2 id="next">Next Steps</h2>
<p>In the next chapter, we will learn the detailed implementation of CGCNN and formation energy prediction using Materials Project.</p>
<div class="nav-buttons">
<a class="nav-button" href="./index.html">‚Üê Series Index</a>
<a class="nav-button" href="chapter-2.html">Chapter 2: CGCNN Implementation ‚Üí</a>
</div>
<hr/>
<h2 id="references">References</h2>
<ol>
<li>Ward, L., Agrawal, A., Choudhary, A., &amp; Wolverton, C. (2016). "A general-purpose machine learning framework for predicting properties of inorganic materials." <em>npj Computational Materials</em>, 2, 16028, pp. 1-7.</li>
<li>Xie, T., &amp; Grossman, J. C. (2018). "Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties." <em>Physical Review Letters</em>, 120(14), 145301, pp. 1-6.</li>
<li>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., &amp; Dahl, G. E. (2017). "Neural Message Passing for Quantum Chemistry." <em>Proceedings of the 34th International Conference on Machine Learning</em>, PMLR 70, pp. 1263-1272.</li>
<li>Fey, M., &amp; Lenssen, J. E. (2019). "Fast Graph Representation Learning with PyTorch Geometric." <em>ICLR Workshop on Representation Learning on Graphs and Manifolds</em>, pp. 1-9.</li>
<li>Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D., Dacek, S., ... &amp; Persson, K. A. (2013). "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation." <em>APL Materials</em>, 1(1), 011002, pp. 1-11.</li>
<li>Ong, S. P., Richards, W. D., Jain, A., Hautier, G., Kocher, M., Cholia, S., ... &amp; Persson, K. A. (2013). "Python Materials Genomics (pymatgen): A robust, open-source python library for materials analysis." <em>Computational Materials Science</em>, 68, pp. 314-319.</li>
<li>Sch√ºtt, K. T., Sauceda, H. E., Kindermans, P. J., Tkatchenko, A., &amp; M√ºller, K. R. (2018). "SchNet ‚Äì A deep learning architecture for molecules and materials." <em>The Journal of Chemical Physics</em>, 148(24), 241722, pp. 1-10.</li>
</ol>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranty, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>

</main>
<footer>
<div class="container">
<p>¬© 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
