<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Hands-on PI with Python - Process Optimization Practice - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../../MI/pi-introduction/index.html">Pi</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Hands-on PI with Python - Process Optimization Practice</h1>
            <p class="subtitle">Implementation and Best Practices for Chemical Process Optimization</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading time: 20-25 minutes</span>
                <span class="meta-item">üìä Level: Intermediate</span>
                <span class="meta-item">üíª Code examples: 0</span>
                <span class="meta-item">üìù Exercises: 0</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: Hands-on PI with Python - Process Optimization Practice</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Combine predictive models with multi-objective optimization to handle yield vs cost trade-offs. Develop practical skills in reducing experimental runs through Bayesian optimization.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> "The optimal solution is not always unique." Use the Pareto front to present options and align decision criteria with stakeholders.</p>




<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master:
- Setting up Python environment and installing PI libraries
- Preprocessing and visualizing process data
- Predicting process characteristics with 5+ regression models
- Optimizing process conditions via Bayesian optimization
- Analyzing trade-offs in multi-objective optimization
- Troubleshooting errors independently</p>
<hr />
<h2>1. Environment Setup: 3 Options</h2>
<p>There are three ways to set up a Python environment for chemical process optimization, depending on your situation.</p>
<h3>1.1 Option 1: Anaconda (Recommended for Beginners)</h3>
<p><strong>Features:</strong>
- Scientific computing libraries included from the start
- Easy environment management (GUI available)
- Windows/Mac/Linux compatible</p>
<p><strong>Installation Steps:</strong></p>
<pre><code class="language-bash"># 1. Download Anaconda
# Official site: https://www.anaconda.com/download
# Select Python 3.11 or higher

# 2. After installation, launch Anaconda Prompt

# 3. Create virtual environment (dedicated PI environment)
conda create -n pi-env python=3.11 numpy pandas matplotlib scikit-learn jupyter scipy

# 4. Activate environment
conda activate pi-env

# 5. Install additional libraries
conda install -c conda-forge lightgbm scikit-optimize pymoo

# 6. Verify installation
python --version
# Output: Python 3.11.x
</code></pre>
<p><strong>Anaconda Advantages:</strong>
- ‚úÖ NumPy, SciPy and others included from the start
- ‚úÖ Fewer dependency issues
- ‚úÖ Visual management via Anaconda Navigator
- ‚ùå Large file size (3GB+)</p>
<h3>1.2 Option 2: venv (Python Standard)</h3>
<p><strong>Features:</strong>
- Python standard tool (no additional installation needed)
- Lightweight (install only what you need)
- Project-by-project environment isolation</p>
<p><strong>Installation Steps:</strong></p>
<pre><code class="language-bash"># 1. Verify Python 3.11+ is installed
python3 --version
# Output: Python 3.11.x or higher required

# 2. Create virtual environment
python3 -m venv pi-env

# 3. Activate environment
# macOS/Linux:
source pi-env/bin/activate

# Windows (PowerShell):
pi-env\Scripts\Activate.ps1

# Windows (Command Prompt):
pi-env\Scripts\activate.bat

# 4. Upgrade pip
pip install --upgrade pip

# 5. Install required libraries
pip install numpy pandas matplotlib scikit-learn scipy jupyter
pip install lightgbm scikit-optimize pymoo

# 6. Verify installation
pip list
</code></pre>
<p><strong>venv Advantages:</strong>
- ‚úÖ Lightweight (tens of MB)
- ‚úÖ Python standard tool (no additional installation needed)
- ‚úÖ Independent per project
- ‚ùå Must manually resolve dependencies</p>
<h3>1.3 Option 3: Google Colab (No Installation Required)</h3>
<p><strong>Features:</strong>
- Runs in browser only
- No installation required (cloud execution)
- Free GPU/TPU access</p>
<p><strong>Usage:</strong></p>
<pre><code>1. Access Google Colab: https://colab.research.google.com
2. Create new notebook
3. Run the following code (required libraries pre-installed)
</code></pre>
<pre><code class="language-python"># Google Colab has the following pre-installed
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Additional libraries requiring installation
!pip install scikit-optimize pymoo lightgbm

print("Library import successful!")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
</code></pre>
<p><strong>Google Colab Advantages:</strong>
- ‚úÖ No installation required (instant start)
- ‚úÖ Free GPU access
- ‚úÖ Google Drive integration (easy data storage)
- ‚ùå Internet connection required
- ‚ùå Sessions reset after 12 hours</p>
<h3>1.4 Environment Selection Guide</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Option</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>First Python environment</td>
<td>Anaconda</td>
<td>Easy setup, fewer issues</td>
</tr>
<tr>
<td>Already have Python environment</td>
<td>venv</td>
<td>Lightweight, project-independent</td>
</tr>
<tr>
<td>Want to try immediately</td>
<td>Google Colab</td>
<td>No installation, instant start</td>
</tr>
<tr>
<td>Large-scale optimization needed</td>
<td>Anaconda or venv</td>
<td>Local execution, no compute limits</td>
</tr>
<tr>
<td>Offline environment</td>
<td>Anaconda or venv</td>
<td>Local execution, no internet needed</td>
</tr>
</tbody>
</table>
<h3>1.5 Installation Verification and Troubleshooting</h3>
<p><strong>Verification Commands:</strong></p>
<pre><code class="language-python"># Executable in all environments
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import scipy

print("===== Environment Check =====")
print(f"Python version: {sys.version}")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"Matplotlib version: {plt.matplotlib.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")
print(f"SciPy version: {scipy.__version__}")

# Verify PI-specific libraries
try:
    import skopt
    print(f"scikit-optimize version: {skopt.__version__}")
except ImportError:
    print("‚ö†Ô∏è scikit-optimize not installed (pip install scikit-optimize)")

try:
    import pymoo
    print(f"pymoo version: {pymoo.__version__}")
except ImportError:
    print("‚ö†Ô∏è pymoo not installed (pip install pymoo)")

print("\n‚úÖ Basic libraries installed successfully!")
</code></pre>
<p><strong>Common Errors and Solutions:</strong></p>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'skopt'</code></td>
<td>scikit-optimize not installed</td>
<td>Run <code>pip install scikit-optimize</code></td>
</tr>
<tr>
<td><code>ImportError: DLL load failed</code> (Windows)</td>
<td>Missing C++ redistributable package</td>
<td>Install Microsoft Visual C++ Redistributable</td>
</tr>
<tr>
<td><code>SSL: CERTIFICATE_VERIFY_FAILED</code></td>
<td>SSL certificate error</td>
<td><code>pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org &lt;package&gt;</code></td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Insufficient memory</td>
<td>Reduce data size or use Google Colab</td>
</tr>
</tbody>
</table>
<hr />
<h2>2. Process Data Preparation and Visualization</h2>
<p>Simulate actual chemical process data and perform preprocessing and visualization.</p>
<h3>2.1 Example 1: Process Data Generation and Loading</h3>
<p><strong>Overview:</strong>
Generate simulated chemical reaction process data (temperature, pressure, catalyst amount ‚Üí yield).</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import time

# Generate simulated chemical reaction process data
np.random.seed(42)
n_samples = 200

# Process conditions (input variables)
temperature = np.random.uniform(300, 500, n_samples)  # Temperature [K]
pressure = np.random.uniform(1, 10, n_samples)  # Pressure [bar]
catalyst = np.random.uniform(0.1, 5.0, n_samples)  # Catalyst amount [wt%]

# Yield model (nonlinear relationship + noise)
# Yield = f(temperature, pressure, catalyst amount) + noise
yield_percentage = (
    20  # Base yield
    + 0.15 * temperature  # Temperature effect (positive correlation)
    - 0.0002 * temperature**2  # Temperature quadratic term (optimal temperature exists)
    + 5.0 * pressure  # Pressure effect (positive correlation)
    + 3.0 * catalyst  # Catalyst effect (positive correlation)
    - 0.3 * catalyst**2  # Catalyst quadratic term (diminishing returns with excess)
    + 0.01 * temperature * pressure  # Temperature-pressure interaction
    + np.random.normal(0, 3, n_samples)  # Noise (measurement error)
)

# Store data in DataFrame
process_data = pd.DataFrame({
    'temperature_K': temperature,
    'pressure_bar': pressure,
    'catalyst_wt%': catalyst,
    'yield_%': yield_percentage
})

print("===== Process Data Check =====")
print(process_data.head(10))
print(f"\nData count: {len(process_data)} records")
print(f"\nBasic statistics:")
print(process_data.describe())

# Save in CSV format (actual processes provide data in this format)
process_data.to_csv('process_data.csv', index=False)
print("\n‚úÖ Data saved to process_data.csv")
</code></pre>
<p><strong>Code Explanation:</strong>
1. <strong>Process Conditions</strong>: Temperature (300-500 K), Pressure (1-10 bar), Catalyst amount (0.1-5.0 wt%)
2. <strong>Yield Model</strong>: Nonlinear relationship (quadratic terms, interaction terms) + noise
3. <strong>Real Data Simulation</strong>: Typical chemical reaction behavior (optimal conditions exist, diminishing returns with excess)</p>
<h3>2.2 Example 2: Data Visualization (Scatter Matrix)</h3>
<pre><code class="language-python">import seaborn as sns

# Confirm relationships between variables with scatter matrix
fig = plt.figure(figsize=(12, 10))
sns.pairplot(
    process_data,
    diag_kind='hist',  # Histogram on diagonal
    plot_kws={'alpha': 0.6, 's': 50},  # Scatter plot settings
    diag_kws={'bins': 20, 'edgecolor': 'black'}  # Histogram settings
)
plt.suptitle('Process Data Scatter Matrix', y=1.01, fontsize=16)
plt.tight_layout()
plt.show()

print("===== Correlation Matrix =====")
correlation_matrix = process_data.corr()
print(correlation_matrix)

# Visualize correlation with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(
    correlation_matrix,
    annot=True,  # Display values
    fmt='.3f',  # 3 decimal places
    cmap='coolwarm',  # Color map
    center=0,  # Center at 0
    square=True,  # Square cells
    linewidths=1,  # Cell borders
    cbar_kws={'label': 'Correlation Coefficient'}
)
plt.title('Process Variable Correlation Heatmap', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Interpretation Points:</strong>
- Temperature vs yield relationship: Curved (optimal temperature exists)
- Pressure vs yield relationship: Positive correlation (pressure‚Üë ‚Üí yield‚Üë)
- Catalyst amount vs yield relationship: Curved (diminishing returns with excess)</p>
<h3>2.3 Example 3: Data Preprocessing (Missing Values & Outliers)</h3>
<pre><code class="language-python"># Check and handle missing values
print("===== Missing Values Check =====")
print(process_data.isnull().sum())

# Artificially add missing values (common in real data)
process_data_with_missing = process_data.copy()
missing_indices = np.random.choice(process_data.index, size=10, replace=False)
process_data_with_missing.loc[missing_indices, 'catalyst_wt%'] = np.nan

print(f"\nAfter adding missing values: {process_data_with_missing.isnull().sum()['catalyst_wt%']} records")

# Impute missing values (with mean)
process_data_filled = process_data_with_missing.copy()
process_data_filled['catalyst_wt%'].fillna(
    process_data_filled['catalyst_wt%'].mean(),
    inplace=True
)

print("‚úÖ Missing values imputed with mean")

# Outlier detection (Z-score method)
from scipy import stats

z_scores = np.abs(stats.zscore(process_data[['yield_%']]))
outliers = (z_scores &gt; 3).any(axis=1)

print(f"\n===== Outlier Detection =====")
print(f"Number of outliers: {outliers.sum()} records")
print(f"Outlier percentage: {outliers.sum() / len(process_data) * 100:.1f}%")

# Remove outliers
process_data_clean = process_data[~outliers].copy()
print(f"Data count after removal: {len(process_data_clean)} records")
</code></pre>
<h3>2.4 Example 4: Feature Engineering (Interaction Terms)</h3>
<pre><code class="language-python"># Create process-specific features
process_data_enhanced = process_data_clean.copy()

# Interaction terms (temperature√ópressure, temperature√ócatalyst)
process_data_enhanced['temp_pressure'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['pressure_bar']
)
process_data_enhanced['temp_catalyst'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['catalyst_wt%']
)

# Quadratic terms (temperature¬≤, catalyst¬≤)
process_data_enhanced['temp_squared'] = process_data_enhanced['temperature_K'] ** 2
process_data_enhanced['catalyst_squared'] = process_data_enhanced['catalyst_wt%'] ** 2

# Ratio (catalyst/pressure)
process_data_enhanced['catalyst_pressure_ratio'] = (
    process_data_enhanced['catalyst_wt%'] / (process_data_enhanced['pressure_bar'] + 1e-10)
)

print("===== Data After Feature Engineering =====")
print(process_data_enhanced.head())
print(f"\nFeature count: {len(process_data_enhanced.columns) - 1} (original 3 ‚Üí 8)")
</code></pre>
<h3>2.5 Example 5: Data Splitting (Train/Test)</h3>
<pre><code class="language-python"># Separate features and target variable
X = process_data_enhanced.drop('yield_%', axis=1)  # Input: Process conditions
y = process_data_enhanced['yield_%']  # Output: Yield

# Split into training and test data (80% vs 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("===== Data Splitting =====")
print(f"Training data: {len(X_train)} records")
print(f"Test data: {len(X_test)} records")
print(f"\nFeature column names:")
print(list(X.columns))
</code></pre>
<h3>2.6 Example 6: Data Standardization</h3>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

# Create standardizer (convert to mean 0, std 1)
scaler = StandardScaler()

# Learn standardization parameters on training data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Transform test data with same parameters

print("===== Standardization =====")
print("Training data (before standardization):")
print(f"  Temperature mean: {X_train['temperature_K'].mean():.1f} K")
print(f"  Temperature std: {X_train['temperature_K'].std():.1f} K")

print("\nTraining data (after standardization):")
print(f"  Temperature mean: {X_train_scaled[:, 0].mean():.3f}")
print(f"  Temperature std: {X_train_scaled[:, 0].std():.3f}")

print("\n‚úÖ Standardization complete: all features now have mean 0, std 1")
</code></pre>
<h3>2.7 Example 7: Time Series Process Data Visualization</h3>
<pre><code class="language-python"># Generate time series process data (batch process example)
np.random.seed(42)
time_hours = np.arange(0, 24, 0.5)  # 24 hours, 0.5 hour intervals
n_points = len(time_hours)

# Time series variation of process variables
temp_time = 350 + 50 * np.sin(2 * np.pi * time_hours / 24) + np.random.normal(0, 2, n_points)
pressure_time = 5 + 2 * np.sin(2 * np.pi * time_hours / 12 + np.pi/4) + np.random.normal(0, 0.3, n_points)
yield_time = 60 + 10 * np.sin(2 * np.pi * time_hours / 24 - np.pi/2) + np.random.normal(0, 1.5, n_points)

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(12, 10))

axes[0].plot(time_hours, temp_time, 'r-', linewidth=2, label='Temperature')
axes[0].set_ylabel('Temperature [K]', fontsize=12)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(time_hours, pressure_time, 'b-', linewidth=2, label='Pressure')
axes[1].set_ylabel('Pressure [bar]', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

axes[2].plot(time_hours, yield_time, 'g-', linewidth=2, label='Yield')
axes[2].set_xlabel('Time [h]', fontsize=12)
axes[2].set_ylabel('Yield [%]', fontsize=12)
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.suptitle('Batch Process Time Series Data', fontsize=16)
plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>3. Yield Prediction with Regression Models</h2>
<p>Implement five machine learning models to predict yield from process conditions.</p>
<h3>3.1 Example 8: Linear Regression (Baseline)</h3>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Build linear regression model
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
training_time_lr = time.time() - start_time

# Prediction
y_pred_lr = model_lr.predict(X_test)

# Evaluation
mae_lr = mean_absolute_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

print("===== Linear Regression Model Performance =====")
print(f"Training time: {training_time_lr:.4f} seconds")
print(f"Mean Absolute Error (MAE): {mae_lr:.2f} %")
print(f"Root Mean Square Error (RMSE): {rmse_lr:.2f} %")
print(f"R¬≤ Score: {r2_lr:.4f}")

# Display learned coefficients
print("\n===== Learned Coefficients (Top 3) =====")
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model_lr.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(coefficients.head(3))
</code></pre>
<h3>3.2 Example 9: Random Forest Regression</h3>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# Build Random Forest model
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
model_rf.fit(X_train, y_train)
training_time_rf = time.time() - start_time

# Prediction and evaluation
y_pred_rf = model_rf.predict(X_test)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("\n===== Random Forest Performance =====")
print(f"Training time: {training_time_rf:.4f} seconds")
print(f"Mean Absolute Error (MAE): {mae_rf:.2f} %")
print(f"Root Mean Square Error (RMSE): {rmse_rf:.2f} %")
print(f"R¬≤ Score: {r2_rf:.4f}")

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print("\n===== Feature Importance (Top 3) =====")
print(feature_importance.head(3))
</code></pre>
<h3>3.3 Example 10: LightGBM Regression</h3>
<pre><code class="language-python">import lightgbm as lgb

# Build LightGBM model
start_time = time.time()
model_lgb = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=10,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1
)
model_lgb.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='rmse',
    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]
)
training_time_lgb = time.time() - start_time

# Prediction and evaluation
y_pred_lgb = model_lgb.predict(X_test)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
r2_lgb = r2_score(y_test, y_pred_lgb)

print("\n===== LightGBM Performance =====")
print(f"Training time: {training_time_lgb:.4f} seconds")
print(f"Mean Absolute Error (MAE): {mae_lgb:.2f} %")
print(f"Root Mean Square Error (RMSE): {rmse_lgb:.2f} %")
print(f"R¬≤ Score: {r2_lgb:.4f}")
</code></pre>
<h3>3.4 Example 11: Support Vector Regression (SVR)</h3>
<pre><code class="language-python">from sklearn.svm import SVR

# Build SVR model (using standardized data)
start_time = time.time()
model_svr = SVR(
    kernel='rbf',
    C=100,
    gamma='scale',
    epsilon=0.1
)
model_svr.fit(X_train_scaled, y_train)
training_time_svr = time.time() - start_time

# Prediction and evaluation
y_pred_svr = model_svr.predict(X_test_scaled)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
r2_svr = r2_score(y_test, y_pred_svr)

print("\n===== SVR Performance =====")
print(f"Training time: {training_time_svr:.4f} seconds")
print(f"Mean Absolute Error (MAE): {mae_svr:.2f} %")
print(f"Root Mean Square Error (RMSE): {rmse_svr:.2f} %")
print(f"R¬≤ Score: {r2_svr:.4f}")
</code></pre>
<h3>3.5 Example 12: Neural Network (MLP)</h3>
<pre><code class="language-python">from sklearn.neural_network import MLPRegressor

# Build MLP model
start_time = time.time()
model_mlp = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate_init=0.01,
    max_iter=500,
    random_state=42,
    early_stopping=True,
    validation_fraction=0.2,
    verbose=False
)
model_mlp.fit(X_train_scaled, y_train)
training_time_mlp = time.time() - start_time

# Prediction and evaluation
y_pred_mlp = model_mlp.predict(X_test_scaled)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))
r2_mlp = r2_score(y_test, y_pred_mlp)

print("\n===== MLP Performance =====")
print(f"Training time: {training_time_mlp:.4f} seconds")
print(f"Mean Absolute Error (MAE): {mae_mlp:.2f} %")
print(f"Root Mean Square Error (RMSE): {rmse_mlp:.2f} %")
print(f"R¬≤ Score: {r2_mlp:.4f}")
print(f"Iterations: {model_mlp.n_iter_}")
</code></pre>
<h3>3.6 Example 13: Model Performance Comparison</h3>
<pre><code class="language-python"># Model performance comparison table
comparison = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'MAE (%)': [mae_lr, mae_rf, mae_lgb, mae_svr, mae_mlp],
    'RMSE (%)': [rmse_lr, rmse_rf, rmse_lgb, rmse_svr, rmse_mlp],
    'R¬≤': [r2_lr, r2_rf, r2_lgb, r2_svr, r2_mlp],
    'Training Time (s)': [training_time_lr, training_time_rf, training_time_lgb,
                  training_time_svr, training_time_mlp]
})

print("\n===== Overall Model Performance Comparison =====")
print(comparison.to_string(index=False))

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAE comparison
axes[0].bar(comparison['Model'], comparison['MAE (%)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[0].set_ylabel('MAE (%)', fontsize=12)
axes[0].set_title('Mean Absolute Error (lower is better)', fontsize=14)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3, axis='y')

# R¬≤ comparison
axes[1].bar(comparison['Model'], comparison['R¬≤'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[1].set_ylabel('R¬≤', fontsize=12)
axes[1].set_title('R¬≤ Score (closer to 1 is better)', fontsize=14)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3, axis='y')

# Training time comparison
axes[2].bar(comparison['Model'], comparison['Training Time (s)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[2].set_ylabel('Training Time (s)', fontsize=12)
axes[2].set_title('Training Time (shorter is better)', fontsize=14)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
</code></pre>
<h3>3.7 Example 14: Predicted vs Actual Plot</h3>
<pre><code class="language-python"># Visualize best model (LightGBM) prediction results
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred_lgb, alpha=0.6, s=100, c='green', edgecolors='k', linewidth=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Yield (%)', fontsize=14)
plt.ylabel('Predicted Yield (%)', fontsize=14)
plt.title('LightGBM: Yield Prediction Accuracy', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# Add performance metrics as text
textstr = f'R¬≤ = {r2_lgb:.3f}\nMAE = {mae_lgb:.2f} %\nRMSE = {rmse_lgb:.2f} %'
plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,
         fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
</code></pre>
<hr />
<h2>4. Process Optimization</h2>
<h3>4.1 Example 15: Grid Search for Optimal Conditions</h3>
<pre><code class="language-python">from scipy.optimize import minimize

# Objective function: Maximize yield (minimize negative yield)
def objective_yield(params):
    """
    params = [temperature, pressure, catalyst]
    """
    # Unpack parameters
    temp = params[0]
    press = params[1]
    cat = params[2]

    # Build features (same order as training)
    features = np.array([[
        temp, press, cat,
        temp * press,  # temp_pressure
        temp * cat,    # temp_catalyst
        temp**2,       # temp_squared
        cat**2,        # catalyst_squared
        cat / (press + 1e-10)  # catalyst_pressure_ratio
    ]])

    # Predict with model (using LightGBM)
    predicted_yield = model_lgb.predict(features)[0]

    # Return negative value for maximization
    return -predicted_yield

# Constraints (process operating range)
bounds = [
    (300, 500),  # Temperature [K]
    (1, 10),     # Pressure [bar]
    (0.1, 5.0)   # Catalyst amount [wt%]
]

# Initial guess
x0 = [400, 5, 2.5]

# Execute optimization
result = minimize(
    objective_yield,
    x0,
    method='L-BFGS-B',
    bounds=bounds
)

print("===== Grid Search Optimization Results =====")
print(f"Optimal conditions:")
print(f"  Temperature: {result.x[0]:.1f} K")
print(f"  Pressure: {result.x[1]:.2f} bar")
print(f"  Catalyst amount: {result.x[2]:.2f} wt%")
print(f"\nMaximum predicted yield: {-result.fun:.2f} %")
print(f"Optimization success: {result.success}")
print(f"Iterations: {result.nit}")
</code></pre>
<h3>4.2 Example 16: Bayesian Optimization (Efficient Search)</h3>
<pre><code class="language-python">from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args

# Define search space
space = [
    Real(300, 500, name='temperature'),
    Real(1, 10, name='pressure'),
    Real(0.1, 5.0, name='catalyst')
]

# Objective function (for Bayesian optimization)
@use_named_args(space)
def objective_bayes(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']

    # Build features
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])

    # Predicted yield (negative for maximization)
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# Execute Bayesian optimization
result_bayes = gp_minimize(
    objective_bayes,
    space,
    n_calls=30,  # 30 evaluations
    random_state=42,
    verbose=False
)

print("\n===== Bayesian Optimization Results =====")
print(f"Optimal conditions:")
print(f"  Temperature: {result_bayes.x[0]:.1f} K")
print(f"  Pressure: {result_bayes.x[1]:.2f} bar")
print(f"  Catalyst amount: {result_bayes.x[2]:.2f} wt%")
print(f"\nMaximum predicted yield: {-result_bayes.fun:.2f} %")

# Optimization convergence history
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(result_bayes.func_vals) + 1),
         -result_bayes.func_vals, 'b-o', linewidth=2, markersize=6)
plt.axhline(y=-result_bayes.fun, color='r', linestyle='--',
            label=f'Best value: {-result_bayes.fun:.2f}%')
plt.xlabel('Evaluation Count', fontsize=12)
plt.ylabel('Predicted Yield (%)', fontsize=12)
plt.title('Bayesian Optimization Convergence History', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>4.3 Example 17: Design of Experiments (DoE)</h3>
<pre><code class="language-python">from itertools import product

# 2-level factorial design (2^3 = 8 experiments)
levels = {
    'temperature': [350, 450],  # Low level, high level
    'pressure': [3, 8],
    'catalyst': [1.0, 4.0]
}

# Generate all combinations
experiments = list(product(levels['temperature'], levels['pressure'], levels['catalyst']))

# Calculate predicted yield for each experiment
results_doe = []
for temp, press, cat in experiments:
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    results_doe.append({
        'Temperature [K]': temp,
        'Pressure [bar]': press,
        'Catalyst [wt%]': cat,
        'Predicted Yield [%]': predicted_yield
    })

# Convert results to DataFrame
df_doe = pd.DataFrame(results_doe).sort_values('Predicted Yield [%]', ascending=False)

print("\n===== Design of Experiments (2^3 DoE) Results =====")
print(df_doe.to_string(index=False))

# Extract best condition
best_condition = df_doe.iloc[0]
print(f"\nBest condition:")
print(f"  Temperature: {best_condition['Temperature [K]']:.0f} K")
print(f"  Pressure: {best_condition['Pressure [bar]']:.0f} bar")
print(f"  Catalyst amount: {best_condition['Catalyst [wt%]']:.1f} wt%")
print(f"  Predicted yield: {best_condition['Predicted Yield [%]']:.2f} %")
</code></pre>
<h3>4.4 Example 18: Response Surface Method</h3>
<pre><code class="language-python">from scipy.interpolate import griddata

# Create grid over temperature and pressure ranges (catalyst amount fixed)
temp_range = np.linspace(300, 500, 50)
press_range = np.linspace(1, 10, 50)
temp_grid, press_grid = np.meshgrid(temp_range, press_range)

# Predict yield at each grid point (catalyst amount fixed to optimal value)
catalyst_fixed = result_bayes.x[2]
yield_grid = np.zeros_like(temp_grid)

for i in range(len(temp_range)):
    for j in range(len(press_range)):
        temp = temp_grid[j, i]
        press = press_grid[j, i]
        cat = catalyst_fixed

        features = np.array([[
            temp, press, cat,
            temp * press,
            temp * cat,
            temp**2,
            cat**2,
            cat / (press + 1e-10)
        ]])

        yield_grid[j, i] = model_lgb.predict(features)[0]

# Visualize response surface
fig = plt.figure(figsize=(14, 6))

# Contour plot
ax1 = fig.add_subplot(1, 2, 1)
contour = ax1.contourf(temp_grid, press_grid, yield_grid, levels=20, cmap='viridis')
ax1.set_xlabel('Temperature [K]', fontsize=12)
ax1.set_ylabel('Pressure [bar]', fontsize=12)
ax1.set_title(f'Response Surface (Catalyst = {catalyst_fixed:.2f} wt%)', fontsize=14)
plt.colorbar(contour, ax=ax1, label='Predicted Yield [%]')

# 3D surface
ax2 = fig.add_subplot(1, 2, 2, projection='3d')
surf = ax2.plot_surface(temp_grid, press_grid, yield_grid,
                        cmap='viridis', alpha=0.8)
ax2.set_xlabel('Temperature [K]', fontsize=10)
ax2.set_ylabel('Pressure [bar]', fontsize=10)
ax2.set_zlabel('Predicted Yield [%]', fontsize=10)
ax2.set_title('3D Response Surface', fontsize=14)
plt.colorbar(surf, ax=ax2, label='Predicted Yield [%]', shrink=0.5)

plt.tight_layout()
plt.show()
</code></pre>
<h3>4.5 Example 19: Constrained Optimization</h3>
<pre><code class="language-python">from scipy.optimize import NonlinearConstraint

# Objective function (maximize yield)
def objective_constrained(params):
    temp, press, cat = params
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# Constraint function: Energy cost < 100 [arbitrary units]
# Cost = 0.1 * temperature + 2.0 * pressure
def energy_cost_constraint(params):
    temp, press, cat = params
    cost = 0.1 * temp + 2.0 * press
    return cost

# Constraint: Energy cost <= 100
constraint = NonlinearConstraint(energy_cost_constraint, -np.inf, 100)

# Execute optimization
result_constrained = minimize(
    objective_constrained,
    x0=[400, 5, 2.5],
    method='SLSQP',
    bounds=bounds,
    constraints=constraint
)

print("\n===== Constrained Optimization Results =====")
print(f"Optimal conditions (under energy cost constraint):")
print(f"  Temperature: {result_constrained.x[0]:.1f} K")
print(f"  Pressure: {result_constrained.x[1]:.2f} bar")
print(f"  Catalyst amount: {result_constrained.x[2]:.2f} wt%")
print(f"\nMaximum predicted yield: {-result_constrained.fun:.2f} %")
print(f"Energy cost: {energy_cost_constraint(result_constrained.x):.2f}")
print(f"Constraint satisfied: {energy_cost_constraint(result_constrained.x) <= 100}")
</code></pre>
<h3>4.6 Example 20: Multi-objective Optimization (Yield vs Cost)</h3>
<pre><code class="language-python">from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.core.problem import Problem
from pymoo.optimize import minimize as pymoo_minimize

# Define multi-objective optimization problem
class ProcessOptimizationProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=3,  # Number of variables (temperature, pressure, catalyst)
            n_obj=2,  # Number of objectives (yield, cost)
            xl=np.array([300, 1, 0.1]),  # Lower bounds
            xu=np.array([500, 10, 5.0])  # Upper bounds
        )

    def _evaluate(self, X, out, *args, **kwargs):
        # X: (n_samples, 3) array
        n_samples = X.shape[0]
        f1 = np.zeros(n_samples)  # Objective 1: -yield (minimize)
        f2 = np.zeros(n_samples)  # Objective 2: cost (minimize)

        for i in range(n_samples):
            temp, press, cat = X[i]

            # Predict yield
            features = np.array([[
                temp, press, cat,
                temp * press,
                temp * cat,
                temp**2,
                cat**2,
                cat / (press + 1e-10)
            ]])
            predicted_yield = model_lgb.predict(features)[0]

            # Objective 1: Maximize yield ‚Üí minimize -yield
            f1[i] = -predicted_yield

            # Objective 2: Minimize cost
            # Cost = Energy cost + Catalyst cost
            energy_cost = 0.1 * temp + 2.0 * press
            catalyst_cost = 5.0 * cat
            f2[i] = energy_cost + catalyst_cost

        out["F"] = np.column_stack([f1, f2])

# Optimize with NSGA-II algorithm
problem = ProcessOptimizationProblem()
algorithm = NSGA2(pop_size=50)

result_nsga2 = pymoo_minimize(
    problem,
    algorithm,
    ('n_gen', 100),  # 100 generations
    verbose=False
)

# Get Pareto optimal solutions
pareto_front = result_nsga2.F
pareto_solutions = result_nsga2.X

print("\n===== Multi-objective Optimization (NSGA-II) Results =====")
print(f"Number of Pareto optimal solutions: {len(pareto_solutions)}")
print(f"\nExamples of Pareto optimal solutions (first 3):")
for i in range(min(3, len(pareto_solutions))):
    temp, press, cat = pareto_solutions[i]
    yield_val = -pareto_front[i, 0]
    cost_val = pareto_front[i, 1]
    print(f"\nSolution {i+1}:")
    print(f"  Temperature: {temp:.1f} K, Pressure: {press:.2f} bar, Catalyst: {cat:.2f} wt%")
    print(f"  Yield: {yield_val:.2f} %, Cost: {cost_val:.2f}")

# Visualize Pareto front
plt.figure(figsize=(10, 6))
plt.scatter(-pareto_front[:, 0], pareto_front[:, 1],
            c='blue', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('Yield [%]', fontsize=12)
plt.ylabel('Cost [arbitrary units]', fontsize=12)
plt.title('Pareto Front (Yield vs Cost)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>4.7 Example 21: Optimization Results Comparison</h3>
<pre><code class="language-python"># Compare all optimization method results
optimization_results = pd.DataFrame({
    'Method': [
        'Grid Search',
        'Bayesian Optimization',
        'DoE (2^3)',
        'Constrained Optimization'
    ],
    'Temperature [K]': [
        result.x[0],
        result_bayes.x[0],
        best_condition['Temperature [K]'],
        result_constrained.x[0]
    ],
    'Pressure [bar]': [
        result.x[1],
        result_bayes.x[1],
        best_condition['Pressure [bar]'],
        result_constrained.x[1]
    ],
    'Catalyst [wt%]': [
        result.x[2],
        result_bayes.x[2],
        best_condition['Catalyst [wt%]'],
        result_constrained.x[2]
    ],
    'Predicted Yield [%]': [
        -result.fun,
        -result_bayes.fun,
        best_condition['Predicted Yield [%]'],
        -result_constrained.fun
    ]
})

print("\n===== Optimization Method Comparison =====")
print(optimization_results.to_string(index=False))
</code></pre>
<h3>4.8 Example 22: Optimization Method Flowchart</h3>
<pre><code class="language-python"># Mermaid flowchart (display in Markdown)
print("""
&lt;div class="mermaid"&gt;
graph TD
    A[Process Optimization Task] --&gt; B{Number of objectives?}
    B --&gt;|Single objective| C{Constraints?}
    B --&gt;|Multi-objective| D[NSGA-II/Genetic Algorithm]

    C --&gt;|None| E{Evaluation cost?}
    C --&gt;|Yes| F[Constrained Optimization&lt;br&gt;SLSQP/COBYLA]

    E --&gt;|Low| G[Grid Search&lt;br&gt;or DoE]
    E --&gt;|High| H[Bayesian Optimization&lt;br&gt;Gaussian Process]

    D --&gt; I[Obtain Pareto Front]
    F --&gt; J[Obtain Optimal Conditions]
    G --&gt; J
    H --&gt; J
    I --&gt; K[Trade-off Analysis]
    J --&gt; L[Experimental Validation]
    K --&gt; L

    style A fill:#e3f2fd
    style D fill:#c8e6c9
    style F fill:#fff9c4
    style H fill:#ffccbc
    style I fill:#f3e5f5
&lt;/div&gt;
""")
</code></pre>
<hr />
<h2>5. Advanced Techniques</h2>
<h3>5.9 Example 23: Hyperparameter Tuning (Grid Search)</h3>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# Random Forest hyperparameter candidates
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

# Grid Search configuration
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1
)

# Execute Grid Search
print("===== Grid Search Started =====")
grid_search.fit(X_train, y_train)

print(f"\n===== Best Hyperparameters =====")
for param, value in grid_search.best_params_.items():
    print(f"{param}: {value}")

print(f"\nCross-validation MAE: {-grid_search.best_score_:.2f} %")

# Evaluate best model on test data
best_model_gs = grid_search.best_estimator_
y_pred_gs = best_model_gs.predict(X_test)
mae_gs = mean_absolute_error(y_test, y_pred_gs)
r2_gs = r2_score(y_test, y_pred_gs)

print(f"\nPerformance on test data:")
print(f"  MAE: {mae_gs:.2f} %")
print(f"  R¬≤: {r2_gs:.4f}")
</code></pre>
<h3>5.10 Example 24: Anomaly Detection in Time Series Process</h3>
<pre><code class="language-python">from sklearn.ensemble import IsolationForest

# Time series process data (using data generated in Example 7)
process_time_series = pd.DataFrame({
    'time_h': time_hours,
    'temperature_K': temp_time,
    'pressure_bar': pressure_time,
    'yield_%': yield_time
})

# Anomaly detection with Isolation Forest
iso_forest = IsolationForest(
    contamination=0.1,  # Assume 10% anomaly rate
    random_state=42
)

# Features (temperature, pressure, yield)
X_anomaly = process_time_series[['temperature_K', 'pressure_bar', 'yield_%']]

# Calculate anomaly scores
anomaly_scores = iso_forest.fit_predict(X_anomaly)
process_time_series['anomaly'] = anomaly_scores

# Extract anomalous data
anomalies = process_time_series[process_time_series['anomaly'] == -1]

print(f"\n===== Anomaly Detection Results =====")
print(f"Anomalous data count: {len(anomalies)} / {len(process_time_series)} records")
print(f"Anomaly rate: {len(anomalies) / len(process_time_series) * 100:.1f}%")

# Visualization
plt.figure(figsize=(14, 5))
plt.plot(process_time_series['time_h'], process_time_series['yield_%'],
         'b-', linewidth=1.5, label='Normal Data')
plt.scatter(anomalies['time_h'], anomalies['yield_%'],
            c='red', s=100, marker='x', linewidth=2, label='Anomalous Data')
plt.xlabel('Time [h]', fontsize=12)
plt.ylabel('Yield [%]', fontsize=12)
plt.title('Time Series Process Anomaly Detection', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>
<h3>5.11 Example 25: Interpretability Analysis with SHAP Values</h3>
<pre><code class="language-python"># Analyze feature influence with SHAP (SHapley Additive exPlanations)
try:
    import shap

    # Create SHAP Explainer (for LightGBM)
    explainer = shap.TreeExplainer(model_lgb)
    shap_values = explainer.shap_values(X_test)

    # SHAP value summary plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
    plt.title('SHAP Value Summary Plot (Feature Influence)', fontsize=14)
    plt.tight_layout()
    plt.show()

    print("\n‚úÖ SHAP value analysis complete")
    print("Visualized how each feature influences predictions.")

except ImportError:
    print("\n‚ö†Ô∏è SHAP library not installed")
    print("Install with: pip install shap")
</code></pre>
<h3>5.12 Example 26: Process Simulation (PID Control)</h3>
<pre><code class="language-python"># Simple PID controller simulation
class PIDController:
    def __init__(self, Kp, Ki, Kd, setpoint):
        self.Kp = Kp  # Proportional gain
        self.Ki = Ki  # Integral gain
        self.Kd = Kd  # Derivative gain
        self.setpoint = setpoint  # Target value
        self.integral = 0
        self.prev_error = 0

    def update(self, measured_value, dt):
        # Calculate error
        error = self.setpoint - measured_value

        # Integral term
        self.integral += error * dt

        # Derivative term
        derivative = (error - self.prev_error) / dt

        # PID output
        output = (
            self.Kp * error +
            self.Ki * self.integral +
            self.Kd * derivative
        )

        # Save error for next step
        self.prev_error = error

        return output

# Process model (first-order lag system)
def process_model(input_val, current_temp, tau=5.0, K=1.0, dt=0.1):
    """
    First-order lag system process model
    tau: time constant, K: gain
    """
    dT = (K * input_val - current_temp) / tau
    new_temp = current_temp + dT * dt
    return new_temp

# Simulation settings
dt = 0.1  # Time step [seconds]
t_end = 50  # Simulation time [seconds]
time_sim = np.arange(0, t_end, dt)

# Initialize PID controller (target temperature: 400 K)
pid = PIDController(Kp=2.0, Ki=0.5, Kd=1.0, setpoint=400)

# Execute simulation
temperature = 350  # Initial temperature [K]
temperatures = []
inputs = []

for t in time_sim:
    # Calculate PID control input
    control_input = pid.update(temperature, dt)

    # Update temperature with process model
    temperature = process_model(control_input, temperature, dt=dt)

    # Record
    temperatures.append(temperature)
    inputs.append(control_input)

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# Temperature evolution
axes[0].plot(time_sim, temperatures, 'b-', linewidth=2, label='Process Temperature')
axes[0].axhline(y=400, color='r', linestyle='--', linewidth=1.5, label='Target Temperature')
axes[0].set_ylabel('Temperature [K]', fontsize=12)
axes[0].set_title('PID Control Simulation', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Control input evolution
axes[1].plot(time_sim, inputs, 'g-', linewidth=2, label='Control Input')
axes[1].set_xlabel('Time [seconds]', fontsize=12)
axes[1].set_ylabel('Control Input', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n===== PID Control Simulation Results =====")
print(f"Final temperature: {temperatures[-1]:.2f} K (target: 400 K)")
print(f"Steady-state error: {abs(400 - temperatures[-1]):.2f} K")
</code></pre>
<hr />
<h2>6. Troubleshooting Guide</h2>
<h3>6.1 Common Errors List (Extended)</h3>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
<th>Additional Hints</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'skopt'</code></td>
<td>scikit-optimize not installed</td>
<td><code>pip install scikit-optimize</code></td>
<td>Anaconda: <code>conda install -c conda-forge scikit-optimize</code></td>
</tr>
<tr>
<td><code>ValueError: Input contains NaN</code></td>
<td>Missing values in data</td>
<td>Remove with <code>df.dropna()</code> or impute with <code>df.fillna()</code></td>
<td>Check missing locations: <code>df.isnull().sum()</code></td>
</tr>
<tr>
<td><code>ConvergenceWarning: lbfgs failed to converge</code></td>
<td>MLP optimization not converging</td>
<td><code>max_iter=1000</code>, <code>learning_rate_init=0.001</code></td>
<td>Prevent overfitting with early_stopping=True</td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Insufficient memory</td>
<td>Reduce data size, batch processing</td>
<td>Check memory: <code>df.memory_usage(deep=True)</code></td>
</tr>
<tr>
<td><code>LinAlgError: Singular matrix</code></td>
<td>Singular matrix (multicollinearity)</td>
<td>Check feature multicollinearity, add regularization</td>
<td>Calculate VIF (see Chapter 2)</td>
</tr>
<tr>
<td><code>ImportError: DLL load failed</code> (Windows)</td>
<td>Missing C++ redistributable package</td>
<td>Install Microsoft Visual C++ Redistributable</td>
<td>https://aka.ms/vs/17/release/vc_redist.x64.exe</td>
</tr>
<tr>
<td><code>RuntimeWarning: overflow encountered</code></td>
<td>Numerical overflow</td>
<td>Data scaling (StandardScaler)</td>
<td>Limit values with <code>np.clip()</code></td>
</tr>
<tr>
<td><code>ValueError: n_samples=1, n_features=8</code></td>
<td>Shape error when predicting single sample</td>
<td><code>model.predict([[x1, x2, ...]])</code> (2D)</td>
<td><code>np.array([values]).reshape(1, -1)</code></td>
</tr>
</tbody>
</table>
<h3>6.2 Debugging Checklist (Extended)</h3>
<p><strong>Step 1: Data Verification</strong></p>
<pre><code class="language-python"># Basic data statistics
print("===== Data Statistics =====")
print(process_data.describe())

# Check missing values
print("\n===== Missing Values =====")
print(process_data.isnull().sum())

# Check data types
print("\n===== Data Types =====")
print(process_data.dtypes)

# Check data shape
print(f"\nData shape: {process_data.shape}")
print(f"Rows: {len(process_data)}, Columns: {len(process_data.columns)}")

# Check memory usage
print(f"\nMemory usage: {process_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
</code></pre>
<p><strong>Step 2: Model Simplification</strong></p>
<pre><code class="language-python"># If complex model fails, try linear regression first
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

model_simple = LinearRegression()
model_simple.fit(X_train, y_train)

y_pred_simple = model_simple.predict(X_test)
r2_simple = r2_score(y_test, y_pred_simple)
mae_simple = mean_absolute_error(y_test, y_pred_simple)

print(f"Linear regression R¬≤: {r2_simple:.4f}")
print(f"Linear regression MAE: {mae_simple:.2f}")

# If R¬≤ < 0.5, data quality may be problematic
if r2_simple < 0.5:
    print("‚ö†Ô∏è Low performance even with linear regression ‚Üí Check data quality")
</code></pre>
<p><strong>Step 3: Scaling Verification</strong></p>
<pre><code class="language-python"># Standardization essential for SVR and MLP
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Compare statistics before/after scaling
print("Before scaling:")
print(f"  Mean: {X_train.mean().values[:3]}")  # First 3 features
print(f"  Std: {X_train.std().values[:3]}")

print("After scaling:")
print(f"  Mean: {X_train_scaled.mean(axis=0)[:3]}")  # Nearly 0
print(f"  Std: {X_train_scaled.std(axis=0)[:3]}")  # Nearly 1
</code></pre>
<p><strong>Step 4: Overfitting Check</strong></p>
<pre><code class="language-python"># Check performance difference between training and test data
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"Training R¬≤: {r2_train:.4f}")
print(f"Test R¬≤: {r2_test:.4f}")

# If training-test difference > 0.1, possible overfitting
if r2_train - r2_test > 0.1:
    print("‚ö†Ô∏è Possible overfitting ‚Üí Add regularization")
</code></pre>
<p><strong>Step 5: Feature Importance Verification</strong></p>
<pre><code class="language-python"># Visualize feature importance with Random Forest
import matplotlib.pyplot as plt

if hasattr(model, 'feature_importances_'):
    importance = model.feature_importances_
    feature_names = X.columns

    plt.figure(figsize=(10, 6))
    plt.barh(feature_names, importance)
    plt.xlabel('Importance')
    plt.title('Feature Importance')
    plt.tight_layout()
    plt.show()

    # Consider removing low importance features (< 0.01)
    low_importance = feature_names[importance < 0.01]
    if len(low_importance) > 0:
        print(f"‚ö†Ô∏è Low importance features: {list(low_importance)}")
</code></pre>
<h3>6.3 Practical Pitfalls</h3>
<p><strong>Pitfall 1: Bayesian Optimization Convergence Issues</strong></p>
<p>Problem: Bayesian optimization getting stuck in local optima</p>
<pre><code class="language-python"># ‚ùå Wrong: Too few evaluations
result = gp_minimize(objective, space, n_calls=10)  # Insufficient

# ‚úÖ Correct: Sufficient evaluations + initial random search
result = gp_minimize(
    objective,
    space,
    n_calls=50,  # Increase evaluation count
    n_initial_points=20,  # More initial random search
    acq_func='EI',  # Expected Improvement (balance exploration/exploitation)
    random_state=42
)
</code></pre>
<p><strong>Pitfall 2: Data Leakage in LSTM Time Series Prediction</strong></p>
<p>Problem: Using future information to predict the past</p>
<pre><code class="language-python"># ‚ùå Wrong: Standardize all data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)  # Using all data
train, test = data_scaled[:800], data_scaled[800:]

# ‚úÖ Correct: Standardize training data only
train, test = data[:800], data[800:]
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train)  # Training only
test_scaled = scaler.transform(test)  # Transform with same parameters
</code></pre>
<p><strong>Pitfall 3: Selecting from Too Many Pareto Solutions</strong></p>
<p>Problem: Too many Pareto optimal solutions to choose from</p>
<pre><code class="language-python"># Select 3 representative solutions from Pareto optimal set
# 1. Objective 1 priority (maximum yield)
idx_yield = np.argmin(pareto_front[:, 0])  # f1 minimum (-yield maximum)
# 2. Objective 2 priority (minimum cost)
idx_cost = np.argmin(pareto_front[:, 1])
# 3. Balance (minimum Euclidean distance)
utopia = np.min(pareto_front, axis=0)  # Ideal point
distances = np.linalg.norm(pareto_front - utopia, axis=1)
idx_balance = np.argmin(distances)

print(f"Yield-priority solution: {pareto_solutions[idx_yield]}")
print(f"Cost-priority solution: {pareto_solutions[idx_cost]}")
print(f"Balanced solution: {pareto_solutions[idx_balance]}")
</code></pre>
<hr />
<h2>End-of-Chapter Checklist (50 items)</h2>
<h3>1. Environment Setup (8 items)</h3>
<ul>
<li>[ ] 1.1 Can install Python 3.11 or higher</li>
<li>[ ] 1.2 Can explain differences between Anaconda, venv, Google Colab</li>
<li>[ ] 1.3 Can create and activate virtual environments</li>
<li>[ ] 1.4 Can install required libraries with pip install</li>
<li>[ ] 1.5 Can run installation verification code</li>
<li>[ ] 1.6 Understand environment selection guide (beginner‚ÜíAnaconda, etc.)</li>
<li>[ ] 1.7 Can resolve common installation errors (DLL load failed, etc.)</li>
<li>[ ] 1.8 Can verify and record library versions</li>
</ul>
<h3>2. Data Preprocessing (8 items)</h3>
<ul>
<li>[ ] 2.1 Can generate and load process data</li>
<li>[ ] 2.2 Can visualize variable relationships with scatter matrix</li>
<li>[ ] 2.3 Can create correlation coefficient heatmaps</li>
<li>[ ] 2.4 Can check and impute missing values (mean, linear interpolation)</li>
<li>[ ] 2.5 Can detect outliers (Z-score method) and remove them</li>
<li>[ ] 2.6 Can implement feature engineering (interaction terms, quadratic terms)</li>
<li>[ ] 2.7 Can correctly execute data splitting (train_test_split)</li>
<li>[ ] 2.8 Can implement standardization (StandardScaler) without leakage</li>
</ul>
<h3>3. Regression Model Building (10 items)</h3>
<ul>
<li>[ ] 3.1 Can build and evaluate linear regression models</li>
<li>[ ] 3.2 Can interpret Random Forest feature importance</li>
<li>[ ] 3.3 Can use LightGBM early_stopping</li>
<li>[ ] 3.4 Understand necessity of standardization for SVR</li>
<li>[ ] 3.5 Can adjust MLP hyperparameters (hidden_layer_sizes, etc.)</li>
<li>[ ] 3.6 Can compare 5 model performances (MAE, RMSE, R¬≤)</li>
<li>[ ] 3.7 Can create and evaluate predicted vs actual plots</li>
<li>[ ] 3.8 Understand model performance benchmarks (R¬≤>0.8 good, >0.9 excellent)</li>
<li>[ ] 3.9 Can detect and address overfitting (train R¬≤ vs test R¬≤ difference)</li>
<li>[ ] 3.10 Can remove low feature importance variables</li>
</ul>
<h3>4. Process Optimization (12 items)</h3>
<ul>
<li>[ ] 4.1 Can optimize with grid search (L-BFGS-B)</li>
<li>[ ] 4.2 Can implement Bayesian optimization (gp_minimize)</li>
<li>[ ] 4.3 Can visualize Bayesian optimization convergence history</li>
<li>[ ] 4.4 Can cover conditions comprehensively with DoE (2^3)</li>
<li>[ ] 4.5 Can create contour plots for 2 variables with response surface method</li>
<li>[ ] 4.6 Can visualize 3D response surfaces</li>
<li>[ ] 4.7 Can implement constrained optimization (NonlinearConstraint)</li>
<li>[ ] 4.8 Can execute multi-objective optimization (NSGA-II)</li>
<li>[ ] 4.9 Can visualize and interpret Pareto fronts</li>
<li>[ ] 4.10 Can select 3 representative solutions from Pareto set</li>
<li>[ ] 4.11 Can draw optimization method flowcharts</li>
<li>[ ] 4.12 Can select methods based on evaluation cost (high/low)</li>
</ul>
<h3>5. Advanced Techniques (6 items)</h3>
<ul>
<li>[ ] 5.1 Can perform hyperparameter tuning with GridSearchCV</li>
<li>[ ] 5.2 Can detect anomalies with Isolation Forest</li>
<li>[ ] 5.3 Can analyze feature influence with SHAP values (optional)</li>
<li>[ ] 5.4 Can simulate PID controllers</li>
<li>[ ] 5.5 Can explain influence of PID parameters (Kp, Ki, Kd)</li>
<li>[ ] 5.6 Can visualize time series process data</li>
</ul>
<h3>6. Troubleshooting (6 items)</h3>
<ul>
<li>[ ] 6.1 Know solutions for 8 common errors</li>
<li>[ ] 6.2 Can execute 5-step debugging checklist</li>
<li>[ ] 6.3 Can resolve Bayesian optimization convergence issues</li>
<li>[ ] 6.4 Can avoid data leakage in LSTM time series prediction</li>
<li>[ ] 6.5 Can select representative solutions from multi-objective Pareto set</li>
<li>[ ] 6.6 Can perform overfitting checks and add regularization</li>
</ul>
<hr />
<h2>7. Project Challenge: Chemical Reactor Optimization</h2>
<p>Integrate what you've learned and tackle a practical project.</p>
<h3>7.1 Project Overview</h3>
<p><strong>Goal:</strong>
Optimize chemical reactor operating conditions to maximize yield</p>
<p><strong>Target Performance:</strong>
- Predictive model: R¬≤ > 0.85
- Optimization: Yield > 90%</p>
<h3>7.2 Step-by-Step Guide</h3>
<p><strong>Step 1: Data Generation (More Realistic Data)</strong></p>
<pre><code class="language-python"># Generate more complex reactor data
np.random.seed(42)
n_reactor = 300

temp_reactor = np.random.uniform(320, 480, n_reactor)
press_reactor = np.random.uniform(2, 12, n_reactor)
cat_reactor = np.random.uniform(0.5, 6.0, n_reactor)
residence_time = np.random.uniform(5, 30, n_reactor)  # Residence time [min]

# More complex yield model (4 variables, interactions, optimal values exist)
yield_reactor = (
    25
    + 0.18 * temp_reactor
    - 0.00025 * temp_reactor**2
    + 6.0 * press_reactor
    - 0.3 * press_reactor**2
    + 4.0 * cat_reactor
    - 0.4 * cat_reactor**2
    + 1.5 * residence_time
    - 0.03 * residence_time**2
    + 0.015 * temp_reactor * press_reactor
    + 0.008 * cat_reactor * residence_time
    + np.random.normal(0, 2.5, n_reactor)
)

reactor_data = pd.DataFrame({
    'temperature': temp_reactor,
    'pressure': press_reactor,
    'catalyst': cat_reactor,
    'residence_time': residence_time,
    'yield': yield_reactor
})

print("===== Reactor Data =====")
print(reactor_data.describe())
</code></pre>
<p><strong>Step 2: Feature Engineering</strong></p>
<pre><code class="language-python"># Add features
reactor_data['temp_press'] = reactor_data['temperature'] * reactor_data['pressure']
reactor_data['cat_time'] = reactor_data['catalyst'] * reactor_data['residence_time']
reactor_data['temp_sq'] = reactor_data['temperature'] ** 2
reactor_data['press_sq'] = reactor_data['pressure'] ** 2

X_reactor = reactor_data.drop('yield', axis=1)
y_reactor = reactor_data['yield']

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reactor, y_reactor, test_size=0.2, random_state=42
)
</code></pre>
<p><strong>Step 3: Model Training (LightGBM)</strong></p>
<pre><code class="language-python">model_reactor = lgb.LGBMRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=15,
    random_state=42,
    verbose=-1
)
model_reactor.fit(X_train_r, y_train_r)

y_pred_r = model_reactor.predict(X_test_r)
r2_reactor = r2_score(y_test_r, y_pred_r)
mae_reactor = mean_absolute_error(y_test_r, y_pred_r)

print(f"\n===== Reactor Model Performance =====")
print(f"R¬≤: {r2_reactor:.3f}")
print(f"MAE: {mae_reactor:.2f}%")

if r2_reactor > 0.85:
    print("üéâ Target achieved! (R¬≤ > 0.85)")
</code></pre>
<p><strong>Step 4: Condition Search with Bayesian Optimization</strong></p>
<pre><code class="language-python"># Search for optimal conditions
space_reactor = [
    Real(320, 480, name='temperature'),
    Real(2, 12, name='pressure'),
    Real(0.5, 6.0, name='catalyst'),
    Real(5, 30, name='residence_time')
]

@use_named_args(space_reactor)
def objective_reactor(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']
    res_time = params['residence_time']

    features = np.array([[
        temp, press, cat, res_time,
        temp * press,
        cat * res_time,
        temp**2,
        press**2
    ]])

    predicted_yield = model_reactor.predict(features)[0]
    return -predicted_yield

result_reactor = gp_minimize(
    objective_reactor,
    space_reactor,
    n_calls=50,
    random_state=42,
    verbose=False
)

print(f"\n===== Optimal Conditions =====")
print(f"Temperature: {result_reactor.x[0]:.1f} K")
print(f"Pressure: {result_reactor.x[1]:.2f} bar")
print(f"Catalyst amount: {result_reactor.x[2]:.2f} wt%")
print(f"Residence time: {result_reactor.x[3]:.1f} min")
print(f"\nMaximum predicted yield: {-result_reactor.fun:.2f}%")

if -result_reactor.fun > 90:
    print("üéâ Target achieved! (Yield > 90%)")
</code></pre>
<hr />
<h2>8. Summary</h2>
<h3>What You Learned in This Chapter</h3>
<ol>
<li>
<p><strong>Environment Setup</strong>
   - Three options: Anaconda, venv, Google Colab
   - Installing PI-specific libraries (scikit-optimize, pymoo)</p>
</li>
<li>
<p><strong>Process Data Handling</strong>
   - Data generation and visualization (scatter matrix, heatmaps)
   - Preprocessing (missing value imputation, outlier removal, standardization)
   - Feature engineering (interaction terms, quadratic terms)</p>
</li>
<li>
<p><strong>5 Regression Models</strong>
   - Linear regression, Random Forest, LightGBM, SVR, MLP
   - Model performance comparison (MAE, RMSE, R¬≤)</p>
</li>
<li>
<p><strong>Process Optimization Techniques</strong>
   - Grid search, Bayesian optimization, DoE, response surface method
   - Constrained optimization, multi-objective optimization (NSGA-II)</p>
</li>
<li>
<p><strong>Advanced Techniques</strong>
   - Hyperparameter tuning
   - Anomaly detection (Isolation Forest)
   - Interpretability analysis (SHAP values)
   - PID control simulation</p>
</li>
</ol>
<h3>Next Steps</h3>
<p><strong>After completing this tutorial, you can:</strong>
- ‚úÖ Preprocess and visualize process data
- ‚úÖ Use 5+ regression models appropriately
- ‚úÖ Optimize process conditions with Bayesian optimization
- ‚úÖ Analyze trade-offs with multi-objective optimization</p>
<p><strong>What to learn next:</strong>
1. <strong>Application to Real Processes</strong>
   - Data acquisition from DCS (Distributed Control Systems)
   - Real-time optimization</p>
<ol start="2">
<li>
<p><strong>Deep Learning Applications</strong>
   - LSTM (time series prediction)
   - Autoencoders (anomaly detection)</p>
</li>
<li>
<p><strong>Autonomous Process Control</strong>
   - Control via reinforcement learning
   - Model Predictive Control (MPC)</p>
</li>
</ol>
<hr />
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>List three reasons why Bayesian optimization is superior to grid search in process optimization.</p>
<details>
<summary>Sample Answer</summary>

**Advantages of Bayesian Optimization:**

1. **Fewer Evaluations**
   - Grid search: Try all combinations (e.g., 10√ó10√ó10 = 1000 trials)
   - Bayesian optimization: Reach optimal solution in ~30-50 trials

2. **Smarter Search**
   - Leverages past evaluation results to prioritize promising regions
   - Grid search blindly searches all combinations

3. **Reduced Experimental Costs**
   - Chemical process experiments take hours to days per trial
   - Fewer evaluations dramatically reduce total experimental time

**Example:**
- Grid search: 1000 experiments √ó 3 hours = 3000 hours (125 days)
- Bayesian optimization: 50 experiments √ó 3 hours = 150 hours (6.25 days)

**~20x time reduction!**

</details>

<h3>Problem 2 (Difficulty: medium)</h3>
<p>Explain what a Pareto front is in multi-objective optimization (NSGA-II) and give one example application in chemical processes.</p>
<details>
<summary>Sample Answer</summary>

**What is a Pareto Front:**

When optimizing multiple objectives simultaneously, there exists a trade-off relationship where improving one objective worsens another. The Pareto front is the "set of solutions where no objective can be improved without worsening another."

**Characteristics:**
- All solutions on the Pareto front are "optimal"
- Which solution to choose depends on decision-maker priorities

**Chemical Process Application Example: Distillation Column Optimization**

**Objective 1**: Minimize energy cost
**Objective 2**: Maximize product purity

**Pareto Front Example:**

| Solution | Energy Cost | Product Purity |
|----------|-------------|----------------|
| A        | Low (100 yen/kg) | Low (95%) |
| B        | Medium (150 yen/kg) | Medium (98%) |
| C        | High (200 yen/kg) | High (99.5%) |

**Selection Criteria:**
- Cost priority ‚Üí Solution A (minimum energy cost)
- Quality priority ‚Üí Solution C (highest purity)
- Balanced ‚Üí Solution B (middle ground)

NSGA-II automatically discovers such Pareto fronts.

</details>

<h3>Problem 3 (Difficulty: hard)</h3>
<p>Explain the influence of PID control parameters (Kp, Ki, Kd) on temperature control behavior, and describe the advantages and disadvantages of increasing each parameter.</p>
<details>
<summary>Sample Answer</summary>

**PID Parameter Influence:**

**1. Increasing Kp (Proportional Gain)**

**Advantages:**
- Faster response (reach target value quicker)
- Smaller steady-state error

**Disadvantages:**
- Larger overshoot (oscillates beyond target)
- Reduced stability (oscillatory behavior)

**2. Increasing Ki (Integral Gain)**

**Advantages:**
- Completely eliminates steady-state error
- Improved long-term accuracy

**Disadvantages:**
- Slower response (integral accumulation takes time)
- Wind-up phenomenon (integral term becomes abnormally large)
- Increased overshoot

**3. Increasing Kd (Derivative Gain)**

**Advantages:**
- Suppresses overshoot (predicts and controls changes)
- Improved stability
- Dampens oscillations

**Disadvantages:**
- Sensitive to noise (amplifies small measurement fluctuations)
- Possible high-frequency oscillations

**Optimal Tuning Methods:**

1. **Ziegler-Nichols Method** (classical)
2. **Auto-tuning** (modern)
3. **Simulation-based Optimization** (apply Bayesian optimization learned in this chapter)

**Implementation Example:**

<pre><code class="language-python"># Auto-adjust PID parameters with Bayesian optimization
space_pid = [
    Real(0.1, 10.0, name='Kp'),
    Real(0.01, 1.0, name='Ki'),
    Real(0.01, 5.0, name='Kd')
]

@use_named_args(space_pid)
def objective_pid(**params):
    # Execute PID simulation
    # Goal: Minimize overshoot + minimize settling time
    overshoot, settling_time = simulate_pid(
        Kp=params['Kp'],
        Ki=params['Ki'],
        Kd=params['Kd']
    )
    return overshoot + 0.1 * settling_time

# Execute optimization
result_pid = gp_minimize(objective_pid, space_pid, n_calls=50)
</code></pre>


</details>

<hr />
<h2>References</h2>
<ol>
<li>
<p>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.
   URL: https://scikit-learn.org</p>
</li>
<li>
<p>Brochu, E., Cora, V. M., &amp; de Freitas, N. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." arXiv:1012.2599.
   URL: https://arxiv.org/abs/1012.2599</p>
</li>
<li>
<p>Deb, K., et al. (2002). "A fast and elitist multiobjective genetic algorithm: NSGA-II." <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197.
   DOI: <a href="https://doi.org/10.1109/4235.996017">10.1109/4235.996017</a></p>
</li>
<li>
<p>Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." <em>Proceedings of the IEEE</em>, 104(1), 148-175.
   DOI: <a href="https://doi.org/10.1109/JPROC.2015.2494218">10.1109/JPROC.2015.2494218</a></p>
</li>
<li>
<p>Lundberg, S. M., &amp; Lee, S. I. (2017). "A Unified Approach to Interpreting Model Predictions." <em>Advances in Neural Information Processing Systems</em>, 30.
   URL: https://github.com/slundberg/shap</p>
</li>
<li>
<p>√Östr√∂m, K. J., &amp; H√§gglund, T. (2006). <em>Advanced PID Control</em>. ISA-The Instrumentation, Systems, and Automation Society.
   ISBN: 978-1556179426</p>
</li>
<li>
<p>scikit-optimize Documentation. (2024). "Bayesian Optimization."
   URL: https://scikit-optimize.github.io/stable/</p>
</li>
<li>
<p>pymoo Documentation. (2024). "Multi-objective Optimization."
   URL: https://pymoo.org/</p>
</li>
</ol>
<hr />
<p><strong>Created</strong>: 2025-10-16
<strong>Version</strong>: 1.0
<strong>Series</strong>: PI Introduction Series v1.0
<strong>Author</strong>: MI Knowledge Hub Project</p><div class="navigation">
    <a href="chapter2-fundamentals.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Back to Series Index</a>
    <a href="chapter4-real-world.html" class="nav-button">Next Chapter ‚Üí</a>
</div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, safety, etc.</li>
            <li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>Even if direct, indirect, incidental, special, consequential, or punitive damages arise from the use, execution, or interpretation of this content, the creators and Tohoku University shall not be liable to the maximum extent permitted by applicable law.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-16</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
