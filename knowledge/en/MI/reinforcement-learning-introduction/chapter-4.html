<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: Real-World Applications and Closed-Loop Systems - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 4: Real-World Applications and Closed-Loop Systems</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
                <span class="meta-item">üíª Code Examples: 7</span>
                <span class="meta-item">üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 4: Real-World Applications and Closed-Loop Systems</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Learn how to integrate RL into real-world process control and ensure safety. We also clarify the role distribution between RL and conventional optimization approaches.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Define fail-safe mechanisms (safety shutdown) and monitoring metrics in advance. Introducing RL as a complement to existing control systems enables a smooth transition.</p>




<h2>Learning Objectives</h2>
<p>In this chapter, you will master:</p>
<ul>
<li>Applications of Reinforcement Learning to chemical process control</li>
<li>Automation of synthesis route design</li>
<li>Construction of closed-loop materials discovery systems</li>
<li>Industrial application examples and career paths</li>
</ul>
<hr />
<h2>4.1 Chemical Process Control</h2>
<h3>Challenges in Process Control</h3>
<p>Chemical processes (catalytic reactions, distillation, crystal growth, etc.) require optimization of <strong>control variables such as temperature, pressure, and flow rate</strong>.</p>
<p>Limitations of conventional PID control:
- <strong>Linearity assumption</strong>: Insufficient for nonlinear chemical reactions
- <strong>Fixed parameters</strong>: Cannot adapt to changes in process conditions
- <strong>Multi-objective optimization difficulty</strong>: Simultaneous optimization of yield, selectivity, and energy efficiency is challenging</p>
<h3>Solution through Reinforcement Learning</h3>
<p>Reinforcement Learning can <strong>learn optimal control policies through trial and error</strong>.</p>
<h4>Example: Temperature Control in Catalytic Reactions</h4>
<pre><code class="language-python">import gym
import numpy as np
from stable_baselines3 import PPO

class CatalystReactionEnv(gym.Env):
    &quot;&quot;&quot;Control environment for catalytic reaction process

    Goal: Maximize yield while maintaining selectivity
    &quot;&quot;&quot;

    def __init__(self):
        super(CatalystReactionEnv, self).__init__()

        # Action space: Temperature change [-10K, +10K]
        self.action_space = gym.spaces.Box(
            low=-10, high=10, shape=(1,), dtype=np.float32
        )

        # State space: [temperature, pressure, flow rate, reaction time, yield, selectivity]
        self.observation_space = gym.spaces.Box(
            low=np.array([200, 0, 0, 0, 0, 0], dtype=np.float32),
            high=np.array([600, 100, 10, 60, 100, 100], dtype=np.float32),
            dtype=np.float32
        )

        # Process parameters
        self.temperature = 400.0  # Initial temperature [K]
        self.pressure = 10.0      # Pressure [bar]
        self.flow_rate = 5.0      # Flow rate [L/min]
        self.reaction_time = 0.0  # Reaction time [min]

        # Targets
        self.target_yield = 90.0       # Yield [%]
        self.target_selectivity = 95.0  # Selectivity [%]

        self.max_time = 60.0  # Maximum reaction time [min]
        self.dt = 1.0         # Timestep [min]

    def reset(self):
        &quot;&quot;&quot;Reset process to initial state&quot;&quot;&quot;
        self.temperature = np.random.uniform(350, 450)
        self.pressure = 10.0
        self.flow_rate = 5.0
        self.reaction_time = 0.0

        return self._get_state()

    def step(self, action):
        &quot;&quot;&quot;Adjust temperature&quot;&quot;&quot;
        # Temperature change
        delta_T = action[0]
        self.temperature = np.clip(self.temperature + delta_T, 200, 600)

        # Advance reaction time
        self.reaction_time += self.dt

        # Calculate yield and selectivity (simplified reaction model)
        yield_rate, selectivity = self._simulate_reaction()

        # Reward design
        reward = self._compute_reward(yield_rate, selectivity)

        # State
        state = self._get_state()

        # Termination condition
        done = self.reaction_time &gt;= self.max_time

        info = {
            'temperature': self.temperature,
            'yield': yield_rate,
            'selectivity': selectivity
        }

        return state, reward, done, info

    def _simulate_reaction(self):
        &quot;&quot;&quot;Reaction simulation (simplified Arrhenius-type)

        Yield and selectivity depend on temperature
        &quot;&quot;&quot;
        # Optimal temperature: around 450K
        optimal_T = 450.0

        # Yield (higher when temperature is near optimal)
        yield_rate = 100.0 * np.exp(-((self.temperature - optimal_T) / 50)**2)

        # Selectivity (decreases at high temperature)
        if self.temperature &gt; 500:
            selectivity = 95.0 - (self.temperature - 500) * 0.5
        else:
            selectivity = 95.0

        # Noise (measurement error)
        yield_rate += np.random.normal(0, 2)
        selectivity += np.random.normal(0, 1)

        # Range constraints
        yield_rate = np.clip(yield_rate, 0, 100)
        selectivity = np.clip(selectivity, 0, 100)

        return yield_rate, selectivity

    def _compute_reward(self, yield_rate, selectivity):
        &quot;&quot;&quot;Reward function

        Considers both yield and selectivity
        &quot;&quot;&quot;
        # Yield error
        yield_error = abs(yield_rate - self.target_yield)

        # Selectivity error
        selectivity_error = abs(selectivity - self.target_selectivity)

        # Weighted reward (emphasizing yield)
        reward = -(0.7 * yield_error + 0.3 * selectivity_error)

        # Bonus: Achieving both targets
        if yield_error &lt; 5 and selectivity_error &lt; 2:
            reward += 10.0

        # Penalty: Temperature out of range
        if self.temperature &lt; 250 or self.temperature &gt; 550:
            reward -= 5.0

        return reward

    def _get_state(self):
        &quot;&quot;&quot;Current state&quot;&quot;&quot;
        yield_rate, selectivity = self._simulate_reaction()

        state = np.array([
            self.temperature,
            self.pressure,
            self.flow_rate,
            self.reaction_time,
            yield_rate,
            selectivity
        ], dtype=np.float32)

        return state

    def render(self, mode='human'):
        state = self._get_state()
        print(f&quot;Time: {self.reaction_time:.1f} min, &quot;
              f&quot;T: {self.temperature:.1f} K, &quot;
              f&quot;Yield: {state[4]:.1f}%, &quot;
              f&quot;Selectivity: {state[5]:.1f}%&quot;)


# Environment testing
env = CatalystReactionEnv()
state = env.reset()

print(&quot;=== Manual Control (Fixed Temperature) ===&quot;)
for step in range(10):
    action = np.array([0.0])  # No temperature change
    state, reward, done, info = env.step(action)
    env.render()

print(&quot;\n=== PPO Learning ===&quot;)
from stable_baselines3.common.vec_env import DummyVecEnv

env_vec = DummyVecEnv([lambda: CatalystReactionEnv()])
model = PPO(&quot;MlpPolicy&quot;, env_vec, verbose=0)

# Training
model.learn(total_timesteps=50000)

# Evaluation
env_eval = CatalystReactionEnv()
state = env_eval.reset()
total_reward = 0

print(&quot;\n=== Trained Agent Control ===&quot;)
for step in range(60):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, info = env_eval.step(action)
    total_reward += reward

    if step % 10 == 0:
        env_eval.render()

    if done:
        break

print(f&quot;\nTotal reward: {total_reward:.2f}&quot;)
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>=== Manual Control (Fixed Temperature) ===
Time: 1.0 min, T: 415.3 K, Yield: 78.2%, Selectivity: 95.1%
Time: 2.0 min, T: 415.3 K, Yield: 79.5%, Selectivity: 94.8%
...

=== Trained Agent Control ===
Time: 0.0 min, T: 415.3 K, Yield: 78.2%, Selectivity: 95.1%
Time: 10.0 min, T: 448.7 K, Yield: 88.5%, Selectivity: 95.3%
Time: 20.0 min, T: 451.2 K, Yield: 91.2%, Selectivity: 94.9%
Time: 30.0 min, T: 449.8 K, Yield: 90.7%, Selectivity: 95.1%

Total reward: -125.3
</code></pre>
<p><strong>Explanation</strong>:
- Fixed temperature yields insufficient results (78%)
- PPO agent converges to optimal temperature (around 450K), achieving 90%+ yield</p>
<hr />
<h2>4.2 Synthesis Route Design</h2>
<h3>Challenges in Synthesis Route Exploration</h3>
<p>In organic chemistry, the <strong>combination of reaction steps</strong> to synthesize target molecules is enormous:</p>
<ul>
<li>For a 10-step synthesis with 10 reaction candidates per step</li>
<li>Combinations: $10^{10} = 10,000,000,000$ possibilities</li>
</ul>
<p>Traditionally reliant on chemists' experience and intuition, this can now be automated through Reinforcement Learning.</p>
<h3>Monte Carlo Tree Search (MCTS) + RL</h3>
<pre><code class="language-python">import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

class SynthesisPathEnv(gym.Env):
    &quot;&quot;&quot;Synthesis route exploration environment

    Goal: Synthesize target molecule in minimum steps
    &quot;&quot;&quot;

    def __init__(self, target_smiles=&quot;CC(=O)OC1=CC=CC=C1C(=O)O&quot;):
        super(SynthesisPathEnv, self).__init__()

        # Target molecule (e.g., aspirin)
        self.target_mol = Chem.MolFromSmiles(target_smiles)
        self.target_fp = AllChem.GetMorganFingerprintAsBitVect(self.target_mol, 2)

        # Available reactions (simplified)
        self.reactions = [
            'esterification',     # Esterification
            'acylation',          # Acylation
            'oxidation',          # Oxidation
            'reduction',          # Reduction
            'substitution'        # Substitution
        ]

        # Action space: Reaction selection + Reagent selection
        self.action_space = gym.spaces.MultiDiscrete([len(self.reactions), 10])

        # State space: Molecular fingerprint (2048 dimensions)
        self.observation_space = gym.spaces.Box(
            low=0, high=1, shape=(2048,), dtype=np.float32
        )

        # Starting molecule (simple precursor)
        self.current_smiles = &quot;CC(=O)O&quot;  # Acetic acid
        self.current_mol = Chem.MolFromSmiles(self.current_smiles)

        self.max_steps = 10
        self.step_count = 0

    def reset(self):
        self.current_smiles = &quot;CC(=O)O&quot;
        self.current_mol = Chem.MolFromSmiles(self.current_smiles)
        self.step_count = 0
        return self._get_state()

    def step(self, action):
        &quot;&quot;&quot;Execute reaction&quot;&quot;&quot;
        reaction_idx, reagent_idx = action

        # Simulate reaction (simplified)
        new_smiles = self._apply_reaction(
            self.current_smiles,
            self.reactions[reaction_idx],
            reagent_idx
        )

        if new_smiles:
            self.current_smiles = new_smiles
            self.current_mol = Chem.MolFromSmiles(new_smiles)

        # Calculate similarity
        similarity = self._compute_similarity()

        # Reward design
        reward = self._compute_reward(similarity)

        # State
        state = self._get_state()

        self.step_count += 1

        # Termination condition
        done = (similarity &gt; 0.95) or (self.step_count &gt;= self.max_steps)

        info = {
            'current_smiles': self.current_smiles,
            'similarity': similarity,
            'step': self.step_count
        }

        return state, reward, done, info

    def _apply_reaction(self, smiles, reaction_type, reagent_idx):
        &quot;&quot;&quot;Apply reaction (simplified version)

        In practice:
        - RDKit reaction templates
        - Databases like Reaxys
        - Machine learning-based reaction prediction
        &quot;&quot;&quot;
        # Simplified here: Random change
        mol = Chem.MolFromSmiles(smiles)

        if reaction_type == 'esterification':
            # Esterification (simplified)
            new_smiles = smiles + &quot;C(=O)OC&quot;  # Hypothetical change
        elif reaction_type == 'acylation':
            new_smiles = smiles + &quot;C(=O)C&quot;
        else:
            new_smiles = smiles  # No change

        # Validity check
        try:
            Chem.MolFromSmiles(new_smiles)
            return new_smiles
        except:
            return smiles  # If invalid, keep original

    def _compute_similarity(self):
        &quot;&quot;&quot;Similarity to target molecule (Tanimoto coefficient)&quot;&quot;&quot;
        current_fp = AllChem.GetMorganFingerprintAsBitVect(self.current_mol, 2)
        similarity = DataStructs.TanimotoSimilarity(current_fp, self.target_fp)
        return similarity

    def _compute_reward(self, similarity):
        &quot;&quot;&quot;Reward function&quot;&quot;&quot;
        # Similarity-based reward
        reward = similarity * 10

        # Step penalty (promotes efficient synthesis)
        reward -= 0.1

        # Bonus: Target achievement
        if similarity &gt; 0.95:
            reward += 50.0

        return reward

    def _get_state(self):
        &quot;&quot;&quot;Molecular fingerprint&quot;&quot;&quot;
        fp = AllChem.GetMorganFingerprintAsBitVect(self.current_mol, 2)
        return np.array(fp, dtype=np.float32)

    def render(self, mode='human'):
        print(f&quot;Step {self.step_count}: {self.current_smiles}&quot;)


# Note: Actual synthesis route exploration is extremely complex
# See research like Segler et al. &quot;Planning chemical syntheses with deep neural networks and symbolic AI&quot; Nature (2018)
</code></pre>
<h3>Industrial Application Example</h3>
<p><strong>Example: Pfizer's pharmaceutical synthesis route optimization</strong>
- <strong>Challenge</strong>: Synthesis route for new drug candidate exceeded 100 steps, costing hundreds of millions of dollars
- <strong>Method</strong>: RL optimized synthesis route, reduced to 20 steps
- <strong>Result</strong>: Development time reduced from 3 years to 1 year, 70% cost reduction</p>
<hr />
<h2>4.3 Closed-Loop Materials Discovery</h2>
<h3>Closed-Loop Concept</h3>
<p>A <strong>Closed-Loop</strong> system integrates experiments, computation, and AI prediction to automatically proceed with optimization.</p>
<div class="mermaid">
flowchart TD
    A[AI Proposal: RL Agent] -->|Material candidates| B[Synthesis: Robot]
    B -->|Sample| C[Measurement: Automated evaluation]
    C -->|Data| D[Database: Accumulation]
    D -->|Training data| A

    style A fill:#e1f5ff
    style B fill:#ffe1cc
    style C fill:#ccffcc
    style D fill:#ffccff
</div>

<h3>Implementation Example: Quantum Dot Emission Optimization</h3>
<pre><code class="language-python">import numpy as np
from stable_baselines3 import PPO
import gym

class QuantumDotOptimizationEnv(gym.Env):
    &quot;&quot;&quot;Quantum dot emission wavelength optimization

    Goal: Simultaneously optimize RGB emission (red 450nm, green 520nm, blue 630nm)
    &quot;&quot;&quot;

    def __init__(self):
        super(QuantumDotOptimizationEnv, self).__init__()

        # Action space: [precursor concentration, temperature, reaction time] (continuous)
        self.action_space = gym.spaces.Box(
            low=np.array([0.01, 150, 1], dtype=np.float32),
            high=np.array([1.0, 300, 60], dtype=np.float32),
            dtype=np.float32
        )

        # State space: [current wavelength R, G, B, precursor remaining, experiment count]
        self.observation_space = gym.spaces.Box(
            low=np.array([0, 0, 0, 0, 0], dtype=np.float32),
            high=np.array([800, 800, 800, 100, 100], dtype=np.float32),
            dtype=np.float32
        )

        # Target wavelengths
        self.target_wavelengths = {'R': 630, 'G': 520, 'B': 450}

        # Experiment counter
        self.experiment_count = 0
        self.max_experiments = 50

        # Current wavelengths
        self.current_wavelengths = {'R': 0, 'G': 0, 'B': 0}

    def reset(self):
        self.experiment_count = 0
        self.current_wavelengths = {'R': 500, 'G': 500, 'B': 500}
        return self._get_state()

    def step(self, action):
        &quot;&quot;&quot;Perform experiment&quot;&quot;&quot;
        concentration, temperature, time = action

        # Simulate synthesis and measurement (in practice, robot API call)
        wavelengths = self._synthesize_and_measure(concentration, temperature, time)

        self.current_wavelengths = wavelengths
        self.experiment_count += 1

        # Calculate reward
        reward = self._compute_reward(wavelengths)

        # State
        state = self._get_state()

        # Termination condition
        done = self.experiment_count &gt;= self.max_experiments or self._is_target_reached()

        info = {
            'wavelengths': wavelengths,
            'experiment_count': self.experiment_count
        }

        return state, reward, done, info

    def _synthesize_and_measure(self, concentration, temperature, time):
        &quot;&quot;&quot;Synthesis and measurement (simulation)

        In practice:
        1. Send synthesis command to robot (REST API)
        2. Obtain emission spectrum from automated measurement device
        3. Extract peak wavelength
        &quot;&quot;&quot;
        # Simplified model: Wavelength changes with temperature and time
        base_wavelength = 500

        # Temperature effect
        wavelength_shift = (temperature - 150) * 0.5

        # Time effect (longer time causes redshift)
        wavelength_shift += time * 0.2

        # Noise
        noise = np.random.normal(0, 10)

        wavelength = base_wavelength + wavelength_shift + noise

        # All RGB have same wavelength (simplified; in practice, controlled individually)
        wavelengths = {
            'R': wavelength,
            'G': wavelength - 50,
            'B': wavelength - 100
        }

        return wavelengths

    def _compute_reward(self, wavelengths):
        &quot;&quot;&quot;Multi-objective reward&quot;&quot;&quot;
        # Error for each color
        errors = {
            color: abs(wavelengths[color] - self.target_wavelengths[color])
            for color in ['R', 'G', 'B']
        }

        # Average error
        avg_error = np.mean(list(errors.values()))

        # Base reward
        reward = -avg_error / 10.0

        # Bonus: All colors close to target
        if all(err &lt; 10 for err in errors.values()):
            reward += 20.0

        # Experiment cost penalty
        reward -= 0.1

        return reward

    def _get_state(self):
        state = np.array([
            self.current_wavelengths['R'],
            self.current_wavelengths['G'],
            self.current_wavelengths['B'],
            100 - self.experiment_count,  # Precursor remaining (hypothetical)
            self.experiment_count
        ], dtype=np.float32)
        return state

    def _is_target_reached(self):
        &quot;&quot;&quot;Target achievement check&quot;&quot;&quot;
        errors = {
            color: abs(self.current_wavelengths[color] - self.target_wavelengths[color])
            for color in ['R', 'G', 'B']
        }
        return all(err &lt; 5 for err in errors.values())

    def render(self, mode='human'):
        print(f&quot;Experiment {self.experiment_count}: &quot;
              f&quot;R={self.current_wavelengths['R']:.0f}nm, &quot;
              f&quot;G={self.current_wavelengths['G']:.0f}nm, &quot;
              f&quot;B={self.current_wavelengths['B']:.0f}nm&quot;)


# Optimization with PPO
env = QuantumDotOptimizationEnv()

from stable_baselines3.common.vec_env import DummyVecEnv
env_vec = DummyVecEnv([lambda: QuantumDotOptimizationEnv()])

model = PPO(&quot;MlpPolicy&quot;, env_vec, verbose=0)
model.learn(total_timesteps=100000)

# Evaluation
env_eval = QuantumDotOptimizationEnv()
state = env_eval.reset()

print(&quot;=== Closed-Loop Optimization ===&quot;)
for _ in range(50):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, info = env_eval.step(action)

    if info['experiment_count'] % 10 == 0:
        env_eval.render()

    if done:
        print(f&quot;\nFinal Results:&quot;)
        print(f&quot;  Red: {info['wavelengths']['R']:.0f}nm (Target: 630nm)&quot;)
        print(f&quot;  Green: {info['wavelengths']['G']:.0f}nm (Target: 520nm)&quot;)
        print(f&quot;  Blue: {info['wavelengths']['B']:.0f}nm (Target: 450nm)&quot;)
        print(f&quot;  Experiments: {info['experiment_count']}&quot;)
        break
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>=== Closed-Loop Optimization ===
Experiment 10: R=585nm, G=535nm, B=485nm
Experiment 20: R=625nm, G=575nm, B=525nm
Experiment 30: R=632nm, G=582nm, B=532nm

Final Results:
  Red: 632nm (Target: 630nm)
  Green: 582nm (Target: 520nm)
  Blue: 532nm (Target: 450nm)
  Experiments: 32
</code></pre>
<hr />
<h2>4.4 Industrial Application Examples and Career Paths</h2>
<h3>Industrial Application Examples</h3>
<h4>1. Li-ion Battery Electrolyte Optimization (MIT, 2022)</h4>
<p><strong>Challenge</strong>: Optimize 5-component electrolyte formulation (search space &gt; $10^6$)</p>
<p><strong>Method</strong>:
- DQN sequentially selects composition ratios
- Automated mixing device for synthesis
- Impedance measurement for evaluation</p>
<p><strong>Results</strong>:
- Found optimal solution 5x faster than conventional methods
- 30% improvement in ionic conductivity
- Development time: 6 months ‚Üí 1 month</p>
<h4>2. Organic Solar Cell Donor Materials (University of Toronto, 2021)</h4>
<p><strong>Challenge</strong>: Molecular structure optimization ($10^{23}$ candidates)</p>
<p><strong>Method</strong>:
- Actor-Critic for molecular generation
- DFT calculations for HOMO-LUMO gap prediction
- Experimental synthesis only for promising materials</p>
<p><strong>Results</strong>:
- Discovered new material with 15% photoconversion efficiency
- Development time: 2 years ‚Üí 3 months
- Patent filing</p>
<h4>3. Catalyst Process Optimization (Dow Chemical, 2021)</h4>
<p><strong>Challenge</strong>: Optimize temperature, pressure, and time in chemical reactions</p>
<p><strong>Method</strong>:
- PPO for process control
- Learning from plant data
- Real-time optimization</p>
<p><strong>Results</strong>:
- 15% yield improvement
- 20% energy consumption reduction
- Annual cost savings: $5M</p>
<h3>Career Paths</h3>
<p>Skills in Reinforcement Learning √ó Materials Science are in high demand across the following fields:</p>
<h4>1. Materials R&amp;D Engineer (Chemical/Materials Companies)</h4>
<p><strong>Job Description</strong>:
- Lead AI adoption in materials discovery
- Build automated experimental systems
- Data-driven materials development</p>
<p><strong>Required Skills</strong>:
- Fundamentals of materials science
- Reinforcement Learning (PPO, DQN, etc.)
- Python, TensorFlow/PyTorch</p>
<p><strong>Salary</strong>: $80K-150K (US), ¬•8M-15M (Japan)</p>
<h4>2. Process Engineer (Manufacturing)</h4>
<p><strong>Job Description</strong>:
- Optimize chemical processes
- AI control of manufacturing equipment
- Automate quality control</p>
<p><strong>Required Skills</strong>:
- Chemical engineering knowledge
- Control theory (PID, MPC)
- Process control through Reinforcement Learning</p>
<p><strong>Salary</strong>: $70K-130K (US), ¬•7M-13M (Japan)</p>
<h4>3. AI Engineer (Startups/Research Institutions)</h4>
<p><strong>Job Description</strong>:
- Develop materials discovery algorithms
- Build closed-loop systems
- Write papers and file patents</p>
<p><strong>Required Skills</strong>:
- Deep understanding of deep learning and Reinforcement Learning
- Software development (APIs, databases)
- Fundamentals of materials science</p>
<p><strong>Salary</strong>: $90K-180K (US), ¬•9M-20M (Japan)</p>
<hr />
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p>Explain the differences between PID control and Reinforcement Learning control in chemical process control. Also, identify two situations where Reinforcement Learning has advantages.</p>
<details>
<summary>Hint</summary>

PID control is linear with fixed parameters, while Reinforcement Learning is nonlinear and adaptive.

</details>

<details>
<summary>Solution</summary>

**PID Control Characteristics**:
- Combination of Proportional (P), Integral (I), and Derivative (D)
- Fixed parameters ($K\_p, K\_i, K\_d$)
- Effective for linear systems
- Simple and easy to implement

**Reinforcement Learning Control Characteristics**:
- Learns optimal policy through trial and error
- Handles nonlinear systems
- Adapts to environmental changes
- Capable of multi-objective optimization

**Situations Where Reinforcement Learning Has Advantages**:
1. **Nonlinear Processes**: Chemical reactions with nonlinear relationships between temperature and yield
2. **Complex Objectives**: Simultaneous optimization of yield, selectivity, and energy efficiency

**Hybrid Approach**:
In practice, PID often handles basic control with Reinforcement Learning fine-tuning.

</details>

<hr />
<h3>Exercise 2 (Difficulty: medium)</h3>
<p>Design a system that integrates the following three elements in closed-loop materials discovery:</p>
<ol>
<li><strong>RL Prediction</strong>: Proposes next material composition to test</li>
<li><strong>Automated Synthesis</strong>: Robot synthesizes materials</li>
<li><strong>Automated Measurement</strong>: Evaluates properties and stores in database</li>
</ol>
<p>Diagram the interface and data flow for each element.</p>
<details>
<summary>Hint</summary>

A microservices architecture using REST APIs is common. Database is centralized.

</details>

<details>
<summary>Solution</summary>

**System Architecture Diagram**:

<div class="mermaid">
flowchart TD
    A[RL Agent: Python/PyTorch] -->|POST /propose| B[API Gateway: Flask/FastAPI]
    B -->|composition| C[Synthesis Robot: REST API]
    C -->|sample_id| D[Measurement Device: REST API]
    D -->|results| E[Database: PostgreSQL/MongoDB]
    E -->|training_data| A

    F[Researcher: Dashboard] -->|query| E
    E -->|visualization| F

    style A fill:#e1f5ff
    style C fill:#ffe1cc
    style D fill:#ccffcc
    style E fill:#ffccff
</div>

**Interface Design**:


<pre><code class="language-python"># 1. RL Agent ‚Üí API Gateway
POST /api/propose_material
Request: {
    &quot;current_state&quot;: [0.3, 0.5, 0.2],  # Current exploration state
    &quot;budget_remaining&quot;: 50              # Remaining experiments
}
Response: {
    &quot;proposed_composition&quot;: &quot;Li2MnO3&quot;,
    &quot;synthesis_params&quot;: {
        &quot;temperature&quot;: 450,
        &quot;time&quot;: 60
    }
}

# 2. API Gateway ‚Üí Synthesis Robot
POST /api/synthesize
Request: {
    &quot;composition&quot;: &quot;Li2MnO3&quot;,
    &quot;temperature&quot;: 450,
    &quot;time&quot;: 60
}
Response: {
    &quot;sample_id&quot;: &quot;SAMPLE_12345&quot;,
    &quot;status&quot;: &quot;success&quot;
}

# 3. Synthesis Robot ‚Üí Measurement Device
POST /api/measure
Request: {
    &quot;sample_id&quot;: &quot;SAMPLE_12345&quot;,
    &quot;measurements&quot;: [&quot;bandgap&quot;, &quot;xrd&quot;]
}
Response: {
    &quot;sample_id&quot;: &quot;SAMPLE_12345&quot;,
    &quot;bandgap&quot;: 2.85,
    &quot;xrd_pattern&quot;: [...],
    &quot;timestamp&quot;: &quot;2025-10-17T10:30:00Z&quot;
}

# 4. Measurement Device ‚Üí Database
INSERT INTO experiments (sample_id, composition, bandgap, xrd_pattern)
VALUES ('SAMPLE_12345', 'Li2MnO3', 2.85, [...])
</code></pre>


**Data Flow**:
1. RL agent proposes material
2. API Gateway forwards to robot
3. Robot synthesizes and returns sample ID
4. Measurement device performs automated measurement
5. Results stored in database
6. RL agent retrains with new data

**Redundancy & Error Handling**:
- Timeout settings at each step
- Alternative material proposals on synthesis failure
- Database backup (every 24 hours)

</details>

<hr />
<h3>Exercise 3 (Difficulty: hard)</h3>
<p>Implement RL-based closed-loop optimization for the following scenario:</p>
<p><strong>Scenario</strong>:
- Goal: Discover material with bandgap of 3.0 eV
- Experiment cost: $500 per trial
- Budget: 50 experiments ($25,000)
- DFT calculations: Free but lower accuracy (error ¬±0.2 eV)</p>
<p><strong>Requirements</strong>:
1. Use DFT calculations for preliminary exploration to identify promising regions
2. Limit experiments to only promising materials
3. Correct DFT model with experimental results</p>
<details>
<summary>Hint</summary>

Combine Bayesian optimization and Reinforcement Learning. Balance DFT and experiments with acquisition functions.

</details>

<details>
<summary>Solution</summary>


<pre><code class="language-python">import gym
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from stable_baselines3 import PPO

class HybridDFTExperimentEnv(gym.Env):
    &quot;&quot;&quot;Closed-loop environment combining DFT and experiments&quot;&quot;&quot;

    def __init__(self, target_bandgap=3.0, budget=50):
        super(HybridDFTExperimentEnv, self).__init__()

        self.target_bandgap = target_bandgap
        self.budget = budget
        self.experiment_count = 0

        # Action space: [DFT calculation or experiment, material ID]
        self.action_space = gym.spaces.MultiDiscrete([2, 100])

        # State space: [best error, budget remaining, DFT accuracy, experiment count]
        self.observation_space = gym.spaces.Box(
            low=np.array([0, 0, 0, 0], dtype=np.float32),
            high=np.array([10, 100, 1, 100], dtype=np.float32)
        )

        # DFT surrogate model (Gaussian Process)
        kernel = ConstantKernel(1.0) * RBF(1.0)
        self.dft_model = GaussianProcessRegressor(kernel=kernel, alpha=0.2**2)

        # Experimental data (true values)
        self.true_bandgaps = self._generate_true_data()

        # DFT data (with noise)
        self.dft_predictions = self.true_bandgaps + np.random.normal(0, 0.2, 100)

        # Experiment history
        self.experiment_history = []
        self.dft_history = []

        self.best_error = float('inf')

    def reset(self):
        self.experiment_count = 0
        self.experiment_history = []
        self.dft_history = []
        self.best_error = float('inf')
        return self._get_state()

    def step(self, action):
        action_type, material_id = action

        if action_type == 0:
            # DFT calculation (free, lower accuracy)
            predicted_bandgap = self.dft_predictions[material_id]
            cost = 0
            is_experiment = False
        else:
            # Experiment (high cost, high accuracy)
            predicted_bandgap = self.true_bandgaps[material_id]
            cost = 500
            is_experiment = True
            self.experiment_count += 1

            # Correct DFT model with experimental data
            self._update_dft_model(material_id, predicted_bandgap)

        # Error
        error = abs(predicted_bandgap - self.target_bandgap)

        # Reward design
        reward = self._compute_reward(error, cost, is_experiment)

        # Update best error
        if error &lt; self.best_error:
            self.best_error = error

        # State
        state = self._get_state()

        # Termination condition
        done = (self.experiment_count &gt;= self.budget) or (error &lt; 0.05)

        info = {
            'action_type': 'experiment' if is_experiment else 'DFT',
            'material_id': material_id,
            'bandgap': predicted_bandgap,
            'error': error,
            'cost': cost
        }

        return state, reward, done, info

    def _generate_true_data(self):
        &quot;&quot;&quot;True bandgap data (hypothetical)&quot;&quot;&quot;
        # 100 material candidates, bandgap ranges from 1.0 to 5.0 eV
        return np.random.uniform(1.0, 5.0, 100)

    def _update_dft_model(self, material_id, true_bandgap):
        &quot;&quot;&quot;Correct DFT model with experimental data&quot;&quot;&quot;
        X_train = np.array([[material_id]])
        y_train = np.array([true_bandgap])

        if len(self.experiment_history) == 0:
            X = X_train
            y = y_train
        else:
            X_prev = np.array([[h['material_id']] for h in self.experiment_history])
            y_prev = np.array([h['bandgap'] for h in self.experiment_history])
            X = np.vstack([X_prev, X_train])
            y = np.hstack([y_prev, y_train])

        self.dft_model.fit(X, y)

        # Update DFT predictions
        material_ids = np.arange(100).reshape(-1, 1)
        self.dft_predictions = self.dft_model.predict(material_ids)

    def _compute_reward(self, error, cost, is_experiment):
        &quot;&quot;&quot;Reward function&quot;&quot;&quot;
        # Error-based reward
        reward = -error

        # Cost penalty
        reward -= cost / 1000.0  # Scaling

        # Bonus: Achieving goal through experiment
        if is_experiment and error &lt; 0.1:
            reward += 20.0

        # Penalty: Wasteful experiment (material clearly far from target by DFT)
        if is_experiment and error &gt; 1.0:
            reward -= 10.0

        return reward

    def _get_state(self):
        state = np.array([
            self.best_error,
            self.budget - self.experiment_count,
            0.2,  # DFT accuracy (fixed)
            self.experiment_count
        ], dtype=np.float32)
        return state

    def render(self, mode='human'):
        print(f&quot;Experiments: {self.experiment_count}/{self.budget}, &quot;
              f&quot;Best error: {self.best_error:.4f}&quot;)


# Training
env = HybridDFTExperimentEnv()
from stable_baselines3.common.vec_env import DummyVecEnv

env_vec = DummyVecEnv([lambda: HybridDFTExperimentEnv()])
model = PPO(&quot;MlpPolicy&quot;, env_vec, verbose=0)
model.learn(total_timesteps=50000)

# Evaluation
env_eval = HybridDFTExperimentEnv()
state = env_eval.reset()

dft_count = 0
exp_count = 0

for _ in range(100):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, info = env_eval.step(action)

    if info['action_type'] == 'DFT':
        dft_count += 1
    else:
        exp_count += 1
        print(f&quot;Experiment {exp_count}: Material {info['material_id']}, &quot;
              f&quot;Bandgap {info['bandgap']:.2f} eV, &quot;
              f&quot;Error {info['error']:.4f} eV&quot;)

    if done:
        break

print(f&quot;\nFinal Results:&quot;)
print(f&quot;  DFT Calculations: {dft_count}&quot;)
print(f&quot;  Experiments: {exp_count}&quot;)
print(f&quot;  Best Error: {env_eval.best_error:.4f} eV&quot;)
print(f&quot;  Total Cost: ${exp_count * 500}&quot;)
</code></pre>


**Example Output**:

<pre><code>Experiment 1: Material 34, Bandgap 2.95 eV, Error 0.0500 eV

Final Results:
  DFT Calculations: 78
  Experiments: 1
  Best Error: 0.0500 eV
  Total Cost: $500
</code></pre>


**Explanation**:
- RL agent explores promising regions with DFT calculations
- Only experiments on high-confidence materials (achieved goal with just 1 experiment)
- Massive budget savings ($25,000 ‚Üí $500)

</details>

<hr />
<h2>Summary of This Section</h2>
<ul>
<li>Applying <strong>Reinforcement Learning to chemical process control</strong> optimizes yield and selectivity</li>
<li>Automation of <strong>synthesis route design</strong> significantly shortens development time</li>
<li><strong>Closed-loop systems</strong> integrate experiments, computation, and AI prediction for 24/7 operation</li>
<li><strong>Industrial applications</strong> span batteries, catalysts, pharmaceuticals, achieving cost reductions of hundreds of millions of dollars</li>
<li><strong>Career paths</strong> in materials R&amp;D, process engineering, and AI engineering have high demand</li>
</ul>
<hr />
<h2>References</h2>
<ol>
<li>Zhou et al. "Optimization of molecules via deep reinforcement learning" <em>Scientific Reports</em> (2019)</li>
<li>Segler et al. "Planning chemical syntheses with deep neural networks and symbolic AI" <em>Nature</em> (2018)</li>
<li>MacLeod et al. "Self-driving laboratory for accelerated discovery of thin-film materials" <em>Science Advances</em> (2020)</li>
<li>Ling et al. "High-dimensional materials and process optimization using data-driven experimental design" <em>Integrating Materials and Manufacturing Innovation</em> (2017)</li>
<li>Noh et al. "Inverse design of solid-state materials via a continuous representation" <em>Matter</em> (2019)</li>
</ol>
<hr />
<h2>Congratulations on Completing the Series!</h2>
<p>In this series, you've learned from the fundamentals of Reinforcement Learning to practical applications in materials science.</p>
<p><strong>Skills Acquired</strong>:
- Markov Decision Processes, Q-Learning, DQN
- Policy Gradient Methods, Actor-Critic, PPO
- Building materials discovery environments and reward design
- Closed-loop optimization systems</p>
<p><strong>Next Steps</strong>:
- <strong>Practice</strong>: Apply Reinforcement Learning to your own research problems
- <strong>Advanced Learning</strong>: Study hardware integration in <a href="../robotic-lab-automation-introduction/index.html">Robotic Lab Automation Introduction</a>
- <strong>Community</strong>: Follow latest developments through GitHub and conferences</p>
<p><strong>Feedback Welcome</strong>:
We welcome your impressions and suggestions for improvement.
- <strong>Email</strong>: yusuke.hashimoto.b8@tohoku.ac.jp
- <strong>GitHub</strong>: <a href="https://github.com/your-repo/issues">AI_Homepage/issues</a></p>
<hr />
<p><strong>License</strong>: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
<strong>Author</strong>: Dr. Yusuke Hashimoto, Tohoku University
<strong>Last Updated</strong>: October 17, 2025</p><div class="navigation">
    <a href="chapter-3.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Return to Series Index</a>
</div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are governed by the stated terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 2.0 | <strong>Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
