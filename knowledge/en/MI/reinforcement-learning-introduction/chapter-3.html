<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Building Materials Discovery Environments - AI Terakoya</title>

        <link rel="stylesheet" href="../../assets/css/knowledge-base.css">

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Building Materials Discovery Environments</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 20-25 min</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
                <span class="meta-item">üíª Code Examples: 7</span>
                <span class="meta-item">üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: Building Materials Discovery Environments</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">This chapter demonstrates the definition of states, actions, and rewards through examples, and provides concrete guidance on structuring exploration strategies. We also examine key points for utilizing simulators.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> We estimate the "reality gap" of simulators and enhance robustness through Domain Randomization.</p>




<h2>Learning Objectives</h2>
<p>In this chapter, you will master:</p>
<ul>
<li>How to implement OpenAI Gym custom environments</li>
<li>Designing material descriptors and state spaces</li>
<li>Design principles for effective reward functions</li>
<li>Integration methods with DFT calculations and experimental equipment</li>
</ul>
<hr />
<h2>3.1 OpenAI Gym Environment Fundamentals</h2>
<h3>Components of a Gym Environment</h3>
<p>OpenAI Gym is the standard interface for reinforcement learning environments. All Gym environments implement the following methods:</p>
<pre><code class="language-python">import gym
import numpy as np

class CustomEnv(gym.Env):
    &quot;&quot;&quot;Template for custom Gym environment&quot;&quot;&quot;

    def __init__(self):
        super(CustomEnv, self).__init__()

        # Define action space and observation space (required)
        self.action_space = gym.spaces.Discrete(4)  # Discrete actions (4 types)
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(4,), dtype=np.float32
        )  # Continuous state (4D, range [0, 10])

    def reset(self):
        &quot;&quot;&quot;Reset environment to initial state

        Returns:
            observation: Initial state
        &quot;&quot;&quot;
        self.state = np.random.uniform(0, 10, 4).astype(np.float32)
        return self.state

    def step(self, action):
        &quot;&quot;&quot;Execute action and advance environment by one step

        Args:
            action: Action to execute

        Returns:
            observation: Next state
            reward: Reward
            done: Episode termination flag
            info: Additional information (dictionary)
        &quot;&quot;&quot;
        # Update state based on action
        self.state = self._update_state(action)

        # Compute reward
        reward = self._compute_reward()

        # Check termination condition
        done = self._is_done()

        # Additional information
        info = {'distance': self._compute_distance()}

        return self.state, reward, done, info

    def render(self, mode='human'):
        &quot;&quot;&quot;Visualize environment (optional)&quot;&quot;&quot;
        print(f&quot;Current state: {self.state}&quot;)

    def _update_state(self, action):
        &quot;&quot;&quot;State update logic&quot;&quot;&quot;
        # Implementation depends on environment
        pass

    def _compute_reward(self):
        &quot;&quot;&quot;Reward calculation logic&quot;&quot;&quot;
        pass

    def _is_done(self):
        &quot;&quot;&quot;Termination condition check&quot;&quot;&quot;
        pass

    def _compute_distance(self):
        &quot;&quot;&quot;Additional information calculation&quot;&quot;&quot;
        pass
</code></pre>
<h3>Defining Action and Observation Spaces</h3>
<p>Gym supports various space types:</p>
<pre><code class="language-python">from gym import spaces

# Discrete actions (integers 0, 1, 2, 3)
action_space = spaces.Discrete(4)

# Continuous actions (real vector [-1, 1]^3)
action_space = spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32)

# Dictionary format (multiple inputs)
observation_space = spaces.Dict({
    'composition': spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32),
    'temperature': spaces.Box(low=0, high=1000, shape=(1,), dtype=np.float32),
    'pressure': spaces.Box(low=0, high=10, shape=(1,), dtype=np.float32)
})

# Tuple format
action_space = spaces.Tuple((
    spaces.Discrete(5),      # Element selection
    spaces.Box(low=0, high=1, shape=(1,))  # Composition ratio
))

# MultiBinary (multiple binary choices)
action_space = spaces.MultiBinary(10)  # ON/OFF for 10 elements
</code></pre>
<hr />
<h2>3.2 Designing Material Descriptors and State Spaces</h2>
<h3>Selecting Material Descriptors</h3>
<p>The state space is a <strong>numerical vector representation of material properties</strong>. Selecting effective descriptors is crucial.</p>
<h4>1. Composition-Based Descriptors</h4>
<p><strong>Element Fractions</strong>:</p>
<pre><code class="language-python"># Example: Composition vector for Li2MnO3
composition = {
    'Li': 2/6,   # 33.3%
    'Mn': 1/6,   # 16.7%
    'O': 3/6     # 50.0%
}

# Vector for entire periodic table (118 dimensions)
state = np.zeros(118)
state[2] = 0.333   # Li (atomic number 3)
state[24] = 0.167  # Mn (atomic number 25)
state[7] = 0.500   # O (atomic number 8)
</code></pre>
<p><strong>Magpie Descriptors</strong> (Ward et al., 2016):</p>
<pre><code class="language-python">from matminer.featurizers.composition import ElementProperty

featurizer = ElementProperty.from_preset(&quot;magpie&quot;)
# Generate 132-dimensional descriptor from composition
# - Average atomic number, electronegativity, ionic radius, etc.
composition = &quot;Li2MnO3&quot;
features = featurizer.featurize(Composition(composition))
</code></pre>
<h4>2. Structure-Based Descriptors</h4>
<p><strong>Lattice Parameters</strong>:</p>
<pre><code class="language-python"># Crystal lattice
state = np.array([
    a, b, c,           # Lattice constants
    alpha, beta, gamma # Angles
])
</code></pre>
<p><strong>Smooth Overlap of Atomic Positions (SOAP)</strong>:</p>
<pre><code class="language-python">from dscribe.descriptors import SOAP
from ase import Atoms

# Generate descriptor from atomic structure
atoms = Atoms('H2O', positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]])
soap = SOAP(species=['H', 'O'], rcut=5.0, nmax=8, lmax=6)
state = soap.create(atoms)  # High-dimensional vector
</code></pre>
<h4>3. Process Parameters</h4>
<p><strong>Synthesis Conditions</strong>:</p>
<pre><code class="language-python"># Synthesis process state
state = np.array([
    temperature,      # Temperature [K]
    pressure,         # Pressure [Pa]
    time,             # Time [s]
    heating_rate,     # Heating rate [K/min]
    atmosphere_O2     # Oxygen partial pressure [Pa]
])
</code></pre>
<h3>Example: Band Gap Discovery Environment</h3>
<pre><code class="language-python">from pymatgen.core import Composition
from matminer.featurizers.composition import ElementProperty

class BandgapDiscoveryEnv(gym.Env):
    &quot;&quot;&quot;Band gap optimization environment

    Goal: Discover materials with specific band gap (e.g., 3.0 eV)
    &quot;&quot;&quot;

    def __init__(self, target_bandgap=3.0, element_pool=None):
        super(BandgapDiscoveryEnv, self).__init__()

        self.target_bandgap = target_bandgap

        # Available elements (default: typical semiconductor elements)
        if element_pool is None:
            self.element_pool = ['Ti', 'Zr', 'Hf', 'V', 'Nb', 'Ta', 'Cr', 'Mo', 'W',
                                  'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge',
                                  'As', 'Se', 'Sr', 'Y', 'In', 'Sn', 'Sb', 'Te', 'O', 'S', 'N']
        else:
            self.element_pool = element_pool

        self.n_elements = len(self.element_pool)

        # Action space: Select 3 elements + ratio for each
        # Simplified: Discrete selection of 3 elements (combinations)
        self.action_space = gym.spaces.MultiDiscrete([self.n_elements] * 3)

        # State space: Magpie descriptors (132 dimensions)
        self.featurizer = ElementProperty.from_preset(&quot;magpie&quot;)
        self.observation_space = gym.spaces.Box(
            low=-10, high=10, shape=(132,), dtype=np.float32
        )

        # History (compositions tested)
        self.history = []
        self.current_composition = None

    def reset(self):
        &quot;&quot;&quot;Random initial composition&quot;&quot;&quot;
        self.history = []
        action = self.action_space.sample()
        self.current_composition = self._action_to_composition(action)
        return self._get_state()

    def step(self, action):
        &quot;&quot;&quot;Try new material composition&quot;&quot;&quot;
        self.current_composition = self._action_to_composition(action)

        # State (descriptor)
        state = self._get_state()

        # Predict band gap (surrogate model or DFT)
        predicted_bandgap = self._predict_bandgap(self.current_composition)

        # Reward: Negative of error from target
        error = abs(predicted_bandgap - self.target_bandgap)
        reward = -error

        # Bonus reward (if close to target)
        if error &lt; 0.1:
            reward += 10.0  # Very close

        # Add to history
        self.history.append({
            'composition': self.current_composition,
            'bandgap': predicted_bandgap,
            'reward': reward
        })

        # Termination condition: Reached target or max steps
        done = error &lt; 0.05 or len(self.history) &gt;= 100

        info = {
            'composition': self.current_composition,
            'predicted_bandgap': predicted_bandgap,
            'error': error
        }

        return state, reward, done, info

    def _action_to_composition(self, action):
        &quot;&quot;&quot;Convert action to composition string

        Args:
            action: [elem1_idx, elem2_idx, elem3_idx]

        Returns:
            Composition string (e.g., &quot;TiO2&quot;)
        &quot;&quot;&quot;
        elements = [self.element_pool[idx] for idx in action]

        # Remove duplicates
        unique_elements = list(set(elements))

        # Simplified: Equal mixture
        if len(unique_elements) == 1:
            comp_str = unique_elements[0]
        elif len(unique_elements) == 2:
            comp_str = f&quot;{unique_elements[0]}{unique_elements[1]}&quot;
        else:
            comp_str = f&quot;{unique_elements[0]}{unique_elements[1]}{unique_elements[2]}&quot;

        return comp_str

    def _get_state(self):
        &quot;&quot;&quot;Generate descriptor from current composition&quot;&quot;&quot;
        try:
            comp = Composition(self.current_composition)
            features = self.featurizer.featurize(comp)
            return np.array(features, dtype=np.float32)
        except:
            # Zero vector for invalid composition
            return np.zeros(132, dtype=np.float32)

    def _predict_bandgap(self, composition):
        &quot;&quot;&quot;Predict band gap

        In practice:
        - Machine learning model (pre-trained)
        - DFT calculation (pymatgen + VASP)
        - Database search (Materials Project)

        Here we use simplified rule-based approach
        &quot;&quot;&quot;
        try:
            comp = Composition(composition)

            # Simple rule: Compounds with oxygen tend to have larger band gaps
            if 'O' in comp:
                base_gap = 2.5
            elif 'S' in comp:
                base_gap = 1.8
            elif 'N' in comp:
                base_gap = 2.0
            else:
                base_gap = 1.0

            # Effect of metal elements
            metals = ['Ti', 'Zr', 'Hf', 'V', 'Nb', 'Ta']
            for metal in metals:
                if metal in comp:
                    base_gap += 0.5

            # Random noise (experimental error)
            noise = np.random.normal(0, 0.2)
            return max(0, base_gap + noise)

        except:
            return 0.0

    def render(self, mode='human'):
        print(f&quot;Current composition: {self.current_composition}&quot;)
        if self.history:
            last = self.history[-1]
            print(f&quot;Predicted bandgap: {last['bandgap']:.2f} eV&quot;)
            print(f&quot;Target: {self.target_bandgap:.2f} eV&quot;)
            print(f&quot;Reward: {last['reward']:.2f}&quot;)


# Test environment
env = BandgapDiscoveryEnv(target_bandgap=3.0)

state = env.reset()
print(f&quot;Initial state: {state.shape}&quot;)

for step in range(10):
    action = env.action_space.sample()
    state, reward, done, info = env.step(action)

    print(f&quot;\nStep {step+1}:&quot;)
    print(f&quot;  Composition: {info['composition']}&quot;)
    print(f&quot;  Predicted band gap: {info['predicted_bandgap']:.2f} eV&quot;)
    print(f&quot;  Reward: {reward:.2f}&quot;)

    if done:
        print(&quot;Target reached!&quot;)
        break
</code></pre>
<p><strong>Sample Output</strong>:</p>
<pre><code>Initial state: (132,)

Step 1:
  Composition: TiO
  Predicted band gap: 3.12 eV
  Reward: -0.12

Step 2:
  Composition: ZrO
  Predicted band gap: 2.95 eV
  Reward: -0.05
Target reached!
</code></pre>
<hr />
<h2>3.3 Designing Effective Reward Functions</h2>
<h3>Principles of Reward Design</h3>
<p>The reward function <strong>defines what the agent should optimize</strong>. Poorly designed rewards can lead to undesired behaviors or learning failures.</p>
<h4>Principle 1: Clear Objectives</h4>
<p><strong>Bad Example</strong>:</p>
<pre><code class="language-python"># Ambiguous reward
reward = 1 if 'good_material' else 0  # &quot;good&quot; is undefined
</code></pre>
<p><strong>Good Example</strong>:</p>
<pre><code class="language-python"># Clear objective (band gap)
target = 3.0
predicted = 2.8
reward = -abs(predicted - target)  # Distance to target
</code></pre>
<h4>Principle 2: Scaling</h4>
<p>Set appropriate reward ranges:</p>
<p><strong>Bad Example</strong>:</p>
<pre><code class="language-python"># Extremely large rewards
reward = 1e10 if success else -1e10  # Learning becomes unstable
</code></pre>
<p><strong>Good Example</strong>:</p>
<pre><code class="language-python"># Normalize to approximately [-1, 1]
reward = -error / max_error  # error ‚àà [0, max_error]
</code></pre>
<h4>Principle 3: Reward Shaping (Intermediate Rewards)</h4>
<p>Transform sparse rewards into dense rewards:</p>
<p><strong>Sparse Reward (difficult to learn)</strong>:</p>
<pre><code class="language-python">reward = 1.0 if distance &lt; 0.1 else 0.0
</code></pre>
<p><strong>Dense Reward (easier to learn)</strong>:</p>
<pre><code class="language-python"># Continuous reward based on distance
reward = -distance

# Further hierarchical reward
if distance &lt; 0.5:
    reward += 5.0  # Close
if distance &lt; 0.1:
    reward += 10.0  # Very close
</code></pre>
<h4>Principle 4: Multi-Objective Optimization</h4>
<p>Weight multiple objectives:</p>
<pre><code class="language-python"># Optimize both band gap and stability
bandgap_error = abs(predicted_bandgap - target_bandgap)
stability = formation_energy  # Negative values are stable

# Weighted reward
w1, w2 = 0.7, 0.3
reward = -w1 * bandgap_error - w2 * max(0, stability)
</code></pre>
<h3>Reward Design Examples</h3>
<h4>Example 1: Maximizing Catalytic Activity</h4>
<pre><code class="language-python">class CatalystOptimizationEnv(gym.Env):
    &quot;&quot;&quot;Environment for maximizing catalytic activity&quot;&quot;&quot;

    def _compute_reward(self, activity, selectivity, stability):
        &quot;&quot;&quot;Multi-objective reward

        Args:
            activity: Catalytic activity (higher is better)
            selectivity: Selectivity to target product (higher is better)
            stability: Stability (negative formation energy, lower is more stable)

        Returns:
            Total reward
        &quot;&quot;&quot;
        # Normalize each metric to [0, 1]
        activity_norm = activity / 100.0  # Assume max 100
        selectivity_norm = selectivity  # Already [0, 1]
        stability_norm = -stability / 5.0  # Assume max -5 eV

        # Weighted sum (emphasize activity)
        weights = {'activity': 0.5, 'selectivity': 0.3, 'stability': 0.2}
        reward = (weights['activity'] * activity_norm +
                  weights['selectivity'] * selectivity_norm +
                  weights['stability'] * stability_norm)

        # Penalty: Unstable materials
        if stability &gt; 0:  # Positive formation energy (unstable)
            reward -= 1.0

        return reward
</code></pre>
<h4>Example 2: Synthesis Cost Constraint</h4>
<pre><code class="language-python">def reward_with_cost_constraint(self, performance, synthesis_cost, max_cost=1000):
    &quot;&quot;&quot;Reward with cost constraint

    Args:
        performance: Material performance
        synthesis_cost: Synthesis cost [USD/kg]
        max_cost: Cost limit

    Returns:
        Reward
    &quot;&quot;&quot;
    # Base reward from performance
    base_reward = performance

    # Penalty for cost constraint violation
    if synthesis_cost &gt; max_cost:
        penalty = (synthesis_cost - max_cost) / max_cost
        base_reward -= 10.0 * penalty

    # Bonus for lower cost
    cost_bonus = max(0, (max_cost - synthesis_cost) / max_cost)
    base_reward += 2.0 * cost_bonus

    return base_reward
</code></pre>
<hr />
<h2>3.4 Integration with DFT Calculations</h2>
<h3>Retrieving Data from Materials Project</h3>
<p>Obtain actual material properties for use in rewards:</p>
<pre><code class="language-python">from mp_api.client import MPRester
import os

class MPIntegratedEnv(gym.Env):
    &quot;&quot;&quot;Materials Project integrated environment&quot;&quot;&quot;

    def __init__(self, mp_api_key=None):
        super(MPIntegratedEnv, self).__init__()

        # Materials Project API key
        if mp_api_key is None:
            mp_api_key = os.getenv(&quot;MP_API_KEY&quot;)

        self.mpr = MPRester(mp_api_key)

        # ... (environment setup) ...

    def _get_bandgap_from_mp(self, composition):
        &quot;&quot;&quot;Retrieve band gap from Materials Project

        Args:
            composition: Composition (e.g., &quot;TiO2&quot;)

        Returns:
            Band gap [eV] (None if no data available)
        &quot;&quot;&quot;
        try:
            # Search by composition
            docs = self.mpr.materials.summary.search(
                formula=composition,
                fields=[&quot;material_id&quot;, &quot;band_gap&quot;, &quot;formation_energy_per_atom&quot;]
            )

            if docs:
                # Select most stable structure (minimum formation energy)
                stable_doc = min(docs, key=lambda x: x.formation_energy_per_atom)
                return stable_doc.band_gap
            else:
                return None

        except Exception as e:
            print(f&quot;Materials Project search error: {e}&quot;)
            return None

    def step(self, action):
        composition = self._action_to_composition(action)

        # Retrieve data from Materials Project
        bandgap = self._get_bandgap_from_mp(composition)

        if bandgap is not None:
            # Calculate reward with real data
            error = abs(bandgap - self.target_bandgap)
            reward = -error
        else:
            # Use prediction model or penalty if no data available
            reward = -10.0  # Penalty for unknown material

        # ... (state, termination condition, etc.) ...

        return state, reward, done, info
</code></pre>
<p><strong>Note</strong>: Avoid excessive requests to Materials Project and utilize local caching.</p>
<h3>DFT Calculation Integration with ASE (Advanced)</h3>
<pre><code class="language-python">from ase import Atoms
from ase.calculators.vasp import Vasp
from ase.optimize import BFGS

class DFTIntegratedEnv(gym.Env):
    &quot;&quot;&quot;DFT calculation integrated environment (high computational cost)&quot;&quot;&quot;

    def _calculate_bandgap_dft(self, composition):
        &quot;&quot;&quot;Obtain band gap from DFT calculation

        Warning: Very time-consuming (hours to days per material)
        Practically, use pre-calculated database

        Args:
            composition: Composition

        Returns:
            Band gap [eV]
        &quot;&quot;&quot;
        # Generate crystal structure (using pymatgen, etc.)
        structure = self._generate_structure(composition)

        # Convert to ASE Atoms object
        atoms = Atoms(
            symbols=structure.species,
            positions=structure.cart_coords,
            cell=structure.lattice.matrix,
            pbc=True
        )

        # VASP calculation settings
        calc = Vasp(
            xc='PBE',
            encut=520,
            kpts=(4, 4, 4),
            ismear=0,
            sigma=0.05,
            directory='vasp_calc'
        )
        atoms.calc = calc

        # Structure optimization
        opt = BFGS(atoms)
        opt.run(fmax=0.05)

        # Band gap calculation
        # ... (Parse VASP OUTCAR) ...

        return bandgap

    def step(self, action):
        # DFT calculations are time-consuming,
        # so in practice the following approaches are needed:
        # 1. Build pre-calculated database
        # 2. Use surrogate model for fast prediction
        # 3. DFT calculation only for important materials via active learning
        pass
</code></pre>
<p><strong>Practical Approach</strong>:
1. <strong>Pre-training</strong>: Train surrogate model with Materials Project data
2. <strong>Reinforcement Learning</strong>: Fast exploration with surrogate model
3. <strong>Validation</strong>: Precise evaluation via DFT calculation only for promising materials</p>
<hr />
<h2>3.5 Integration with Experimental Equipment (Closed-Loop)</h2>
<h3>Automated Experimental Equipment Control via REST API</h3>
<pre><code class="language-python">import requests

class RoboticLabEnv(gym.Env):
    &quot;&quot;&quot;Robotic laboratory equipment integrated environment&quot;&quot;&quot;

    def __init__(self, api_endpoint=&quot;http://lab-robot.example.com/api&quot;):
        super(RoboticLabEnv, self).__init__()
        self.api_endpoint = api_endpoint

        # ... (environment setup) ...

    def _synthesize_and_measure(self, composition, temperature, time):
        &quot;&quot;&quot;Synthesize material and measure properties

        Args:
            composition: Composition
            temperature: Synthesis temperature [K]
            time: Synthesis time [min]

        Returns:
            Measurement results (band gap, XRD pattern, etc.)
        &quot;&quot;&quot;
        # Request synthesis from robot
        payload = {
            'composition': composition,
            'temperature': temperature,
            'time': time,
            'measurement': ['bandgap', 'xrd']
        }

        response = requests.post(
            f&quot;{self.api_endpoint}/synthesize&quot;,
            json=payload,
            headers={'Authorization': 'Bearer YOUR_API_KEY'}
        )

        if response.status_code == 200:
            result = response.json()
            return result['bandgap'], result['xrd_pattern']
        else:
            raise Exception(f&quot;Experiment failed: {response.text}&quot;)

    def step(self, action):
        &quot;&quot;&quot;Action = synthesis conditions&quot;&quot;&quot;
        composition, temperature, time = self._decode_action(action)

        # Execute experiment (minutes to hours)
        bandgap, xrd = self._synthesize_and_measure(composition, temperature, time)

        # Calculate reward
        reward = -abs(bandgap - self.target_bandgap)

        # Update state (including experiment history)
        state = self._update_state(composition, temperature, time, bandgap, xrd)

        done = len(self.history) &gt;= self.max_experiments

        return state, reward, done, {'bandgap': bandgap}
</code></pre>
<p><strong>Challenges</strong>:
- <strong>Experimental Cost</strong>: Several thousand to tens of thousands of dollars per experiment
- <strong>Time</strong>: Hours to days for synthesis and measurement
- <strong>Safety</strong>: Robot malfunctions, handling hazardous materials</p>
<p><strong>Solutions</strong>:
- <strong>Simulation First</strong>: Pre-exploration with surrogate model
- <strong>Combined with Bayesian Optimization</strong>: Efficient experimental point selection
- <strong>Batch Experiments</strong>: Synthesize multiple materials in parallel</p>
<hr />
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Explain the difference between the following two reward functions and indicate which is easier to learn with reasoning.</p>
<p><strong>Reward A</strong>:</p>
<pre><code class="language-python">reward = 10.0 if abs(bandgap - 3.0) &lt; 0.1 else 0.0
</code></pre>
<p><strong>Reward B</strong>:</p>
<pre><code class="language-python">reward = -abs(bandgap - 3.0)
</code></pre>
<details>
<summary>Hint</summary>

Reward A is sparse, while Reward B is dense. Consider the frequency of learning signals.

</details>

<details>
<summary>Solution</summary>

**Characteristics of Reward A**:
- **Sparse Reward**: Reward of 10.0 only when band gap is in range 2.9-3.1 eV, otherwise 0.0
- **Difficult to Learn**: Reward is 0 for most exploration, unclear which direction to proceed
- **Inefficient Exploration**: Becomes close to random search

**Characteristics of Reward B**:
- **Dense Reward**: Reward obtained for all actions (distance to target)
- **Easy to Learn**: Clear gradient as reward improves when approaching target
- **Efficient Exploration**: Can learn from reward changes

**Conclusion**: **Reward B is easier to learn**

However, Reward B has the drawback of being prone to local optima. In practice, a hybrid design based on Reward B with bonus elements like Reward A is effective.


<pre><code class="language-python"># Hybrid reward
reward = -abs(bandgap - 3.0)  # Dense reward
if abs(bandgap - 3.0) &lt; 0.1:
    reward += 10.0  # Bonus (sparse reward element)
</code></pre>


</details>

<hr />
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>For materials discovery, compare the following three state representations and describe the advantages and disadvantages of each.</p>
<ol>
<li><strong>Composition Only</strong>: <code>["Li2MnO3"]</code> (string)</li>
<li><strong>Element Fractions</strong>: <code>[0.33, 0.17, 0.50]</code> (Li, Mn, O fractions)</li>
<li><strong>Magpie Descriptors</strong>: 132-dimensional vector (average atomic number, electronegativity, etc.)</li>
</ol>
<details>
<summary>Hint</summary>

Neural networks require numerical inputs. Also consider the relationship between descriptor dimensionality and learning complexity.

</details>

<details>
<summary>Solution</summary>

**1. Advantages and Disadvantages of Composition Strings**:

**Advantages**:
- Human-readable
- Can be used directly for database searches

**Disadvantages**:
- Cannot be directly input to neural networks (requires numerical conversion)
- Difficult to capture relationships between similar compositions (hard to learn that "TiO2" and "ZrO2" are similar)

**2. Advantages and Disadvantages of Element Fractions**:

**Advantages**:
- Numerical vector, can be input to NNs
- Low-dimensional (3D, etc.) and easy to handle

**Disadvantages**:
- Does not reflect chemical properties of elements (cannot express that Ti and Zr are similar)
- Element order is arbitrary ([Li, Mn, O] and [O, Mn, Li] become different vectors)

**3. Advantages and Disadvantages of Magpie Descriptors**:

**Advantages**:
- Reflects chemical properties of elements (electronegativity, ionic radius, etc.)
- Similar compositions become similar vectors
- High predictive performance in machine learning

**Disadvantages**:
- High-dimensional (132D) and complex learning
- Low interpretability (not intuitive what each dimension represents)

**Recommendations**:
- **Initial Exploration**: Magpie descriptors (high versatility)
- **Specific Tasks**: Task-specific descriptors (e.g., d-orbital occupancy for catalysis)
- **Hybrid**: Composition + process parameters

</details>

<hr />
<h3>Problem 3 (Difficulty: Hard)</h3>
<p>For the band gap discovery environment, implement the following improvements:</p>
<ol>
<li><strong>History-Aware State</strong>: Include information about previously tested materials in the state</li>
<li><strong>Exploration Bonus</strong>: Additional reward for exploring unknown regions</li>
<li><strong>Early Termination</strong>: End episode if no improvement for 10 consecutive steps</li>
</ol>
<details>
<summary>Hint</summary>

Store history in dictionary format and add "distance to best material" to the state. Exploration bonus can be calculated based on similarity to past materials.

</details>

<details>
<summary>Solution</summary>


<pre><code class="language-python">import numpy as np
from scipy.spatial.distance import euclidean

class ImprovedBandgapEnv(gym.Env):
    &quot;&quot;&quot;Improved band gap discovery environment&quot;&quot;&quot;

    def __init__(self, target_bandgap=3.0):
        super(ImprovedBandgapEnv, self).__init__()

        self.target_bandgap = target_bandgap

        # Action and state spaces (simplified)
        self.action_space = gym.spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = gym.spaces.Box(low=-10, high=10, shape=(15,), dtype=np.float32)

        # History
        self.history = []
        self.best_error = float('inf')
        self.no_improvement_count = 0

    def reset(self):
        self.history = []
        self.best_error = float('inf')
        self.no_improvement_count = 0

        initial_state = self._get_state(np.random.uniform(0, 1, 10))
        return initial_state

    def step(self, action):
        # Predict band gap (simplified model)
        predicted_bandgap = np.sum(action) * 3.0  # Hypothetical prediction

        # Error
        error = abs(predicted_bandgap - self.target_bandgap)

        # Base reward
        reward = -error

        # Improvement 1: History-aware state
        state = self._get_state(action)

        # Improvement 2: Exploration bonus
        exploration_bonus = self._compute_exploration_bonus(action)
        reward += 0.1 * exploration_bonus

        # Improvement 3: Early termination
        if error &lt; self.best_error:
            self.best_error = error
            self.no_improvement_count = 0
        else:
            self.no_improvement_count += 1

        done = error &lt; 0.05 or self.no_improvement_count &gt;= 10 or len(self.history) &gt;= 100

        # Add to history
        self.history.append({
            'action': action,
            'bandgap': predicted_bandgap,
            'error': error
        })

        info = {'bandgap': predicted_bandgap, 'exploration_bonus': exploration_bonus}

        return state, reward, done, info

    def _get_state(self, action):
        &quot;&quot;&quot;History-aware state

        State composition:
        - Current action (10D)
        - Distance to best material (1D)
        - History size (1D)
        - Consecutive no-improvement count (1D)
        - Average error (1D)
        - Best error (1D)
        &quot;&quot;&quot;
        state = np.zeros(15, dtype=np.float32)

        # Current action
        state[:10] = action

        # Distance to best material
        if self.history:
            best_action = min(self.history, key=lambda x: x['error'])['action']
            state[10] = euclidean(action, best_action) / 10.0  # Normalized
        else:
            state[10] = 1.0

        # History size
        state[11] = len(self.history) / 100.0  # Normalized

        # Consecutive no-improvement count
        state[12] = self.no_improvement_count / 10.0

        # Average error
        if self.history:
            state[13] = np.mean([h['error'] for h in self.history])
        else:
            state[13] = 10.0

        # Best error
        state[14] = self.best_error

        return state

    def _compute_exploration_bonus(self, action):
        &quot;&quot;&quot;Exploration bonus

        Higher bonus for actions farther from past actions
        &quot;&quot;&quot;
        if not self.history:
            return 1.0  # Always explore first

        # Minimum distance to past actions
        min_distance = min(
            euclidean(action, h['action'])
            for h in self.history
        )

        # Higher bonus for larger distance (max 1.0)
        bonus = min(1.0, min_distance / 5.0)

        return bonus


# Test
env = ImprovedBandgapEnv()
state = env.reset()

for step in range(50):
    action = env.action_space.sample()
    state, reward, done, info = env.step(action)

    print(f&quot;Step {step+1}: Bandgap={info['bandgap']:.2f}, &quot;
          f&quot;Reward={reward:.2f}, Exploration={info['exploration_bonus']:.2f}&quot;)

    if done:
        print(f&quot;Terminated: Best error={env.best_error:.4f}, &quot;
              f&quot;Consecutive no-improvement={env.no_improvement_count}&quot;)
        break
</code></pre>


**Key Points**:
- Including history information in state allows agent to leverage past experience
- Exploration bonus encourages exploring unknown regions
- Early termination reduces wasted exploration

</details>

<hr />
<h2>Summary of This Section</h2>
<ul>
<li><strong>OpenAI Gym</strong> is the standard interface for reinforcement learning environments</li>
<li><strong>State Space</strong> is designed with material descriptors (composition, structure, process parameters)</li>
<li><strong>Reward Function</strong> requires clear objectives, appropriate scaling, and intermediate rewards</li>
<li><strong>DFT Integration</strong> accelerates with surrogate models, precise calculation only for important materials</li>
<li><strong>Experimental Equipment Integration</strong> achieves closed-loop optimization via REST API</li>
</ul>
<p>In the next chapter, we will learn about real-world application cases including chemical process control and synthesis route design.</p>
<hr />
<h2>References</h2>
<ol>
<li>Brockman et al. "OpenAI Gym" <em>arXiv</em> (2016) - Gym environment standard</li>
<li>Ward et al. "A general-purpose machine learning framework for predicting properties of inorganic materials" <em>npj Computational Materials</em> (2016) - Magpie descriptors</li>
<li>Brockherde et al. "Bypassing the Kohn-Sham equations with machine learning" <em>Nature Communications</em> (2017) - DFT acceleration</li>
<li>Ng et al. "Policy invariance under reward transformations" <em>ICML</em> (1999) - Reward shaping theory</li>
</ol>
<hr />
<p><strong>Next Chapter</strong>: <a href="chapter-4.html">Chapter 4: Real-World Applications and Closed-Loop Systems</a></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Back to Series Index</a>
    <a href="chapter-4.html" class="nav-button">Next Chapter ‚Üí</a>
</div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The authors and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>To the maximum extent permitted by applicable law, the authors and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to stated conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
        </ul>
    </section>

<footer>
        <p><strong>Authors</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 2.0 | <strong>Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
