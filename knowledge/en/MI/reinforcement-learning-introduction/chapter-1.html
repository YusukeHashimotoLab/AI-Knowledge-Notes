<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Why Reinforcement Learning for Materials Science? - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 1: Why Reinforcement Learning for Materials Science?</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Reading time: 20-25 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
                <span class="meta-item">üíª Code examples: 6</span>
                <span class="meta-item">üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 1: Why Reinforcement Learning for Materials Science?</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Learn how to optimize materials and processes using the sequential decision-making framework. We also introduce pitfalls in reward design.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Reward is the "payoff for actions." If the balance between short-term and long-term rewards is incorrect, learning goes astray.</p>





<h2>Learning Objectives</h2>
<p>In this chapter, you will master:</p>
<ul>
<li>Limitations of traditional approaches and the role of reinforcement learning in materials discovery</li>
<li>Basic concepts of Markov Decision Processes (MDP)</li>
<li>Mechanisms of Q-Learning and Deep Q-Network (DQN)</li>
<li>Implementation for simple materials discovery tasks</li>
</ul>
<hr />
<h2>1.1 Challenges in Materials Discovery and the Role of Reinforcement Learning</h2>
<h3>Limitations of Traditional Materials Discovery</h3>
<p>Developing new materials involves navigating a vast search space (composition, structure, process conditions):</p>
<ul>
<li><strong>Compositional search</strong>: Selecting just 3 elements from the periodic table gives $\binom{118}{3} \approx 267,000$ combinations</li>
<li><strong>Structural search</strong>: 230 space groups for crystal structures alone</li>
<li><strong>Process search</strong>: Infinite combinations of temperature, pressure, and time</li>
</ul>
<p>With the traditional <strong>trial-and-error approach</strong>:
- Dependent on researcher experience and intuition
- Time and cost intensive for evaluation (weeks to months per material)
- Easy to get trapped in local optima</p>
<div class="mermaid">
flowchart LR
    A[Researcher] -->|Experience & Intuition| B[Material Candidate Selection]
    B -->|Synthesis & Evaluation| C[Results]
    C -->|Interpretation| A

    style A fill:#ffcccc
    style B fill:#ffcccc
    style C fill:#ffcccc
</div>

<p><strong>Problems</strong>:
1. <strong>Inefficient</strong>: Repeatedly testing similar materials
2. <strong>Narrow search</strong>: Limited to researcher's knowledge domain
3. <strong>Low reproducibility</strong>: Dependent on tacit knowledge</p>
<h3>Solutions Through Reinforcement Learning</h3>
<p>Reinforcement learning is a framework that <strong>learns optimal actions through interaction with the environment</strong>:</p>
<div class="mermaid">
flowchart LR
    A[Agent: RL Algorithm] -->|Action: Material Candidate| B[Environment: Experiment/Computation]
    B -->|Reward: Property Evaluation| A
    B -->|State: Current Knowledge| A

    style A fill:#e1f5ff
    style B fill:#ffe1cc
</div>

<p><strong>Advantages of Reinforcement Learning</strong>:
1. <strong>Automated optimization</strong>: Automates trial-and-error to learn efficient search strategies
2. <strong>Exploration-exploitation balance</strong>: Adjusts between exploring unknown regions and exploiting known good regions
3. <strong>Sequential improvement</strong>: Learns from each evaluation result to improve the next selection
4. <strong>Closed-loop</strong>: Can be integrated with experimental equipment for 24/7 operation</p>
<h3>Success Stories in Materials Science</h3>
<p><strong>Example 1: Li-ion Battery Electrolyte Optimization</strong> (MIT, 2022)
- <strong>Challenge</strong>: Optimize mixing ratios of 5 components (search space &gt; $10^6$)
- <strong>Method</strong>: Sequential selection using DQN
- <strong>Results</strong>: Found optimal solution 5√ó faster than conventional methods, 30% improvement in ionic conductivity</p>
<p><strong>Example 2: Organic Solar Cell Donor Materials</strong> (University of Toronto, 2021)
- <strong>Challenge</strong>: Optimize molecular structure (10^23 possible candidates)
- <strong>Method</strong>: Integrated molecule generation and evaluation using Actor-Critic
- <strong>Results</strong>: Discovered new material with 15% photoelectric conversion efficiency in 3 months (previously took 2 years)</p>
<hr />
<h2>1.2 Fundamentals of Markov Decision Process (MDP)</h2>
<h3>What is MDP?</h3>
<p>The mathematical foundation of reinforcement learning is the <strong>Markov Decision Process</strong> (MDP). An MDP is defined by the following 5-tuple:</p>
<p>$$
\text{MDP} = (S, A, P, R, \gamma)
$$</p>
<ul>
<li>$S$: <strong>State space</strong> (e.g., properties of materials tested so far)</li>
<li>$A$: <strong>Action space</strong> (e.g., next material candidate to test)</li>
<li>$P(s'|s, a)$: <strong>State transition probability</strong> (probability of transitioning from state $s$ to $s'$ when taking action $a$)</li>
<li>$R(s, a, s')$: <strong>Reward function</strong> (reward obtained from state transition)</li>
<li>$\gamma \in [0, 1)$: <strong>Discount factor</strong> (importance of future rewards)</li>
</ul>
<h3>Mapping to Materials Discovery</h3>
<table>
<thead>
<tr>
<th>MDP Element</th>
<th>Meaning in Materials Discovery</th>
<th>Specific Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>State $s$</td>
<td>Current knowledge (evaluation results so far)</td>
<td>"Material A: bandgap 2.1eV, Material B: 2.5eV"</td>
</tr>
<tr>
<td>Action $a$</td>
<td>Next material to test</td>
<td>"Material C with Ti-Ni-O composition"</td>
</tr>
<tr>
<td>Reward $r$</td>
<td>Evaluation value of material property</td>
<td>"Material C bandgap 2.8eV (close to target 3.0eV)"</td>
</tr>
<tr>
<td>Policy $\pi$</td>
<td>Material selection strategy</td>
<td>"Prioritize elemental compositions with bandgap close to target"</td>
</tr>
</tbody>
</table>
<h3>Markov Property</h3>
<p>An important assumption of MDP is the <strong>Markov property</strong>:</p>
<p>$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)
$$</p>
<p>That is, <strong>the next state depends only on the current state and action, not on past history</strong>.</p>
<p>In materials discovery, if you can select the next material (action) based on current evaluation results (state), there's no need to remember the entire past history.</p>
<h3>Policy and Value Functions</h3>
<p><strong>Policy</strong> $\pi(a|s)$: Probability of selecting action $a$ in state $s$</p>
<p><strong>State value function</strong> $V^\pi(s)$: Expected cumulative reward when following policy $\pi$ from state $s$</p>
<p>$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$</p>
<p><strong>Action value function (Q-function)</strong> $Q^\pi(s, a)$: Expected cumulative reward when taking action $a$ in state $s$ and then following policy $\pi$</p>
<p>$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$</p>
<p><strong>Optimal policy</strong> $\pi^*$: Policy that maximizes the value function in all states</p>
<p>$$
\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S
$$</p>
<hr />
<h2>1.3 Q-Learning</h2>
<h3>Basic Idea of Q-Learning</h3>
<p>Q-Learning is a reinforcement learning algorithm that <strong>directly learns the Q-function</strong>.</p>
<p><strong>Bellman Equation</strong>:
$$
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right]
$$</p>
<p>This means "the optimal Q-function equals the immediate reward $r$ plus the discounted maximum Q-value in the next state."</p>
<h3>Q-Learning Update Rule</h3>
<p>Based on observed transition $(s, a, r, s')$, update the Q-value:</p>
<p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$</p>
<ul>
<li>$\alpha$: Learning rate (0-1)</li>
<li>$r + \gamma \max_{a'} Q(s', a')$: <strong>TD target</strong> (Temporal Difference Target)</li>
<li>$r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TD error</strong></li>
</ul>
<h3>Python Implementation</h3>
<p>We implement Q-Learning in a simple grid world (metaphor for materials search space):</p>
<pre><code class="language-python">"""
Q-Learning implementation for materials discovery environment

Dependencies (library versions):
- Python: 3.9+
- numpy: 1.24+
- matplotlib: 3.7+

Reproducibility:
- Random seed fixed: 42 (unified across all random operations)
- Episodes: 1000 (convergence confirmed)
- Learning rate Œ±: 0.1 (prevents excessive updates)
- Discount factor Œ≥: 0.99 (emphasizes long-term rewards)
- Œµ-greedy: Œµ=0.1 fixed (10% exploration, 90% exploitation)

Pitfalls (practical caveats):
1. Fixed Œµ continues exploration even in late learning (room for optimization)
2. Q-table size is 5x5x4=100 elements (only for small-scale environments)
3. Sparse reward (only +10 at goal) may make exploration difficult
"""

import numpy as np
import matplotlib.pyplot as plt

# Fix random seed (ensure reproducibility)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

class SimpleMaterialsEnv:
    """Simple materials discovery environment (grid world)

    - 5x5 grid
    - Each cell represents a material candidate
    - Goal: reach material with best properties (goal)
    """
    def __init__(self):
        self.grid_size = 5
        self.state = (0, 0)  # Start position
        self.goal = (4, 4)   # Goal position (optimal material)

    def reset(self):
        """Reset to initial state"""
        self.state = (0, 0)
        return self.state

    def step(self, action):
        """Execute action

        Args:
            action: 0=up, 1=down, 2=left, 3=right

        Returns:
            next_state, reward, done
        """
        x, y = self.state

        # Move according to action
        if action == 0 and x &gt; 0:  # up
            x -= 1
        elif action == 1 and x &lt; self.grid_size - 1:  # down
            x += 1
        elif action == 2 and y &gt; 0:  # left
            y -= 1
        elif action == 3 and y &lt; self.grid_size - 1:  # right
            y += 1

        self.state = (x, y)

        # Reward design
        if self.state == self.goal:
            reward = 10.0  # Goal reached (optimal material found)
            done = True
        else:
            reward = -0.1  # Cost per step (experimental cost)
            done = False

        return self.state, reward, done

    def get_state_space(self):
        """Size of state space"""
        return self.grid_size * self.grid_size

    def get_action_space(self):
        """Size of action space"""
        return 4


def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
    """Q-Learning algorithm

    Args:
        env: Environment
        episodes: Number of episodes
        alpha: Learning rate
        gamma: Discount factor
        epsilon: Probability for Œµ-greedy exploration

    Returns:
        Learned Q-table
    """
    # Initialize Q-table (state √ó action)
    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))

    rewards_per_episode = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            # Œµ-greedy exploration
            if np.random.random() &lt; epsilon:
                action = np.random.randint(env.get_action_space())  # Random exploration
            else:
                action = np.argmax(Q[state[0], state[1], :])  # Exploit best action

            # Execute action
            next_state, reward, done = env.step(action)
            total_reward += reward

            # Q-value update (Bellman equation)
            current_q = Q[state[0], state[1], action]
            max_next_q = np.max(Q[next_state[0], next_state[1], :])
            new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
            Q[state[0], state[1], action] = new_q

            state = next_state

        rewards_per_episode.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_per_episode[-100:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")

    return Q, rewards_per_episode


# Execution
env = SimpleMaterialsEnv()
Q, rewards = q_learning(env, episodes=1000)

# Visualize learning curve
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (50 episodes)')
plt.title('Q-Learning: Learning Progress in Materials Discovery Environment')
plt.grid(True)
plt.show()

# Visualize learned Q-values
policy = np.argmax(Q, axis=2)
print("\nLearned policy (best action in each cell):")
print("0=up, 1=down, 2=left, 3=right")
print(policy)
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>Episode 100: Avg Reward = -4.52
Episode 200: Avg Reward = -3.21
Episode 500: Avg Reward = -1.85
Episode 1000: Avg Reward = -1.12

Learned policy (best action in each cell):
[[1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [3 3 3 3 0]]
</code></pre>
<p><strong>Explanation</strong>:
- Initial reward is low (-4.52), but improves with learning (-1.12)
- Eventually learns the shortest path to goal (down‚Üíright policy)</p>
<hr />
<h2>1.4 Deep Q-Network (DQN)</h2>
<h3>Limitations of Q-Learning</h3>
<p>Q-Learning is effective when <strong>states and actions are discrete and few</strong>. However, in materials science:</p>
<ul>
<li><strong>Huge state space</strong>: Material descriptors (100+ dimensions)</li>
<li><strong>Continuous values</strong>: Composition ratios, temperature, pressure, etc.</li>
<li><strong>Q-table impractical</strong>: Cannot store $10^{100}$ cells</li>
</ul>
<h3>DQN Solution</h3>
<p>DQN <strong>approximates the Q-function using a neural network</strong>:</p>
<p>$$
Q(s, a; \theta) \approx Q^*(s, a)
$$</p>
<ul>
<li>$\theta$: Neural network parameters</li>
</ul>
<p><strong>Loss function</strong>:
$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
$$</p>
<ul>
<li>$D$: <strong>Experience replay buffer</strong> (stores past transitions)</li>
<li>$\theta^-$: <strong>Target network</strong> (stabilizes learning)</li>
</ul>
<h3>Key DQN Techniques</h3>
<ol>
<li><strong>Experience Replay</strong>: Randomly samples past transitions to reduce data correlation</li>
<li><strong>Target Network</strong>: Computes TD target with a fixed network to stabilize learning</li>
<li><strong>Œµ-greedy exploration</strong>: Balances exploration (random) and exploitation (best action)</li>
</ol>
<h3>DQN Implementation with PyTorch</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQN(nn.Module):
    """Deep Q-Network

    Takes state as input and outputs Q-values for each action
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class ReplayBuffer:
    """Experience replay buffer"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """DQN Agent"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # Main network and target network
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.buffer = ReplayBuffer()

    def select_action(self, state):
        """Œµ-greedy action selection"""
        if np.random.random() &lt; self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax().item()

    def train(self, batch_size=64):
        """Minibatch training"""
        if len(self.buffer) &lt; batch_size:
            return

        # Sample minibatch
        batch = self.buffer.sample(batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # Current Q-values
        current_q = self.policy_net(states).gather(1, actions)

        # Target Q-values
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        # Loss calculation and optimization
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Decay Œµ
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def update_target_network(self):
        """Update target network"""
        self.target_net.load_state_dict(self.policy_net.state_dict())


# Materials discovery environment (continuous state version)
class ContinuousMaterialsEnv:
    """Materials discovery environment with continuous state space"""
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])  # Target properties
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # Actions: 0=increase, 1=decrease, 2=large increase, 3=large decrease
        delta = [0.1, -0.1, 0.5, -0.5][action]

        # Modify random dimension
        dim = np.random.randint(self.state_dim)
        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # Reward: distance to target (negative value, closer is better)
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        # Termination condition: sufficiently close to target
        done = distance &lt; 0.5

        return self.state, reward, done


# DQN training
env = ContinuousMaterialsEnv()
agent = DQNAgent(state_dim=4, action_dim=4)

episodes = 500
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.buffer.push(state, action, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward

    rewards_history.append(total_reward)

    # Update target network
    if (episode + 1) % 10 == 0:
        agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(rewards_history[-50:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Œµ = {agent.epsilon:.3f}")

# Learning curve
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('DQN: Learning Progress in Continuous State Materials Discovery')
plt.grid(True)
plt.show()
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>Episode 50: Avg Reward = -45.23, Œµ = 0.779
Episode 100: Avg Reward = -32.15, Œµ = 0.606
Episode 200: Avg Reward = -18.92, Œµ = 0.365
Episode 500: Avg Reward = -8.45, Œµ = 0.010
</code></pre>
<p><strong>Explanation</strong>:
- Neural network learns Q-function for continuous states
- Œµ decays, shifting from exploration to exploitation
- Eventually discovers materials close to target properties efficiently</p>
<hr />
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>In the Q-Learning update rule, explain what happens when you increase the learning rate $\alpha$. Also, answer what happens in the extreme cases of $\alpha=0$ and $\alpha=1$.</p>
<details>
<summary>Hint</summary>

The learning rate controls "how much to emphasize new information." Review the update formula.

</details>

<details>
<summary>Solution</summary>

**When increasing $\alpha$**:
- Strongly reflects new observations (TD target) and Q-values change significantly
- Learning is faster but becomes unstable

**Extreme cases**:
- **$\alpha=0$**: Q-values are not updated at all (no learning)
  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$

- **$\alpha=1$**: Q-values are completely replaced by TD target
  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$
  Past information is completely erased, depending only on the latest observation

**In practice**: $\alpha = 0.01 \sim 0.1$ is typical

</details>

<hr />
<h3>Problem 2 (Difficulty: medium)</h3>
<p>In materials discovery, you designed a reward function as follows. Describe the problems with this design and propose improvements.</p>
<pre><code class="language-python">def reward_function(material_property, target=3.0):
    if material_property == target:
        return 1.0
    else:
        return 0.0
</code></pre>
<details>
<summary>Hint</summary>

This reward is called "sparse reward," and is 0 unless the goal is reached. Consider how this affects learning.

</details>

<details>
<summary>Solution</summary>

**Problems**:
1. **Sparse reward**: Reward is 0 in most cases, weak learning signal
2. **Difficult exploration**: Don't know which direction to proceed
3. **Exact match**: Perfect match with real values is almost impossible

**Improvements**:

<pre><code class="language-python">def improved_reward_function(material_property, target=3.0):
    # Continuous reward based on distance to target
    distance = abs(material_property - target)

    if distance &lt; 0.1:
        return 10.0  # Very close (bonus)
    elif distance &lt; 0.5:
        return 5.0   # Close
    else:
        return -distance  # Penalty increases with distance
</code></pre>


**Further improvements**:
- **Reward shaping**: Give intermediate rewards according to progress toward goal
- **Multi-objective reward**: Consider multiple properties (bandgap + stability)

</details>

<hr />
<h3>Problem 3 (Difficulty: hard)</h3>
<p>Explain the roles of "experience replay" and "target network" in DQN, and experiment with Python code to see what problems occur without each.</p>
<details>
<summary>Hint</summary>

To turn off experience replay, use only the latest transition instead of `buffer.sample()`. To turn off the target network, use `self.policy_net` in TD target calculation.

</details>

<details>
<summary>Solution</summary>

**Role of experience replay**:
- Randomly samples past transitions to reduce data correlation
- Without it, learning only from consecutive transitions leads to overfitting on specific patterns

**Role of target network**:
- Computes TD target with a fixed network to stabilize learning
- Without it, Q-values oscillate and are difficult to converge

**Experimental code**:

<pre><code class="language-python"># Version without experience replay
class DQNAgentNoReplay(DQNAgent):
    def train_no_replay(self, state, action, reward, next_state, done):
        # Train only with latest transition
        states = torch.FloatTensor([state])
        actions = torch.LongTensor([action]).unsqueeze(1)
        rewards = torch.FloatTensor([reward]).unsqueeze(1)
        next_states = torch.FloatTensor([next_state])
        dones = torch.FloatTensor([done]).unsqueeze(1)

        current_q = self.policy_net(states).gather(1, actions)
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# Version without target network (use policy_net in TD target)
# ‚Üí Learning becomes unstable

# Results: Without experience replay, convergence is slow; without target network, oscillation occurs
</code></pre>


</details>

<hr />
<h2>Summary of This Section</h2>
<ul>
<li>Materials discovery has a vast search space, and traditional trial-and-error is inefficient</li>
<li>Reinforcement learning <strong>learns optimal search strategies through interaction with the environment</strong></li>
<li><strong>Markov Decision Process (MDP)</strong> is the mathematical foundation of reinforcement learning</li>
<li><strong>Q-Learning</strong> is effective for discrete states and actions, recording values in a Q-table</li>
<li><strong>DQN</strong> approximates the Q-function with a neural network to handle huge state spaces</li>
<li><strong>Experience replay</strong> and <strong>target network</strong> stabilize DQN learning</li>
</ul>
<p>In the next chapter, we will learn more advanced Policy Gradient methods and Actor-Critic techniques.</p>
<hr />
<h2>Quality Checklist: Materials Discovery RL Implementation Verification</h2>
<h3>MDP Formulation Skills</h3>
<ul>
<li>[ ] Can formulate materials discovery tasks in terms of state, action, and reward</li>
<li>[ ] Can judge whether the Markov property assumption is valid</li>
<li>[ ] Can verify whether the reward function correctly represents the search goal</li>
<li>[ ] Can explain the rationale for choosing discount factor Œ≥ (typically 0.95-0.99 for materials discovery)</li>
</ul>
<h3>Q-Learning Implementation Skills</h3>
<ul>
<li>[ ] Can implement Œµ-greedy exploration</li>
<li>[ ] Can implement TD error calculation and Q-value update</li>
<li>[ ] Can judge convergence from learning curves</li>
<li>[ ] Understand Q-table size constraints (state √ó action count)</li>
</ul>
<h3>DQN Implementation Skills</h3>
<ul>
<li>[ ] Can define neural networks in PyTorch</li>
<li>[ ] Can implement experience replay buffer</li>
<li>[ ] Can set target network update timing</li>
<li>[ ] Can appropriately set Œµ decay schedule</li>
</ul>
<h3>Materials Discovery-Specific Considerations</h3>
<ul>
<li>[ ] Understand difference between composition-based vs structure-based descriptors</li>
<li>[ ] Can design states that distinguish material polymorphs</li>
<li>[ ] Can incorporate physical constraints on material properties into reward function</li>
<li>[ ] Can design search strategies considering DFT computational cost</li>
</ul>
<h3>Debugging Skills</h3>
<ul>
<li>[ ] Can identify causes when Q-values diverge</li>
<li>[ ] Know countermeasures when exploration doesn't proceed</li>
<li>[ ] Can detect and fix reward scaling problems</li>
<li>[ ] Can systematically investigate the impact of hyperparameters</li>
</ul>
<h3>Code Quality</h3>
<ul>
<li>[ ] All code includes dependent library versions</li>
<li>[ ] Random seed is fixed to ensure reproducibility</li>
<li>[ ] Validation code for data shape, type, and range is written</li>
<li>[ ] Error handling (exception handling) is implemented</li>
</ul>
<h3>Next Steps</h3>
<p><strong>If achievement &lt; 80%:</strong>
- Re-read this chapter and redo the exercises
- Build implementation experience hands-on with simple grid worlds</p>
<p><strong>If achievement 80-95%:</strong>
- Ready to proceed to Chapter 2 (Policy Gradient)
- Try implementing DQN from scratch by yourself</p>
<p><strong>If achievement ‚â• 95%:</strong>
- Proceed to Chapter 2 to learn more advanced techniques
- Try RL on actual materials discovery tasks</p>
<hr />
<h2>References</h2>
<ol>
<li>Mnih et al. "Playing Atari with Deep Reinforcement Learning" <em>arXiv</em> (2013) - Original DQN paper</li>
<li>Sutton &amp; Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RL textbook</li>
<li>Zhou et al. "Optimization of molecules via deep reinforcement learning" <em>Scientific Reports</em> (2019)</li>
<li>Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" <em>Integrating Materials and Manufacturing Innovation</em> (2017)</li>
</ol>
<hr />
<p><strong>Next Chapter</strong>: <a href="chapter-2.html">Chapter 2: Fundamentals of Reinforcement Learning Theory</a></p><div class="navigation">
    <a href="index.html" class="nav-button">Back to Series Index</a>
    <a href="chapter-2.html" class="nav-button">Next Chapter ‚Üí</a>
</div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
            <li>The author and Tohoku University are not liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
            <li>The content of this material is subject to change, update, or discontinuation without notice.</li>
            <li>The copyright and license of this content follow the specified conditions (e.g., CC BY 4.0). Such licenses typically contain warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 2.0 | <strong>Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
