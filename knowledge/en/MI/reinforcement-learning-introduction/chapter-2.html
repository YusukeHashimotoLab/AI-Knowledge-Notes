<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Fundamentals of Reinforcement Learning Theory - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/reinforcement-learning-introduction/index.html">Reinforcement Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/reinforcement-learning-introduction/chapter-2.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 2: Fundamentals of Reinforcement Learning Theory</h1>
<p class="subtitle"></p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-25 min</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code examples: 8</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 2: Fundamentals of Reinforcement Learning Theory</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Organize the intuition and differences of representative methods such as Q-Learning, DQN, and PPO. Get a sense of which method to try for which problem.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Policy gradient methods like PPO work well for continuous control. For discrete selection, start with Q-based methods.</p>
<h2>Learning Objectives</h2>
<p>In this chapter, you will master:</p>
<ul>
<li>Theory and implementation of Policy Gradient Methods</li>
<li>Actor-Critic architecture mechanisms</li>
<li>Proximal Policy Optimization (PPO) details</li>
<li>Practical implementation using Stable Baselines3</li>
</ul>
<hr/>
<h2>2.1 Policy Gradient Methods</h2>
<h3>Limitations of Q-Learning</h3>
<p>The Q-Learning and DQN from Chapter 1 were <strong>value-based</strong> methods. These have the following limitations:</p>
<ol>
<li><strong>Discrete actions only</strong>: $\arg\max_a Q(s,a)$ is difficult for continuous action spaces</li>
<li><strong>Deterministic policy</strong>: Always selects the same action (cannot learn stochastic policies)</li>
<li><strong>Fragile to small changes</strong>: Small changes in Q-values can drastically change the policy</li>
</ol>
<p>In materials science, <strong>continuous control</strong> (raising temperature by 0.5 degrees, changing composition ratio by 2%) is important.</p>
<h3>Basic Idea of Policy Gradient Methods</h3>
<p>Policy gradient methods <strong>directly optimize the policy</strong>:</p>
<p>$$
\pi_\theta(a|s) = P(a|s; \theta)
$$</p>
<ul>
<li>$\theta$: Policy parameters (neural network weights)</li>
</ul>
<p><strong>Objective</strong>: Maximize expected cumulative reward $J(\theta)$</p>
<p>$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]
$$</p>
<ul>
<li>$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: Trajectory</li>
</ul>
<h3>Policy Gradient Theorem</h3>
<p>The <strong>REINFORCE</strong> algorithm (Williams, 1992) computes the gradient as:</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right]
$$</p>
<ul>
<li>$R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: Cumulative reward (return) from time $t$</li>
</ul>
<p><strong>Intuitive meaning</strong>:
- Increase probability of actions that received high rewards
- Decrease probability of actions that received low rewards</p>
<h3>REINFORCE Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

class PolicyNetwork(nn.Module):
    """Policy Network

    Takes state as input, outputs probability for each action
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)  # Probability distribution


class REINFORCEAgent:
    """REINFORCE Algorithm"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # Store logs within episode
        self.log_probs = []
        self.rewards = []

    def select_action(self, state):
        """Sample action according to policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        # Sample from probability distribution
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        # Store log probability (for gradient computation)
        self.log_probs.append(action_dist.log_prob(action))

        return action.item()

    def store_reward(self, reward):
        """Store reward"""
        self.rewards.append(reward)

    def update(self):
        """Update policy after episode ends"""
        # Compute returns (cumulative rewards)
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)

        # Normalize (stabilize learning)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Policy gradient
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)

        # Gradient descent
        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # Reset logs
        self.log_probs = []
        self.rewards = []


# Simple materials exploration environment (discrete action version)
class DiscreteMaterialsEnv:
    """Discrete action materials exploration environment"""
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # Action: 0=increase dim 0, 1=decrease dim 0, 2=increase dim 1, 3=decrease dim 1
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # Reward: distance to target
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        done = distance &lt; 0.5

        return self.state, reward, done


# REINFORCE training
env = DiscreteMaterialsEnv()
agent = REINFORCEAgent(state_dim=4, action_dim=4)

episodes = 1000
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.store_reward(reward)
        state = next_state
        total_reward += reward

    # Update after episode ends
    agent.update()
    rewards_history.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_history[-100:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")

# Learning curve
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('REINFORCE: Materials Exploration with Policy Gradient')
plt.grid(True)
plt.show()
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>Episode 100: Avg Reward = -38.24
Episode 200: Avg Reward = -28.15
Episode 500: Avg Reward = -15.32
Episode 1000: Avg Reward = -7.89
</code></pre>
<hr/>
<h2>2.2 Baselines and Variance Reduction</h2>
<h3>Problems with REINFORCE</h3>
<p>REINFORCE suffers from <strong>high variance</strong>. With the same policy, returns $R_t$ can vary greatly depending on luck.</p>
<h3>Introducing Baselines</h3>
<p>Subtracting a <strong>baseline</strong> $b(s)$ reduces variance:</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right]
$$</p>
<p><strong>Optimal baseline</strong>: State value function $V(s)$</p>
<p>$$
b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right]
$$</p>
<p><strong>Advantage function</strong> $A(s, a)$:
$$
A(s, a) = Q(s, a) - V(s) = R_t - V(s_t)
$$</p>
<p>It represents "how much better is this action than average".</p>
<h3>REINFORCE with Baseline</h3>
<pre><code class="language-python">class ValueNetwork(nn.Module):
    """Value Network (Baseline)"""
    def __init__(self, state_dim, hidden_dim=64):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # Output state value

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class REINFORCEWithBaseline:
    """REINFORCE with Baseline"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)

        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)

        self.log_probs = []
        self.rewards = []
        self.states = []

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        self.log_probs.append(action_dist.log_prob(action))
        self.states.append(state)

        return action.item()

    def store_reward(self, reward):
        self.rewards.append(reward)

    def update(self):
        # Compute returns
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)
        states = torch.FloatTensor(self.states)

        # Value network output (baseline)
        values = self.value(states).squeeze()

        # Advantage = returns - baseline
        advantages = returns - values.detach()

        # Policy gradient loss
        policy_loss = []
        for log_prob, adv in zip(self.log_probs, advantages):
            policy_loss.append(-log_prob * adv)

        # Value network loss (MSE)
        value_loss = nn.MSELoss()(values, returns)

        # Optimization
        self.policy_optimizer.zero_grad()
        torch.stack(policy_loss).sum().backward()
        self.policy_optimizer.step()

        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        # Reset
        self.log_probs = []
        self.rewards = []
        self.states = []


# Training (with baseline)
agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)

rewards_baseline = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent_baseline.select_action(state)
        next_state, reward, done = env.step(action)

        agent_baseline.store_reward(reward)
        state = next_state
        total_reward += reward

    agent_baseline.update()
    rewards_baseline.append(total_reward)

# Comparison
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')
plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('Learning Stabilization with Baseline')
plt.legend()
plt.grid(True)
plt.show()
</code></pre>
<p><strong>Result</strong>: Baseline makes learning <strong>more stable</strong> and convergence <strong>faster</strong>.</p>
<hr/>
<h2>2.3 Actor-Critic Architecture</h2>
<h3>Actor-Critic Concept</h3>
<p>Learn <strong>Actor (policy)</strong> and <strong>Critic (value function)</strong> simultaneously:</p>
<ul>
<li><strong>Actor</strong> $\pi_\theta(a|s)$: Selects actions</li>
<li><strong>Critic</strong> $V_\phi(s)$: Evaluates state values</li>
</ul>
<div class="mermaid">
flowchart LR
    S[State s] --&gt; A[Actor: œÄŒ∏]
    S --&gt; C[Critic: Vœï]
    A --&gt;|Action a| E[Environment]
    E --&gt;|Reward r| C
    C --&gt;|TD Error| A
    C --&gt;|Value evaluation| C

    style A fill:#e1f5ff
    style C fill:#ffe1cc
</div>
<h3>TD Error and Advantage</h3>
<p><strong>TD Error</strong> (Temporal Difference Error):
$$
\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$</p>
<p>This can be used as a <strong>1-step advantage estimate</strong>.</p>
<h3>A2C (Advantage Actor-Critic)</h3>
<pre><code class="language-python">class A2CAgent:
    """Advantage Actor-Critic"""
    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):
        self.gamma = gamma

        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = ValueNetwork(state_dim)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.actor(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        return action.item(), action_dist.log_prob(action)

    def update(self, state, action_log_prob, reward, next_state, done):
        """Update every step"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

        # Current and next state values
        value = self.critic(state_tensor)
        next_value = self.critic(next_state_tensor)

        # TD target and TD error
        td_target = reward + (1 - done) * self.gamma * next_value.item()
        td_error = td_target - value.item()

        # Critic loss (MSE)
        critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)

        # Actor loss (policy gradient √ó advantage)
        actor_loss = -action_log_prob * td_error

        # Optimization
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()


# A2C training
agent_a2c = A2CAgent(state_dim=4, action_dim=4)

rewards_a2c = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, log_prob = agent_a2c.select_action(state)
        next_state, reward, done = env.step(action)

        # Update every step
        agent_a2c.update(state, log_prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    rewards_a2c.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_a2c[-100:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")
</code></pre>
<p><strong>Advantages</strong>:
- <strong>Online learning</strong> without waiting for episode completion
- <strong>Low variance</strong> using TD error</p>
<hr/>
<h2>2.4 Proximal Policy Optimization (PPO)</h2>
<h3>Trust Region Methods</h3>
<p>In policy gradient methods, <strong>too large updates can collapse the policy</strong>.</p>
<p><strong>Trust Region Policy Optimization (TRPO)</strong> constrains policy changes:</p>
<p>$$
\max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} | \pi_\theta) \leq \delta
$$</p>
<p>However, optimizing with KL divergence constraints is complex.</p>
<h3>PPO Simplification</h3>
<p><strong>PPO</strong> (Schulman et al., 2017) implements the constraint through <strong>clipping in the loss function</strong>:</p>
<p>$$
L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$</p>
<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: Importance ratio</li>
<li>$\epsilon$: Clipping range (typically 0.1-0.2)</li>
</ul>
<p><strong>Intuition</strong>:
- When advantage is positive (good action) ‚Üí increase $r_t$, but cap at $1+\epsilon$
- When advantage is negative (bad action) ‚Üí decrease $r_t$, but floor at $1-\epsilon$
- Prevents drastic policy changes</p>
<h3>Entropy Bonus</h3>
<p>To encourage exploration, add <strong>entropy</strong> to the loss:</p>
<p>$$
L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta]
$$</p>
<ul>
<li>$L^{\text{VF}}$: Value function loss</li>
<li>$H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: Entropy (uncertainty of probability distribution)</li>
<li>$c_2$: Entropy coefficient (typically 0.01)</li>
</ul>
<h3>PPO Implementation (Using Stable Baselines3)</h3>
<pre><code class="language-python">from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym

# Gym environment wrapper
class GymMaterialsEnv(gym.Env):
    """OpenAI Gym-compatible materials exploration environment"""
    def __init__(self):
        super(GymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # Define action and observation spaces
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.5

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# Create environment
env = DummyVecEnv([lambda: GymMaterialsEnv()])

# PPO model
model = PPO(
    "MlpPolicy",                # Multi-layer perceptron policy
    env,
    learning_rate=3e-4,
    n_steps=2048,               # Steps before update
    batch_size=64,
    n_epochs=10,                # Optimization epochs per update
    gamma=0.99,
    gae_lambda=0.95,            # GAE (Generalized Advantage Estimation)
    clip_range=0.2,             # PPO clipping range
    ent_coef=0.01,              # Entropy coefficient
    verbose=1,
    tensorboard_log="./ppo_materials_tensorboard/"
)

# Train
model.learn(total_timesteps=100000)

# Save
model.save("ppo_materials_agent")

# Evaluation
eval_env = GymMaterialsEnv()
state = eval_env.reset()
total_reward = 0

for _ in range(100):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, _ = eval_env.step(action)
    total_reward += reward

    if done:
        break

print(f"Evaluation result: Total Reward = {total_reward:.2f}")
print(f"Final state: {state}")
print(f"Target: {eval_env.target}")
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 1024     |
|    iterations      | 50       |
|    time_elapsed    | 97       |
|    total_timesteps | 102400   |
---------------------------------

Evaluation result: Total Reward = -5.23
Final state: [3.02 4.98 2.47 3.95]
Target: [3.  5.  2.5 4. ]
</code></pre>
<p><strong>Explanation</strong>:
- Stable Baselines3 implements PPO in just a few lines
- Learning progress can be visualized with TensorBoard
- Discovers materials very close to the target</p>
<hr/>
<h2>2.5 Extension to Continuous Action Spaces</h2>
<h3>Gaussian Policies</h3>
<p>In materials science, <strong>continuous control</strong> of temperature, composition ratios, etc. is necessary.</p>
<p>For continuous actions, use <strong>Gaussian distribution policies</strong>:</p>
<p>$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))
$$</p>
<ul>
<li>$\mu_\theta(s)$: Mean (output by neural network)</li>
<li>$\sigma_\theta(s)$: Standard deviation (learnable or fixed)</li>
</ul>
<h3>Continuous Action PPO</h3>
<pre><code class="language-python"># Continuous action environment
class ContinuousGymMaterialsEnv(gym.Env):
    """Continuous action materials exploration environment"""
    def __init__(self):
        super(ContinuousGymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # Continuous action space (4D vector, range [-1, 1])
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(self.state_dim,), dtype=np.float32
        )
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        # Map action to state change (-1~1 ‚Üí -0.5~0.5)
        delta = action * 0.5
        self.state = np.clip(self.state + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance &lt; 0.3

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


# Continuous action PPO
env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])

model_continuous = PPO(
    "MlpPolicy",
    env_continuous,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    clip_range=0.2,
    verbose=1
)

model_continuous.learn(total_timesteps=100000)

# Evaluation
eval_env_cont = ContinuousGymMaterialsEnv()
state = eval_env_cont.reset()

for _ in range(50):
    action, _ = model_continuous.predict(state, deterministic=True)
    state, reward, done, _ = eval_env_cont.step(action)

    if done:
        break

print(f"Final state: {state}")
print(f"Target: {eval_env_cont.target}")
print(f"Distance: {np.linalg.norm(state - eval_env_cont.target):.4f}")
</code></pre>
<p><strong>Example output</strong>:</p>
<pre><code>Final state: [3.001 5.003 2.498 3.997]
Target: [3.  5.  2.5 4. ]
Distance: 0.0054
</code></pre>
<p><strong>Explanation</strong>: Continuous actions enable <strong>precise control</strong> to the target</p>
<hr/>
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p>Explain why using a baseline reduces variance, using the following expressions.</p>
<p>$$
\text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)]
$$</p>
<details>
<summary>Hint</summary>

For variance properties: $\text{Var}[X - c] = \text{Var}[X]$ (subtracting a constant $c$ doesn't change variance), but $b(s\_t)$ is state-dependent, not a constant.

</details>
<details>
<summary>Solution</summary>

When baseline $b(s\_t)$ is close to state value $V(s\_t)$:

- **Return** $R\_t$ varies greatly with state (heavily influenced by luck)
- **Advantage** $R\_t - V(s\_t)$ is "deviation from average" so varies less

Mathematically:
$$
\text{Var}[R\_t - V(s\_t)] \leq \text{Var}[R\_t]
$$

This is because $V(s\_t)$ is the "expected cumulative reward from state $s\_t$", canceling out luck effects.

**Concrete example**:
- Returns from state A: 100, 105, 95 ‚Üí variance = 25
- Value of state A: 100
- Advantage: 0, 5, -5 ‚Üí variance = 25 (same)

However, considering multiple states:
- Returns from state A: 100¬±5
- Returns from state B: 50¬±5
- Overall variance: large

By subtracting the state-specific average baseline, differences between states disappear, reducing variance.

</details>
<hr/>
<h3>Exercise 2 (Difficulty: medium)</h3>
<p>Explain what happens when PPO's clipping range $\epsilon$ is increased, and what happens in the extreme case of $\epsilon=0$.</p>
<details>
<summary>Hint</summary>

Review the clipping formula and consider how changes in $r\_t(\theta)$ are limited.

</details>
<details>
<summary>Solution</summary>

**When $\epsilon$ is increased**:
- Clipping range widens, allowing larger policy changes
- Learning is faster but more unstable
- In extreme cases, policy may collapse

**When $\epsilon=0$**:
$$
\text{clip}(r\_t, 1, 1) = 1
$$

- Importance ratio is always clipped to 1
- Policy is never updated (forces $\pi\_\theta = \pi\_{\theta\_{\text{old}}}$)

**Practical values**: $\epsilon = 0.1 \sim 0.2$ is common

**Experimental code**:

<pre><code class="language-python"># Œµ=0.05 (strict constraint)
model_tight = PPO("MlpPolicy", env, clip_range=0.05)

# Œµ=0.5 (loose constraint)
model_loose = PPO("MlpPolicy", env, clip_range=0.5)

# Compare learning curves
# ‚Üí model_tight is stable but slow
# ‚Üí model_loose is fast but oscillates
</code></pre>
</details>
<hr/>
<h3>Exercise 3 (Difficulty: hard)</h3>
<p>Compare the following two reward designs for materials exploration, describe their advantages and disadvantages, and conduct an experiment with code.</p>
<p><strong>Reward A (Sparse reward)</strong>: Reward of 1 only when reaching the target, 0 otherwise
<strong>Reward B (Dense reward)</strong>: Continuous reward based on distance to target</p>
<details>
<summary>Hint</summary>

Sparse rewards make exploration difficult, but dense rewards are prone to local optima. Consider the impact of entropy bonus.

</details>
<details>
<summary>Solution</summary>

**Reward A (Sparse reward) advantages and disadvantages**:

**Advantages**:
- Clear objective (no ambiguity)
- Less prone to local optima (not misled by intermediate rewards)

**Disadvantages**:
- Exploration is very difficult (weak learning signal)
- Takes long to learn

**Reward B (Dense reward) advantages and disadvantages**:

**Advantages**:
- Easy exploration (feedback every step)
- Fast learning

**Disadvantages**:
- Difficult reward design (distance alone may be insufficient)
- Prone to local optima

**Experimental code**:

<pre><code class="language-python"># Reward A (sparse reward)
class SparseRewardEnv(gym.Env):
    def step(self, action):
        # ... (state update) ...
        distance = np.linalg.norm(self.state - self.target)

        if distance &lt; 0.5:
            reward = 1.0  # Reached
            done = True
        else:
            reward = 0.0  # Otherwise
            done = False

        return self.state, reward, done, {}

# Reward B (dense reward)
class DenseRewardEnv(gym.Env):
    def step(self, action):
        # ... (state update) ...
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance  # Continuous reward
        done = distance &lt; 0.5

        return self.state, reward, done, {}

# Comparison experiment
model_sparse = PPO("MlpPolicy", DummyVecEnv([lambda: SparseRewardEnv()]))
model_dense = PPO("MlpPolicy", DummyVecEnv([lambda: DenseRewardEnv()]))

model_sparse.learn(total_timesteps=100000)
model_dense.learn(total_timesteps=100000)

# Result: model_dense learns faster, but
# in complex environments, model_sparse may find better solutions
</code></pre>


**Best practice**: Start with dense rewards, consider sparse rewards or **reward shaping** (adding intermediate rewards) depending on the problem.

</details>
<hr/>
<h2>Summary of This Section</h2>
<ul>
<li><strong>Policy gradient methods</strong> directly optimize the policy and handle continuous actions</li>
<li><strong>REINFORCE algorithm</strong> has high variance but is improved with baselines</li>
<li><strong>Actor-Critic</strong> simultaneously learns Actor and Critic, achieving low variance and online learning</li>
<li><strong>PPO</strong> achieves stable learning through clipping, a state-of-the-art practical method</li>
<li><strong>Stable Baselines3</strong> enables PPO implementation in just a few lines</li>
<li>For continuous action spaces, use <strong>Gaussian policies</strong></li>
</ul>
<p>In the next chapter, we'll learn to build custom environments specialized for materials exploration and reward design.</p>
<hr/>
<h2>Quality Checklist: Implementation Verification for Policy Gradient Methods</h2>
<h3>Theoretical Understanding Skills</h3>
<ul>
<li>[ ] Can explain the policy gradient theorem mathematically</li>
<li>[ ] Can derive REINFORCE update equations</li>
<li>[ ] Can explain why baselines reduce variance</li>
<li>[ ] Understand the role of PPO clipping</li>
</ul>
<h3>Implementation Skills</h3>
<ul>
<li>[ ] Can implement policy networks in PyTorch</li>
<li>[ ] Can compute returns (cumulative rewards)</li>
<li>[ ] Can compute advantage functions</li>
<li>[ ] Can use PPO with Stable Baselines3</li>
</ul>
<h3>Application to Materials Exploration</h3>
<ul>
<li>[ ] Can design continuous control variables (temperature, pressure) as action spaces</li>
<li>[ ] Can appropriately weight multi-objective rewards (yield, selectivity)</li>
<li>[ ] Can incorporate safety constraints (temperature limits) into rewards</li>
</ul>
<h3>Debugging Skills</h3>
<ul>
<li>[ ] Know how to handle high variance in policy gradients</li>
<li>[ ] Can identify causes when PPO doesn't converge</li>
<li>[ ] Can adjust entropy bonus</li>
</ul>
<hr/>
<h2>References</h2>
<ol>
<li>Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" <em>Machine Learning</em> (1992) - REINFORCE</li>
<li>Mnih et al. "Asynchronous methods for deep reinforcement learning" <em>ICML</em> (2016) - A3C/A2C</li>
<li>Schulman et al. "Proximal policy optimization algorithms" <em>arXiv</em> (2017) - PPO</li>
<li>Schulman et al. "Trust region policy optimization" <em>ICML</em> (2015) - TRPO</li>
<li>Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" <em>JMLR</em> (2021)</li>
</ol>
<hr/>
<p><strong>Next chapter</strong>: <a href="chapter-3.html">Chapter 3: Building Materials Exploration Environments</a></p><div class="navigation">
<a class="nav-button" href="chapter-1.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Index</a>
<a class="nav-button" href="chapter-3.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified terms (e.g., CC BY 4.0), which typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 2.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
