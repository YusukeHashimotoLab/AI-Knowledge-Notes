<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: The Need for High-Throughput Computing and Workflow Design - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/high-throughput-computing-introduction/index.html">High Throughput Computing</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/high-throughput-computing-introduction/chapter-1.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 1: The Need for High-Throughput Computing and Workflow Design</h1>
<p class="subtitle"></p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code examples: 0</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 1: The Need for High-Throughput Computing and Workflow Design</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">From one-off execution to "systems that run calculations." Quickly grasp the value and scope of HTC.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> The shift from "people running tasks" to "systems running tasks." Even small automation yields cumulative benefits.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master:</p>
<ul>
<li>‚úÖ Quantitatively understand the vastness of materials exploration space</li>
<li>‚úÖ Explain the four elements of High-Throughput Computing (automation, parallelization, standardization, data management)</li>
<li>‚úÖ Analyze success factors of Materials Project, AFLOW, and others</li>
<li>‚úÖ Understand and apply principles of effective workflow design</li>
<li>‚úÖ Quantitatively evaluate cost reduction effects</li>
</ul>
<hr/>
<h2>1.1 Challenges in Materials Discovery: Why High-Throughput Computing is Necessary</h2>
<h3>The Vastness of the Exploration Space</h3>
<p>The greatest challenge in materials science is that <strong>the space to be explored is enormously vast</strong>.</p>
<p><strong>Example: Ternary alloy exploration</strong></p>
<p>Consider Li-Ni-Co oxide battery materials (Li‚ÇìNi·µßCo·µßO‚ÇÇ):
- Li composition: 0.0-1.0 (10 levels)
- Ni composition: 0.0-1.0 (10 levels)
- Co composition: 0.0-1.0 (10 levels)</p>
<p>Simple calculation gives <strong>10¬≥ = 1,000 combinations</strong>.</p>
<p><strong>Example: Quinary high-entropy alloys</strong></p>
<p>For CoCrFeNiMn systems with varying composition ratios:
- Each element: 0-100% (11 levels at 10% intervals)
- Constraint that total composition = 100%</p>
<p>The combinations reach <strong>tens of thousands</strong>.</p>
<p><strong>Typical scale of materials exploration</strong></p>
<div class="mermaid">
flowchart LR
    A[Single material] --&gt;|Substitution| B[10-100 candidates]
    B --&gt;|Composite materials| C[1,000-10,000 candidates]
    C --&gt;|High-dimensional search| D[100,000-1,000,000 candidates]
    D --&gt;|Exhaustive search| E[10^12-10^60 candidates]

    style E fill:#ff6b6b
</div>
<p>In actual materials exploration, there exist <strong>10¬π¬≤ (1 trillion) to 10‚Å∂‚Å∞ combinations</strong>.</p>
<h3>Limitations of Traditional Methods</h3>
<p><strong>Cost per material (traditional experiment-driven approach)</strong></p>
<table>
<thead>
<tr>
<th>Item</th>
<th>Time</th>
<th>Cost (Estimate)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Literature survey</td>
<td>1-2 weeks</td>
<td>$10,000</td>
</tr>
<tr>
<td>Sample synthesis</td>
<td>1-4 weeks</td>
<td>$30,000-100,000</td>
</tr>
<tr>
<td>Characterization</td>
<td>2-8 weeks</td>
<td>$50,000-200,000</td>
</tr>
<tr>
<td>Data analysis</td>
<td>1-2 weeks</td>
<td>$10,000-30,000</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>2-4 months</strong></td>
<td><strong>$100,000-340,000</strong></td>
</tr>
</tbody>
</table>
<p><strong>Annual exploration capacity</strong></p>
<ul>
<li>Experienced researcher (1 person): <strong>10-50 materials/year</strong></li>
<li>Research group (5-10 people): <strong>50-200 materials/year</strong></li>
</ul>
<p><strong>Problems</strong></p>
<ol>
<li><strong>Time</strong>: New material development takes 15-20 years</li>
<li><strong>Cost</strong>: Hundreds of thousands of dollars per material</li>
<li><strong>Scalability</strong>: Adding personnel only improves linearly</li>
<li><strong>Reproducibility</strong>: Complete recording of experimental conditions is difficult</li>
</ol>
<h3>Materials Genome Initiative (MGI) Proposal</h3>
<p>In 2011, the Obama administration announced the <strong>Materials Genome Initiative (MGI)</strong>:</p>
<p><strong>Goals</strong>:
- <strong>Halve</strong> new material development time (20 years ‚Üí 10 years)
- <strong>Closed loop</strong> of computational science and experiments
- <strong>Public resource</strong> databases and computational infrastructure</p>
<p><strong>Budget</strong>: $100M in first year, $250M over 10 years</p>
<p><strong>Outcomes</strong> (2011-2021):
- Materials Project: DFT calculations for <strong>140,000 materials</strong>
- AFLOW: Automated analysis of <strong>3,500,000 crystal structures</strong>
- Development period: Actually achieved <strong>30-50% reduction</strong></p>
<hr/>
<h2>1.2 Definition of High-Throughput Computing and Its Four Elements</h2>
<h3>Definition</h3>
<blockquote>
<p><strong>High-Throughput Computing (HTC)</strong> is a methodology that efficiently executes large volumes of computational tasks through automation and parallelization, under standardized workflows and data management.</p>
</blockquote>
<h3>Four Elements</h3>
<div class="mermaid">
flowchart TD
    A[High-Throughput Computing] --&gt; B[1. Automation]
    A --&gt; C[2. Parallelization]
    A --&gt; D[3. Standardization]
    A --&gt; E[4. Data Management]

    B --&gt; B1[Automated input generation]
    B --&gt; B2[Automated job submission]
    B --&gt; B3[Automated error handling]

    C --&gt; C1[Simultaneous multi-material calculation]
    C --&gt; C2[MPI node parallelization]
    C --&gt; C3[Task and data parallelism]

    D --&gt; D1[Unified computational conditions]
    D --&gt; D2[Unified file formats]
    D --&gt; D3[Quality control standards]

    E --&gt; E1[Database design]
    E --&gt; E2[Metadata recording]
    E --&gt; E3[Search and visualization]

    style A fill:#4ecdc4
    style B fill:#ffe66d
    style C fill:#ff6b6b
    style D fill:#95e1d3
    style E fill:#a8e6cf
</div>
<h4>Element 1: Automation</h4>
<p>Systems that execute and manage calculations without human intervention.</p>
<p><strong>Example: Structure optimization automation</strong></p>
<p>Manual case:
1. Prepare initial structure ‚Üí Create input file ‚Üí Submit job
2. Check completion ‚Üí Review results ‚Üí Judge convergence
3. If not converged, change settings ‚Üí Resubmit
4. If converged, move to next material</p>
<p><strong>30 minutes to 2 hours per material</strong> (50-200 hours for 100 materials)</p>
<p>After automation:</p>
<pre><code class="language-python">for structure in structures:
    optimize_structure(structure)  # Fully automatic
    if converged:
        calculate_properties(structure)
# 100 materials completed overnight (zero human effort)
</code></pre>
<h4>Element 2: Parallelization</h4>
<p>Improving throughput by executing multiple calculations simultaneously.</p>
<p><strong>Three parallelization levels</strong></p>
<ol>
<li>
<p><strong>Task parallelism</strong>: Simultaneous calculation of different materials
   - 1000 materials parallel execution on 100 nodes ‚Üí 10x speedup</p>
</li>
<li>
<p><strong>Data parallelism</strong>: k-point parallel calculation for same material
   - 2-4x speedup with VASP KPAR settings</p>
</li>
<li>
<p><strong>MPI parallelism</strong>: Distributing single calculation across multiple cores
   - 10-20x speedup on 48 cores (50-70% scaling efficiency)</p>
</li>
</ol>
<p><strong>Parallel efficiency examples</strong></p>
<table>
<thead>
<tr>
<th>Parallelization method</th>
<th>Nodes</th>
<th>Speedup ratio</th>
<th>Efficiency</th>
</tr>
</thead>
<tbody>
<tr>
<td>Task parallelism only</td>
<td>100</td>
<td>100x</td>
<td>100%</td>
</tr>
<tr>
<td>MPI parallelism only</td>
<td>4</td>
<td>3.2x</td>
<td>80%</td>
</tr>
<tr>
<td>Hybrid</td>
<td>100x4</td>
<td>320x</td>
<td>80%</td>
</tr>
</tbody>
</table>
<h4>Element 3: Standardization</h4>
<p>Unifying computational conditions, data formats, and quality standards.</p>
<p><strong>Materials Project standard settings</strong></p>
<pre><code class="language-python"># VASP settings example (Materials Project)
{
    "ENCUT": 520,  # Energy cutoff (eV)
    "EDIFF": 1e-5,  # Energy convergence criterion
    "K-point density": 1000,  # k-point density (√Ö‚Åª¬≥)
    "ISMEAR": -5,  # Tetrahedron method
}
</code></pre>
<p><strong>Benefits</strong>:
- <strong>Fair comparison</strong> between different materials
- Ensuring calculation <strong>reproducibility</strong>
- <strong>Ease</strong> of error detection</p>
<h4>Element 4: Data Management</h4>
<p>Structuring and storing computational results for searchability.</p>
<p><strong>Database schema example</strong></p>
<pre><code class="language-json">{
  "material_id": "mp-1234",
  "formula": "LiCoO2",
  "structure": {...},
  "energy": -45.67,  // eV/atom
  "band_gap": 2.3,   // eV
  "calculation_metadata": {
    "vasp_version": "6.3.0",
    "encut": 520,
    "kpoints": [12, 12, 8],
    "calculation_date": "2025-10-17"
  }
}
</code></pre>
<p><strong>Search example</strong>:</p>
<pre><code class="language-python"># Search for oxides with band gap 1.5-2.5 eV
results = db.find({
    "band_gap": {"$gte": 1.5, "$lte": 2.5},
    "elements": {"$all": ["O"]}
})
</code></pre>
<hr/>
<h2>1.3 Success Stories: Materials Project, AFLOW, OQMD</h2>
<h3>Materials Project (USA)</h3>
<p><strong>Scale</strong> (as of 2025):
- Number of materials: <strong>140,000+</strong>
- Calculation tasks: <strong>5,000,000+</strong>
- DFT computation time: Over <strong>500 million CPU hours</strong></p>
<p><strong>Technology stack</strong>:
- Calculation code: VASP
- Workflow: FireWorks + Atomate
- Database: MongoDB
- API: pymatgen + RESTful API</p>
<p><strong>Achievements</strong>:
- Li-ion battery materials: <strong>67% development time reduction</strong>
- Thermoelectric materials: <strong>90% prediction accuracy</strong> for ZT values
- Perovskite solar cells: <strong>Screening of 50,000 candidates</strong></p>
<p><strong>Impact</strong>:
- Citations: <strong>20,000+ times</strong> (Google Scholar)
- Industrial use: Tesla, Panasonic, Samsung, etc.
- Users: <strong>100,000+</strong> (API registrations)</p>
<h3>AFLOW (Duke University)</h3>
<p><strong>Scale</strong>:
- Crystal structures: <strong>3,500,000+</strong>
- Prototypes: <strong>1,000,000+</strong>
- Calculated properties: Band gaps, elastic constants, thermodynamic stability</p>
<p><strong>Features</strong>:
- <strong>Crystal symmetry analysis</strong>: Automatic space group identification
- <strong>Prototype database</strong>: Generation from known structures
- <strong>AFLOW-ML</strong>: Machine learning integration</p>
<p><strong>Applications</strong>:
- High-entropy alloys: <strong>Phase stability prediction</strong>
- Superconducting materials: <strong>Tc prediction</strong></p>
<h3>OQMD (Northwestern University)</h3>
<p><strong>Scale</strong>:
- Number of materials: <strong>815,000+</strong>
- DFT calculations: Quantum ESPRESSO</p>
<p><strong>Features</strong>:
- <strong>Thermodynamic data</strong>: Formation energy, phase equilibria
- <strong>Chemical potential diagrams</strong>: Stability visualization</p>
<h3>JARVIS (NIST)</h3>
<p><strong>Scale</strong>:
- Number of materials: <strong>40,000+</strong>
- Diverse properties: Optical, elastic, magnetic, topological</p>
<p><strong>Features</strong>:
- <strong>Machine learning models</strong>: Pre-trained model provision
- <strong>2D materials</strong>: Large database of monolayer materials</p>
<h3>Comparison table</h3>
<table>
<thead>
<tr>
<th>Project</th>
<th>Materials</th>
<th>Calculation code</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>Materials Project</td>
<td>140k+</td>
<td>VASP</td>
<td>Comprehensive, industrial use</td>
</tr>
<tr>
<td>AFLOW</td>
<td>3.5M+</td>
<td>VASP</td>
<td>Crystal structure focused</td>
</tr>
<tr>
<td>OQMD</td>
<td>815k+</td>
<td>QE</td>
<td>Thermodynamic data</td>
</tr>
<tr>
<td>JARVIS</td>
<td>40k+</td>
<td>VASP</td>
<td>Diverse properties</td>
</tr>
</tbody>
</table>
<hr/>
<h2>1.4 Principles of Workflow Design</h2>
<p>Effective High-Throughput Computing requires appropriate workflow design.</p>
<h3>Principle 1: Modularity</h3>
<p>Divide each task into independent modules for reusability.</p>
<p><strong>Good example</strong>:</p>
<pre><code class="language-python"># Modularized workflow
structure = generate_structure(formula)
relaxed = relax_structure(structure)
energy = static_calculation(relaxed)
band_gap = calculate_band_structure(relaxed)
dos = calculate_dos(relaxed)
</code></pre>
<p><strong>Bad example</strong>:</p>
<pre><code class="language-python"># Monolithic script
# Difficult to reuse parts
run_everything(formula)  # Black box
</code></pre>
<h3>Principle 2: Error Handling</h3>
<p>Calculation failures are inevitable, requiring appropriate error handling.</p>
<p><strong>Error classification</strong></p>
<div class="mermaid">
flowchart TD
    A[Calculation errors] --&gt; B[Retryable]
    A --&gt; C[Requires setting changes]
    A --&gt; D[Fatal errors]

    B --&gt; B1[Temporary I/O errors]
    B --&gt; B2[Job timeout]

    C --&gt; C1[Convergence issues]
    C --&gt; C2[Memory shortage]

    D --&gt; D1[Structure abnormality]
    D --&gt; D2[Software bug]

    B1 --&gt;|Auto retry| E[Success]
    C1 --&gt;|Relax settings| E
    D1 --&gt;|Skip| F[Next material]
</div>
<p><strong>Implementation example</strong>:</p>
<pre><code class="language-python">def robust_calculation(structure, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = run_vasp(structure)
            if result.converged:
                return result
            else:
                # Convergence issue ‚Üí Change settings
                structure = adjust_parameters(structure)
        except MemoryError:
            # Memory shortage ‚Üí Reduce cores
            reduce_cores()
        except TimeoutError:
            # Timeout ‚Üí Extend time limit
            extend_time_limit()

    # Finally failed
    log_failure(structure)
    return None
</code></pre>
<h3>Principle 3: Reproducibility</h3>
<p>Enable other researchers to obtain the same results.</p>
<p><strong>Essential recording items</strong>:</p>
<ol>
<li><strong>Calculation conditions</strong>: All parameters</li>
<li><strong>Software version</strong>: VASP 6.3.0, etc.</li>
<li><strong>Pseudopotentials</strong>: PBE, PAW, etc.</li>
<li><strong>Computational environment</strong>: OS, compiler, libraries</li>
</ol>
<p><strong>Implementation example</strong>:</p>
<pre><code class="language-python"># Provenance recording
metadata = {
    "software": "VASP 6.3.0",
    "potcar": "PBE_54",
    "encut": 520,
    "kpoints": [12, 12, 8],
    "convergence": {
        "energy": 1e-5,
        "force": 0.01
    },
    "compute_environment": {
        "hostname": "hpc.university.edu",
        "nodes": 4,
        "cores_per_node": 48,
        "date": "2025-10-17T10:30:00Z"
    }
}
</code></pre>
<h3>Principle 4: Scalability</h3>
<p>Design that can scale from 10 materials ‚Üí 1,000 materials ‚Üí 100,000 materials.</p>
<p><strong>Scalability checklist</strong>:</p>
<ul>
<li>[ ] Can the database handle large volumes of data (MongoDB, PostgreSQL)</li>
<li>[ ] Can the file system withstand hundreds of thousands of files</li>
<li>[ ] Is network bandwidth sufficient</li>
<li>[ ] Are job scheduler limits (maximum jobs) acceptable</li>
<li>[ ] Is data analysis parallelized</li>
</ul>
<p><strong>Scalability testing</strong>:</p>
<pre><code class="language-python"># Small-scale test: 10 materials
test_workflow(n_materials=10)  # 1 hour

# Medium-scale test: 100 materials
test_workflow(n_materials=100)  # 10 hours

# Large-scale test: 1,000 materials
test_workflow(n_materials=1000)  # 100 hours

# Check scaling efficiency
scaling_efficiency = (time_10 * 100) / time_1000
# Ideal is 1.0 (linear scaling)
</code></pre>
<hr/>
<h2>1.5 Quantitative Analysis of Costs and Benefits</h2>
<h3>Traditional Method vs High-Throughput Computing</h3>
<p><strong>Scenario</strong>: Screening 1,000 materials</p>
<h4>Traditional method (experiment-driven)</h4>
<table>
<thead>
<tr>
<th>Item</th>
<th>Unit price</th>
<th>Quantity</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Researcher personnel costs</td>
<td>$80,000/year</td>
<td>5 people √ó 2 years</td>
<td>$800,000</td>
</tr>
<tr>
<td>Sample synthesis</td>
<td>$50,000</td>
<td>1,000 materials</td>
<td>$50,000,000</td>
</tr>
<tr>
<td>Characterization</td>
<td>$30,000</td>
<td>1,000 materials</td>
<td>$30,000,000</td>
</tr>
<tr>
<td><strong>Total cost</strong></td>
<td></td>
<td></td>
<td><strong>$80,800,000</strong></td>
</tr>
<tr>
<td><strong>Duration</strong></td>
<td></td>
<td></td>
<td><strong>2 years</strong></td>
</tr>
</tbody>
</table>
<h4>High-Throughput Computing</h4>
<table>
<thead>
<tr>
<th>Item</th>
<th>Unit price</th>
<th>Quantity</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Researcher personnel costs</td>
<td>$80,000/year</td>
<td>2 people √ó 6 months</td>
<td>$80,000</td>
</tr>
<tr>
<td>Computing resources</td>
<td>$0.5/CPU hour</td>
<td>1M CPU hours</td>
<td>$500,000</td>
</tr>
<tr>
<td>Storage</td>
<td>$0.02/GB/month</td>
<td>10TB √ó 6 months</td>
<td>$10,000</td>
</tr>
<tr>
<td>Software licenses</td>
<td>$300,000/year</td>
<td>1 year</td>
<td>$300,000</td>
</tr>
<tr>
<td><strong>Total cost</strong></td>
<td></td>
<td></td>
<td><strong>$890,000</strong></td>
</tr>
<tr>
<td><strong>Duration</strong></td>
<td></td>
<td></td>
<td><strong>6 months</strong></td>
</tr>
</tbody>
</table>
<p><strong>Reduction effects</strong>:
- Cost reduction: <strong>89%</strong> ($80.8M ‚Üí $0.89M)
- Time reduction: <strong>75%</strong> (2 years ‚Üí 6 months)</p>
<h3>ROI (Return on Investment) Calculation</h3>
<p><strong>Initial investment</strong>:
- Environment setup: $50,000
- Personnel training: $30,000
- <strong>Total</strong>: $80,000</p>
<p><strong>Annual savings</strong> (for 1,000 materials/year):
- Experimental cost reduction: $80M/year
- Personnel cost reduction: $240,000/year
- <strong>Total</strong>: $80.24M/year</p>
<p><strong>ROI</strong>:</p>
<pre><code>ROI = (Annual savings - Operating costs) / Initial investment
    = ($80.24M - $0.89M) / $0.08M
    = 991x
</code></pre>
<p><strong>Payback period</strong>: About 1 day</p>
<h3>Non-monetary Benefits</h3>
<ol>
<li><strong>Innovation acceleration</strong>: Trial-and-error cycle 2 years ‚Üí 6 months</li>
<li><strong>Competitive advantage</strong>: Market entry 6-12 months ahead of competitors</li>
<li><strong>Data assets</strong>: Accumulated database becomes valuable property</li>
<li><strong>Human resource development</strong>: Acquisition of computational materials science skills</li>
</ol>
<hr/>
<h2>1.6 Workflow Design Examples</h2>
<h3>Example 1: Band Gap Screening</h3>
<p><strong>Goal</strong>: Discover oxides with band gaps of 1.5-2.5 eV</p>
<p><strong>Workflow</strong>:</p>
<div class="mermaid">
flowchart TD
    A[Structure generation] --&gt; B[Structure optimization]
    B --&gt; C[Static calculation]
    C --&gt; D[Band structure calculation]
    D --&gt; E[Band gap extraction]
    E --&gt; F{1.5-2.5 eV?}
    F --&gt;|Yes| G[Candidate material list]
    F --&gt;|No| H[Exclude]

    B --&gt;|Convergence failure| I[Error handling]
    I --&gt;|Change settings| B
    I --&gt;|Fatal| H
</div>
<p><strong>Python pseudocode</strong>:</p>
<pre><code class="language-python">candidate_materials = []

for formula in oxide_formulas:
    # Step 1: Structure generation
    structure = generate_structure(formula)

    # Step 2: Structure optimization
    try:
        relaxed = relax_structure(structure)
    except ConvergenceError:
        relaxed = relax_structure(structure, strict=False)

    # Step 3: Static calculation
    energy, forces = static_calculation(relaxed)

    # Step 4: Band structure
    band_gap = calculate_band_gap(relaxed)

    # Step 5: Filtering
    if 1.5 &lt;= band_gap &lt;= 2.5:
        candidate_materials.append({
            "formula": formula,
            "band_gap": band_gap,
            "energy": energy
        })
</code></pre>
<h3>Example 2: Thermodynamic Stability Screening</h3>
<p><strong>Goal</strong>: Discover materials with negative (stable) formation energy</p>
<p><strong>Workflow</strong>:</p>
<pre><code class="language-python">stable_materials = []

for composition in compositions:
    # Formation energy calculation
    E_compound = calculate_energy(composition)
    E_elements = sum([calculate_energy(el) for el in composition.elements])

    E_formation = E_compound - E_elements

    if E_formation &lt; 0:
        # Check decomposition energy too
        E_decomp = calculate_decomposition_energy(composition)

        if E_decomp &gt; 0:  # Does not decompose
            stable_materials.append({
                "composition": composition,
                "E_formation": E_formation,
                "E_decomp": E_decomp
            })
</code></pre>
<hr/>
<h2>1.7 Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p><strong>Problem</strong>: Consider band gap calculations for 100 materials. Estimate the time required for traditional methods (manual) and High-Throughput Computing.</p>
<p><strong>Conditions</strong>:
- Calculation time per material: 8 hours (48-core parallelism)
- Manual work: 20 min input preparation, 5 min job submission, 10 min result check
- High-Throughput: Input preparation is automatic, 100 materials can be submitted simultaneously</p>
<details>
<summary>Hint</summary>

Manual case:
- Calculation time: 8 hours √ó 100 = 800 hours (serial)
- Manual work: 35 min √ó 100 = 3,500 min = 58.3 hours

High-Throughput:
- Calculation time: 8 hours (parallel)
- Manual work: 1 hour (workflow setup only)

</details>
<details>
<summary>Solution</summary>

**Traditional method**:
- Total time = Calculation time (serial) + Manual work
- = 800 hours + 58.3 hours
- = **858.3 hours ‚âà 36 days**

**High-Throughput**:
- Total time = Calculation time (parallel) + Initial setup
- = 8 hours + 1 hour
- = **9 hours**

**Efficiency improvement**: 858.3 / 9 ‚âà **95x**

</details>
<h3>Exercise 2 (Difficulty: medium)</h3>
<p><strong>Problem</strong>: Estimate the CPU time required to calculate Materials Project's 140,000 materials.</p>
<p><strong>Conditions</strong>:
- Average per material: 8 hours structure optimization + 2 hours static calculation + 2 hours band structure = 12 hours
- Cores used: Average 48 cores/material</p>
<details>
<summary>Hint</summary>

Total CPU time = Number of materials √ó CPU time per material
CPU time per material = Calculation time √ó Number of cores

</details>
<details>
<summary>Solution</summary>

**Calculation**:

<pre><code>Total CPU time = 140,000 materials √ó 12 hours √ó 48 cores
         = 140,000 √ó 576 CPU hours
         = 80,640,000 CPU hours
         ‚âà 80.64 million CPU hours
</code></pre>


**Real-time conversion** (using 1,000 nodes, each with 48 cores):

<pre><code>Real time = 80.64M CPU hours / (1,000 nodes √ó 48 cores)
      = 1,680 hours
      ‚âà 70 days
</code></pre>


Materials Project has actually accumulated over 10+ years, averaging about 14,000 materials per year.

</details>
<h3>Exercise 3 (Difficulty: hard)</h3>
<p><strong>Problem</strong>: Propose a High-Throughput Computing system design.</p>
<p><strong>Scenario</strong>:
- Goal: Screen 10,000 materials (within 6 months)
- Budget: $2,000,000
- Calculation content: Structure optimization + static calculation (12 hours per material, 48 cores)</p>
<p><strong>Items to propose</strong>:
1. Required computational resources (number of nodes, cores)
2. Workflow design (tool selection)
3. Data management strategy
4. Cost estimate</p>
<details>
<summary>Hint</summary>

1. Calculate total CPU time
2. Determine required parallelism to complete in 6 months
3. Compare cloud vs on-premise
4. Consider tools like FireWorks

</details>
<details>
<summary>Solution</summary>

**1. Required resources**

Total CPU time:

<pre><code>10,000 materials √ó 12 hours √ó 48 cores = 5,760,000 CPU hours
</code></pre>


To complete in 6 months (180 days, 24 hours operation):

<pre><code>Required cores = 5,760,000 / (180 days √ó 24 hours)
          = 1,333 cores
          ‚âà 28 nodes (48 cores/node)
</code></pre>


**2. Workflow design**

- **Tool**: FireWorks + Atomate
  - Reason: Materials Project track record, VASP integration
- **Job Scheduler**: SLURM
- **Database**: MongoDB

**3. Data management**

- Calculation data: 100MB per material ‚Üí 1TB total
- Database: Metadata 10GB
- Backup: 2TB (redundancy)

**4. Cost estimate**

**Option A: Cloud (AWS)**

| Item | Unit price | Quantity | Total |
|----|------|-----|------|
| EC2 (c5.12xlarge, 48 cores) | $2.04/hour | 28 nodes √ó 4,320 hours | $247,000 |
| Storage (EBS) | $0.10/GB/month | 2TB √ó 6 months | $1,200 |
| Data transfer | $0.09/GB | 500GB | $45 |
| **Total** | | | **$248,245** |

**Option B: On-premise HPC usage**

| Item | Unit price | Quantity | Total |
|----|------|-----|------|
| HPC usage fee | $0.1/CPU hour | 5.76M CPU hours | $576,000 |
| Personnel costs | $80,000/year | 1 person √ó 0.5 year | $40,000 |
| Software | $300,000/year | 0.5 year | $150,000 |
| **Total** | | | **$766,000** |

**Recommendation**: Option B (On-premise HPC)
- Within budget ($2,000,000)
- Utilize university HPC clusters
- Invest surplus budget in experimental validation

</details>
<hr/>
<h2>1.8 Summary</h2>
<p>In this chapter, we learned about the necessity of High-Throughput Computing and principles of workflow design.</p>
<p><strong>Key points</strong>:</p>
<ol>
<li><strong>Vastness of exploration space</strong>: 10¬π¬≤-10‚Å∂‚Å∞ combinations</li>
<li><strong>Four elements</strong>: Automation, parallelization, standardization, data management</li>
<li><strong>Success stories</strong>: Materials Project (140k materials), AFLOW (3.5M structures)</li>
<li><strong>Design principles</strong>: Modularity, error handling, reproducibility, scalability</li>
<li><strong>Cost reduction</strong>: 89% cost reduction, 75% time reduction</li>
</ol>
<p><strong>Next steps</strong>:</p>
<p>In Chapter 2, we will practice <strong>DFT calculation automation</strong> using ASE and pymatgen. We will learn automatic generation of VASP and Quantum ESPRESSO input files, error detection and restart, and automated result analysis.</p>
<p><strong><a href="./chapter-2.html">Chapter 2: DFT Calculation Automation ‚Üí</a></strong></p>
<hr/>
<hr/>
<h2>Data License and Citation</h2>
<h3>Datasets Used</h3>
<p>License information for databases mentioned in this chapter:</p>
<table>
<thead>
<tr>
<th>Database</th>
<th>License</th>
<th>Citation requirements</th>
<th>Access</th>
</tr>
</thead>
<tbody>
<tr>
<td>Materials Project</td>
<td>CC BY 4.0</td>
<td>Paper citation required</td>
<td>https://materialsproject.org</td>
</tr>
<tr>
<td>AFLOW</td>
<td>Open data</td>
<td>Paper citation recommended</td>
<td>http://aflowlib.org</td>
</tr>
<tr>
<td>OQMD</td>
<td>Open data</td>
<td>Paper citation recommended</td>
<td>http://oqmd.org</td>
</tr>
<tr>
<td>JARVIS</td>
<td>NIST Public Data</td>
<td>Paper citation recommended</td>
<td>https://jarvis.nist.gov</td>
</tr>
</tbody>
</table>
<p><strong>Notes on data usage</strong>:
- Always cite original papers when writing papers
- Check terms of use for each database for commercial use
- Original license applies to data redistribution</p>
<h3>Citation Methods</h3>
<p><strong>When using Materials Project</strong>:</p>
<pre><code>Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D., Dacek, S., ... &amp; Persson, K. A. (2013).
Commentary: The Materials Project: A materials genome approach to accelerating materials innovation.
APL materials, 1(1), 011002.
</code></pre>
<p><strong>To cite this chapter</strong>:</p>
<pre><code>Hashimoto, Y. (2025). "The Need for High-Throughput Computing and Workflow Design"
High-Throughput Computing Introduction Series Chapter 1. Materials Informatics Dojo Project.
</code></pre>
<hr/>
<h2>Practical Pitfalls</h2>
<p>Common problems and solutions when implementing High-Throughput Computing:</p>
<h3>Pitfall 1: Over-allocation of computational resources</h3>
<p><strong>Problem</strong>: Allocating maximum resources (48 cores, 24 hours) to all materials</p>
<p><strong>Symptoms</strong>:
- Small structures (few atoms) still use 48 cores
- Actual calculation takes 1 hour but 24 hours reserved
- Resource waste, increased wait times</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python">def estimate_resources(structure):
    """
    Estimate appropriate resources based on structure size
    """
    n_atoms = len(structure)

    if n_atoms &lt; 10:
        return {'cores': 12, 'time': '4:00:00'}
    elif n_atoms &lt; 50:
        return {'cores': 24, 'time': '12:00:00'}
    else:
        return {'cores': 48, 'time': '24:00:00'}

# Use in SLURM script generation
resources = estimate_resources(structure)
</code></pre>
<p><strong>Lesson</strong>: Dynamically adjust resources according to structure size and k-point count</p>
<h3>Pitfall 2: Neglecting error logs</h3>
<p><strong>Problem</strong>: Not noticing that 20 out of 100 materials have failed</p>
<p><strong>Symptoms</strong>:
- Only checking completion status, not verifying errors
- Treating unconverged calculations as "complete"
- Panic when noticed during paper writing</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python">def comprehensive_check(directory):
    """
    Multi-faceted calculation health check
    """
    checks = {
        'file_exists': os.path.exists(f"{directory}/OUTCAR"),
        'converged': False,
        'energy_reasonable': False,
        'forces_converged': False
    }

    if checks['file_exists']:
        with open(f"{directory}/OUTCAR", 'r') as f:
            content = f.read()

            # Convergence check
            checks['converged'] = 'reached required accuracy' in content

            # Energy check (detect abnormal values)
            energy = extract_energy(content)
            checks['energy_reasonable'] = -100 &lt; energy &lt; 0  # eV/atom

            # Force convergence check
            max_force = extract_max_force(content)
            checks['forces_converged'] = max_force &lt; 0.05  # eV/√Ö

    # Only successful if all True
    return all(checks.values()), checks

# Usage example
success, details = comprehensive_check('calculations/LiCoO2')
if not success:
    print(f"Error details: {details}")
</code></pre>
<p><strong>Lesson</strong>: Enforce quality control with automated checking scripts</p>
<h3>Pitfall 3: File system limits</h3>
<p><strong>Problem</strong>: 10,000 materials √ó 50 files each = 500,000 files make file system slow</p>
<p><strong>Symptoms</strong>:
- <code>ls</code> command takes several minutes
- File deletion takes hours
- Backup failures</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Bad example
calculations/
  ‚îú‚îÄ‚îÄ material_0001/
  ‚îú‚îÄ‚îÄ material_0002/
  ...
  ‚îî‚îÄ‚îÄ material_10000/  # 10,000 directories at same level

# Good example (hierarchical)
calculations/
  ‚îú‚îÄ‚îÄ 00/
  ‚îÇ   ‚îú‚îÄ‚îÄ 00/material_0000/
  ‚îÇ   ‚îú‚îÄ‚îÄ 01/material_0001/
  ‚îÇ   ...
  ‚îÇ   ‚îî‚îÄ‚îÄ 99/material_0099/
  ‚îú‚îÄ‚îÄ 01/
  ‚îÇ   ‚îú‚îÄ‚îÄ 00/material_0100/
  ...
</code></pre>
<pre><code class="language-python">def get_hierarchical_path(material_id):
    """
    Generate hierarchical path
    """
    # material_id = 1234 ‚Üí calculations/12/34/material_1234
    id_str = f"{material_id:06d}"
    level1 = id_str[:2]
    level2 = id_str[2:4]

    path = f"calculations/{level1}/{level2}/material_{id_str}"
    os.makedirs(path, exist_ok=True)

    return path
</code></pre>
<p><strong>Lesson</strong>: Use hierarchical directory structures for large-scale calculations</p>
<h3>Pitfall 4: Network file system overload</h3>
<p><strong>Problem</strong>: All nodes writing to NFS simultaneously, causing I/O bottleneck</p>
<p><strong>Symptoms</strong>:
- Calculations complete but waiting to write results
- Frequent file system errors
- Parallel efficiency below 20%</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Utilize local disk (fast)
#!/bin/bash
#SBATCH ...

# Use local scratch directory
LOCAL_SCRATCH=/scratch/job_${SLURM_JOB_ID}
mkdir -p $LOCAL_SCRATCH

# Calculate locally
cd $LOCAL_SCRATCH
cp $SLURM_SUBMIT_DIR/INCAR .
cp $SLURM_SUBMIT_DIR/POSCAR .
cp $SLURM_SUBMIT_DIR/KPOINTS .

mpirun -np 48 vasp_std

# Copy only results to NFS after completion
cp OUTCAR CONTCAR vasprun.xml $SLURM_SUBMIT_DIR/
</code></pre>
<p><strong>Lesson</strong>: Calculate on local storage, copy only results to shared storage</p>
<h3>Pitfall 5: Missing dependency records</h3>
<p><strong>Problem</strong>: Cannot reproduce environment when trying 6 months later</p>
<p><strong>Symptoms</strong>:
- "It worked back then..."
- Library versions unknown
- Cannot recall calculation settings</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python">import json
from datetime import datetime
import subprocess

def record_environment():
    """
    Complete environment recording
    """
    env_record = {
        'timestamp': datetime.now().isoformat(),
        'python_version': subprocess.check_output(['python', '--version']).decode(),
        'packages': subprocess.check_output(['pip', 'freeze']).decode().split('\n'),
        'hostname': subprocess.check_output(['hostname']).decode().strip(),
        'slurm_version': subprocess.check_output(['sinfo', '--version']).decode(),
        'git_commit': subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip(),
    }

    with open('environment_snapshot.json', 'w') as f:
        json.dump(env_record, f, indent=2)

    return env_record

# Record at calculation start
record_environment()
</code></pre>
<p><strong>Lesson</strong>: Record environment snapshots for all calculations</p>
<hr/>
<h2>Quality Checklist</h2>
<p>Items to check before calculation start and after completion:</p>
<h3>Pre-calculation Checklist</h3>
<p><strong>Project Planning</strong>
- [ ] Estimated total CPU time (number of materials √ó time per material)
- [ ] Checked budget (if cloud)
- [ ] Calculated data storage capacity (assuming 100MB per material)
- [ ] Set completion date</p>
<p><strong>Workflow Design</strong>
- [ ] Implemented error handling
- [ ] Added automatic restart functionality
- [ ] Prepared progress monitoring script
- [ ] Adopted hierarchical directory structure</p>
<p><strong>Calculation Settings</strong>
- [ ] Explicitly stated convergence criteria (EDIFF, EDIFFG, etc.)
- [ ] Unified k-point density
- [ ] Determined energy cutoff
- [ ] Managed calculation settings with Git</p>
<p><strong>Reproducibility</strong>
- [ ] Recorded software version
- [ ] Prepared environment setup script
- [ ] Created README.md
- [ ] Version controlled input file generation scripts</p>
<h3>Post-calculation Checklist</h3>
<p><strong>Quality Control</strong>
- [ ] Verified all calculations converged
- [ ] Confirmed energies in reasonable range (-100 ~ 0 eV/atom)
- [ ] Detected and investigated outliers
- [ ] Checked error logs for failed calculations</p>
<p><strong>Data Management</strong>
- [ ] Saved results to database
- [ ] Backed up raw data (minimum 2 locations)
- [ ] Recorded metadata (date/time, settings)
- [ ] Made data searchable (JSON, MongoDB, etc.)</p>
<p><strong>Documentation</strong>
- [ ] Documented calculation conditions
- [ ] Recorded failure causes and countermeasures
- [ ] Described reproduction procedures in README
- [ ] Created result summary</p>
<p><strong>Sharing and Publishing</strong>
- [ ] Prepared data for publication on NOMAD, etc.
- [ ] Created figures and tables for papers
- [ ] Published code on GitHub (if possible)
- [ ] Obtained DOI (when publishing dataset)</p>
<hr/>
<h2>Code Reproducibility Specifications</h2>
<p>Environment required to reproduce code examples in this chapter:</p>
<h3>Software Versions</h3>
<pre><code class="language-bash"># Python environment
Python 3.10+
numpy==1.24.0
scipy==1.10.0
matplotlib==3.7.0
pandas==2.0.0

# DFT calculation codes (either)
VASP 6.3.0 or higher (commercial license)
Quantum ESPRESSO 7.0 or higher (open source)

# Job scheduler
SLURM 22.05 or higher
or PBS Pro 2021+
</code></pre>
<h3>Verified Environments</h3>
<p><strong>On-premise HPC</strong>:
- TSUBAME3.0 (Tokyo Institute of Technology)
- Fugaku (RIKEN)
- University clusters (general SLURM systems)</p>
<p><strong>Cloud HPC</strong>:
- AWS Parallel Cluster 3.6.0
- Google Cloud HPC Toolkit 1.25.0</p>
<h3>Installation Script</h3>
<pre><code class="language-bash"># conda environment setup
conda create -n htc-env python=3.10
conda activate htc-env

# Essential packages
pip install numpy scipy matplotlib pandas
pip install ase pymatgen

# Optional (workflow management)
pip install fireworks atomate
</code></pre>
<h3>Troubleshooting</h3>
<p><strong>Issue 1</strong>: <code>ImportError: No module named 'ase'</code></p>
<pre><code class="language-bash"># Solution
pip install ase
# or
conda install -c conda-forge ase
</code></pre>
<p><strong>Issue 2</strong>: VASP not found</p>
<pre><code class="language-bash"># Solution: Add VASP executable to PATH
export PATH=/path/to/vasp/bin:$PATH
# or add to .bashrc
echo 'export PATH=/path/to/vasp/bin:$PATH' &gt;&gt; ~/.bashrc
</code></pre>
<p><strong>Issue 3</strong>: SLURM commands unavailable</p>
<pre><code class="language-bash"># Solution: Login to HPC system
ssh username@hpc.university.edu
# SLURM commands cannot be used on local PC
</code></pre>
<hr/>
<h2>References</h2>
<h3>Essential References (Cited in this chapter)</h3>
<ol>
<li>
<p><strong>Materials Project</strong>
   Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D., Dacek, S., ... &amp; Persson, K. A. (2013).
   "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation."
   <em>APL Materials</em>, 1(1), 011002.
   DOI: <a href="https://doi.org/10.1063/1.4812323">10.1063/1.4812323</a></p>
</li>
<li>
<p><strong>AFLOW</strong>
   Curtarolo, S., Setyawan, W., Hart, G. L., Jahnatek, M., Chepulskii, R. V., Taylor, R. H., ... &amp; Levy, O. (2012).
   "AFLOW: An automatic framework for high-throughput materials discovery."
   <em>Computational Materials Science</em>, 58, 218-226.
   DOI: <a href="https://doi.org/10.1016/j.commatsci.2012.02.005">10.1016/j.commatsci.2012.02.005</a></p>
</li>
<li>
<p><strong>OQMD</strong>
   Saal, J. E., Kirklin, S., Aykol, M., Meredig, B., &amp; Wolverton, C. (2013).
   "Materials design and discovery with high-throughput density functional theory: the open quantum materials database (OQMD)."
   <em>JOM</em>, 65(11), 1501-1509.
   DOI: <a href="https://doi.org/10.1007/s11837-013-0755-4">10.1007/s11837-013-0755-4</a></p>
</li>
<li>
<p><strong>JARVIS</strong>
   Choudhary, K., Garrity, K. F., Reid, A. C., DeCost, B., Biacchi, A. J., Hight Walker, A. R., ... &amp; Tavazza, F. (2020).
   "The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design."
   <em>npj Computational Materials</em>, 6(1), 173.
   DOI: <a href="https://doi.org/10.1038/s41524-020-00440-1">10.1038/s41524-020-00440-1</a></p>
</li>
<li>
<p><strong>Materials Genome Initiative</strong>
   Materials Genome Initiative for Global Competitiveness (2011).
   Office of Science and Technology Policy, USA.
   URL: https://www.mgi.gov/</p>
</li>
</ol>
<h3>Recommended References (Advanced Learning)</h3>
<ol start="6">
<li>
<p><strong>Theory of High-Throughput Computing</strong>
   Hautier, G., Jain, A., &amp; Ong, S. P. (2012).
   "From the computer to the laboratory: materials discovery and design using first-principles calculations."
   <em>Journal of Materials Science</em>, 47(21), 7317-7340.</p>
</li>
<li>
<p><strong>Workflow Design Practice</strong>
   Mathew, K., Montoya, J. H., Faghaninia, A., Dwarakanath, S., Aykol, M., Tang, H., ... &amp; Persson, K. A. (2017).
   "Atomate: A high-level interface to generate, execute, and analyze computational materials science workflows."
   <em>Computational Materials Science</em>, 139, 140-152.</p>
</li>
<li>
<p><strong>Parallel Computing Optimization</strong>
   Gropp, W., Lusk, E., &amp; Skjellum, A. (2014).
   <em>Using MPI: portable parallel programming with the message-passing interface.</em> MIT press.</p>
</li>
</ol>
<h3>Online Resources</h3>
<ul>
<li><strong>Materials Project Documentation</strong>: https://docs.materialsproject.org/</li>
<li><strong>AFLOW Tutorial</strong>: http://aflowlib.org/tutorial/</li>
<li><strong>OQMD API</strong>: http://oqmd.org/documentation/</li>
<li><strong>SLURM Documentation</strong>: https://slurm.schedmd.com/documentation.html</li>
</ul>
<hr/>
<h2>Next Steps</h2>
<p>In Chapter 2, we will practice <strong>DFT calculation automation</strong> using ASE and pymatgen.</p>
<p><strong><a href="./chapter-2.html">Chapter 2: DFT Calculation Automation ‚Üí</a></strong></p>
<hr/>
<p><strong>License</strong>: CC BY 4.0
<strong>Creation date</strong>: 2025-10-17
<strong>Last updated</strong>: 2025-10-19
<strong>Author</strong>: Dr. Yusuke Hashimoto, Tohoku University
<strong>Version</strong>: 1.1</p><div class="navigation">
<a class="nav-button" href="index.html">Back to series index</a>
<a class="nav-button" href="chapter-2.html">Next chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>Copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Creation date</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
