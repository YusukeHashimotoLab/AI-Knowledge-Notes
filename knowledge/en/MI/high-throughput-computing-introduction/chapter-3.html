<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 3: Job Scheduling and Parallelization (SLURM, PBS) - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/high-throughput-computing-introduction/index.html">High Throughput Computing</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 3: Job Scheduling and Parallelization (SLURM, PBS)</h1>
<p class="subtitle"></p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 min</span>
<span class="meta-item">üìä Difficulty: Advanced</span>
<span class="meta-item">üíª Code Examples: 5-6</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 3: Job Scheduling and Parallelization (SLURM, PBS)</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Understand dependency management and reproducibility design in FireWorks/AiiDA. Also cover essential logging and observability concepts.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Visualize the overall picture with DAG (flowcharts). Determining where to save intermediate artifacts speeds up recovery.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master the following:</p>
<ul>
<li>‚úÖ Create SLURM scripts and submit jobs</li>
<li>‚úÖ Evaluate MPI parallel computing efficiency</li>
<li>‚úÖ Write job management scripts in Python</li>
<li>‚úÖ Design parallel computations for 1000-material scale</li>
<li>‚úÖ Perform tuning through benchmarking</li>
</ul>
<hr/>
<h2>3.1 Job Scheduler Fundamentals</h2>
<h3>SLURM vs PBS vs Torque</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>SLURM</th>
<th>PBS Pro</th>
<th>Torque</th>
</tr>
</thead>
<tbody>
<tr>
<td>Developer</td>
<td>SchedMD</td>
<td>Altair</td>
<td>Adaptive Computing</td>
</tr>
<tr>
<td>License</td>
<td>GPL (partial commercial)</td>
<td>Commercial</td>
<td>Open Source</td>
</tr>
<tr>
<td>Adoption</td>
<td>TSUBAME, many TOP500</td>
<td>NASA, DOE national labs</td>
<td>Many universities</td>
</tr>
<tr>
<td>Commands</td>
<td><code>sbatch</code>, <code>squeue</code></td>
<td><code>qsub</code>, <code>qstat</code></td>
<td><code>qsub</code>, <code>qstat</code></td>
</tr>
<tr>
<td>Recommended Use</td>
<td>Large-scale HPC</td>
<td>Enterprise</td>
<td>Small to medium HPC</td>
</tr>
</tbody>
</table>
<h3>SLURM Basic Concepts</h3>
<div class="mermaid">
flowchart TD
    A[User] --&gt;|sbatch| B[Job Queue]
    B --&gt; C{Scheduler}
    C --&gt;|Resource Allocation| D[Compute Node 1]
    C --&gt;|Resource Allocation| E[Compute Node 2]
    C --&gt;|Resource Allocation| F[Compute Node N]

    D --&gt; G[Job Completion]
    E --&gt; G
    F --&gt; G

    style C fill:#4ecdc4
</div>
<p><strong>Key Commands</strong>:</p>
<pre><code class="language-bash"># Submit job
sbatch job.sh

# Check job status
squeue -u username

# Cancel job
scancel job_id

# Node information
sinfo

# Job details
scontrol show job job_id
</code></pre>
<hr/>
<h2>3.2 Creating SLURM Scripts</h2>
<h3>Basic SLURM Script</h3>
<pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=vasp_relax       # Job name
#SBATCH --output=slurm-%j.out       # Standard output (%j=job ID)
#SBATCH --error=slurm-%j.err        # Standard error
#SBATCH --nodes=1                   # Number of nodes
#SBATCH --ntasks-per-node=48        # MPI processes per node
#SBATCH --cpus-per-task=1           # Threads per task
#SBATCH --time=24:00:00             # Time limit (24 hours)
#SBATCH --partition=standard        # Partition (queue)
#SBATCH --account=project_name      # Project name

# Environment setup
module purge
module load intel/2021.2
module load vasp/6.3.0

# Working directory
cd $SLURM_SUBMIT_DIR

# Run VASP
echo "Job started: $(date)"
echo "Host: $(hostname)"
echo "Number of nodes: $SLURM_JOB_NUM_NODES"
echo "Number of processes: $SLURM_NTASKS"

mpirun -np $SLURM_NTASKS vasp_std

echo "Job finished: $(date)"
</code></pre>
<h3>Parallel Execution with Array Jobs</h3>
<p><strong>Parallel computation of 100 materials</strong>:</p>
<pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=vasp_array
#SBATCH --output=logs/slurm-%A_%a.out  # %A=array job ID, %a=task ID
#SBATCH --error=logs/slurm-%A_%a.err
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=48
#SBATCH --time=24:00:00
#SBATCH --array=1-100%10              # Tasks 1-100, max 10 concurrent

# Environment setup
module load vasp/6.3.0

# Load material list
MATERIAL_LIST="materials.txt"
MATERIAL=$(sed -n "${SLURM_ARRAY_TASK_ID}p" $MATERIAL_LIST)

echo "Processing: TaskID=${SLURM\_ARRAY\_TASK\_ID}, Material=${MATERIAL}"

# Working directory
WORK_DIR="calculations/${MATERIAL}"
cd $WORK_DIR

# Run VASP
mpirun -np 48 vasp_std

# Convergence check
if grep -q "reached required accuracy" OUTCAR; then
    echo "SUCCESS: ${MATERIAL}" &gt;&gt; ../completed.log
else
    echo "FAILED: ${MATERIAL}" &gt;&gt; ../failed.log
fi
</code></pre>
<p><strong>Example materials.txt</strong>:</p>
<pre><code>LiCoO2
LiNiO2
LiMnO2
LiFePO4
...(100 lines)
</code></pre>
<h3>Job Chains with Dependencies</h3>
<pre><code class="language-bash"># Step 1: Structure relaxation
JOB1=$(sbatch --parsable relax.sh)
echo "Relaxation job ID: $JOB1"

# Step 2: Static calculation (run after relaxation)
JOB2=$(sbatch --parsable --dependency=afterok:$JOB1 static.sh)
echo "Static calculation job ID: $JOB2"

# Step 3: Band structure (run after static calculation)
JOB3=$(sbatch --parsable --dependency=afterok:$JOB2 band.sh)
echo "Band structure job ID: $JOB3"

# Step 4: Data analysis (run after all complete)
sbatch --dependency=afterok:$JOB3 analysis.sh
</code></pre>
<hr/>
<h2>3.3 MPI Parallel Computing</h2>
<h3>Types of Parallelization</h3>
<pre><code class="language-python"># 1. Task Parallelism (Recommended: High-throughput computing)
# Compute 100 materials simultaneously on 100 nodes
# Scaling efficiency: 100%

# 2. Data Parallelism (VASP: KPAR setting)
# Divide k-points into 4 groups
INCAR: KPAR = 4
# Scaling efficiency: 80-90%

# 3. MPI Parallelism (inter-process communication)
# Distribute 1 calculation across 48 cores
mpirun -np 48 vasp_std
# Scaling efficiency: 50-70%
</code></pre>
<h3>Measuring Scaling Efficiency</h3>
<pre><code class="language-python">import subprocess
import time
import numpy as np
import matplotlib.pyplot as plt

def benchmark_scaling(structure, core_counts=[1, 2, 4, 8, 16, 32, 48]):
    """
    Benchmark parallelization efficiency

    Parameters:
    -----------
    structure : str
        Test structure
    core_counts : list
        Core counts to test

    Returns:
    --------
    efficiency : dict
        Efficiency for each core count
    """
    timings = {}

    for n_cores in core_counts:
        # Run VASP
        start = time.time()

        result = subprocess.run(
            [f"mpirun -np {n_cores} vasp_std"],
            shell=True,
            cwd=f"benchmark_{n_cores}cores"
        )

        elapsed = time.time() - start
        timings[n_cores] = elapsed

        print(f"{n_cores} cores: {elapsed:.1f} seconds")

    # Calculate efficiency
    base_time = timings[1]
    efficiency = {}

    for n_cores, t in timings.items():
        ideal_time = base_time / n_cores
        actual_speedup = base_time / t
        ideal_speedup = n_cores

        eff = (actual_speedup / ideal_speedup) * 100
        efficiency[n_cores] = eff

    return efficiency, timings

# Plot results
def plot_scaling(efficiency, timings):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    cores = list(timings.keys())
    times = list(timings.values())
    effs = [efficiency[c] for c in cores]

    # Speedup
    ax1.plot(cores, [timings[1]/t for t in times], 'o-', label='Actual')
    ax1.plot(cores, cores, '--', label='Ideal (linear)')
    ax1.set_xlabel('Number of Cores')
    ax1.set_ylabel('Speedup')
    ax1.set_xscale('log', base=2)
    ax1.set_yscale('log', base=2)
    ax1.legend()
    ax1.grid(True)

    # Efficiency
    ax2.plot(cores, effs, 'o-', color='green')
    ax2.axhline(y=80, color='red', linestyle='--', label='80% target')
    ax2.set_xlabel('Number of Cores')
    ax2.set_ylabel('Parallelization Efficiency (%)')
    ax2.set_xscale('log', base=2)
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('scaling_benchmark.png', dpi=300)
    plt.show()
</code></pre>
<h3>Optimizing VASP Parallelization Parameters</h3>
<pre><code class="language-python">def optimize_vasp_parallelization(n_kpoints, n_bands, n_cores=48):
    """
    Optimize VASP parallelization parameters

    Parameters:
    -----------
    n_kpoints : int
        Number of k-points
    n_bands : int
        Number of bands
    n_cores : int
        Available cores

    Returns:
    --------
    params : dict
        Optimal parameters
    """
    # KPAR: k-point parallelism (most efficient)
    # Power of 2, divisor of k-point count
    kpar_options = [1, 2, 4, 8, 16]
    valid_kpar = [k for k in kpar_options if n_kpoints % k == 0 and k &lt;= n_cores]

    # NCORE: band parallelism
    # Typically 4-8 is optimal
    ncore = min(4, n_cores // max(valid_kpar))

    # Recommended settings
    recommended = {
        'KPAR': max(valid_kpar),
        'NCORE': ncore,
        'cores_per_kpar_group': n_cores // max(valid_kpar),
    }

    print("Recommended parallelization parameters:")
    print(f"  KPAR = {recommended['KPAR']} (k-point parallelism)")
    print(f"  NCORE = {recommended['NCORE']} (band parallelism)")
    print(f"  {recommended['cores_per_kpar_group']} cores per KPAR group")

    return recommended

# Usage example
params = optimize_vasp_parallelization(n_kpoints=64, n_bands=200, n_cores=48)
# Recommended:
#   KPAR = 16 (k-point parallelism)
#   NCORE = 3 (band parallelism)
#   3 cores per KPAR group
</code></pre>
<hr/>
<h2>3.4 Job Management with Python</h2>
<h3>Job Submission and Monitoring</h3>
<pre><code class="language-python">import subprocess
import time
import re

class SLURMJobManager:
    """SLURM job management class"""

    def submit_job(self, script_path):
        """
        Submit a job

        Returns:
        --------
        job_id : int
            Job ID
        """
        result = subprocess.run(
            ['sbatch', script_path],
            capture_output=True,
            text=True
        )

        # Extract ID from "Submitted batch job 12345"
        match = re.search(r'(\d+)', result.stdout)
        if match:
            job_id = int(match.group(1))
            print(f"Job submitted: ID={job_id}")
            return job_id
        else:
            raise RuntimeError(f"Job submission failed: {result.stderr}")

    def get_job_status(self, job_id):
        """
        Get job status

        Returns:
        --------
        status : str
            'PENDING', 'RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED'
        """
        result = subprocess.run(
            ['squeue', '-j', str(job_id), '-h', '-o', '%T'],
            capture_output=True,
            text=True
        )

        if result.stdout.strip():
            return result.stdout.strip()
        else:
            # Not in queue ‚Üí completed or failed
            result = subprocess.run(
                ['sacct', '-j', str(job_id), '-X', '-n', '-o', 'State'],
                capture_output=True,
                text=True
            )
            return result.stdout.strip()

    def wait_for_completion(self, job_id, check_interval=60):
        """
        Wait for job completion

        Parameters:
        -----------
        job_id : int
            Job ID
        check_interval : int
            Check interval (seconds)
        """
        while True:
            status = self.get_job_status(job_id)

            if status in ['COMPLETED', 'FAILED', 'CANCELLED']:
                print(f"Job {job_id}: {status}")
                return status

            print(f"Job {job_id}: {status}...waiting")
            time.sleep(check_interval)

    def submit_array_job(self, script_path, n_tasks, max_concurrent=10):
        """
        Submit array job

        Parameters:
        -----------
        n_tasks : int
            Number of tasks
        max_concurrent : int
            Maximum concurrent tasks
        """
        result = subprocess.run(
            ['sbatch', f'--array=1-{n_tasks}%{max_concurrent}', script_path],
            capture_output=True,
            text=True
        )

        match = re.search(r'(\d+)', result.stdout)
        if match:
            job_id = int(match.group(1))
            print(f"Array job submitted: ID={job_id}, Tasks={n_tasks}")
            return job_id
        else:
            raise RuntimeError(f"Submission failed: {result.stderr}")

# Usage example
manager = SLURMJobManager()

# Single job
job_id = manager.submit_job('relax.sh')
status = manager.wait_for_completion(job_id)

# Array job
array_id = manager.submit_array_job('array_job.sh', n_tasks=100, max_concurrent=20)
</code></pre>
<h3>Large-Scale Job Management (1000 Materials)</h3>
<pre><code class="language-python">import os
from pathlib import Path
import json

def manage_large_scale_calculation(materials, batch_size=100):
    """
    Efficiently manage 1000 materials

    Parameters:
    -----------
    materials : list
        List of materials
    batch_size : int
        Batch size
    """
    manager = SLURMJobManager()
    n_materials = len(materials)

    print(f"Total materials: {n_materials}")
    print(f"Batch size: {batch_size}")

    # Divide into batches
    n_batches = (n_materials + batch_size - 1) // batch_size
    print(f"Number of batches: {n_batches}")

    job_ids = []

    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, n_materials)
        batch_materials = materials[start_idx:end_idx]

        print(f"\nBatch {batch_idx+1}/{n_batches}")
        print(f"  Materials: {len(batch_materials)}")

        # Create material list for batch
        list_file = f"batch_{batch_idx+1}_materials.txt"
        with open(list_file, 'w') as f:
            for mat in batch_materials:
                f.write(f"{mat}\n")

        # Submit array job
        job_id = manager.submit_array_job(
            'vasp_array.sh',
            n_tasks=len(batch_materials),
            max_concurrent=20
        )

        job_ids.append(job_id)

    # Monitor progress
    print("\nMonitoring progress...")
    completed = 0

    while completed &lt; len(job_ids):
        time.sleep(300)  # Check every 5 minutes

        for i, job_id in enumerate(job_ids):
            status = manager.get_job_status(job_id)

            if status == 'COMPLETED' and i not in completed_jobs:
                completed += 1
                print(f"Batch {i+1} completed ({completed}/{len(job_ids)})")

    print("All batches completed!")

# Usage example
materials = [f"material_{i:04d}" for i in range(1, 1001)]
manage_large_scale_calculation(materials, batch_size=100)
</code></pre>
<hr/>
<h2>3.5 Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p><strong>Problem</strong>: Create a SLURM script with the following conditions:</p>
<ul>
<li>Job name: <code>si_bandgap</code></li>
<li>Number of nodes: 2</li>
<li>Processes per node: 24</li>
<li>Time limit: 12 hours</li>
<li>Partition: <code>gpu</code></li>
</ul>
<details>
<summary>Solution</summary>
<pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=si_bandgap
#SBATCH --output=slurm-%j.out
#SBATCH --error=slurm-%j.err
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=24
#SBATCH --time=12:00:00
#SBATCH --partition=gpu

module load vasp/6.3.0

cd $SLURM_SUBMIT_DIR

mpirun -np 48 vasp_std
</code></pre>
</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p><strong>Problem</strong>: Create an array job for 50 structure relaxation calculations, with 10 running concurrently.</p>
<details>
<summary>Solution</summary>
<pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=relax_array
#SBATCH --output=logs/slurm-%A_%a.out
#SBATCH --error=logs/slurm-%A_%a.err
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=48
#SBATCH --time=24:00:00
#SBATCH --array=1-50%10

module load vasp/6.3.0

MATERIAL=$(sed -n "${SLURM_ARRAY_TASK_ID}p" materials.txt)

cd calculations/${MATERIAL}

mpirun -np 48 vasp_std
</code></pre>
</details>
<h3>Problem 3 (Difficulty: hard)</h3>
<p><strong>Problem</strong>: Create a Python script to manage VASP calculations for 1000 materials. Requirements:</p>
<ol>
<li>Process in batches of 50 materials</li>
<li>20 tasks running concurrently per batch</li>
<li>Log completed and failed tasks</li>
<li>Automatically retry failed tasks</li>
</ol>
<details>
<summary>Solution</summary>
<pre><code class="language-python">import os
import time
import json
from pathlib import Path

class HighThroughputManager:
    def __init__(self, materials, batch_size=50, max_concurrent=20):
        self.materials = materials
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent
        self.manager = SLURMJobManager()

        self.results = {
            'completed': [],
            'failed': [],
            'retry': []
        }

    def run_batch(self, batch_materials, batch_id):
        """Run batch"""
        # Create material list
        list_file = f"batch_{batch_id}.txt"
        with open(list_file, 'w') as f:
            for mat in batch_materials:
                f.write(f"{mat}\n")

        # Submit job
        job_id = self.manager.submit_array_job(
            'vasp_array.sh',
            n_tasks=len(batch_materials),
            max_concurrent=self.max_concurrent
        )

        # Wait for completion
        status = self.manager.wait_for_completion(job_id)

        # Check results
        self.check_results(batch_materials, batch_id)

    def check_results(self, batch_materials, batch_id):
        """Check results"""
        for mat in batch_materials:
            outcar = f"calculations/{mat}/OUTCAR"

            if not os.path.exists(outcar):
                self.results['failed'].append(mat)
                continue

            with open(outcar, 'r') as f:
                content = f.read()
                if 'reached required accuracy' in content:
                    self.results['completed'].append(mat)
                else:
                    self.results['failed'].append(mat)

        # Save log
        with open(f'batch_{batch_id}_results.json', 'w') as f:
            json.dump(self.results, f, indent=2)

    def retry_failed(self, max_retries=2):
        """Retry failed tasks"""
        for retry_count in range(max_retries):
            if not self.results['failed']:
                break

            print(f"\nRetry {retry_count+1}/{max_retries}")
            print(f"  Failed tasks: {len(self.results['failed'])}")

            failed_materials = self.results['failed'].copy()
            self.results['failed'] = []

            self.run_batch(failed_materials, f'retry_{retry_count+1}')

    def execute_all(self):
        """Execute all materials"""
        n_batches = (len(self.materials) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(n_batches):
            start = batch_idx * self.batch_size
            end = min((batch_idx + 1) * self.batch_size, len(self.materials))
            batch_materials = self.materials[start:end]

            print(f"\nBatch {batch_idx+1}/{n_batches}")
            self.run_batch(batch_materials, batch_idx+1)

        # Retry
        self.retry_failed(max_retries=2)

        # Final report
        print("\nFinal results:")
        print(f"  Completed: {len(self.results['completed'])}")
        print(f"  Failed: {len(self.results['failed'])}")

        return self.results

# Execute
materials = [f"material_{i:04d}" for i in range(1, 1001)]
manager = HighThroughputManager(materials, batch_size=50, max_concurrent=20)
results = manager.execute_all()
</code></pre>
</details>
<hr/>
<h2>3.6 Summary</h2>
<p><strong>Key Points</strong>:</p>
<ol>
<li><strong>SLURM</strong>: Widely used job scheduler for large-scale HPC</li>
<li><strong>Array Jobs</strong>: Efficiently execute many materials in parallel</li>
<li><strong>MPI Parallelism</strong>: Measure and optimize scaling efficiency</li>
<li><strong>Python Management</strong>: Automate and monitor large-scale calculations</li>
</ol>
<p><strong>Next Steps</strong>:</p>
<p>In Chapter 4, we will learn about <strong>workflow management with FireWorks and AiiDA</strong>.</p>
<p><strong><a href="chapter-4.html">Chapter 4: Data Management and Post-Processing ‚Üí</a></strong></p>
<hr/>
<p><strong>License</strong>: CC BY 4.0
<strong>Date Created</strong>: 2025-10-17
<strong>Author</strong>: Dr. Yusuke Hashimoto, Tohoku University</p><div class="navigation">
<a class="nav-button" href="chapter-2.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Return to Series Index</a>
<a class="nav-button" href="chapter-4.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantee, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material is subject to change, update, or discontinuation without notice.</li>
<li>The copyright and license of this content shall be subject to the specified terms (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Date Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
