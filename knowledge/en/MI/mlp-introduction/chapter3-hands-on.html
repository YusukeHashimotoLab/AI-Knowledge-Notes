<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="From environment setup to training and MLP-MD" name="description"/>
<title>Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial - MI Knowledge Hub</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Mermaid for diagrams -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/mlp-introduction/index.html">MLP</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/mlp-introduction/chapter3-hands-on.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial</h1>
<div class="meta">
<span>üìñ Reading Time: Unknown</span>
<span>üìä Level: beginner-intermediate</span>
</div>
</div>
</header>
<main class="container">
<h1 id="3pythonmlp-schnetpack">Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Complete the entire training and evaluation workflow with a small dataset. Clarify checkpoints for reproducibility and overfitting prevention.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Supplement:</strong> The basic three-piece set includes seed fixing, data splitting, and learning curve recording. Stabilize with early stopping and weight decay.</p>
<h2 id="_1">Learning Objectives</h2>
<p>By reading this chapter, you will master the following:<br/>
- Install SchNetPack in Python environment and set up the environment<br/>
- Train an MLP model using a small-scale dataset (aspirin molecule from MD17)<br/>
- Evaluate trained model accuracy and verify energy/force prediction errors<br/>
- Execute MLP-MD simulations and analyze trajectories<br/>
- Understand common errors and their solutions</p>
<hr/>
<h2 id="31">3.1 Environment Setup: Installing Required Tools</h2>
<p>To practice MLP, you need to set up a Python environment and SchNetPack.</p>
<h3 id="_2">Required Software</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Version</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>3.9-3.11</td>
<td>Base language</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>2.0+</td>
<td>Deep learning framework</td>
</tr>
<tr>
<td><strong>SchNetPack</strong></td>
<td>2.0+</td>
<td>MLP training and inference</td>
</tr>
<tr>
<td><strong>ASE</strong></td>
<td>3.22+</td>
<td>Atomic structure manipulation, MD execution</td>
</tr>
<tr>
<td><strong>NumPy/Matplotlib</strong></td>
<td>Latest</td>
<td>Data analysis and visualization</td>
</tr>
</tbody>
</table>
<h3 id="_3">Installation Steps</h3>
<p><strong>Step 1: Create Conda Environment</strong></p>
<pre class="codehilite"><code class="language-bash"># Create new Conda environment (Python 3.10)
conda create -n mlp-tutorial python=3.10 -y
conda activate mlp-tutorial
</code></pre>
<p><strong>Step 2: Install PyTorch</strong></p>
<pre class="codehilite"><code class="language-bash"># CPU version (local machine, lightweight)
conda install pytorch cpuonly -c pytorch

# GPU version (if CUDA available)
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>
<p><strong>Step 3: Install SchNetPack and ASE</strong></p>
<pre class="codehilite"><code class="language-bash"># SchNetPack (pip recommended)
pip install schnetpack

# ASE (Atomic Simulation Environment)
pip install ase

# Visualization tools
pip install matplotlib seaborn
</code></pre>
<p><strong>Step 4: Verify Installation</strong></p>
<pre class="codehilite"><code class="language-python"># Example 1: Environment verification script (5 lines)
import torch
import schnetpack as spk
print(f"PyTorch: {torch.__version__}")
print(f"SchNetPack: {spk.__version__}")
print(f"GPU available: {torch.cuda.is_available()}")
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre class="codehilite"><code>PyTorch: 2.1.0
SchNetPack: 2.0.3
GPU available: False  # For CPU
</code></pre>
<hr/>
<h2 id="32-md17">3.2 Data Preparation: Obtaining MD17 Dataset</h2>
<p>SchNetPack includes the <strong>MD17</strong> benchmark dataset for small molecules.</p>
<h3 id="md17">What is MD17 Dataset</h3>
<ul>
<li><strong>Content</strong>: Molecular dynamics trajectories from DFT calculations</li>
<li><strong>Target molecules</strong>: 10 types including aspirin, benzene, ethanol</li>
<li><strong>Data count</strong>: Approximately 100,000 configurations per molecule</li>
<li><strong>Accuracy</strong>: PBE/def2-SVP level (DFT)</li>
<li><strong>Purpose</strong>: Benchmarking MLP methods</li>
</ul>
<h3 id="_4">Downloading and Loading Data</h3>
<p><strong>Example 2: Loading MD17 Dataset (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from schnetpack.datasets import MD17
from schnetpack.data import AtomsDataModule

# Download aspirin molecule dataset (approximately 100,000 configurations)
dataset = MD17(
    datapath='./data',
    molecule='aspirin',
    download=True
)

print(f"Total samples: {len(dataset)}")
print(f"Properties: {dataset.available_properties}")
print(f"First sample: {dataset[0]}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre class="codehilite"><code>Total samples: 211762
Properties: ['energy', 'forces']
First sample: {'_atomic_numbers': tensor([...]), 'energy': tensor(-1234.5), 'forces': tensor([...])}
</code></pre>
<h3 id="_5">Data Splitting</h3>
<p><strong>Example 3: Splitting into Train/Validation/Test Sets (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Split data into train:val:test = 70%:15%:15%
data_module = AtomsDataModule(
    datapath='./data',
    dataset=dataset,
    batch_size=32,
    num_train=100000,      # Number of training data
    num_val=10000,          # Number of validation data
    num_test=10000,         # Number of test data
    split_file='split.npz', # Save split information
)
data_module.prepare_data()
data_module.setup()
</code></pre>
<p><strong>Explanation</strong>:<br/>
- <code>batch_size=32</code>: Process 32 configurations at once (memory efficiency)<br/>
- <code>num_train=100000</code>: Large data improves generalization<br/>
- <code>split_file</code>: Save split to file (ensure reproducibility)</p>
<hr/>
<h2 id="33-schnetpack">3.3 Model Training with SchNetPack</h2>
<p>Train a SchNet model to learn energy and forces.</p>
<h3 id="schnet">SchNet Architecture Configuration</h3>
<p><strong>Example 4: Defining SchNet Model (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import schnetpack.transform as trn
from schnetpack.representation import SchNet
from schnetpack.model import AtomisticModel
from schnetpack.task import ModelOutput

# 1. SchNet representation layer (atomic configuration ‚Üí feature vector)
representation = SchNet(
    n_atom_basis=128,      # Dimension of atomic feature vectors
    n_interactions=6,      # Number of message passing layers
    cutoff=5.0,            # Cutoff radius (√Ö)
    n_filters=128          # Number of filters
)

# 2. Output layer (energy prediction)
output = ModelOutput(
    name='energy',
    loss_fn=torch.nn.MSELoss(),
    metrics={'MAE': spk.metrics.MeanAbsoluteError()}
)
</code></pre>
<p><strong>Parameter Explanation</strong>:<br/>
- <code>n_atom_basis=128</code>: Each atom's feature vector is 128-dimensional (typical value)<br/>
- <code>n_interactions=6</code>: 6 layers of message passing (deeper captures longer-range interactions)<br/>
- <code>cutoff=5.0√Ö</code>: Ignore atomic interactions beyond this distance (computational efficiency)</p>
<h3 id="_6">Running Training</h3>
<p><strong>Example 5: Setting Up Training Loop (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import pytorch_lightning as pl
from schnetpack.task import AtomisticTask

# Define training task
task = AtomisticTask(
    model=AtomisticModel(representation, [output]),
    outputs=[output],
    optimizer_cls=torch.optim.AdamW,
    optimizer_args={'lr': 1e-4}  # Learning rate
)

# Configure Trainer
trainer = pl.Trainer(
    max_epochs=50,               # Maximum 50 epochs
    accelerator='cpu',           # Use CPU (GPU: 'gpu')
    devices=1,
    default_root_dir='./training'
)

# Start training
trainer.fit(task, datamodule=data_module)
</code></pre>
<p><strong>Training Time Estimate</strong>:<br/>
- CPU (4 cores): Approximately 2-3 hours (100,000 configurations)<br/>
- GPU (RTX 3090): Approximately 15-20 minutes</p>
<h3 id="_7">Monitoring Training Progress</h3>
<p><strong>Example 6: Visualization with TensorBoard (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Launch TensorBoard (separate terminal)
# tensorboard --logdir=./training/lightning_logs

# Check logs from Python
import pandas as pd

metrics = pd.read_csv('./training/lightning_logs/version_0/metrics.csv')
print(metrics[['epoch', 'train_loss', 'val_loss']].tail(10))
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre class="codehilite"><code>   epoch  train_loss  val_loss
40    40      0.0023    0.0031
41    41      0.0022    0.0030
42    42      0.0021    0.0029
...
</code></pre>
<p><strong>Observation Points</strong>:<br/>
- Both <code>train_loss</code> and <code>val_loss</code> decreasing ‚Üí Normal learning progress<br/>
- <code>val_loss</code> starts increasing ‚Üí Sign of <strong>overfitting</strong> ‚Üí Consider Early Stopping</p>
<hr/>
<h2 id="34">3.4 Accuracy Verification: Energy and Force Prediction Accuracy</h2>
<p>Evaluate whether the trained model achieves DFT accuracy.</p>
<h3 id="_8">Test Set Evaluation</h3>
<p><strong>Example 7: Test Set Evaluation (12 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Evaluate on test set
test_results = trainer.test(task, datamodule=data_module)

# Display results
print(f"Energy MAE: {test_results[0]['test_energy_MAE']:.4f} eV")
print(f"Energy RMSE: {test_results[0]['test_energy_RMSE']:.4f} eV")

# Force evaluation (requires separate calculation)
from schnetpack.metrics import MeanAbsoluteError
force_mae = MeanAbsoluteError(target='forces')
# ... Force evaluation code
</code></pre>
<p><strong>Good Accuracy Benchmarks</strong> (aspirin molecule, 21 atoms):<br/>
- <strong>Energy MAE</strong>: &lt; 1 kcal/mol (&lt; 0.043 eV)<br/>
- <strong>Force MAE</strong>: &lt; 1 kcal/mol/√Ö (&lt; 0.043 eV/√Ö)</p>
<h3 id="_9">Correlation Plot of Predictions vs True Values</h3>
<p><strong>Example 8: Visualizing Prediction Accuracy (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

# Predict on test data
model = task.model
predictions, targets = [], []

for batch in data_module.test_dataloader():
    pred = model(batch)['energy'].detach().numpy()
    true = batch['energy'].numpy()
    predictions.extend(pred)
    targets.extend(true)

# Scatter plot
plt.scatter(targets, predictions, alpha=0.5, s=1)
plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')
plt.xlabel('DFT Energy (eV)')
plt.ylabel('MLP Predicted Energy (eV)')
plt.title('Energy Prediction Accuracy')
plt.show()
</code></pre>
<p><strong>Ideal Result</strong>:<br/>
- Points concentrate along red diagonal line (y=x)<br/>
- R¬≤ &gt; 0.99 (coefficient of determination)</p>
<hr/>
<h2 id="35-mlp-md">3.5 MLP-MD Simulation: Running Molecular Dynamics</h2>
<p>Execute MD simulations 10‚Å¥ times faster than DFT using trained MLP.</p>
<h3 id="asemlp-md">MLP-MD Setup with ASE</h3>
<p><strong>Example 9: Preparing MLP-MD Calculation (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase import units
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.verlet import VelocityVerlet
import schnetpack.interfaces.ase_interface as spk_ase

# Wrap MLP as ASE Calculator
calculator = spk_ase.SpkCalculator(
    model_file='./training/best_model.ckpt',
    device='cpu'
)

# Prepare initial structure (first configuration from MD17)
atoms = dataset.get_atoms(0)
atoms.calc = calculator
</code></pre>
<h3 id="_10">Initial Velocity Setup and Equilibration</h3>
<p><strong>Example 10: Temperature Initialization (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Set velocity distribution at 300K
temperature = 300  # K
MaxwellBoltzmannDistribution(atoms, temperature_K=temperature)

# Zero momentum (remove overall translation)
from ase.md.velocitydistribution import Stationary
Stationary(atoms)

print(f"Initial kinetic energy: {atoms.get_kinetic_energy():.3f} eV")
print(f"Initial potential energy: {atoms.get_potential_energy():.3f} eV")
</code></pre>
<h3 id="md">Running MD Simulation</h3>
<p><strong>Example 11: MD Execution and Trajectory Saving (12 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io.trajectory import Trajectory

# Configure MD simulator
timestep = 0.5 * units.fs  # 0.5 femtoseconds
dyn = VelocityVerlet(atoms, timestep=timestep)

# Output trajectory file
traj = Trajectory('aspirin_md.traj', 'w', atoms)
dyn.attach(traj.write, interval=10)  # Save every 10 steps

# Run MD for 10,000 steps (5 picoseconds)
dyn.run(10000)
print("MD simulation completed!")
</code></pre>
<p><strong>Computational Time Estimate</strong>:<br/>
- CPU (4 cores): Approximately 5 minutes (10,000 steps)<br/>
- With DFT: Approximately 1 week (10,000 steps)<br/>
- <strong>Achieved 10‚Å¥√ó speedup!</strong></p>
<h3 id="_11">Trajectory Analysis</h3>
<p><strong>Example 12: Energy Conservation and RDF Calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io import read
import numpy as np

# Read trajectory
traj_data = read('aspirin_md.traj', index=':')

# Check energy conservation
energies = [a.get_total_energy() for a in traj_data]
plt.plot(energies)
plt.xlabel('Time step')
plt.ylabel('Total Energy (eV)')
plt.title('Energy Conservation Check')
plt.show()

# Calculate energy drift (monotonic increase/decrease)
drift = (energies[-1] - energies[0]) / len(energies)
print(f"Energy drift: {drift:.6f} eV/step")
</code></pre>
<p><strong>Good Simulation Indicators</strong>:<br/>
- Energy drift: &lt; 0.001 eV/step<br/>
- Total energy oscillates with time (conservation law)</p>
<hr/>
<h2 id="36">3.6 Physical Property Calculations: Vibrational Spectra and Diffusion Coefficients</h2>
<p>Calculate physical properties from MLP-MD.</p>
<h3 id="_12">Vibrational Spectrum (Power Spectrum)</h3>
<p><strong>Example 13: Vibrational Spectrum Calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from scipy.fft import fft, fftfreq

# Extract velocity time series for one atom
atom_idx = 0  # First atom
velocities = np.array([a.get_velocities()[atom_idx] for a in traj_data])

# Fourier transform of x-direction velocity
vx = velocities[:, 0]
freq = fftfreq(len(vx), d=timestep)
spectrum = np.abs(fft(vx))**2

# Plot positive frequencies only
mask = freq &gt; 0
plt.plot(freq[mask] * 1e15 / (2 * np.pi), spectrum[mask])  # Hz ‚Üí THz conversion
plt.xlabel('Frequency (THz)')
plt.ylabel('Power Spectrum')
plt.title('Vibrational Spectrum')
plt.xlim(0, 100)
plt.show()
</code></pre>
<p><strong>Interpretation</strong>:<br/>
- Peaks correspond to molecular vibrational modes<br/>
- Compare with DFT-calculated vibrational spectrum for accuracy verification</p>
<h3 id="msd">Mean Square Displacement (MSD) and Diffusion Coefficient</h3>
<p><strong>Example 14: MSD Calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">def calculate_msd(traj, atom_idx=0):
    """Calculate mean square displacement"""
    positions = np.array([a.positions[atom_idx] for a in traj])
    msd = np.zeros(len(positions))

    for t in range(len(positions)):
        displacement = positions[t:] - positions[:-t or None]
        msd[t] = np.mean(np.sum(displacement**2, axis=1))

    return msd

# Calculate and plot MSD
msd = calculate_msd(traj_data)
time_ps = np.arange(len(msd)) * timestep / units.fs * 1e-3  # picoseconds

plt.plot(time_ps, msd)
plt.xlabel('Time (ps)')
plt.ylabel('MSD (≈≤)')
plt.title('Mean Square Displacement')
plt.show()
</code></pre>
<p><strong>Diffusion Coefficient Calculation</strong>:</p>
<pre class="codehilite"><code class="language-python"># Calculate diffusion coefficient from linear region of MSD (Einstein relation)
# D = lim_{t‚Üí‚àû} MSD(t) / (6t)
linear_region = slice(100, 500)
fit = np.polyfit(time_ps[linear_region], msd[linear_region], deg=1)
D = fit[0] / 6  # ≈≤/ps ‚Üí cm¬≤/s conversion needed
print(f"Diffusion coefficient: {D:.6f} ≈≤/ps")
</code></pre>
<hr/>
<h2 id="37-active-learning">3.7 Active Learning: Efficient Data Addition</h2>
<p>Automatically detect configurations where the model is uncertain and add DFT calculations.</p>
<h3 id="_13">Ensemble Uncertainty Evaluation</h3>
<p><strong>Example 15: Prediction Uncertainty (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Train multiple independent models (omitted: run Example 5 three times)
models = [model1, model2, model3]  # 3 independent models

def predict_with_uncertainty(atoms, models):
    """Ensemble prediction with uncertainty"""
    predictions = []
    for model in models:
        atoms.calc = spk_ase.SpkCalculator(model_file=model, device='cpu')
        predictions.append(atoms.get_potential_energy())

    mean = np.mean(predictions)
    std = np.std(predictions)
    return mean, std

# Evaluate uncertainty for each configuration in MD trajectory
uncertainties = []
for atoms in traj_data[::100]:  # Every 100 frames
    _, std = predict_with_uncertainty(atoms, models)
    uncertainties.append(std)

# Identify configurations with high uncertainty
threshold = np.percentile(uncertainties, 95)
high_uncertainty_frames = np.where(np.array(uncertainties) &gt; threshold)[0]
print(f"High uncertainty frames: {high_uncertainty_frames}")
</code></pre>
<p><strong>Next Steps</strong>:<br/>
- Add configurations with high uncertainty to DFT calculations<br/>
- Update dataset and retrain model<br/>
- Verify accuracy improvement</p>
<hr/>
<h2 id="38">3.8 Troubleshooting: Common Errors and Solutions</h2>
<p>Introduce problems and solutions frequently encountered in practice.</p>
<table>
<thead>
<tr>
<th>Error</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Out of Memory (OOM)</strong></td>
<td>Batch size too large</td>
<td>Reduce <code>batch_size</code> from 32‚Üí16‚Üí8</td>
</tr>
<tr>
<td><strong>Loss becomes NaN</strong></td>
<td>Learning rate too high</td>
<td>Lower <code>lr=1e-4</code>‚Üí<code>1e-5</code></td>
</tr>
<tr>
<td><strong>Energy drift in MD</strong></td>
<td>Timestep too large</td>
<td>Reduce <code>timestep=0.5fs</code>‚Üí<code>0.25fs</code></td>
</tr>
<tr>
<td><strong>Poor generalization</strong></td>
<td>Training data biased</td>
<td>Diversify data with Active Learning</td>
</tr>
<tr>
<td><strong>CUDA error</strong></td>
<td>GPU compatibility issue</td>
<td>Verify PyTorch and CUDA versions</td>
</tr>
</tbody>
</table>
<h3 id="_14">Debugging Best Practices</h3>
<pre class="codehilite"><code class="language-python"># 1. Test with small-scale data
data_module.num_train = 1000  # Quick test with 1,000 configurations

# 2. Check overfitting on 1 batch
trainer = pl.Trainer(max_epochs=100, overfit_batches=1)
# If training error approaches 0, model has learning capacity

# 3. Gradient clipping
task = AtomisticTask(..., gradient_clip_val=1.0)  # Prevent gradient explosion
</code></pre>
<hr/>
<h2 id="39">3.9 Chapter Summary</h2>
<h3 id="_15">What You Learned</h3>
<ol>
<li>
<p><strong>Environment Setup</strong><br/>
   - Installing Conda environment, PyTorch, SchNetPack<br/>
   - Choosing GPU/CPU environment</p>
</li>
<li>
<p><strong>Data Preparation</strong><br/>
   - Downloading and loading MD17 dataset<br/>
   - Splitting into training/validation/test sets</p>
</li>
<li>
<p><strong>Model Training</strong><br/>
   - Configuring SchNet architecture (6 layers, 128 dimensions)<br/>
   - Training for 50 epochs (CPU: 2-3 hours)<br/>
   - Monitoring progress with TensorBoard</p>
</li>
<li>
<p><strong>Accuracy Verification</strong><br/>
   - Verifying Energy MAE &lt; 1 kcal/mol achieved<br/>
   - Correlation plot of predictions vs true values<br/>
   - High accuracy with R¬≤ &gt; 0.99</p>
</li>
<li>
<p><strong>MLP-MD Execution</strong><br/>
   - Integration as ASE Calculator<br/>
   - Running MD for 10,000 steps (5 picoseconds)<br/>
   - Experiencing 10‚Å¥√ó speedup over DFT</p>
</li>
<li>
<p><strong>Physical Property Calculations</strong><br/>
   - Vibrational spectrum (Fourier transform)<br/>
   - Diffusion coefficient (calculated from mean square displacement)</p>
</li>
<li>
<p><strong>Active Learning</strong><br/>
   - Configuration selection by ensemble uncertainty<br/>
   - Automated data addition strategy</p>
</li>
</ol>
<h3 id="_16">Important Points</h3>
<ul>
<li><strong>SchNetPack is easy to implement</strong>: MLP training possible with a few dozen lines of code</li>
<li><strong>Practical accuracy achieved with small data (100,000 configurations)</strong>: MD17 is an excellent benchmark</li>
<li><strong>MLP-MD is practical</strong>: 10‚Å¥√ó faster than DFT, executable on personal PCs</li>
<li><strong>Efficiency with Active Learning</strong>: Automatically discover important configurations, reduce data collection costs</li>
</ul>
<h3 id="_17">To the Next Chapter</h3>
<p>In Chapter 4, you'll learn about the latest MLP methods (NequIP, MACE) and actual research applications:<br/>
- Theory of E(3) equivariant graph neural networks<br/>
- Dramatic improvement in data efficiency (100,000‚Üí3,000 configurations)<br/>
- Application cases to catalytic reactions and battery materials<br/>
- Realization of large-scale simulations (1 million atoms)</p>
<hr/>
<h2 id="_18">Exercises</h2>
<h3 id="1easy">Exercise 1 (Difficulty: easy)</h3>
<p>In Example 4's SchNet configuration, change <code>n_interactions</code> (number of message passing layers) to 3, 6, and 9, train the models, and predict how test MAE changes.</p>
<details>
<summary>Hint</summary>

The deeper the layers, the better they capture long-range atomic interactions. However, too deep increases overfitting risk.

</details>
<details>
<summary>Sample Answer</summary>

**Expected Results**:

| `n_interactions` | Predicted Test MAE | Training Time | Characteristics |
|-----------------|-------------|---------|------|
| **3** | 0.8-1.2 kcal/mol | 1 hour | Shallow, cannot fully capture long-range interactions |
| **6** | 0.5-0.8 kcal/mol | 2-3 hours | Well balanced (recommended) |
| **9** | 0.6-1.0 kcal/mol | 4-5 hours | Overfitting risk, accuracy decreases with insufficient training data |

**Experimental Method**:

<pre class="codehilite"><code class="language-python">for n in [3, 6, 9]:
    representation = SchNet(n_interactions=n, ...)
    task = AtomisticTask(...)
    trainer.fit(task, datamodule=data_module)
    results = trainer.test(task, datamodule=data_module)
    print(f"n={n}: MAE={results[0]['test_energy_MAE']:.4f} eV")
</code></pre>



**Conclusion**: For small molecules (21 aspirin atoms), `n_interactions=6` is optimal. For large-scale systems (100+ atoms), 9-12 layers may be effective.

</details>
<h3 id="2medium">Exercise 2 (Difficulty: medium)</h3>
<p>In Example 11's MLP-MD, if energy drift exceeds acceptable range (e.g., 0.01 eV/step), what countermeasures can you consider? List three.</p>
<details>
<summary>Hint</summary>

Consider from three perspectives: timestep, training accuracy, and MD algorithm.

</details>
<details>
<summary>Sample Answer</summary>

**Countermeasure 1: Reduce Timestep**

<pre class="codehilite"><code class="language-python">timestep = 0.25 * units.fs  # Halve from 0.5fs to 0.25fs
dyn = VelocityVerlet(atoms, timestep=timestep)
</code></pre>


- **Reason**: Smaller timestep reduces numerical integration error
- **Drawback**: 2√ó longer computation time

**Countermeasure 2: Improve Model Training Accuracy**

<pre class="codehilite"><code class="language-python"># Train with more data
data_module.num_train = 200000  # Increase from 100,000 to 200,000 configurations

# Or increase weight of force loss function
task = AtomisticTask(..., loss_weights={'energy': 1.0, 'forces': 1000})
</code></pre>


- **Reason**: Low force prediction accuracy causes unstable MD
- **Target**: Force MAE &lt; 0.05 eV/√Ö

**Countermeasure 3: Switch to Langevin Dynamics (Heat Bath Coupling)**

<pre class="codehilite"><code class="language-python">from ase.md.langevin import Langevin
dyn = Langevin(atoms, timestep=0.5*units.fs,
               temperature_K=300, friction=0.01)
</code></pre>


- **Reason**: Heat bath absorbs energy drift
- **Caution**: No longer strictly microcanonical ensemble (NVE)

**Priority**: Countermeasure 2 (improve accuracy) ‚Üí Countermeasure 1 (timestep) ‚Üí Countermeasure 3 (Langevin)

</details>
<hr/>
<h2 id="310">3.10 Data License and Reproducibility</h2>
<p>Information on datasets and tool versions needed to reproduce the hands-on code in this chapter.</p>
<h3 id="3101">3.10.1 Datasets Used</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Description</th>
<th>License</th>
<th>Access Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MD17</strong></td>
<td>Small molecule MD trajectories (10 types including aspirin, benzene)</td>
<td>CC0 1.0 (Public Domain)</td>
<td>Built into SchNetPack (<code>MD17(molecule='aspirin')</code>)</td>
</tr>
<tr>
<td><strong>Aspirin molecule</strong></td>
<td>211,762 configurations, DFT (PBE/def2-SVP)</td>
<td>CC0 1.0</td>
<td><a href="http://sgdml.org/#datasets">sgdml.org</a></td>
</tr>
</tbody>
</table>
<p><strong>Notes</strong>:<br/>
- <strong>Commercial Use</strong>: CC0 license allows all commercial use, modification, and redistribution freely<br/>
- <strong>Paper Citation</strong>: When using MD17, cite the following<br/>
  Chmiela, S., et al. (2017). "Machine learning of accurate energy-conserving molecular force fields." <em>Science Advances</em>, 3(5), e1603015.<br/>
- <strong>Data Integrity</strong>: SchNetPack's download function verifies with SHA256 checksum</p>
<h3 id="3102">3.10.2 Environment Information for Code Reproducibility</h3>
<p>To accurately reproduce the code examples in this chapter, use the following versions.</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Recommended Version</th>
<th>Installation Command</th>
<th>Compatibility</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>3.10.x</td>
<td><code>conda create -n mlp python=3.10</code></td>
<td>Tested on 3.9-3.11</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>2.1.0</td>
<td><code>conda install pytorch=2.1.0</code></td>
<td>2.0 or higher required</td>
</tr>
<tr>
<td><strong>SchNetPack</strong></td>
<td>2.0.3</td>
<td><code>pip install schnetpack==2.0.3</code></td>
<td>API differs between 2.0 and 1.x series</td>
</tr>
<tr>
<td><strong>ASE</strong></td>
<td>3.22.1</td>
<td><code>pip install ase==3.22.1</code></td>
<td>3.20 or higher recommended</td>
</tr>
<tr>
<td><strong>PyTorch Lightning</strong></td>
<td>2.1.0</td>
<td><code>pip install pytorch-lightning==2.1.0</code></td>
<td>Compatible with SchNetPack 2.0.3</td>
</tr>
<tr>
<td><strong>NumPy</strong></td>
<td>1.24.3</td>
<td><code>pip install numpy==1.24.3</code></td>
<td>1.20 or higher</td>
</tr>
<tr>
<td><strong>Matplotlib</strong></td>
<td>3.7.1</td>
<td><code>pip install matplotlib==3.7.1</code></td>
<td>3.5 or higher</td>
</tr>
</tbody>
</table>
<p><strong>Saving Environment File</strong>:</p>
<pre class="codehilite"><code class="language-bash"># Save current environment in reproducible form
conda env export &gt; environment.yml

# Reproduce in another environment
conda env create -f environment.yml
</code></pre>
<p><strong>Ensuring Reproducibility with Docker</strong> (recommended):</p>
<pre class="codehilite"><code class="language-dockerfile"># Dockerfile example
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
RUN pip install schnetpack==2.0.3 ase==3.22.1 pytorch-lightning==2.1.0
</code></pre>
<h3 id="3103">3.10.3 Complete Record of Training Hyperparameters</h3>
<p>Complete record of hyperparameters used in Examples 4 and 5 (for paper reproduction):</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>n_atom_basis</code></td>
<td>128</td>
<td>Dimension of atomic feature vectors</td>
</tr>
<tr>
<td><code>n_interactions</code></td>
<td>6</td>
<td>Number of message passing layers</td>
</tr>
<tr>
<td><code>cutoff</code></td>
<td>5.0 √Ö</td>
<td>Cutoff radius for atomic interactions</td>
</tr>
<tr>
<td><code>n_filters</code></td>
<td>128</td>
<td>Number of convolution filters</td>
</tr>
<tr>
<td><code>batch_size</code></td>
<td>32</td>
<td>Mini-batch size</td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td>1e-4</td>
<td>Initial learning rate (AdamW)</td>
</tr>
<tr>
<td><code>max_epochs</code></td>
<td>50</td>
<td>Maximum training epochs</td>
</tr>
<tr>
<td><code>num_train</code></td>
<td>100,000</td>
<td>Number of training data</td>
</tr>
<tr>
<td><code>num_val</code></td>
<td>10,000</td>
<td>Number of validation data</td>
</tr>
<tr>
<td><code>num_test</code></td>
<td>10,000</td>
<td>Number of test data</td>
</tr>
<tr>
<td><code>random_seed</code></td>
<td>42</td>
<td>Random seed (data split reproducibility)</td>
</tr>
</tbody>
</table>
<p><strong>Complete Reproduction Code</strong>:</p>
<pre class="codehilite"><code class="language-python">import torch
torch.manual_seed(42)  # Ensure reproducibility

representation = SchNet(
    n_atom_basis=128, n_interactions=6, cutoff=5.0, n_filters=128
)
task = AtomisticTask(
    model=AtomisticModel(representation, [output]),
    optimizer_cls=torch.optim.AdamW,
    optimizer_args={'lr': 1e-4, 'weight_decay': 0.01}
)
trainer = pl.Trainer(max_epochs=50, deterministic=True)
</code></pre>
<h3 id="3104">3.10.4 Energy and Force Unit Conversion Table</h3>
<p>Unit conversions for physical quantities used in this chapter (SchNetPack and ASE standard units):</p>
<table>
<thead>
<tr>
<th>Physical Quantity</th>
<th>SchNetPack/ASE</th>
<th>eV</th>
<th>kcal/mol</th>
<th>Hartree</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Energy</strong></td>
<td>eV</td>
<td>1.0</td>
<td>23.06</td>
<td>0.03674</td>
</tr>
<tr>
<td><strong>Force</strong></td>
<td>eV/√Ö</td>
<td>1.0</td>
<td>23.06</td>
<td>0.01945</td>
</tr>
<tr>
<td><strong>Distance</strong></td>
<td>√Ö</td>
<td>-</td>
<td>-</td>
<td>1.889726 Bohr</td>
</tr>
<tr>
<td><strong>Time</strong></td>
<td>fs (femtoseconds)</td>
<td>-</td>
<td>-</td>
<td>0.02419 a.u.</td>
</tr>
</tbody>
</table>
<p><strong>Unit Conversion Example</strong>:</p>
<pre class="codehilite"><code class="language-python">from ase import units

# Energy conversion
energy_ev = 1.0  # eV
energy_kcal = energy_ev * 23.06052  # kcal/mol
energy_hartree = energy_ev * 0.036749  # Hartree

# Using ASE unit constants (recommended)
print(f"{energy_ev} eV = {energy_ev * units.eV / units.kcal * units.mol} kcal/mol")
</code></pre>
<hr/>
<h2 id="311">3.11 Practical Precautions: Common Failure Patterns in Hands-On</h2>
<h3 id="3111">3.11.1 Pitfalls in Environment Setup and Installation</h3>
<details>
<summary><strong>Failure 1: PyTorch and CUDA Version Mismatch</strong></summary>
<p><strong>Problem</strong>:</p>
<pre class="codehilite"><code>RuntimeError: CUDA error: no kernel image is available for execution on the device
</code></pre>
<p><strong>Cause</strong>:<br/>
PyTorch 2.1.0 is compiled with CUDA 11.8 or 12.1, but system CUDA is older version like 10.2</p>
<p><strong>Diagnostic Code</strong>:</p>
<pre class="codehilite"><code class="language-python">import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version (PyTorch): {torch.version.cuda}")

# Check system CUDA version (terminal)
# nvcc --version
</code></pre>
<p><strong>Solution</strong>:</p>
<pre class="codehilite"><code class="language-bash"># For system CUDA 11.8
conda install pytorch==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia

# Switch to CPU version if CUDA unavailable
conda install pytorch==2.1.0 cpuonly -c pytorch
</code></pre>
<p><strong>Prevention</strong>:<br/>
Check GPU driver and CUDA version with <code>nvidia-smi</code> before environment setup</p>
</details>
<details>
<summary><strong>Failure 2: Confusing SchNetPack 1.x and 2.x APIs</strong></summary>
<p><strong>Problem</strong>:</p>
<pre class="codehilite"><code>AttributeError: module 'schnetpack' has no attribute 'AtomsData'
</code></pre>
<p><strong>Cause</strong>:<br/>
Running old tutorial code from SchNetPack 1.x series on 2.x series</p>
<p><strong>Version Check</strong>:</p>
<pre class="codehilite"><code class="language-python">import schnetpack as spk
print(spk.__version__)  # If 2.0.3, this chapter's code works
</code></pre>
<p><strong>Main API Changes</strong>:</p>
<table>
<thead>
<tr>
<th>SchNetPack 1.x</th>
<th>SchNetPack 2.x</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>spk.AtomsData</code></td>
<td><code>spk.data.AtomsDataModule</code></td>
</tr>
<tr>
<td><code>spk.atomistic.Atomwise</code></td>
<td><code>spk.task.ModelOutput</code></td>
</tr>
<tr>
<td><code>spk.train.Trainer</code></td>
<td><code>pytorch_lightning.Trainer</code></td>
</tr>
</tbody>
</table>
<p><strong>Solution</strong>:<br/>
Use this chapter's code examples (2.x series) or refer to SchNetPack official documentation (<a href="https://schnetpack.readthedocs.io">schnetpack.readthedocs.io</a>) 2.x series tutorials</p>
</details>
<details>
<summary><strong>Failure 3: Misdiagnosis of Memory Shortage (OOM)</strong></summary>
<p><strong>Problem</strong>:</p>
<pre class="codehilite"><code>RuntimeError: CUDA out of memory. Tried to allocate 1.50 GiB
</code></pre>
<p><strong>Common Misconception</strong>:<br/>
"GPU memory shortage so need to buy new GPU" ‚Üí <strong>Wrong</strong></p>
<p><strong>Diagnostic Procedure</strong>:</p>
<pre class="codehilite"><code class="language-python"># 1. Check batch size
print(f"Current batch size: {data_module.batch_size}")

# 2. Check GPU memory usage
if torch.cuda.is_available():
    print(f"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
</code></pre>
<p><strong>Solutions (Priority Order)</strong>:</p>
<ol>
<li><strong>Reduce batch size</strong>: <code>batch_size=32</code> ‚Üí <code>16</code> ‚Üí <code>8</code> ‚Üí <code>4</code></li>
<li><strong>Gradient accumulation</strong>: Accumulate small batches multiple times to simulate large batch</li>
</ol>
<pre class="codehilite"><code class="language-python">trainer = pl.Trainer(accumulate_grad_batches=4)  # Update every 4 batches
</code></pre>
<ol start="3">
<li><strong>Mixed Precision training</strong>: Halve memory usage</li>
</ol>
<pre class="codehilite"><code class="language-python">trainer = pl.Trainer(precision=16)  # Use float16
</code></pre>
<p><strong>Guidelines</strong>:<br/>
- GPU 4GB: batch_size=4-8<br/>
- GPU 8GB: batch_size=16-32<br/>
- GPU 24GB: batch_size=64-128</p>
</details>
<h3 id="3112">3.11.2 Training and Debugging Pitfalls</h3>
<details>
<summary><strong>Failure 4: Training Error Not Decreasing (NaN Loss)</strong></summary>
<p><strong>Problem</strong>:</p>
<pre class="codehilite"><code>Epoch 5: train_loss=nan, val_loss=nan
</code></pre>
<p><strong>Top 3 Causes</strong>:</p>
<ol>
<li><strong>Learning rate too high</strong>: Gradient explosion ‚Üí Parameters become NaN</li>
<li><strong>Lack of data normalization</strong>: Energy absolute values too large (e.g., -1000 eV)</li>
<li><strong>Inappropriate force loss coefficient</strong>: Force loss too dominant</li>
</ol>
<p><strong>Diagnostic Code</strong>:</p>
<pre class="codehilite"><code class="language-python"># Check model output immediately after training starts
for batch in data_module.train_dataloader():
    output = task.model(batch)
    print(f"Energy prediction: {output['energy'][:5]}")  # First 5 samples
    print(f"Energy target: {batch['energy'][:5]}")
    break

# NaN check
print(f"Has NaN in prediction: {torch.isnan(output['energy']).any()}")
</code></pre>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Lower learning rate</strong>:</li>
</ol>
<pre class="codehilite"><code class="language-python">optimizer_args={'lr': 1e-5}  # Decrease from 1e-4 to 1e-5
</code></pre>
<ol start="2">
<li><strong>Gradient clipping</strong>:</li>
</ol>
<pre class="codehilite"><code class="language-python">trainer = pl.Trainer(gradient_clip_val=1.0)  # Clip gradient norm to ‚â§1.0
</code></pre>
<ol start="3">
<li><strong>Data normalization</strong> (automatic in SchNetPack 2.x, but verify manually):</li>
</ol>
<pre class="codehilite"><code class="language-python">import schnetpack.transform as trn
data_module.train_transforms = [
    trn.SubtractCenterOfMass(),
    trn.RemoveOffsets('energy', remove_mean=True)  # Remove energy offset
]
</code></pre>
</details>
<details>
<summary><strong>Failure 5: Missing Overfitting Signs</strong></summary>
<p><strong>Problem</strong>:<br/>
Training error decreases but validation error stagnates or increases</p>
<pre class="codehilite"><code>Epoch 30: train_loss=0.001, val_loss=0.050  # val_loss worsening
</code></pre>
<p><strong>Cause</strong>:<br/>
Model memorizes training data, generalization performance to unseen data deteriorates</p>
<p><strong>Diagnostic Graph</strong>:</p>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt
import pandas as pd

metrics = pd.read_csv('./training/lightning_logs/version_0/metrics.csv')
plt.plot(metrics['epoch'], metrics['train_loss'], label='Train')
plt.plot(metrics['epoch'], metrics['val_loss'], label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
</code></pre>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Early Stopping</strong> (automatic stopping):</li>
</ol>
<pre class="codehilite"><code class="language-python">from pytorch_lightning.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min')
trainer = pl.Trainer(callbacks=[early_stop])
</code></pre>
<ol start="2">
<li><strong>Data augmentation</strong> (increase training data):</li>
</ol>
<pre class="codehilite"><code class="language-python">data_module.num_train = 200000  # Increase from 100,000 to 200,000
</code></pre>
<ol start="3">
<li><strong>Dropout (not recommended, less effective for MLP)</strong>: Instead reduce model parameter count</li>
</ol>
<pre class="codehilite"><code class="language-python">representation = SchNet(n_atom_basis=64, n_interactions=4)  # Reduce from 128‚Üí64
</code></pre>
</details>
<h3 id="3113">3.11.3 MLP-MD Simulation Pitfalls</h3>
<details>
<summary><strong>Failure 6: Underestimating Energy Drift</strong></summary>
<p><strong>Problem</strong>:<br/>
Energy monotonically increases/decreases during MD simulation (breaking conservation law)</p>
<p><strong>Misconception About Acceptable Range</strong>:</p>
<ul>
<li>"A little drift is unavoidable" ‚Üí <strong>Dangerous</strong></li>
<li>0.01 eV/step drift results in 100 eV energy change over 10,000 steps (unrealistic)</li>
</ul>
<p><strong>Quantitative Diagnosis</strong>:</p>
<pre class="codehilite"><code class="language-python">from ase.io import read

traj = read('aspirin_md.traj', index=':')
energies = [a.get_total_energy() for a in traj]

# Calculate drift (linear fit)
import numpy as np
time_steps = np.arange(len(energies))
drift_rate, offset = np.polyfit(time_steps, energies, deg=1)
print(f"Energy drift: {drift_rate:.6f} eV/step")

# Check acceptable range
if abs(drift_rate) &gt; 0.001:
    print("‚ö†Ô∏è WARNING: Excessive energy drift detected!")
</code></pre>
<p><strong>Solutions (Detailed from Exercise 2)</strong>:</p>
<ol>
<li><strong>Improve force training accuracy</strong> (most important):</li>
</ol>
<pre class="codehilite"><code class="language-python"># Significantly increase weight of force loss function
task = AtomisticTask(
    loss_weights={'energy': 0.01, 'forces': 0.99}  # 99% weight on forces
)
</code></pre>
<ol start="2">
<li><strong>Optimize timestep</strong>:</li>
</ol>
<pre class="codehilite"><code class="language-bash"># Stability test
for dt in [0.1, 0.25, 0.5, 1.0]:  # fs
    # If stable at dt=0.5 but diverges at dt=1.0, adopt dt=0.5
</code></pre>
<ol start="3">
<li><strong>Recognize MLP accuracy limits</strong>:<br/>
If force MAE &gt; 0.1 eV/√Ö, long-time MD (&gt;10 ps) has reduced reliability<br/>
‚Üí Add training data with Active Learning</li>
</ol>
</details>
<details>
<summary><strong>Failure 7: Not Verifying Physical Validity of MD Results</strong></summary>
<p><strong>Problem</strong>:<br/>
Misconception that "MD completed so it's successful" ‚Üí Actually unphysical structural changes</p>
<p><strong>Items to Verify</strong>:</p>
<p><strong>1. Temperature Control Check</strong>:</p>
<pre class="codehilite"><code class="language-python">temperatures = [a.get_temperature() for a in traj]
print(f"Average T: {np.mean(temperatures):.1f} K (target: 300 K)")
print(f"Std T: {np.std(temperatures):.1f} K")
# Standard deviation &gt; 30K indicates abnormality
</code></pre>
<p><strong>2. Structure Disruption Check</strong>:</p>
<pre class="codehilite"><code class="language-python">from ase.geometry.analysis import Analysis

# Compare initial and final structures
ana_init = Analysis(traj[0])
ana_final = Analysis(traj[-1])

# Check if bonds are broken
bonds_init = ana_init.all_bonds[0]
bonds_final = ana_final.all_bonds[0]
print(f"Initial bonds: {len(bonds_init)}, Final bonds: {len(bonds_final)}")

# Change in bond count ‚Üí Possible structure disruption
</code></pre>
<p><strong>3. Validity of Radial Distribution Function (RDF)</strong>:</p>
<pre class="codehilite"><code class="language-python"># Check if first peak position matches DFT calculations or X-ray diffraction data
# (Implementation advanced, omitted)
</code></pre>
<p><strong>Solution</strong>:<br/>
If physically valid results not obtained, likely extrapolation beyond training data range<br/>
‚Üí Add those configurations to training data with Active Learning</p>
</details>
<hr/>
<h2 id="312">3.12 Chapter Completion Checklist: Hands-On Quality Assurance</h2>
<p>After completing this chapter, verify the following items. If you can check all items, you're ready to apply MLP in actual research projects.</p>
<h3 id="3121">3.12.1 Conceptual Understanding</h3>
<p><strong>Understanding Tools and Environment</strong>:</p>
<ul>
<li>‚ñ° Can explain SchNetPack's role (MLP training library)</li>
<li>‚ñ° Understand relationship between PyTorch and SchNetPack (PyTorch-based MLP implementation)</li>
<li>‚ñ° Can explain ASE's role (atomic structure manipulation, MD execution)</li>
<li>‚ñ° Understand MD17 dataset characteristics (10 small molecules, DFT accuracy)</li>
</ul>
<p><strong>Understanding Model Training</strong>:</p>
<ul>
<li>‚ñ° Can explain meaning of SchNet hyperparameters (<code>n_atom_basis</code>, <code>n_interactions</code>, <code>cutoff</code>)</li>
<li>‚ñ° Understand roles of training/validation/test sets</li>
<li>‚ñ° Can identify overfitting signs (validation error increase)</li>
<li>‚ñ° Understand that Energy MAE &lt; 1 kcal/mol is high accuracy benchmark</li>
</ul>
<p><strong>Understanding MLP-MD</strong>:</p>
<ul>
<li>‚ñ° Understand mechanism of MLP integration as ASE Calculator</li>
<li>‚ñ° Can explain difference between energy conservation law and energy drift</li>
<li>‚ñ° Understand why timestep (0.5 fs) affects stability</li>
<li>‚ñ° Can explain why MLP-MD is 10‚Å¥√ó faster than DFT</li>
</ul>
<h3 id="3122">3.12.2 Practical Skills</h3>
<p><strong>Environment Setup</strong>:</p>
<ul>
<li>‚ñ° Can create Conda environment and install Python 3.10</li>
<li>‚ñ° Can correctly install PyTorch (CPU/GPU versions)</li>
<li>‚ñ° Can install SchNetPack 2.0.3 and ASE 3.22.1</li>
<li>‚ñ° Can run environment verification script and check versions</li>
</ul>
<p><strong>Data Preparation and Training</strong>:</p>
<ul>
<li>‚ñ° Can download MD17 dataset and split into 100,000 configurations</li>
<li>‚ñ° Can define SchNet model and set hyperparameters</li>
<li>‚ñ° Can run 50-epoch training and monitor progress with TensorBoard</li>
<li>‚ñ° Can evaluate MAE on test set and achieve target accuracy (&lt; 1 kcal/mol)</li>
</ul>
<p><strong>MLP-MD Simulation</strong>:</p>
<ul>
<li>‚ñ° Can wrap trained model as ASE Calculator</li>
<li>‚ñ° Can set initial velocities with Maxwell-Boltzmann distribution</li>
<li>‚ñ° Can run MD for 10,000 steps (5 picoseconds)</li>
<li>‚ñ° Can save trajectory and verify energy conservation</li>
</ul>
<p><strong>Analysis and Troubleshooting</strong>:</p>
<ul>
<li>‚ñ° Can calculate vibrational spectrum (power spectrum)</li>
<li>‚ñ° Can calculate diffusion coefficient from mean square displacement (MSD)</li>
<li>‚ñ° Can handle Out of Memory (OOM) errors (reduce batch size)</li>
<li>‚ñ° Can diagnose NaN loss causes and adjust learning rate</li>
</ul>
<h3 id="3123">3.12.3 Application Skills</h3>
<p><strong>Application Plan to Your Research</strong>:</p>
<ul>
<li>‚ñ° Can design MD17-equivalent dataset for your research target (molecules, materials)</li>
<li>‚ñ° Can estimate required DFT calculation count (from target accuracy and system size)</li>
<li>‚ñ° Can develop strategy to optimize SchNet hyperparameters for your system</li>
<li>‚ñ° Can clearly define physical properties to obtain from MLP-MD (diffusion coefficient, vibrational spectrum, reaction path)</li>
</ul>
<p><strong>Problem Solving and Debugging</strong>:</p>
<ul>
<li>‚ñ° Can execute diagnostic procedure when training doesn't converge (learning rate, data normalization, gradient clipping)</li>
<li>‚ñ° Can identify causes of energy drift and select countermeasures</li>
<li>‚ñ° Can detect overfitting and apply Early Stopping or Data Augmentation</li>
<li>‚ñ° Can optimize batch size and training time according to GPU/CPU resources</li>
</ul>
<p><strong>Preparation for Advanced Techniques</strong>:</p>
<ul>
<li>‚ñ° Understand Active Learning concept (Example 15) and can explain implementation flow</li>
<li>‚ñ° Understand importance of configuration selection by ensemble uncertainty</li>
<li>‚ñ° Have expectations for how data efficiency improves in next chapter (NequIP, MACE)</li>
<li>‚ñ° Can continue self-study using SchNetPack documentation (<a href="https://schnetpack.readthedocs.io">schnetpack.readthedocs.io</a>)</li>
</ul>
<p><strong>Bridge to Next Chapter</strong>:</p>
<ul>
<li>‚ñ° Recognize SchNet limitations (data efficiency, rotational equivariance)</li>
<li>‚ñ° Interested in how E(3) equivariant architectures (NequIP, MACE) improve</li>
<li>‚ñ° Ready to learn actual research applications (catalysts, batteries, drug discovery) in Chapter 4</li>
</ul>
<hr/>
<h2 id="_19">References</h2>
<ol>
<li>
<p>Sch√ºtt, K. T., et al. (2019). "SchNetPack: A Deep Learning Toolbox For Atomistic Systems." <em>Journal of Chemical Theory and Computation</em>, 15(1), 448-455.<br/>
   DOI: <a href="https://doi.org/10.1021/acs.jctc.8b00908">10.1021/acs.jctc.8b00908</a></p>
</li>
<li>
<p>Chmiela, S., et al. (2017). "Machine learning of accurate energy-conserving molecular force fields." <em>Science Advances</em>, 3(5), e1603015.<br/>
   DOI: <a href="https://doi.org/10.1126/sciadv.1603015">10.1126/sciadv.1603015</a></p>
</li>
<li>
<p>Larsen, A. H., et al. (2017). "The atomic simulation environment‚Äîa Python library for working with atoms." <em>Journal of Physics: Condensed Matter</em>, 29(27), 273002.<br/>
   DOI: <a href="https://doi.org/10.1088/1361-648X/aa680e">10.1088/1361-648X/aa680e</a></p>
</li>
<li>
<p>Paszke, A., et al. (2019). "PyTorch: An imperative style, high-performance deep learning library." <em>Advances in Neural Information Processing Systems</em>, 32.<br/>
   arXiv: <a href="https://arxiv.org/abs/1912.01703">1912.01703</a></p>
</li>
<li>
<p>Zhang, L., et al. (2020). "Active learning of uniformly accurate interatomic potentials for materials simulation." <em>Physical Review Materials</em>, 3(2), 023804.<br/>
   DOI: <a href="https://doi.org/10.1103/PhysRevMaterials.3.023804">10.1103/PhysRevMaterials.3.023804</a></p>
</li>
<li>
<p>Sch√ºtt, K. T., et al. (2017). "Quantum-chemical insights from deep tensor neural networks." <em>Nature Communications</em>, 8(1), 13890.<br/>
   DOI: <a href="https://doi.org/10.1038/ncomms13890">10.1038/ncomms13890</a></p>
</li>
</ol>
<hr/>
<h2 id="_20">Author Information</h2>
<p><strong>Created by</strong>: MI Knowledge Hub Content Team<br/>
<strong>Created on</strong>: 2025-10-17<br/>
<strong>Version</strong>: 1.1 (Chapter 3 quality improvement)<br/>
<strong>Series</strong>: MLP Introduction Series</p>
<p><strong>Update History</strong>:<br/>
- 2025-10-19: v1.1 Quality improvement revision<br/>
  - Added data license and reproducibility section (MD17 dataset, aspirin molecule information)<br/>
  - Code reproducibility information (Python 3.10.x, PyTorch 2.1.0, SchNetPack 2.0.3, ASE 3.22.1)<br/>
  - Complete record of training hyperparameters (11 items, for paper reproduction)<br/>
  - Energy and force unit conversion table (eV, kcal/mol, Hartree mutual conversion)<br/>
  - Added practical precautions section (7 failure patterns: CUDA mismatch, API confusion, OOM, NaN loss, overfitting, energy drift, physical validity)<br/>
  - Added chapter completion checklist (12 conceptual understanding items, 16 practical skill items, 16 application skill items)<br/>
- 2025-10-17: v1.0 Chapter 3 initial release<br/>
  - Python environment setup (Conda, PyTorch, SchNetPack)<br/>
  - MD17 dataset preparation and splitting<br/>
  - SchNet model training (15 code examples)<br/>
  - MLP-MD execution and analysis (trajectory, vibrational spectrum, MSD)<br/>
  - Active Learning uncertainty evaluation<br/>
  - Troubleshooting table (5 items)<br/>
  - 2 exercises (easy, medium)<br/>
  - 6 references</p>
<p><strong>License</strong>: Creative Commons BY-NC-SA 4.0</p>
<div class="nav-buttons">
<a class="nav-button" href="index.html">‚Üê Back to Series Index</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is for educational, research, and informational purposes only and does not provide professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any express or implied warranties, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University assume no liability for direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty provisions.</li>
</ul>
</section>
<footer>
<div class="container">
<p>¬© 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
