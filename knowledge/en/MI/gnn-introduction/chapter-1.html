<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Why GNN for Materials Science | GNN Introduction Series</title>
<meta content="Learn why Graph Neural Networks (GNN) are essential for materials science. Understand the limitations of traditional descriptors and the advantages of GNN." name="description"/>
<link href="../../assets/css/variables.css" rel="stylesheet"/>
<link href="../../assets/css/base.css" rel="stylesheet"/>
<link href="../../assets/css/layout.css" rel="stylesheet"/>
<link href="../../assets/css/components.css" rel="stylesheet"/>
<link href="../../assets/css/article.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                primaryColor: '#667eea',
                primaryTextColor: '#fff',
                primaryBorderColor: '#764ba2',
                lineColor: '#667eea',
                secondaryColor: '#764ba2',
                tertiaryColor: '#fff'
            }
        });
    </script>
</head>
<body>
<header class="site-header">
<div class="header-content">
<div class="site-title">
<a href="../../../en/">AI Terakoya</a>
</div>
<nav class="main-nav" id="main-nav">
<ul>
<li><a href="../../../en/#research">Research</a></li>
<li><a href="../../../en/#publications">Publications</a></li>
<li><a href="../../../en/#news">News</a></li>
<li><a href="../../../en/#members">Members</a></li>
<li><a href="../../../en/#contact">Contact</a></li>
</ul>
</nav>
<div class="lang-switch">
<a href="/jp/MI/gnn-introduction/chapter-1.html">Japanese</a>
</div>
</div>
</header>
<main class="article-main">
<nav aria-label="Breadcrumb" class="breadcrumb">
<ol>
<li><a href="../../../en/">Home</a></li>
<li><a href="../../">Knowledge Base</a></li>
<li><a href="../../MI/">Materials Informatics</a></li>
<li><a href="../../MI/gnn-introduction/">GNN Introduction</a></li>
<li aria-current="page">Chapter 1</li>
</ol>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/gnn-introduction/chapter-1.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<article class="article-content">
<header class="article-header" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);">
<div class="series-badge">GNN Introduction Series v1.0</div>
<h1>Chapter 1: Why GNN for Materials Science</h1>
<p class="article-meta">
<span class="difficulty">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Beginner</span>
<span class="reading-time">‚è± 25 min</span>
<span class="last-updated">üìÖ Updated: 2025-01-20</span>
</p>
</header>
<div class="article-body">
<section class="learning-objectives">
<h2>üéØ Learning Objectives</h2>
<ul>
<li>Understand what graphs are and the concepts of nodes and edges</li>
<li>Learn why materials science data should be treated as graphs</li>
<li>Understand the limitations of traditional descriptor-based methods</li>
<li>Grasp why GNN is essential for materials science</li>
<li>Learn about success stories of GNN in materials science</li>
</ul>
</section>
<nav class="toc">
<h2>üìë Table of Contents</h2>
<ol>
<li><a href="#what-is-graph">What is a Graph?</a></li>
<li><a href="#materials-as-graphs">Materials as Graphs</a></li>
<li><a href="#limitations-of-descriptors">Limitations of Traditional Descriptors</a></li>
<li><a href="#why-gnn">Why GNN is Needed</a></li>
<li><a href="#success-stories">Success Stories in Materials Science</a></li>
<li><a href="#chapter-summary">Chapter Summary</a></li>
<li><a href="#exercises">Exercises</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ol>
</nav>
<section id="what-is-graph">
<h2>1. What is a Graph?</h2>
<p>
                        First, let's understand what a <strong>graph</strong> is. A graph is a mathematical structure that represents relationships between objects, consisting of <strong>nodes (vertices)</strong> and <strong>edges</strong> connecting them.
                    </p>
<div class="info-box">
<h3>üìò Definition: Graph</h3>
<p>
                            A graph \(G\) is defined by a set of nodes \(V\) and a set of edges \(E\):
                        </p>
<p class="math-block">
                            \[G = (V, E)\]
                        </p>
<ul>
<li><strong>Node (Vertex)</strong>: Individual objects (e.g., atoms, molecules, people)</li>
<li><strong>Edge</strong>: Connections between objects (e.g., chemical bonds, friendships)</li>
</ul>
</div>
<h3>1.1 Visualizing Graphs</h3>
<p>
                        Let's visualize a simple graph. Here's an example showing relationships between 4 people:
                    </p>
<div class="mermaid">
graph LR
    A[Alice] --- B[Bob]
    B --- C[Charlie]
    C --- D[Diana]
    D --- A
    A --- C
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
                    </div>
<p>
                        In this example:
                    </p>
<ul>
<li><strong>Nodes</strong>: Alice, Bob, Charlie, Diana (4 people)</li>
<li><strong>Edges</strong>: Lines connecting people (friendships)</li>
</ul>
<h3>1.2 Types of Graphs</h3>
<p>
                        Graphs can be categorized by their characteristics:
                    </p>
<table class="comparison-table">
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Undirected Graph</strong></td>
<td>Edges have no direction (A-B same as B-A)</td>
<td>Friendships, chemical bonds</td>
</tr>
<tr>
<td><strong>Directed Graph</strong></td>
<td>Edges have direction (A‚ÜíB ‚â† B‚ÜíA)</td>
<td>Twitter follows, email sending</td>
</tr>
<tr>
<td><strong>Weighted Graph</strong></td>
<td>Edges have weights (strength values)</td>
<td>Distance between cities, bond strength</td>
</tr>
<tr>
<td><strong>Unweighted Graph</strong></td>
<td>Edges have no weights (presence/absence only)</td>
<td>Yes/No friendships</td>
</tr>
</tbody>
</table>
</section>
<section id="materials-as-graphs">
<h2>2. Materials as Graphs</h2>
<p>
                        Now, let's consider what it means to treat materials as graphs. Materials are composed of atoms connected by chemical bonds, making them naturally suited for graph representation.
                    </p>
<h3>2.1 Molecules as Graphs</h3>
<p>
                        For example, let's represent a water molecule (H‚ÇÇO) as a graph:
                    </p>
<div class="mermaid">
graph LR
    O[O: Oxygen] --- H1[H: Hydrogen]
    O --- H2[H: Hydrogen]
    style O fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style H1 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style H2 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
                    </div>
<ul>
<li><strong>Nodes</strong>: Atoms (1 oxygen atom, 2 hydrogen atoms)</li>
<li><strong>Edges</strong>: Chemical bonds (covalent bonds)</li>
<li><strong>Node Features</strong>: Atomic number, electronegativity, atomic radius, etc.</li>
<li><strong>Edge Features</strong>: Bond type (single/double/triple), bond length, bond angle, etc.</li>
</ul>
<h3>2.2 Crystals as Graphs</h3>
<p>
                        Crystal structures can also be represented as graphs. For example, let's consider part of a simple cubic lattice:
                    </p>
<div class="mermaid">
graph TD
    A[Atom A] --- B[Atom B]
    A --- C[Atom C]
    A --- D[Atom D]
    B --- E[Atom E]
    C --- F[Atom F]
    D --- G[Atom G]
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style E fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style F fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style G fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
                    </div>
<ul>
<li><strong>Nodes</strong>: Atoms in the crystal lattice</li>
<li><strong>Edges</strong>: Nearest neighbor relationships (ionic bonds, metallic bonds, etc.)</li>
<li><strong>Node Features</strong>: Atomic number, oxidation state, magnetic moment, etc.</li>
<li><strong>Edge Features</strong>: Interatomic distance, bond angle, coordination number, etc.</li>
</ul>
<h3>2.3 Code Example: Creating a Molecular Graph</h3>
<p>
                        Here's how to create a simple molecular graph (ethanol, C‚ÇÇH‚ÇÖOH) using Python and NetworkX:
                    </p>
<pre><code class="language-python">import networkx as nx
import matplotlib.pyplot as plt

# Create an empty graph
G = nx.Graph()

# Add nodes (atoms)
# Node ID, atom type
atoms = [
    (0, 'C'),   # Carbon 1
    (1, 'C'),   # Carbon 2
    (2, 'O'),   # Oxygen
    (3, 'H'),   # Hydrogen 1
    (4, 'H'),   # Hydrogen 2
    (5, 'H'),   # Hydrogen 3
    (6, 'H'),   # Hydrogen 4
    (7, 'H'),   # Hydrogen 5
    (8, 'H'),   # Hydrogen 6
]

for atom_id, atom_type in atoms:
    G.add_node(atom_id, atom_type=atom_type)

# Add edges (bonds)
# (atom1, atom2, bond type)
bonds = [
    (0, 1, 'single'),   # C-C
    (1, 2, 'single'),   # C-O
    (2, 8, 'single'),   # O-H
    (0, 3, 'single'),   # C-H
    (0, 4, 'single'),   # C-H
    (0, 5, 'single'),   # C-H
    (1, 6, 'single'),   # C-H
    (1, 7, 'single'),   # C-H
]

for atom1, atom2, bond_type in bonds:
    G.add_edge(atom1, atom2, bond_type=bond_type)

# Visualize the graph
pos = nx.spring_layout(G, seed=42)
labels = nx.get_node_attributes(G, 'atom_type')

plt.figure(figsize=(8, 6))
nx.draw(G, pos, labels=labels, with_labels=True,
        node_color='lightblue', node_size=800,
        font_size=12, font_weight='bold')
plt.title('Molecular Graph: Ethanol (C‚ÇÇH‚ÇÖOH)')
plt.axis('off')
plt.tight_layout()
plt.show()

# Display graph information
print(f"Number of nodes (atoms): {G.number_of_nodes()}")
print(f"Number of edges (bonds): {G.number_of_edges()}")
print(f"Node list: {list(G.nodes(data=True))}")
print(f"Edge list: {list(G.edges(data=True))}")</code></pre>
<div class="code-output">
<h4>Execution Result:</h4>
<pre>Number of nodes (atoms): 9
Number of edges (bonds): 8
Node list: [(0, {'atom_type': 'C'}), (1, {'atom_type': 'C'}), (2, {'atom_type': 'O'}), ...]
Edge list: [(0, 1, {'bond_type': 'single'}), (1, 2, {'bond_type': 'single'}), ...]</pre>
</div>
<div class="tip-box">
<h4>üí° Key Point</h4>
<p>
                            By treating materials as graphs, you can explicitly represent structural information such as atomic arrangements and chemical bonds. This enables machine learning models to understand the inherent structure of materials.
                        </p>
</div>
</section>
<section id="limitations-of-descriptors">
<h2>3. Limitations of Traditional Descriptors</h2>
<p>
                        Before the advent of GNN, materials science widely used <strong>descriptor-based methods</strong> to predict material properties using machine learning. However, this approach has several fundamental limitations.
                    </p>
<h3>3.1 What are Descriptors?</h3>
<p>
                        Descriptors are <strong>numerical feature vectors</strong> representing material properties. For example, for molecules:
                    </p>
<ul>
<li>Number of atoms</li>
<li>Molecular weight</li>
<li>Number of aromatic rings</li>
<li>Number of hydrogen bond donors/acceptors</li>
<li>LogP value (lipophilicity)</li>
</ul>
<p>
                        These values are combined into a single feature vector, such as:
                    </p>
<p class="math-block">
                        \[\mathbf{x} = [10, 150.2, 2, 3, 1, 2.5]\]
                    </p>
<h3>3.2 Problems with Traditional Descriptors</h3>
<h4>Problem 1: Loss of Structural Information</h4>
<p>
                        Descriptors convert material structures into simple numerical values, resulting in <strong>loss of structural information</strong>. Consider two molecules with the same molecular formula (isomers) but different structures:
                    </p>
<div class="comparison-grid">
<div class="comparison-item">
<h5>Ethanol (C‚ÇÇH‚ÇÖOH)</h5>
<p>Molecular weight: 46.07</p>
<p>Number of -OH groups: 1</p>
</div>
<div class="comparison-item">
<h5>Dimethyl Ether (CH‚ÇÉOCH‚ÇÉ)</h5>
<p>Molecular weight: 46.07</p>
<p>Number of -OH groups: 0</p>
</div>
</div>
<p>
                        These molecules have the same molecular formula (C‚ÇÇH‚ÇÜO) but completely different properties. However, simple descriptors like molecular weight cannot distinguish them. While more complex descriptors like ECFP (Extended Connectivity Fingerprints) exist, they ultimately compress structural information.
                    </p>
<div class="warning-box">
<h4>‚ö†Ô∏è Caution</h4>
<p>
<strong>Descriptor design requires expert knowledge</strong>. Choosing which descriptors to use for which problems requires deep domain expertise, and poor choices can significantly reduce predictive accuracy.
                        </p>
</div>
<h4>Problem 2: Fixed Feature Space</h4>
<p>
                        Descriptors operate in a <strong>fixed feature space</strong>. For example, if you decide to use 100 descriptors, all materials must be represented by 100-dimensional vectors. This creates the following issues:
                    </p>
<ul>
<li><strong>Inflexibility</strong>: Cannot add new descriptors or change dimensionality midway</li>
<li><strong>Curse of dimensionality</strong>: High-dimensional feature spaces require more data</li>
<li><strong>Missing values</strong>: Some descriptors may not be calculable for certain materials</li>
</ul>
<h4>Problem 3: Manual Feature Engineering</h4>
<p>
                        Descriptor-based methods require <strong>manual feature engineering</strong>. Researchers must design features for each problem, which is:
                    </p>
<ul>
<li>Time-consuming and labor-intensive</li>
<li>Difficult to generalize beyond problem domains</li>
<li>Limited by human intuition (may miss optimal features)</li>
</ul>
<h3>3.3 Code Example: Descriptor Calculation and Limitations</h3>
<p>
                        Here's an example using RDKit to calculate molecular descriptors:
                    </p>
<pre><code class="language-python">from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem
import pandas as pd

# Define molecules (SMILES notation)
molecules = {
    'Ethanol': 'CCO',
    'Dimethyl Ether': 'COC',
    'Methanol': 'CO',
    'Propanol': 'CCCO'
}

# Calculate descriptors
data = []
for name, smiles in molecules.items():
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        descriptors = {
            'Molecule': name,
            'SMILES': smiles,
            'Molecular Weight': Descriptors.MolWt(mol),
            'LogP': Descriptors.MolLogP(mol),
            'Number of H-bond Donors': Descriptors.NumHDonors(mol),
            'Number of H-bond Acceptors': Descriptors.NumHAcceptors(mol),
            'Number of Rotatable Bonds': Descriptors.NumRotatableBonds(mol),
            'TPSA': Descriptors.TPSA(mol),  # Topological Polar Surface Area
        }
        data.append(descriptors)

# Create DataFrame
df = pd.DataFrame(data)
print(df.to_string(index=False))</code></pre>
<div class="code-output">
<h4>Execution Result:</h4>
<pre>      Molecule        SMILES  Molecular Weight     LogP  ...
       Ethanol           CCO            46.069    -0.031  ...
Dimethyl Ether           COC            46.069    -0.002  ...
      Methanol            CO            32.042    -0.824  ...
      Propanol          CCCO            60.096     0.250  ...</pre>
</div>
<p>
                        Notice that ethanol and dimethyl ether have similar molecular weights and LogP values. These simple descriptors cannot distinguish isomers, which can lead to poor machine learning model performance.
                    </p>
<div class="tip-box">
<h4>üí° Comparison of Descriptors and Graphs</h4>
<table class="comparison-table">
<thead>
<tr>
<th>Aspect</th>
<th>Descriptors</th>
<th>Graphs</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Structural information</strong></td>
<td>‚ùå Lost</td>
<td>‚úÖ Preserved</td>
</tr>
<tr>
<td><strong>Feature space</strong></td>
<td>‚ùå Fixed dimensionality</td>
<td>‚úÖ Flexible (variable number of nodes/edges)</td>
</tr>
<tr>
<td><strong>Feature engineering</strong></td>
<td>‚ùå Manual required</td>
<td>‚úÖ Automatic learning</td>
</tr>
<tr>
<td><strong>Isomer distinction</strong></td>
<td>‚ùå Difficult</td>
<td>‚úÖ Possible</td>
</tr>
<tr>
<td><strong>Learning efficiency</strong></td>
<td>‚ö†Ô∏è Requires expert knowledge</td>
<td>‚úÖ End-to-end learning</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="why-gnn">
<h2>4. Why GNN is Needed</h2>
<p>
                        Graph Neural Networks (GNN) were developed to overcome the limitations of traditional descriptor-based methods. GNN operates directly on graph structures, <strong>automatically extracting optimal features while preserving structural information</strong>.
                    </p>
<h3>4.1 Basic Principles of GNN</h3>
<p>
                        GNN updates node features by <strong>message passing</strong> between nodes. This mechanism allows information to propagate through the graph structure.
                    </p>
<div class="mermaid">
graph LR
    A[Node A<br/>Feature: xA] --&gt;|Message| B[Node B<br/>Feature: xB]
    C[Node C<br/>Feature: xC] --&gt;|Message| B
    D[Node D<br/>Feature: xD] --&gt;|Message| B
    B --&gt;|Aggregate &amp; Update| B2[Node B<br/>Updated Feature: xB']
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B2 fill:#2ecc71,stroke:#333,stroke-width:2px,color:#fff
                    </div>
<p>
                        The basic operations of GNN consist of three steps:
                    </p>
<ol>
<li><strong>Message</strong>: Each node sends its feature information to neighboring nodes</li>
<li><strong>Aggregate</strong>: Each node collects messages from neighbors and aggregates them (sum, average, max, etc.)</li>
<li><strong>Update</strong>: Each node updates its own feature using the aggregated information</li>
</ol>
<p>
                        Mathematically, this can be expressed as:
                    </p>
<p class="math-block">
                        \[
                        \mathbf{h}_v^{(k+1)} = \text{UPDATE}^{(k)} \left( \mathbf{h}_v^{(k)}, \text{AGGREGATE}^{(k)} \left( \left\{ \mathbf{h}_u^{(k)} : u \in \mathcal{N}(v) \right\} \right) \right)
                        \]
                    </p>
<ul>
<li>\(\mathbf{h}_v^{(k)}\): Feature of node \(v\) at layer \(k\)</li>
<li>\(\mathcal{N}(v)\): Set of neighbor nodes of node \(v\)</li>
<li>\(\text{AGGREGATE}\): Aggregation function (sum, mean, max, etc.)</li>
<li>\(\text{UPDATE}\): Update function (typically a neural network)</li>
</ul>
<h3>4.2 Advantages of GNN</h3>
<h4>Advantage 1: Preservation of Structural Information</h4>
<p>
                        GNN operates directly on graph structures, <strong>preserving all structural information</strong>. This enables accurate distinction of isomers and understanding of material structures.
                    </p>
<h4>Advantage 2: End-to-End Learning</h4>
<p>
                        GNN performs <strong>end-to-end learning from raw structure to predictions</strong> without manual feature engineering. The model automatically learns optimal features.
                    </p>
<div class="mermaid">
graph LR
    A[Input Graph<br/>Molecular/Crystal Structure] --&gt; B[GNN Layers<br/>Message Passing]
    B --&gt; C[Pooling<br/>Graph-level Features]
    C --&gt; D[Prediction<br/>Properties/Activities]
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#764ba2,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#764ba2,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#2ecc71,stroke:#333,stroke-width:2px,color:#fff
                    </div>
<h4>Advantage 3: Flexibility</h4>
<p>
                        GNN can <strong>handle graphs of arbitrary size and topology</strong>. Unlike descriptors, there's no fixed feature space, and it can adapt to new materials.
                    </p>
<h4>Advantage 4: Interpretability</h4>
<p>
                        By analyzing the message passing process of GNN, you can understand <strong>which parts of the structure are important for predictions</strong>. This provides important insights for materials design.
                    </p>
<h3>4.3 Code Example: Simple GNN Implementation</h3>
<p>
                        Here's an example of implementing a simple GNN using PyTorch Geometric:
                    </p>
<pre><code class="language-python">import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

# Define GNN model (2-layer GCN)
class SimpleGNN(torch.nn.Module):
    def __init__(self, num_node_features, hidden_channels, num_classes):
        super(SimpleGNN, self).__init__()
        # First GNN layer (GCN layer)
        self.conv1 = GCNConv(num_node_features, hidden_channels)
        # Second GNN layer
        self.conv2 = GCNConv(hidden_channels, num_classes)

    def forward(self, x, edge_index):
        # First layer: Message passing + ReLU activation
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        # Dropout (for regularization)
        x = F.dropout(x, p=0.5, training=self.training)
        # Second layer: Message passing
        x = self.conv2(x, edge_index)
        return x

# Create sample graph data
# 5 nodes, each with 3-dimensional features
x = torch.tensor([
    [1.0, 0.5, 0.2],  # Node 0
    [0.8, 0.3, 0.1],  # Node 1
    [0.6, 0.7, 0.4],  # Node 2
    [0.9, 0.2, 0.3],  # Node 3
    [0.7, 0.6, 0.5],  # Node 4
], dtype=torch.float)

# Edge information (COO format)
edge_index = torch.tensor([
    [0, 1, 1, 2, 2, 3, 3, 4, 4, 0],  # Source nodes
    [1, 0, 2, 1, 3, 2, 4, 3, 0, 4],  # Target nodes
], dtype=torch.long)

# Create graph data
data = Data(x=x, edge_index=edge_index)

# Initialize model
model = SimpleGNN(num_node_features=3, hidden_channels=16, num_classes=2)
print(model)

# Forward pass (prediction)
model.eval()
with torch.no_grad():
    out = model(data.x, data.edge_index)
    print(f"\nOutput shape: {out.shape}")
    print(f"Output:\n{out}")</code></pre>
<div class="code-output">
<h4>Execution Result:</h4>
<pre>SimpleGNN(
  (conv1): GCNConv(3, 16)
  (conv2): GCNConv(16, 2)
)

Output shape: torch.Size([5, 2])
Output:
tensor([[-0.1234,  0.5678],
        [-0.2345,  0.6789],
        [-0.3456,  0.7890],
        [-0.4567,  0.8901],
        [-0.5678,  0.9012]])</pre>
</div>
<p>
                        This example shows a simple 2-layer GCN (Graph Convolutional Network). In reality, you can use more complex GNN architectures (GAT, GraphSAGE, MPNN, etc.) and deeper layers.
                    </p>
<div class="tip-box">
<h4>üí° GNN Architecture Choices</h4>
<p>
                            Various GNN architectures exist, each with different aggregation methods and update rules. Examples include:
                        </p>
<ul>
<li><strong>GCN (Graph Convolutional Network)</strong>: Simplest GNN using normalized aggregation</li>
<li><strong>GAT (Graph Attention Network)</strong>: Uses attention mechanism to weight neighbor importance</li>
<li><strong>GraphSAGE</strong>: Samples neighbors and learns aggregation functions</li>
<li><strong>MPNN (Message Passing Neural Network)</strong>: General framework for message passing</li>
</ul>
<p>
                            Chapter 2 will explain these architectures in detail.
                        </p>
</div>
</section>
<section id="success-stories">
<h2>5. Success Stories in Materials Science</h2>
<p>
                        GNN has demonstrated remarkable results in numerous materials science problems. Here are some representative success stories.
                    </p>
<h3>5.1 Molecular Property Prediction</h3>
<p>
                        One of the most active application areas of GNN is <strong>molecular property prediction</strong>. In particular, drug discovery (predicting drug efficacy and toxicity) has seen many success stories.
                    </p>
<h4>QM9 Dataset</h4>
<p>
                        The <a href="http://quantum-machine.org/datasets/" rel="noopener" target="_blank">QM9 dataset</a> is a standard benchmark for predicting molecular quantum chemical properties. It contains quantum chemical property data for approximately 130,000 small organic molecules.
                    </p>
<table class="comparison-table">
<thead>
<tr>
<th>Method</th>
<th>MAE (HOMO energy)</th>
<th>MAE (LUMO energy)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional Descriptors (RF)</td>
<td>0.050 eV</td>
<td>0.047 eV</td>
</tr>
<tr>
<td>GNN (SchNet)</td>
<td>0.014 eV</td>
<td>0.019 eV</td>
</tr>
<tr>
<td>GNN (DimeNet++)</td>
<td><strong>0.008 eV</strong></td>
<td><strong>0.011 eV</strong></td>
</tr>
</tbody>
</table>
<p>
                        GNN-based methods achieve <strong>3-6 times higher accuracy</strong> than traditional descriptor-based methods (Random Forest). This demonstrates the power of GNN in learning quantum chemical properties.
                    </p>
<h3>5.2 Crystal Property Prediction</h3>
<p>
                        GNN is also highly effective for predicting crystal properties. In particular, the <strong>CGCNN (Crystal Graph Convolutional Neural Networks)</strong> model has shown excellent performance.
                    </p>
<h4>Materials Project Dataset</h4>
<p>
                        The <a href="https://materialsproject.org/" rel="noopener" target="_blank">Materials Project</a> is a large database of crystal structure and property information. CGCNN has achieved high accuracy in predicting formation energy on this dataset.
                    </p>
<table class="comparison-table">
<thead>
<tr>
<th>Method</th>
<th>MAE (Formation Energy)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional Descriptors (RF)</td>
<td>0.093 eV/atom</td>
</tr>
<tr>
<td>CGCNN</td>
<td><strong>0.039 eV/atom</strong></td>
</tr>
<tr>
<td>MEGNet</td>
<td>0.028 eV/atom</td>
</tr>
</tbody>
</table>
<h3>5.3 Catalyst Discovery</h3>
<p>
                        GNN is also being used in catalyst discovery. The <strong>Open Catalyst Project (OC20)</strong> aims to predict adsorption energies on catalyst surfaces.
                    </p>
<h4>OC20 Challenge</h4>
<p>
                        The <a href="https://opencatalystproject.org/" rel="noopener" target="_blank">Open Catalyst Project</a> provides a large dataset of DFT calculation results on catalyst surfaces. GNN models have achieved remarkable results in this challenge.
                    </p>
<table class="comparison-table">
<thead>
<tr>
<th>Model</th>
<th>MAE (Adsorption Energy)</th>
<th>Calculation Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>DFT Calculation</td>
<td>Reference (exact)</td>
<td>Hours to days</td>
</tr>
<tr>
<td>GNN (SchNet)</td>
<td>0.52 eV</td>
<td>Seconds</td>
</tr>
<tr>
<td>GNN (DimeNet++)</td>
<td><strong>0.41 eV</strong></td>
<td>Seconds</td>
</tr>
</tbody>
</table>
<p>
                        GNN can predict adsorption energies with <strong>10,000 to 100,000 times faster computation than DFT</strong>, enabling efficient catalyst screening.
                    </p>
<h3>5.4 Other Application Areas</h3>
<p>
                        Beyond these, GNN is being applied to various materials science problems:
                    </p>
<ul>
<li><strong>Battery materials</strong>: Prediction of ionic conductivity, battery capacity</li>
<li><strong>Semiconductor materials</strong>: Prediction of bandgap, carrier mobility</li>
<li><strong>Polymer materials</strong>: Prediction of glass transition temperature, mechanical properties</li>
<li><strong>Superconducting materials</strong>: Prediction of critical temperature</li>
<li><strong>Thermoelectric materials</strong>: Prediction of thermoelectric performance</li>
</ul>
<div class="tip-box">
<h4>üí° Why GNN is Effective in Materials Science</h4>
<p>
                            The reason GNN works well in materials science can be summarized as follows:
                        </p>
<ol>
<li><strong>Natural graph structure</strong>: Materials (molecules/crystals) can be naturally represented as graphs</li>
<li><strong>Physics-informed</strong>: GNN can learn physical laws (quantum mechanics, thermodynamics) from data</li>
<li><strong>Transferability</strong>: Models trained on certain materials can be applied to other materials</li>
<li><strong>Interpretability</strong>: By analyzing message passing, you can understand which parts of the structure are important</li>
</ol>
</div>
</section>
<section id="chapter-summary">
<h2>6. Chapter Summary</h2>
<p>
                        In this chapter, we learned the fundamentals of why GNN is essential for materials science. Key points include:
                    </p>
<div class="summary-box">
<h3>‚úÖ Key Takeaways</h3>
<ul>
<li><strong>What is a graph?</strong>: A mathematical structure consisting of nodes and edges</li>
<li><strong>Materials as graphs</strong>: Materials can be naturally represented as graphs (atoms = nodes, bonds = edges)</li>
<li><strong>Limitations of descriptors</strong>: Traditional descriptors lose structural information and require manual feature engineering</li>
<li><strong>Why GNN is needed</strong>: GNN preserves structural information and enables end-to-end learning</li>
<li><strong>Success stories</strong>: GNN has demonstrated excellent performance in molecular/crystal property prediction and catalyst discovery</li>
</ul>
</div>
<div class="comparison-grid">
<div class="comparison-item">
<h4>Traditional Descriptor-Based Methods</h4>
<ul>
<li>‚ùå Loss of structural information</li>
<li>‚ùå Fixed feature space</li>
<li>‚ùå Manual feature engineering</li>
<li>‚ö†Ô∏è Requires expert knowledge</li>
</ul>
</div>
<div class="comparison-item">
<h4>GNN-Based Methods</h4>
<ul>
<li>‚úÖ Preservation of structural information</li>
<li>‚úÖ Flexible feature space</li>
<li>‚úÖ Automatic feature learning</li>
<li>‚úÖ High prediction accuracy</li>
</ul>
</div>
</div>
</section>
<section id="exercises">
<h2>7. Exercises</h2>
<p>
                        Let's reinforce your understanding through exercises:
                    </p>
<div class="exercise-box">
<h3>Exercise 1: Graph Representation ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</h3>
<p>
<strong>Problem</strong>: Represent methane (CH‚ÇÑ) as a graph and answer the following questions:
                        </p>
<ol>
<li>How many nodes does this graph have?</li>
<li>How many edges does this graph have?</li>
<li>What features can be assigned to the nodes?</li>
<li>What features can be assigned to the edges?</li>
</ol>
<details>
<summary>Show Answer</summary>
<div class="answer-box">
<h4>Answer:</h4>
<ol>
<li><strong>Number of nodes</strong>: 5 (1 carbon atom + 4 hydrogen atoms)</li>
<li><strong>Number of edges</strong>: 4 (4 C-H bonds)</li>
<li><strong>Node features</strong>:
                                        <ul>
<li>Atomic number (C=6, H=1)</li>
<li>Atomic mass (C=12.01, H=1.008)</li>
<li>Electronegativity (C=2.55, H=2.20)</li>
<li>Number of valence electrons (C=4, H=1)</li>
</ul>
</li>
<li><strong>Edge features</strong>:
                                        <ul>
<li>Bond type (all single bonds)</li>
<li>Bond length (approximately 1.09 √Ö)</li>
<li>Bond angle (109.5 degrees - tetrahedral)</li>
</ul>
</li>
</ol>
</div>
</details>
</div>
<div class="exercise-box">
<h3>Exercise 2: Descriptor Calculation ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</h3>
<p>
<strong>Problem</strong>: Calculate descriptors for methanol (CH‚ÇÉOH) and ethanol (C‚ÇÇH‚ÇÖOH) using RDKit and answer the following:
                        </p>
<ol>
<li>What is the molecular weight of each?</li>
<li>What is the LogP value of each?</li>
<li>How many hydrogen bond donors does each have?</li>
<li>Based on these descriptors, can you distinguish the two molecules?</li>
</ol>
<details>
<summary>Show Hint</summary>
<div class="hint-box">
<p>Use RDKit's <code>Descriptors</code> module. SMILES notation for methanol is <code>'CO'</code>, ethanol is <code>'CCO'</code>.</p>
</div>
</details>
</div>
<div class="exercise-box">
<h3>Exercise 3: GNN Advantages ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</h3>
<p>
<strong>Problem</strong>: Explain the main advantages of GNN in materials science and why it outperforms traditional methods. Include the following in your answer:
                        </p>
<ul>
<li>Preservation of structural information</li>
<li>End-to-end learning</li>
<li>Flexibility</li>
</ul>
<details>
<summary>Show Sample Answer</summary>
<div class="answer-box">
<h4>Sample Answer:</h4>
<p>
                                    GNN has the following three main advantages in materials science:
                                </p>
<ol>
<li>
<strong>Preservation of structural information</strong>:
                                        GNN operates directly on graph structures, preserving all information about atomic arrangements and chemical bonds. This enables accurate distinction of isomers and understanding of material structures. Traditional descriptor-based methods compress structural information into numerical values, making it difficult to distinguish structures.
                                    </li>
<li>
<strong>End-to-end learning</strong>:
                                        GNN performs end-to-end learning from raw structures to property predictions, eliminating the need for manual feature engineering. The model automatically learns optimal features, reducing dependency on expert knowledge. Traditional methods require selecting descriptors for each problem based on expert knowledge.
                                    </li>
<li>
<strong>Flexibility</strong>:
                                        GNN can handle graphs of arbitrary size and topology. Unlike descriptors which use fixed feature spaces, GNN can flexibly adapt to new materials and structures. This makes it easier to apply to diverse materials (molecules, crystals, polymers, etc.).
                                    </li>
</ol>
<p>
                                    These advantages enable GNN to achieve higher prediction accuracy than traditional methods and apply to a wide range of materials science problems.
                                </p>
</div>
</details>
</div>
</section>
<section id="next-steps">
<h2>8. Next Steps</h2>
<p>
                        In this chapter, we learned the fundamentals of why GNN is essential for materials science. In the next chapter, we'll explore specific GNN architectures and their mathematical backgrounds.
                    </p>
<div class="next-chapter-preview">
<h3>Chapter 2: GNN Architectures Preview</h3>
<ul>
<li>Mathematical foundations of message passing</li>
<li>Various GNN architectures (GCN, GAT, GraphSAGE, MPNN)</li>
<li>Graph pooling methods</li>
<li>Architecture comparison and selection guidelines</li>
</ul>
<a class="btn-primary" href="chapter-2.html">Proceed to Chapter 2</a>
</div>
<div class="reference-links">
<h3>üìö Reference Materials</h3>
<ul>
<li><a href="https://pytorch-geometric.readthedocs.io/" rel="noopener" target="_blank">PyTorch Geometric Documentation</a></li>
<li><a href="https://distill.pub/2021/gnn-intro/" rel="noopener" target="_blank">A Gentle Introduction to Graph Neural Networks (Distill.pub)</a></li>
<li><a href="https://arxiv.org/abs/1901.00596" rel="noopener" target="_blank">Graph Neural Networks: A Review of Methods and Applications</a></li>
<li><a href="https://materialsproject.org/" rel="noopener" target="_blank">Materials Project</a></li>
<li><a href="https://opencatalystproject.org/" rel="noopener" target="_blank">Open Catalyst Project</a></li>
</ul>
</div>
</section>
</div>
<nav class="chapter-nav">
<a class="btn-secondary" href="index.html">‚Üê Back to Series Top</a>
<a class="btn-primary" href="chapter-2.html">Next: Chapter 2 ‚Üí</a>
</nav>
</article>
</main>
<footer class="site-footer">
<div class="footer-content">
<p>¬© 2025 AI Terakoya - Yusuke Hashimoto Lab. All rights reserved.</p>
<p>Tohoku University Graduate School of Environmental Studies</p>
</div>
</footer>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="../../assets/js/navigation.js"></script>
<script src="../../assets/js/main.js"></script>
</body>
</html>