<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="Introduction to Composition-Based Features - Accelerate Materials Discovery with Magpie and Machine Learning" name="description"/>
<title>Introduction to Composition-Based Features Series v1.0 - AI Terakoya</title>
<!-- CSS Styling -->
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<!-- Mermaid for diagrams -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({ startOnLoad: true, theme: 'default' });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
<!-- MathJax for mathematical expressions -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Introduction to Composition-Based Features</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/composition-features-introduction/index.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="container">
<h1>Introduction to Composition-Based Features Series v1.0</h1>
<p class="subtitle">Accelerate Materials Discovery with Magpie and Machine Learning</p>
<div class="meta">
<span>üìñ Total Study Time: 150-180 minutes</span>
<span>üìä Level: Beginner to Intermediate</span>
<span>üë• Target Audience: Python basics acquired, materials science fundamentals</span>
</div>
</div>
</header>
<main class="container">
<div class="highlight-box">
<h4>üéØ What You Will Learn in This Series</h4>
<p><strong>Composition-based features</strong> are classical yet powerful methods for predicting material properties from chemical composition (types and ratios of elements). Centered on Magpie descriptors, this series systematically covers everything from utilizing elemental property databases to Python implementation with matminer.</p>
</div>
<h2 id="overview">Series Overview</h2>
<p>In materials discovery, <strong>chemical composition is the most fundamental and important information</strong>. However, a composition formula like "Fe<sub>2</sub>O<sub>3</sub>" alone cannot be input into machine learning models. This is where <strong>composition-based features</strong> play a crucial role by combining periodic table information (ionization energy, electronegativity, atomic radius, etc.) to convert composition into numerical vectors.</p>
<p>This series comprehensively covers the following, centered on the widely-used <strong>Magpie descriptors</strong>:</p>
<ul>
<li>‚úÖ <strong>Theoretical Foundation</strong>: Mathematical definitions and materials science significance of composition-based features</li>
<li>‚úÖ <strong>Practical Skills</strong>: Feature generation workflows using the matminer library</li>
<li>‚úÖ <strong>Comparative Analysis</strong>: When to use composition-based vs structure-based features (CGCNN/MPNN and other GNNs)</li>
<li>‚úÖ <strong>Latest Trends</strong>: Limitations of Magpie and evolution to GNN methods</li>
</ul>
<h2 id="why-composition">Why Composition-Based Features Are Important</h2>
<div class="highlight-box">
<h4>üí° Composition-Based vs Structure-Based</h4>
<p>Material features have two main approaches:</p>
<ul>
<li><strong>Composition-Based</strong> (this series): Generate features from chemical composition only (no structure information required)</li>
<li><strong>Structure-Based</strong> (<a href="/AI-Knowledge-Notes/knowledge/en/MI/gnn-introduction/index.html">GNN Introduction Series</a>): Learn from 3D structures including atomic coordinates and bonding information</li>
</ul>
<p><strong>Strengths of Composition-Based</strong>: Effective for exploring new materials with unknown structures, high-speed screening, and cases with limited data</p>
</div>
<h3>Typical Applications of Composition-Based Features</h3>
<ol>
<li><strong>High-Speed Materials Screening</strong>: Formation energy prediction for 1 million compounds (10-100√ó faster than GNNs)</li>
<li><strong>Experimental Data-Driven Exploration</strong>: Property prediction from limited experimental data (combined with transfer learning)</li>
<li><strong>Hybrid Models</strong>: Improved accuracy by combining composition features + GNN features</li>
</ol>
<h2 id="learning">How to Study</h2>
<h3>Recommended Learning Flow</h3>
<div class="mermaid">
timeline
    title Introduction to Composition-Based Features Learning Flow
    section Chapter 1 : Fundamentals
        What are Composition-Based Features : Definitions and history
        Limitations of Conventional Descriptors : Density and symmetry are insufficient
        Background of Magpie : Utilizing elemental properties
    section Chapter 2 : Magpie Details
        Types of Statistical Descriptors : Mean, variance, max, min
        145 Elemental Properties : Periodic table database
        Mathematical Implementation : Weighted statistics
    section Chapter 3 : Databases
        Elemental Property Databases : Magpie/Deml/Jarvis
        Choosing Featurizers : matminer API
        Custom Featurizer Creation : Adding original descriptors
    section Chapter 4 : Machine Learning Integration
        Model Selection : Random Forest/XGBoost/NN
        Hyperparameter Optimization : Optuna/GridSearch
        Feature Importance Analysis : SHAP/LIME
    section Chapter 5 : Python Practice
        matminer Workflow : Data prep ‚Üí Feature generation ‚Üí Model training
        Materials Project Data : Property prediction with real data
        Performance Evaluation and Benchmarking : Comparison with GNNs
        </div>
<p><strong>For Beginners (Learning composition features for the first time):</strong><br/>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 4 ‚Üí Chapter 5 (all chapters recommended)<br/>
        - Time required: 150-180 minutes</p>
<p><strong>For Intermediate Learners (Machine learning experience, want to use matminer):</strong><br/>
        - Chapter 2 ‚Üí Chapter 3 ‚Üí Chapter 5<br/>
        - Time required: 90-120 minutes</p>
<p><strong>For GNN Learners (Want to compare composition vs structure):</strong><br/>
        - Chapter 1 ‚Üí Chapter 2 ‚Üí Chapter 5 ‚Üí <br/>
        - Time required: 120-150 minutes</p>
<h2 id="chapters">Chapter Details</h2>
<div class="chapter-card">
<h3><a href="chapter-1.html">Chapter 1: Fundamentals of Composition-Based Features</a></h3>
<p><strong>Difficulty</strong>: Introductory<br/>
<strong>Reading Time</strong>: 25-30 minutes<br/>
<strong>Code Examples</strong>: 5</p>
<h4>Learning Content</h4>
<ol>
<li><strong>What Are Composition-Based Features</strong> - Converting chemical composition to numerical vectors</li>
<li><strong>Historical Background</strong> - Before and after Ward (2016) Magpie paper</li>
<li><strong>Limitations of Conventional Descriptors</strong> - Density, symmetry, and lattice parameters are insufficient</li>
<li><strong>Utilizing Elemental Properties</strong> - The power of periodic table databases</li>
<li><strong>Success Stories</strong> - Applications in OQMD and Materials Project</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Explain the definition and role of composition-based features</li>
<li>‚úÖ Demonstrate differences from conventional material descriptors with examples</li>
<li>‚úÖ Understand why Magpie is widely used</li>
</ul>
<p><strong><a class="nav-button" href="chapter-1.html">Read Chapter 1 ‚Üí</a></strong></p>
</div>
<hr/>
<div class="chapter-card">
<h3><a href="chapter-2.html">Chapter 2: Magpie and Statistical Descriptors</a></h3>
<p><strong>Difficulty</strong>: Beginner to Intermediate<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 8</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Mathematical Definition of Magpie Descriptors</strong> - 145-dimensional statistics</li>
<li><strong>Types of Statistical Descriptors</strong> - Mean, variance, maximum, minimum, range, mode</li>
<li><strong>Weighted vs Unweighted</strong> - Effect of composition ratio weighting</li>
<li><strong>22 Types of Elemental Properties</strong> - Ionization energy, electronegativity, atomic radius, etc.</li>
<li><strong>Implementation Example</strong> - Manual calculation using NumPy</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Understand Magpie descriptor calculation methods with formulas</li>
<li>‚úÖ List the 22 types of elemental properties</li>
<li>‚úÖ Explain the significance of weighted statistics</li>
</ul>
<p><strong><a class="nav-button" href="chapter-2.html">Read Chapter 2 ‚Üí</a></strong></p>
</div>
<hr/>
<div class="chapter-card">
<h3><a href="chapter-3.html">Chapter 3: Elemental Property Databases and Featurizers</a></h3>
<p><strong>Difficulty</strong>: Intermediate<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 10</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Types of Elemental Property Databases</strong> - Magpie, Deml, Jarvis, Matscholar</li>
<li><strong>matminer Featurizer API</strong> - ElementProperty, Stoichiometry, OxidationStates</li>
<li><strong>Choosing Featurizers</strong> - Selection criteria based on application</li>
<li><strong>Custom Featurizer Creation</strong> - How to add original elemental properties</li>
<li><strong>Feature Preprocessing</strong> - Standardization, missing value handling</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Choose among 3+ elemental property databases appropriately</li>
<li>‚úÖ Select matminer Featurizers based on application</li>
<li>‚úÖ Implement custom Featurizers</li>
</ul>
<p><strong><a class="nav-button" href="chapter-3.html">Read Chapter 3 ‚Üí</a></strong></p>
</div>
<hr/>
<div class="chapter-card">
<h3><a href="chapter-4.html">Chapter 4: Integration with Machine Learning Models</a></h3>
<p><strong>Difficulty</strong>: Intermediate to Advanced<br/>
<strong>Reading Time</strong>: 30-35 minutes<br/>
<strong>Code Examples</strong>: 12</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Model Selection Criteria</strong> - Random Forest, XGBoost, LightGBM, Neural Networks</li>
<li><strong>Hyperparameter Optimization</strong> - Optuna, GridSearchCV, BayesSearchCV</li>
<li><strong>Feature Importance Analysis</strong> - SHAP, LIME, Permutation Importance</li>
<li><strong>Ensemble Methods</strong> - Bagging, Boosting, Stacking</li>
<li><strong>Performance Evaluation Metrics</strong> - MAE, RMSE, R¬≤, Cross-validation</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Select appropriate machine learning models based on tasks</li>
<li>‚úÖ Execute hyperparameter optimization with Optuna</li>
<li>‚úÖ Interpret feature importance using SHAP values</li>
</ul>
<p><strong><a class="nav-button" href="chapter-4.html">Read Chapter 4 ‚Üí</a></strong></p>
</div>
<hr/>
<div class="chapter-card">
<h3><a href="chapter-5.html">Chapter 5: Python Practice - matminer Workflow</a></h3>
<p><strong>Difficulty</strong>: Intermediate to Advanced<br/>
<strong>Reading Time</strong>: 35-45 minutes<br/>
<strong>Code Examples</strong>: 15 (all executable)</p>
<h4>Learning Content</h4>
<ol>
<li><strong>Environment Setup</strong> - Anaconda, pip, Google Colab</li>
<li><strong>Data Preparation</strong> - Materials Project API, OQMD dataset</li>
<li><strong>Feature Generation Pipeline</strong> - Composition formula ‚Üí Magpie descriptors ‚Üí Standardization</li>
<li><strong>Model Training and Evaluation</strong> - Formation energy prediction, band gap prediction</li>
<li><strong>Performance Comparison with GNNs</strong> - Accuracy, speed, interpretability</li>
<li><strong>Hybrid Approach</strong> - Composition features + Structure features</li>
</ol>
<h4>Learning Objectives</h4>
<ul>
<li>‚úÖ Build end-to-end prediction workflows with matminer</li>
<li>‚úÖ Perform property prediction with Materials Project data (R¬≤ &gt; 0.85)</li>
<li>‚úÖ Quantitatively compare composition-based and GNN performance</li>
<li>‚úÖ Achieve improved accuracy with hybrid models</li>
</ul>
<p><strong><a class="nav-button" href="chapter-5.html">Read Chapter 5 ‚Üí</a></strong></p>
</div>
<hr/>
<h2 id="outcomes">Overall Learning Outcomes</h2>
<p>Upon completing this series, you will acquire the following skills and knowledge:</p>
<h3>Knowledge Level (Understanding)</h3>
<ul>
<li>‚úÖ Explain the theoretical foundation and history of composition-based features</li>
<li>‚úÖ Understand the mathematical definition of Magpie descriptors</li>
<li>‚úÖ Know the types and characteristics of elemental property databases</li>
<li>‚úÖ Understand criteria for choosing between composition-based and GNN approaches</li>
</ul>
<h3>Practical Skills (Doing)</h3>
<ul>
<li>‚úÖ Generate feature vectors from composition using matminer</li>
<li>‚úÖ Extend features by combining multiple Featurizers</li>
<li>‚úÖ Perform property prediction with machine learning models (RF/XGBoost/NN)</li>
<li>‚úÖ Interpret prediction rationale using SHAP values</li>
<li>‚úÖ Quantitatively compare performance with GNNs</li>
</ul>
<h3>Application Ability (Applying)</h3>
<ul>
<li>‚úÖ Design appropriate features for new materials discovery tasks</li>
<li>‚úÖ Build hybrid models (composition + structure)</li>
<li>‚úÖ Design prediction workflows combining experimental data</li>
<li>‚úÖ Apply feature engineering to industrial applications (battery materials, catalysts)</li>
</ul>
<hr/>
<h2 id="faq">Frequently Asked Questions (FAQ)</h2>
<details>
<summary>Q1: Should I use composition-based features or GNNs (structure-based)?</summary>
<p><strong>A:</strong> It depends on the task and data characteristics:</p>
<ul>
<li><strong>Composition-based</strong> advantages: (1) Materials discovery with unknown structures, (2) High-speed screening (1 million compound scale), (3) Limited data cases (&lt;1000 samples)</li>
<li><strong>GNN</strong> advantages: (1) Properties with strong structure dependence (elastic modulus, thermal conductivity), (2) Accuracy priority, (3) Sufficient data available (&gt;10000 samples)</li>
<li><strong>Hybrid</strong> is strongest: Using both composition and GNN features improves accuracy (implemented in Chapter 5)</li>
</ul>
</details>
<details>
<summary>Q2: Aren't 145 dimensions of Magpie descriptors too many? Concerns about overfitting?</summary>
<p><strong>A:</strong> This is rarely a problem in practice:</p>
<ul>
<li>145 dimensions is considered low in modern machine learning (GNNs learn thousands of dimensions in embeddings)</li>
<li>Elemental properties have physical meaning, unlike random high dimensions</li>
<li>Dimensionality reduction possible with regularization (L1/L2) or feature selection</li>
<li>Good performance reported experimentally even with 100-1000 samples</li>
</ul>
</details>
<details>
<summary>Q3: Are there libraries other than matminer?</summary>
<p><strong>A:</strong> Yes, these are available:</p>
<ul>
<li><strong>DScribe</strong>: SOAP, MBTR, ACSF descriptors (supports molecules and crystals)</li>
<li><strong>CFID</strong>: Composition-based Feature Identifier</li>
<li><strong>Pymatgen</strong>: matminer's foundation library (low-level API)</li>
<li><strong>XenonPy</strong>: Integrates deep learning and feature generation</li>
</ul>
<p>This series focuses on the most widely-used matminer.</p>
</details>
<details>
<summary>Q4: Can Magpie descriptors be calculated automatically from chemical formulas (e.g., Fe2O3)?</summary>
<p><strong>A:</strong> Yes, it's easy with matminer:</p>
<pre><code>from matminer.featurizers.composition import ElementProperty

featurizer = ElementProperty.from_preset("magpie")
features = featurizer.featurize_dataframe(df, col_id="composition")
# Automatically generates 145-dimensional vectors from df["composition"] column (Fe2O3, etc.)</code></pre>
<p>Chapter 5 provides detailed code examples.</p>
</details>
<details>
<summary>Q5: What is the prediction accuracy difference between GNNs (CGCNN, MPNN) and composition-based features?</summary>
<p><strong>A:</strong> Depends on the dataset and task, but representative benchmark results:</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Magpie + RF</th>
<th>CGCNN</th>
<th>Hybrid</th>
</tr>
</thead>
<tbody>
<tr>
<td>Formation Energy (OQMD)</td>
<td>MAE 0.12 eV</td>
<td>MAE 0.039 eV</td>
<td>MAE 0.035 eV</td>
</tr>
<tr>
<td>Band Gap (Materials Project)</td>
<td>MAE 0.45 eV</td>
<td>MAE 0.39 eV</td>
<td>MAE 0.36 eV</td>
</tr>
<tr>
<td>Inference Speed (1M compounds)</td>
<td>10 min</td>
<td>100 min</td>
<td>110 min</td>
</tr>
</tbody>
</table>
<p>Detailed comparison experiments are conducted in Chapter 5.</p>
</details>
<details>
<summary>Q6: What are the differences between elemental property databases (Magpie/Deml/Jarvis)?</summary>
<p><strong>A:</strong> Characteristics of each database:</p>
<ul>
<li><strong>Magpie</strong> (Ward+ 2016): 22 elemental properties, most widely used, proven track record in Materials Project</li>
<li><strong>Deml</strong> (Deml+ 2016): Considers oxidation states, particularly strong for oxides</li>
<li><strong>Jarvis</strong> (Choudhary+ 2020): Includes DFT calculation values, latest elemental properties</li>
<li><strong>Matscholar</strong> (Tshitoyan+ 2019): Element embeddings extracted from 2 million papers using NLP</li>
</ul>
<p>Chapter 3 implements usage of each database.</p>
</details>
<details>
<summary>Q7: Can composition-based features be used for transfer learning?</summary>
<p><strong>A:</strong> Yes, the following approaches are effective:</p>
<ul>
<li><strong>Pre-training</strong>: Train on Materials Project (60k compounds) ‚Üí Fine-tune on experimental data (100 samples)</li>
<li><strong>Domain Adaptation</strong>: Train on inorganic materials ‚Üí Apply to organic-inorganic hybrids</li>
<li><strong>Meta-learning</strong>: Learn common feature representations across multiple tasks (formation energy, band gap, elastic modulus)</li>
</ul>
<p>Chapter 4 implements transfer learning using XGBoost and neural networks.</p>
</details>
<details>
<summary>Q8: Can custom elemental properties (e.g., rarity, cost) be added?</summary>
<p><strong>A:</strong> Yes, you can create custom Featurizers by inheriting matminer's BaseFeaturizer:</p>
<pre><code>from matminer.featurizers.base import BaseFeaturizer

class CustomElementProperty(BaseFeaturizer):
    def featurize(self, comp):
        # Reference custom elemental property database
        rarity = get_element_rarity(comp)
        cost = get_element_cost(comp)
        return [rarity, cost]</code></pre>
<p>Chapter 3 provides detailed custom Featurizer implementation examples.</p>
</details>
<details>
<summary>Q9: How is interpretability (Explainability) with composition-based features?</summary>
<p><strong>A:</strong> There are aspects that are easier to interpret than GNNs:</p>
<ul>
<li><strong>SHAP Values</strong>: Clear which elemental properties (e.g., average electronegativity) contributed to predictions</li>
<li><strong>Physical Meaning</strong>: Chemical interpretations possible, such as "higher ionization energy leads to lower formation energy"</li>
<li><strong>Integration with Domain Knowledge</strong>: Direct utilization of periodic table knowledge</li>
</ul>
<p>Chapter 4 implements interpretability analysis using SHAP/LIME.</p>
</details>
<details>
<summary>Q10: After completing this series, what learning resources should I pursue next?</summary>
<p><strong>A:</strong> The following learning paths are recommended:</p>
<ul>
<li><strong>Comparison with GNNs</strong>:  ‚Üí Quantitatively compare both methods</li>
<li><strong>Extension to Deep Learning</strong>: <a href="/AI-Knowledge-Notes/knowledge/en/MI/gnn-introduction/index.html">GNN Introduction Series</a> ‚Üí Learn CGCNN, MPNN</li>
<li><strong>Practical Application</strong>:  ‚Üí Apply to actual projects</li>
<li><strong>Paper Implementation</strong>: Reproduce Ward+ (2016) "A general-purpose machine learning framework for predicting properties of inorganic materials"</li>
</ul>
</details>
<hr/>
<h2 id="prerequisites">Prerequisites</h2>
<p>To effectively learn this series, the following prerequisites are recommended:</p>
<h3>Required (Must Have)</h3>
<ul>
<li>‚úÖ <strong>Python Basics</strong>: Basic operations with NumPy, Pandas, Matplotlib</li>
<li>‚úÖ <strong>Materials Science Basics</strong>: Concepts of chemical composition, periodic table, elemental properties</li>
<li>‚úÖ <strong>Machine Learning Basics</strong>: Concepts of supervised learning, regression/classification, cross-validation</li>
</ul>
<h3>Recommended (Nice to Have)</h3>
<ul>
<li>üìö <strong>scikit-learn</strong>: Experience with Random Forest, model evaluation</li>
<li>üìö <strong>Statistics</strong>: Understanding of mean, variance, standard deviation</li>
<li>üìö <strong>Crystallography</strong>: Basics of lattices, symmetry (reviewed in Chapter 1)</li>
</ul>
<h3>Not Required</h3>
<ul>
<li>‚ùå <strong>Deep Learning</strong>: GNN knowledge not required (composition-based focuses on classical machine learning)</li>
<li>‚ùå <strong>Quantum Chemistry</strong>: DFT calculation experience not required</li>
</ul>
<hr/>
<h2 id="related">Related Series</h2>
<div class="highlight-box">
<h4>üîó Integrated Learning with GNN Series</h4>
<p>By learning this series together with the <a href="/AI-Knowledge-Notes/knowledge/en/MI/gnn-introduction/index.html">GNN Introduction Series</a>, you can grasp the complete picture of material features:</p>
<ul>
<li><strong>Introduction to Composition-Based Features</strong> (this series): Property prediction from chemical composition</li>
<li><strong>GNN Introduction Series</strong>: Property prediction from 3D structures (CGCNN, MPNN, SchNet)</li>
<li><strong>Composition vs GNN Comparison Series</strong> (coming soon): Quantitative benchmarking of both methods</li>
</ul>
</div>
<h3>Recommended Learning Order</h3>
<ol>
<li><strong>Introduction to Composition-Based Features</strong> (this series) ‚Üí Build fundamentals</li>
<li><strong><a href="/AI-Knowledge-Notes/knowledge/en/MI/gnn-introduction/index.html">GNN Introduction Series</a></strong> ‚Üí Learn structure-based methods</li>
<li><strong>Composition vs GNN Comparison Series</strong> (coming soon) ‚Üí Master when to use which</li>
<li><strong>Materials Screening Workflow</strong> (coming soon) ‚Üí Practical application</li>
</ol>
<hr/>
<h2 id="start">Let's Get Started!</h2>
<p>Are you ready? Start with Chapter 1 and begin your journey into the world of composition-based features!</p>
<p><strong><a class="nav-button" href="chapter-1.html">Chapter 1: Fundamentals of Composition-Based Features ‚Üí</a></strong></p>
<hr/>
<p><strong>Update History</strong></p>
<ul>
<li><strong>2025-11-02</strong>: v1.0 Initial release</li>
</ul>
<hr/>
<p><strong>Your materials discovery journey starts here!</strong></p>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranties, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<div class="container">
<p>¬© 2025 AI Terakoya - Dr. Yusuke Hashimoto, Tohoku University</p>
<p>Licensed under CC BY 4.0</p>
</div>
</footer>
</body>
</html>
