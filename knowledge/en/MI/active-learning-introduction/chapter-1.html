<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: The Need for Active Learning - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/active-learning-introduction/index.html">Active Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 1: The Need for Active Learning</h1>
<p class="subtitle">Dramatically Reduce Experiment Count Through Active Data Selection</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 7</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 1: The Need for Active Learning</h1>
<p><strong>Dramatically Reduce Experiment Count Through Active Data Selection</strong></p>
<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Explain the definition and advantages of Active Learning</li>
<li>‚úÖ Understand the four main query strategy techniques</li>
<li>‚úÖ Explain the exploration-exploitation tradeoff</li>
<li>‚úÖ Provide three or more successful examples in materials science</li>
<li>‚úÖ Perform quantitative comparisons with random sampling</li>
</ul>
<p><strong>Reading Time</strong>: 20-25 minutes
<strong>Code Examples</strong>: 7
<strong>Exercises</strong>: 3</p>
<hr/>
<h2>1.1 What is Active Learning?</h2>
<h3>Definition: Efficient Learning Through Active Data Selection</h3>
<p><strong>Active Learning</strong> is a method where machine learning models actively select "which data to acquire next," enabling the construction of high-accuracy models with minimal training data.</p>
<p><strong>Differences from Passive Learning</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Passive Learning</th>
<th>Active Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Selection</td>
<td>Random or existing datasets</td>
<td>Actively selected by model</td>
</tr>
<tr>
<td>Learning Efficiency</td>
<td>Low (requires large data)</td>
<td>High (high accuracy with small data)</td>
</tr>
<tr>
<td>Data Acquisition Cost</td>
<td>Not considered</td>
<td>Considered</td>
</tr>
<tr>
<td>Application Scenarios</td>
<td>Data is inexpensive</td>
<td>Data is expensive</td>
</tr>
</tbody>
</table>
<p><strong>Importance in Materials Science</strong>:
- Single experiments take days to weeks
- High experimental costs (catalyst synthesis, DFT calculations, etc.)
- Vast search spaces (10^6 to 10^60 candidates)</p>
<h3>Basic Active Learning Cycle</h3>
<div class="mermaid">
flowchart LR
    A["Initial Data<br/>Few samples"] --&gt; B["Model Training<br/>Build prediction model"]
    B --&gt; C["Candidate Evaluation<br/>Query Strategy"]
    C --&gt; D["Select most<br/>informative sample"]
    D --&gt; E["Experiment Execution<br/>Data Acquisition"]
    E --&gt; F{"Stopping criteria?<br/>Goal achieved or<br/>budget limit"}
    F --&gt;|No| B
    F --&gt;|Yes| G["Final Model"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style G fill:#4CAF50,color:#fff
</div>
<p><strong>Key Points</strong>:
1. <strong>Start with small initial data</strong> (typically 10-20 samples)
2. <strong>Intelligently select next sample</strong> using query strategy
3. <strong>Execute experiments</strong> adding data one at a time
4. <strong>Repeat model updates</strong>
5. <strong>Continue until goal achieved</strong></p>
<hr/>
<h2>1.2 Query Strategy Fundamentals</h2>
<h3>1.2.1 Uncertainty Sampling</h3>
<p><strong>Principle</strong>: Select samples where the model's prediction is most uncertain</p>
<p><strong>Formula</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Uncertainty}(x)
$$</p>
<p>where $\mathcal{U}$ is the set of unlabeled samples</p>
<p><strong>Uncertainty Measurement Methods</strong>:</p>
<p><strong>Regression Problems</strong>:
$$
\text{Uncertainty}(x) = \sigma(x)
$$
(standard deviation of prediction)</p>
<p><strong>Classification Problems (2-class)</strong>:
$$
\text{Uncertainty}(x) = 1 - |P(y=1|x) - P(y=0|x)|
$$
(inverse of absolute probability difference; closer to 0.5 means more uncertain)</p>
<p><strong>Code Example 1: Uncertainty Sampling Implementation</strong></p>
<pre>__PROTECTED_CODE_0__</pre>
<p><strong>Output</strong>:</p>
<pre>__PROTECTED_CODE_1__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Simple and intuitive
- ‚úÖ Low computational cost
- ‚úÖ Effective for many problems</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è Does not consider diversity of search space
- ‚ö†Ô∏è May be biased toward local regions</p>
<hr/>
<h3>1.2.2 Diversity Sampling</h3>
<p><strong>Principle</strong>: Select samples that are different (diverse) from existing data</p>
<p><strong>Formula</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \min_{x_i \in \mathcal{L}} d(x, x_i)
$$</p>
<p>where $\mathcal{L}$ is the set of labeled samples, and $d(\cdot, \cdot)$ is a distance function</p>
<p><strong>Distance Measurement Methods</strong>:
- Euclidean distance: $d(x_i, x_j) = |x_i - x_j|_2$
- Mahalanobis distance: $d(x_i, x_j) = \sqrt{(x_i - x_j)^T \Sigma^{-1} (x_i - x_j)}$
- Cosine distance: $d(x_i, x_j) = 1 - \frac{x_i \cdot x_j}{|x_i| |x_j|}$</p>
<p><strong>Code Example 2: Diversity Sampling Implementation</strong></p>
<pre>__PROTECTED_CODE_2__</pre>
<p><strong>Output</strong>:</p>
<pre>__PROTECTED_CODE_3__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Covers wide range of search space
- ‚úÖ Prevents bias toward local optima
- ‚úÖ Works well with clustering</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è Does not consider model uncertainty
- ‚ö†Ô∏è Slightly higher computational cost</p>
<hr/>
<h3>1.2.3 Query-by-Committee</h3>
<p><strong>Principle</strong>: Select samples where multiple models (committee) disagree the most</p>
<p><strong>Formula</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Disagreement}(C, x)
$$</p>
<p>where $C = {M_1, M_2, ..., M_K}$ is a set of models (committee)</p>
<p><strong>Disagreement Measurement</strong>:</p>
<p><strong>Regression Problems (Variance)</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K (M_k(x) - \bar{M}(x))^2
$$</p>
<p><strong>Classification Problems (Kullback-Leibler Divergence)</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K KL(P_k(\cdot|x) | P_C(\cdot|x))
$$</p>
<p><strong>Code Example 3: Query-by-Committee Implementation</strong></p>
<pre>__PROTECTED_CODE_4__</pre>
<p><strong>Output</strong>:</p>
<pre>__PROTECTED_CODE_5__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Leverages knowledge from diverse models
- ‚úÖ Reduces model bias
- ‚úÖ Robust uncertainty estimation</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è High computational cost (training multiple models)
- ‚ö†Ô∏è Depends on model selection</p>
<hr/>
<h3>1.2.4 Expected Model Change</h3>
<p><strong>Principle</strong>: Select samples that cause the largest change in model parameters</p>
<p><strong>Formula</strong> (gradient-based):
$$
x^* = \arg\max_{x \in \mathcal{U}} |\nabla_\theta \mathcal{L}(\theta; x, \hat{y})|
$$</p>
<p>where $\theta$ is model parameters, $\mathcal{L}$ is loss function, $\hat{y}$ is predicted value</p>
<p><strong>Advantages</strong>:
- ‚úÖ Directly evaluates impact on model improvement
- ‚úÖ Enables efficient learning</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è High computational cost
- ‚ö†Ô∏è Limited to models with computable gradients</p>
<hr/>
<h2>1.3 Exploration vs Exploitation</h2>
<h3>The Tradeoff Concept</h3>
<p>One of the most important concepts in active learning is the <strong>exploration-exploitation tradeoff</strong>.</p>
<p><strong>Exploration</strong>:
- Explore unknown regions
- Collect diverse samples
- Acquire new information
- Take risks</p>
<p><strong>Exploitation</strong>:
- Intensively investigate known good regions
- Prioritize high uncertainty regions
- Maximize use of existing knowledge
- Improve safely</p>
<h3>Visualizing the Tradeoff</h3>
<div class="mermaid">
flowchart TB
    subgraph Exploration_Focused [Exploration-focused]
    A["Aggressively sample
unknown regions"]
    A --&gt; B["High discovery potential"]
    A --&gt; C["Slow learning"]
    end

    subgraph Exploitation_Focused [Exploitation-focused]
    D["Intensively sample
high uncertainty regions"]
    D --&gt; E["Fast convergence"]
    D --&gt; F["Risk of
local optima"]
    end

    subgraph Balance
    G["Balanced exploration
and exploitation"]
    G --&gt; H["Efficient learning"]
    G --&gt; I["Wide and
deep understanding"]
    end

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style G fill:#e8f5e9
    style I fill:#4CAF50,color:#fff
</div>
<h3>Œµ-greedy Approach</h3>
<p><strong>Principle</strong>: Explore with probability $\epsilon$, exploit with probability $1-\epsilon$</p>
<p><strong>Algorithm</strong>:</p>
<pre>__PROTECTED_CODE_6__</pre>
<p><strong>Code Example 4: Œµ-greedy Active Learning</strong></p>
<pre>__PROTECTED_CODE_7__</pre>
<p><strong>Output</strong>:</p>
<pre>__PROTECTED_CODE_8__</pre>
<p><strong>Choosing Œµ</strong>:
- $\epsilon = 0$: Full exploitation (risk of local optima)
- $\epsilon = 1$: Full exploration (random sampling)
- $\epsilon = 0.1 \sim 0.2$: Well-balanced (recommended)</p>
<hr/>
<h3>Upper Confidence Bound (UCB)</h3>
<p><strong>Principle</strong>: Prediction mean + uncertainty bonus</p>
<p><strong>Formula</strong>:
$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$</p>
<ul>
<li>$\mu(x)$: Prediction mean</li>
<li>$\sigma(x)$: Prediction standard deviation</li>
<li>$\kappa$: Exploration parameter (typically 1.0-3.0)</li>
</ul>
<p><strong>Code Example 5: Sample Selection Using UCB</strong></p>
<pre>__PROTECTED_CODE_9__</pre>
<p><strong>Output</strong>:</p>
<pre>__PROTECTED_CODE_10__</pre>
<p><strong>Impact of Œ∫</strong>:
- Large $\kappa$ ‚Üí Exploration-focused
- Small $\kappa$ ‚Üí Exploitation-focused
- Recommended: $\kappa = 2.0 \sim 2.5$</p>
<hr/>
<h2>1.4 Case Study: Catalyst Activity Prediction</h2>
<h3>Problem Setup</h3>
<p><strong>Objective</strong>: Predict catalyst reaction activity and discover the most active catalyst in 10 experiments</p>
<p><strong>Dataset</strong>:
- Candidate catalysts: 500 types
- Features: Metal composition (3 elements), loading, calcination temperature
- Target variable: Reaction rate constant (k)</p>
<p><strong>Constraints</strong>:
- Single experiment takes 3 days
- Budget limited to maximum 10 experiments</p>
<h3>Random Sampling vs Active Learning</h3>
<p><strong>Code Example 6: Comparative Experiment for Catalyst Activity Prediction</strong></p>
<pre>__PROTECTED_CODE_11__</pre>
<p><strong>Expected Output</strong>:</p>
<pre>__PROTECTED_CODE_12__</pre>
<p><strong>Important Observations</strong>:
- ‚úÖ Active Learning reaches 97.5% of true optimal value in 10 experiments
- ‚úÖ Random Sampling only reaches 79.3%
- ‚úÖ <strong>23% performance improvement</strong>
- ‚úÖ R¬≤ score steadily improves (0.512 ‚Üí 0.843)</p>
<hr/>
<h2>1.5 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Active Learning Definition</strong>
   - Efficient learning through active data selection
   - Differences from passive learning
   - Importance in materials science (experimental cost reduction)</p>
</li>
<li>
<p><strong>Query Strategies</strong>
   - <strong>Uncertainty Sampling</strong>: Select samples with uncertain predictions
   - <strong>Diversity Sampling</strong>: Select diverse samples
   - <strong>Query-by-Committee</strong>: Leverage disagreement between models
   - <strong>Expected Model Change</strong>: Select by impact on model updates</p>
</li>
<li>
<p><strong>Exploration-Exploitation</strong>
   - Œµ-greedy: Probabilistically switch between exploration and exploitation
   - UCB: Prediction mean + uncertainty bonus
   - Importance of balance</p>
</li>
<li>
<p><strong>Practical Examples</strong>
   - 23% performance improvement in catalyst activity prediction
   - 97.5% achievement rate in 10 experiments
   - 1.3√ó efficiency over random sampling</p>
</li>
</ol>
<h3>Key Takeaways</h3>
<ul>
<li>‚úÖ Active learning excels in <strong>problems with high data acquisition costs</strong></li>
<li>‚úÖ Query strategy selection <strong>greatly affects exploration efficiency</strong></li>
<li>‚úÖ <strong>Balance in exploration-exploitation is crucial</strong></li>
<li>‚úÖ Can <strong>reduce experiments by 50-90%</strong> in materials science</li>
<li>‚úÖ <strong>Significant improvements achievable in 10-20 experiments</strong></li>
</ul>
<h3>Next Chapter</h3>
<p>In Chapter 2, we will learn the core <strong>uncertainty estimation techniques</strong> for active learning:
- Ensemble methods (Random Forest, LightGBM)
- Dropout methods (Bayesian Neural Networks)
- Gaussian Processes (rigorous uncertainty quantification)</p>
<p><strong><a href="chapter-2.html">Chapter 2: Uncertainty Estimation Techniques ‚Üí</a></strong></p>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>For the following situations, determine which query strategy is most appropriate and explain your reasoning.</p>
<p><strong>Situation A</strong>: Predicting tensile strength of alloys. 10,000 candidate materials, 50 initial data samples, budget allows 20 additional experiments. Search space is vast, but strength varies relatively smoothly with composition.</p>
<p><strong>Situation B</strong>: Discovery of novel organic semiconductor materials. 100,000 candidate molecules, 10 initial data samples, budget allows 10 additional experiments. Properties vary very complexly with molecular structure.</p>
<details>
<summary>Hint</summary>

- Situation A: Vast search space ‚Üí ?
- Situation B: Little data, complex function ‚Üí ?
- Review characteristics of query strategies

</details>
<details>
<summary>Example Solution</summary>

**Situation A: Diversity Sampling is optimal**

**Reasoning**:
1. Search space is vast (10,000 types), difficult to cover entirely with 20 experiments
2. 50 initial samples available, sufficient for reasonable model construction
3. Strength varies smoothly, so covering wide range enables grasping overall picture
4. Diversity sampling provides even coverage of search space

**Alternative**: UCB sampling (with large exploration parameter Œ∫)

**Situation B: Uncertainty Sampling (or Query-by-Committee) is optimal**

**Reasoning**:
1. Very few initial data samples (10 samples)
2. Properties vary complexly, so should prioritize high uncertainty regions
3. Budget is limited (10 experiments), requiring efficient learning
4. Uncertainty sampling selects most informative samples

**Alternative**: Query-by-Committee (handles complex functions through model diversity)

</details>
<hr/>
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Implement Œµ-greedy Active Learning and compare exploration efficiency for different Œµ values (0.0, 0.1, 0.2, 0.5).</p>
<p><strong>Tasks</strong>:
1. Generate synthetic material properties dataset (500 samples)
2. Execute Œµ-greedy AL with 10 initial samples and 15 additional experiments
3. Plot best value discovered for each Œµ
4. Select optimal Œµ and explain reasoning</p>
<details>
<summary>Hint</summary>

- Reference Code Example 4 to implement Œµ-greedy
- Run 5 trials for each Œµ and average results
- Plot: x-axis = experiment count, y-axis = best value discovered

</details>
<details>
<summary>Example Solution</summary>
<pre>__PROTECTED_CODE_13__</pre>


**Expected Output**:

<pre>__PROTECTED_CODE_14__</pre>


**Conclusion**:
- **Œµ = 0.2 is optimal** (99.0% achievement rate)
- Œµ = 0.0 prone to local optima (89.2%)
- Œµ = 0.5 over-explores inefficiently (93.4%)
- **Moderate exploration (Œµ=0.1-0.2) provides good balance**

</details>
<hr/>
<h3>Problem 3 (Difficulty: Hard)</h3>
<p>Compare three query strategies (Uncertainty, Diversity, Query-by-Committee) on the same dataset and select the most efficient method.</p>
<p><strong>Requirements</strong>:
1. Generate synthetic multi-objective material data (1,000 samples, 10 dimensions)
2. Execute each method with 20 initial samples and 30 additional experiments
3. Evaluate using these metrics:
   - Best value discovered
   - R¬≤ score (prediction accuracy on all data)
   - Computation time
4. Select most efficient method overall</p>
<details>
<summary>Hint</summary>

- Implement each method independently
- Run 5 trials and average results
- Measure computation time with `time.time()`
- Consider tradeoffs (accuracy vs computation time)

</details>
<details>
<summary>Example Solution</summary>
<pre>__PROTECTED_CODE_15__</pre>


**Expected Output**:

<pre>__PROTECTED_CODE_16__</pre>


**Conclusion**:
1. **Query-by-Committee (QBC)** achieves highest performance (97.4% achievement, R¬≤=0.856)
2. However, computation time is over 3√ó longer (38.12s vs 12.34s)
3. **Uncertainty Sampling provides best overall balance**
   - 96.2% achievement (only 1.2% difference from QBC)
   - R¬≤=0.834 (only 0.022 difference from QBC)
   - Computation time is 1/3

**Recommendations**:
- **No time constraints**: QBC
- **Balance priority**: Uncertainty Sampling
- **Computation cost priority**: Diversity Sampling

</details>
<hr/>
<h2>Data Licenses and Citations</h2>
<h3>Benchmark Datasets</h3>
<p>Benchmark datasets for active learning that can be used in this chapter's code examples:</p>
<h4>1. UCI Machine Learning Repository</h4>
<ul>
<li><strong>License</strong>: CC BY 4.0</li>
<li><strong>Citation</strong>: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. University of California, Irvine, School of Information and Computer Sciences.</li>
<li><strong>Recommended Datasets</strong>:</li>
<li><code>make_regression()</code> (built-in scikit-learn)</li>
<li>Wine Quality Dataset</li>
<li>Boston Housing Dataset</li>
</ul>
<h4>2. Materials Project API</h4>
<ul>
<li><strong>License</strong>: CC BY 4.0</li>
<li><strong>Citation</strong>: Jain, A. et al. (2013). "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation." <em>APL Materials</em>, 1(1), 011002.</li>
<li><strong>Usage</strong>: Active learning experiments in materials science (band gap, formation energy)</li>
<li><strong>API Access</strong>: https://materialsproject.org/api</li>
</ul>
<h4>3. Matbench Datasets</h4>
<ul>
<li><strong>License</strong>: MIT License</li>
<li><strong>Citation</strong>: Dunn, A. et al. (2020). "Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm." <em>npj Computational Materials</em>, 6(1), 138.</li>
<li><strong>Usage</strong>: Active learning for material property prediction</li>
</ul>
<h3>Library Licenses</h3>
<p>Licenses of major libraries used in this chapter:</p>
<table>
<thead>
<tr>
<th>Library</th>
<th>Version</th>
<th>License</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>modAL</td>
<td>0.4.1</td>
<td>MIT</td>
<td>Active Learning Framework</td>
</tr>
<tr>
<td>scikit-learn</td>
<td>1.3.0</td>
<td>BSD-3-Clause</td>
<td>Machine Learning &amp; Preprocessing</td>
</tr>
<tr>
<td>numpy</td>
<td>1.24.3</td>
<td>BSD-3-Clause</td>
<td>Numerical Computing</td>
</tr>
<tr>
<td>matplotlib</td>
<td>3.7.1</td>
<td>PSF (BSD-like)</td>
<td>Visualization</td>
</tr>
</tbody>
</table>
<p><strong>License Compliance</strong>:
- All are available for commercial use
- Maintain original license notices when redistributing
- Cite appropriately in academic publications</p>
<hr/>
<h2>Ensuring Reproducibility</h2>
<h3>Random Seed Configuration</h3>
<p>To make active learning experiments reproducible, set the following random seeds in all code:</p>
<pre>__PROTECTED_CODE_18__</pre>
<p><strong>Important Points</strong>:
- Data splitting: <code>train_test_split(..., random_state=SEED)</code>
- Model initialization: <code>RandomForestRegressor(..., random_state=SEED)</code>
- Initial sample selection: Set seed before <code>np.random.choice(..., replace=False)</code></p>
<h3>Library Version Management</h3>
<p>To fully reproduce experimental environment, create <code>requirements.txt</code>:</p>
<pre>__PROTECTED_CODE_23__</pre>
<h3>Recording Experimental Logs</h3>
<p>Record all active learning iterations:</p>
<pre>__PROTECTED_CODE_24__</pre>
<hr/>
<h2>Common Pitfalls and Solutions</h2>
<h3>1. Cold Start Problem (Insufficient Initial Data)</h3>
<p><strong>Problem</strong>: Too few initial labeled data leads to unstable uncertainty estimation</p>
<p><strong>Symptoms</strong>:</p>
<pre>__PROTECTED_CODE_25__</pre>
<p><strong>Solution</strong>:</p>
<pre>__PROTECTED_CODE_26__</pre>
<p><strong>Recommended Rules</strong>:
- Minimum 10 samples
- Ideally 3-5√ó number of features
- More complex models (NN, GP) require more initial data</p>
<hr/>
<h3>2. Query Selection Bias</h3>
<p><strong>Problem</strong>: Using only uncertainty sampling leads to selecting same regions repeatedly</p>
<p><strong>Symptoms</strong>:</p>
<pre>__PROTECTED_CODE_27__</pre>
<p><strong>Solution 1: Œµ-greedy</strong>:</p>
<pre>__PROTECTED_CODE_28__</pre>
<p><strong>Solution 2: Batch Diversity</strong>:</p>
<pre>__PROTECTED_CODE_29__</pre>
<hr/>
<h3>3. Stopping Criteria Errors</h3>
<p><strong>Problem</strong>: Unclear when to stop active learning</p>
<p><strong>Bad Example</strong>: Fixed iteration count only</p>
<pre>__PROTECTED_CODE_30__</pre>
<p><strong>Solution: Multiple stopping criteria</strong>:</p>
<pre>__PROTECTED_CODE_31__</pre>
<hr/>
<h3>4. Distribution Shift</h3>
<p><strong>Problem</strong>: Distribution differs between labeled and unlabeled pools</p>
<p><strong>Symptoms</strong>:</p>
<pre>__PROTECTED_CODE_32__</pre>
<p><strong>Solution</strong>:</p>
<pre>__PROTECTED_CODE_33__</pre>
<hr/>
<h3>5. Label Noise Handling</h3>
<p><strong>Problem</strong>: When experimental measurements contain noise, learning incorrect samples</p>
<p><strong>Solution 1: Uncertainty threshold</strong>:</p>
<pre>__PROTECTED_CODE_34__</pre>
<p><strong>Solution 2: Ensemble Robustness</strong>:</p>
<pre>__PROTECTED_CODE_35__</pre>
<hr/>
<h3>6. Computational Cost of Uncertainty Estimation</h3>
<p><strong>Problem</strong>: Uncertainty estimation takes too long (e.g., GP's N^3 computational complexity)</p>
<p><strong>Solution: Pre-filter candidate pool</strong>:</p>
<pre>__PROTECTED_CODE_36__</pre>
<hr/>
<h2>Quality Checklist</h2>
<h3>Experimental Design Checklist</h3>
<h4>Initialization Phase</h4>
<ul>
<li>[ ] Random seed configured (<code>np.random.seed(SEED)</code>)</li>
<li>[ ] Appropriate initial sample count (minimum 10, ideally features √ó 3-5)</li>
<li>[ ] Data split uses stratified sampling (avoid distribution shift)</li>
<li>[ ] Library versions recorded in <code>requirements.txt</code></li>
</ul>
<h4>Query Strategy Selection</h4>
<ul>
<li>[ ] Select appropriate method for task</li>
<li>Wide exploration ‚Üí Diversity Sampling</li>
<li>Efficient convergence ‚Üí Uncertainty Sampling</li>
<li>Model robustness ‚Üí Query-by-Committee</li>
<li>[ ] Set exploration-exploitation balance (Œµ-greedy, UCB)</li>
<li>[ ] Consider diversity when batch selecting</li>
</ul>
<h4>Stopping Criteria Design</h4>
<ul>
<li>[ ] Set maximum iteration count</li>
<li>[ ] Define target performance metrics (R¬≤, RMSE, etc.)</li>
<li>[ ] Set early stopping conditions (patience=5-10)</li>
<li>[ ] Clarify budget limits (experimental cost, time)</li>
</ul>
<h4>Model Selection</h4>
<ul>
<li>[ ] Select models capable of uncertainty estimation</li>
<li>Ensemble methods (RF, LightGBM)</li>
<li>MC Dropout (NN)</li>
<li>Gaussian Process</li>
<li>[ ] Select model based on data size</li>
<li>Small (&lt;1000) ‚Üí GP</li>
<li>Medium (1000-10000) ‚Üí RF, LightGBM</li>
<li>Large (&gt;10000) ‚Üí MC Dropout</li>
</ul>
<h3>Implementation Quality Checklist</h3>
<h4>Data Preprocessing</h4>
<ul>
<li>[ ] Missing values handled (deletion or imputation)</li>
<li>[ ] Outliers detected and addressed (IQR method, etc.)</li>
<li>[ ] Feature scaling applied (standardization or normalization)</li>
<li>[ ] No data leakage (test data separated)</li>
</ul>
<h4>Code Quality</h4>
<ul>
<li>[ ] Type hints added to functions (<code>def func(x: np.ndarray) -&gt; float:</code>)</li>
<li>[ ] Docstrings written (arguments, return values, purpose)</li>
<li>[ ] Error handling implemented (try-except)</li>
<li>[ ] Logging output implemented (experiment tracking)</li>
</ul>
<h4>Evaluation and Validation</h4>
<ul>
<li>[ ] Multiple evaluation metrics calculated (R¬≤, RMSE, MAE)</li>
<li>[ ] Learning curves plotted (iteration count vs performance)</li>
<li>[ ] Compared with random sampling</li>
<li>[ ] Statistical significance verified (mean ¬± std of multiple trials)</li>
</ul>
<h3>Materials Science-Specific Checklist</h3>
<h4>Physical Constraints</h4>
<ul>
<li>[ ] Check physical validity of search space</li>
<li>Temperature range: 0-1500¬∞C</li>
<li>Composition ratio: Total 100%</li>
<li>pH range: 0-14</li>
<li>[ ] Verify unit consistency (nm, eV, GPa, etc.)</li>
<li>[ ] Synthesizability constraints (experimental feasibility)</li>
</ul>
<h4>Domain Knowledge Integration</h4>
<ul>
<li>[ ] Leverage physical prior knowledge</li>
<li>Kernel selection (periodicity, smoothness)</li>
<li>Feature engineering (descriptors)</li>
<li>[ ] Verify consistency with known physical laws</li>
<li>Band Gap &gt; 0</li>
<li>Density &gt; 0</li>
</ul>
<h4>Experimental Integration</h4>
<ul>
<li>[ ] Account for measurement errors (noise terms)</li>
<li>[ ] Define experimental cost function</li>
<li>[ ] Design batch experiments (parallelization potential)</li>
</ul>
<hr/>
<h2>Additional Practice Exercise Guide</h2>
<h3>Complete Solution Example for Exercise 1 (CNT Electrical Conductivity Prediction)</h3>
<details>
<summary>Click to show complete code</summary>
<pre>__PROTECTED_CODE_40__</pre>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>Settles, B. (2009). "Active Learning Literature Survey." <em>Computer Sciences Technical Report 1648</em>, University of Wisconsin-Madison.</p>
</li>
<li>
<p>Lookman, T. et al. (2019). "Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design." <em>npj Computational Materials</em>, 5(1), 1-17. DOI: <a href="https://doi.org/10.1038/s41524-019-0153-8">10.1038/s41524-019-0153-8</a></p>
</li>
<li>
<p>Raccuglia, P. et al. (2016). "Machine-learning-assisted materials discovery using failed experiments." <em>Nature</em>, 533(7601), 73-76. DOI: <a href="https://doi.org/10.1038/nature17439">10.1038/nature17439</a></p>
</li>
<li>
<p>Ren, F. et al. (2018). "Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments." <em>Science Advances</em>, 4(4), eaaq1566. DOI: <a href="https://doi.org/10.1126/sciadv.aaq1566">10.1126/sciadv.aaq1566</a></p>
</li>
<li>
<p>Kusne, A. G. et al. (2020). "On-the-fly closed-loop materials discovery via Bayesian active learning." <em>Nature Communications</em>, 11(1), 5966. DOI: <a href="https://doi.org/10.1038/s41467-020-19597-w">10.1038/s41467-020-19597-w</a></p>
</li>
</ol>
<hr/>
<h2>Navigation</h2>
<h3>Next Chapter</h3>
<p><strong><a href="chapter-2.html">Chapter 2: Uncertainty Estimation Techniques ‚Üí</a></strong></p>
<h3>Series Index</h3>
<p><strong><a href="./index.html">‚Üê Return to Series Index</a></strong></p>
<hr/>
<h2>Author Information</h2>
<p><strong>Creator</strong>: AI Terakoya Content Team
<strong>Created</strong>: 2025-10-18
<strong>Version</strong>: 1.0</p>
<p><strong>Update History</strong>:
- 2025-10-18: v1.0 Initial release</p>
<p><strong>Feedback</strong>:
- GitHub Issues: <a href="https://github.com/your-repo/AI_Homepage/issues">AI_Homepage/issues</a>
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr/>
<p><strong>Let's learn the details of Uncertainty Estimation in the next chapter!</strong></p><div class="navigation">
<a class="nav-button" href="index.html">Return to Series Index</a>
<a class="nav-button" href="chapter-2.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes, and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creator and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the creator and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Creator</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-18</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
