<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 2: Uncertainty Estimation Techniques - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Uncertainty Estimation Techniques - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/active-learning-introduction/index.html">Active Learning</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/active-learning-introduction/chapter-2.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 2: Uncertainty Estimation Techniques</h1>
<p class="subtitle">Prediction Confidence Intervals with Ensemble, Dropout, and Gaussian Process</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 25-30 minutes</span>
<span class="meta-item">üìä Difficulty: Intermediate to Advanced</span>
<span class="meta-item">üíª Code Examples: 8 examples</span>
<span class="meta-item">üìù Exercises: 3 problems</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 2: Uncertainty Estimation Techniques</h1><p class="chapter-description">This chapter covers Uncertainty Estimation Techniques. You will learn principles of three uncertainty estimation methods, Ensemble methods (Random Forest), and MC Dropout to neural networks.</p>


<p><strong>Prediction Confidence Intervals with Ensemble, Dropout, and Gaussian Process</strong></p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Understand the principles of three uncertainty estimation methods</li>
<li>‚úÖ Implement Ensemble methods (Random Forest)</li>
<li>‚úÖ Apply MC Dropout to neural networks</li>
<li>‚úÖ Calculate prediction variance with Gaussian Process</li>
<li>‚úÖ Explain the criteria for selecting among these methods</li>
</ul>
<p><strong>Reading Time</strong>: 25-30 minutes
<strong>Code Examples</strong>: 8 examples
<strong>Exercises</strong>: 3 problems</p>
<hr/>
<h2>2.1 Uncertainty Estimation with Ensemble Methods</h2>
<h3>Why Uncertainty Estimation is Important</h3>
<p>In active learning, it is necessary to quantify "how confident the model is in its predictions." Uncertainty estimation is a core technology for query strategies.</p>
<p><strong>Two Types of Uncertainty</strong>:</p>
<ol>
<li>
<p><strong>Aleatoric Uncertainty</strong>
   - Noise inherent in the data itself
   - Measurement errors, environmental variations, etc.
   - Does not decrease even with more data</p>
</li>
<li>
<p><strong>Epistemic Uncertainty</strong>
   - Uncertainty due to the model's lack of knowledge
   - Caused by insufficient data
   - Decreases with more data</p>
</li>
</ol>
<p><strong>Uncertainty Focused on by Active Learning</strong>:
‚Üí <strong>Epistemic Uncertainty</strong> (can be improved by adding data)</p>
<h3>Principle of Ensemble Methods</h3>
<p><strong>Basic Idea</strong>: Measure uncertainty by the variation in predictions from multiple models</p>
<p><strong>Formula</strong>:
$$
\mu(x) = \frac{1}{M} \sum_{m=1}^M f_m(x)
$$</p>
<p>$$
\sigma^2(x) = \frac{1}{M} \sum_{m=1}^M (f_m(x) - \mu(x))^2
$$</p>
<ul>
<li>$f_m(x)$: Prediction from the m-th model</li>
<li>$M$: Number of models (ensemble size)</li>
<li>$\mu(x)$: Prediction mean</li>
<li>$\sigma^2(x)$: Prediction variance (uncertainty)</li>
</ul>
<h3>Implementation with Random Forest</h3>
<p><strong>Code Example 1: Uncertainty Estimation with Random Forest</strong></p>
<pre>__PROTECTED_CODE_0__</pre>
<p><strong>OutputExample</strong>:</p>
<pre>__PROTECTED_CODE_1__</pre>
<h3>Implementation with LightGBM</h3>
<p><strong>Code Example 2: Uncertainty Estimation with LightGBM</strong></p>
<pre>__PROTECTED_CODE_2__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Simple to implement
- ‚úÖ Relatively low computational cost
- ‚úÖ Easy to interpret
- ‚úÖ Strong performance on tabular data</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è Depends on ensemble size
- ‚ö†Ô∏è Difficult to apply to deep learning
- ‚ö†Ô∏è May require uncertainty calibration</p>
<hr/>
<h2>2.2 Uncertainty Estimation with Dropout Methods</h2>
<h3>MC Dropout (Monte Carlo Dropout)</h3>
<p><strong>Principle</strong>: Apply dropout during inference as well and measure variation through multiple predictions</p>
<p><strong>Regular Dropout</strong> (training only):</p>
<pre>__PROTECTED_CODE_3__</pre>
<p><strong>MC Dropout</strong> (dropout during inference too):</p>
<pre>__PROTECTED_CODE_4__</pre>
<h3>Implementation Example</h3>
<p><strong>Code Example 3: MC Dropout with PyTorch</strong></p>
<pre>__PROTECTED_CODE_5__</pre>
<p><strong>OutputExample</strong>:</p>
<pre>__PROTECTED_CODE_6__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Easy to apply to existing neural networks
- ‚úÖ No additional training required (dropout only)
- ‚úÖ Well-suited for deep learning</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è Computational cost depends on sampling count (T)
- ‚ö†Ô∏è Choice of dropout rate is important
- ‚ö†Ô∏è May require uncertainty calibration</p>
<hr/>
<h2>2.3 Uncertainty Estimation with Gaussian Process (GP)</h2>
<h3>Fundamentals of GP</h3>
<p>Gaussian Process is a powerful method for defining probability distributions over functions.</p>
<p><strong>Definition</strong>:
$$
f(\mathbf{x}) \sim \mathcal{GP}(\mu(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$</p>
<ul>
<li>$\mu(\mathbf{x})$: Mean function (usually 0)</li>
<li>$k(\mathbf{x}, \mathbf{x}')$: Kernel function (covariance function)</li>
</ul>
<p><strong>Predictive Distribution</strong>:
$$
p(f^* | \mathbf{X}, \mathbf{y}, \mathbf{x}^*) = \mathcal{N}(\mu^*, \sigma^{*2})
$$</p>
<p>$$
\mu^* = k(\mathbf{x}^*, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} \mathbf{y}
$$</p>
<p>$$
\sigma^{*2} = k(\mathbf{x}^*, \mathbf{x}^*) - k(\mathbf{x}^*, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} k(\mathbf{X}, \mathbf{x}^*)
$$</p>
<h3>Kernel Functions</h3>
<p><strong>RBF (Radial Basis Function) Kernel</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2 \exp\left(-\frac{|\mathbf{x}_i - \mathbf{x}_j|^2}{2\ell^2}\right)
$$</p>
<ul>
<li>$\sigma_f^2$: Signal variance</li>
<li>$\ell$: Length scale (smoothness)</li>
</ul>
<p><strong>Mat√©rn Kernel</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu} r}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu} r}{\ell}\right)
$$</p>
<h3>Implementation with GPyTorch</h3>
<p><strong>Code Example 4: Uncertainty Estimation with GPyTorch</strong></p>
<pre>__PROTECTED_CODE_7__</pre>
<p><strong>OutputExample</strong>:</p>
<pre>__PROTECTED_CODE_8__</pre>
<p><strong>Advantages</strong>:
- ‚úÖ Rigorous uncertainty quantification
- ‚úÖ High accuracy with small datasets
- ‚úÖ Flexibility through kernel selection
- ‚úÖ Strong theoretical foundation</p>
<p><strong>Disadvantages</strong>:
- ‚ö†Ô∏è Not suitable for large-scale data (O(n¬≥))
- ‚ö†Ô∏è Kernel and hyperparameter selection is important
- ‚ö†Ô∏è Performance degrades with high-dimensional data</p>
<hr/>
<h2>2.4 Case Study: Band Gap Prediction</h2>
<h3>Problem Setup</h3>
<p><strong>Objective</strong>: Predict the band gap of inorganic materials and prioritize calculations for samples with high uncertainty</p>
<p><strong>Dataset</strong>: Materials Project (DFT calculations completed)
- Number of samples: 5,000 materials
- Features: Compositional descriptors (20-dimensional)
- Target variable: Band Gap (eV)</p>
<h3>Comparison of Three Methods</h3>
<p><strong>Code Example 5: Comparison of Uncertainty Estimation for Band Gap Prediction</strong></p>
<pre>__PROTECTED_CODE_9__</pre>
<p><strong>OutputExample</strong>:</p>
<pre>__PROTECTED_CODE_10__</pre>
<hr/>
<h2>2.5 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Ensemble Methods</strong>
   - Uncertainty estimation with Random Forest and LightGBM
   - Quantify uncertainty through prediction variance
   - Simple to implement, moderate computational cost</p>
</li>
<li>
<p><strong>MC Dropout</strong>
   - Apply dropout during inference as well
   - Easy to implement with neural networks
   - Sampling count and dropout rate are important</p>
</li>
<li>
<p><strong>Gaussian Process</strong>
   - Rigorous uncertainty quantification
   - Flexibility through kernel functions
   - High accuracy with small data, not suitable for large-scale data</p>
</li>
</ol>
<h3>Selecting the Right Method</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Recommended Case</th>
<th>Data Size</th>
<th>Computational Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Forest</td>
<td>Tabular data, medium-scale</td>
<td>100-10,000</td>
<td>Low to Medium</td>
</tr>
<tr>
<td>MC Dropout</td>
<td>Deep learning, images/text</td>
<td>1,000-100,000</td>
<td>Medium to High</td>
</tr>
<tr>
<td>Gaussian Process</td>
<td>Small datasets, rigorous uncertainty</td>
<td>10-1,000</td>
<td>Medium to High</td>
</tr>
</tbody>
</table>
<h3>Next Chapter</h3>
<p>In Chapter 3, we will learn about <strong>acquisition function design</strong> that leverages uncertainty:
- Expected Improvement (EI)
- Probability of Improvement (PI)
- Upper Confidence Bound (UCB)
- Multi-objective and constrained acquisition functions</p>
<p><strong><a href="chapter-3.html">Chapter 3: Acquisition Function Design ‚Üí</a></strong></p>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>(Omitted: Detailed implementation of exercises)</p>
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>(Omitted: Detailed implementation of exercises)</p>
<h3>Problem 3 (Difficulty: Hard)</h3>
<p>(Omitted: Detailed implementation of exercises)</p>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>Gal, Y., &amp; Ghahramani, Z. (2016). "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning." <em>ICML</em>, 1050-1059.</p>
</li>
<li>
<p>Rasmussen, C. E., &amp; Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</p>
</li>
<li>
<p>Lakshminarayanan, B. et al. (2017). "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles." <em>NeurIPS</em>.</p>
</li>
</ol>
<hr/>
<h2>Navigation</h2>
<h3>Previous Chapter</h3>
<p><strong><a href="chapter-1.html">‚Üê Chapter 1: The Need for Active Learning</a></strong></p>
<h3>Next Chapter</h3>
<p><strong><a href="chapter-3.html">Chapter 3: Acquisition Function Design ‚Üí</a></strong></p>
<h3>Series Index</h3>
<p><strong><a href="./index.html">‚Üê Back to Series Index</a></strong></p>
<hr/>
<p><strong>Let's learn about acquisition function design in the next chapter!</strong></p>
<div class="navigation">
<a class="nav-button" href="chapter-1.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Index</a>
<a class="nav-button" href="chapter-3.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, either express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The creators and Tohoku University are not responsible for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the creators and Tohoku University are not responsible for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the stated terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Creator</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-18</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
