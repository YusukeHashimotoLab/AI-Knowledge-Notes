<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Building Custom Databases - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/materials-databases-introduction/index.html">Materials Databases</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/materials-databases-introduction/chapter-4.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 4: Building Custom Databases</h1>
<p class="subtitle">From SQLite to PostgreSQL - Structuring and Publishing Experimental Data</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 15-20 min</span>
<span class="meta-item">üìä Difficulty: Beginner to Intermediate</span>
<span class="meta-item">üíª Code examples: 10</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 4: Building Custom Databases</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">This chapter shows the path from starting small with a local database to sharing and publishing data. We also cover fundamental operational rules and security considerations.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Clearly define roles (registration, approval, viewing). Decide on backup and access control from the start.</p>
<p><strong>From SQLite to PostgreSQL - Structuring and Publishing Experimental Data</strong></p>
<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Design schemas for materials databases</li>
<li>‚úÖ Build practical databases with SQLite</li>
<li>‚úÖ Manage large-scale data with PostgreSQL</li>
<li>‚úÖ Implement backup and version control</li>
<li>‚úÖ Publish data and obtain DOIs</li>
</ul>
<p><strong>Reading time</strong>: 15-20 min
<strong>Code examples</strong>: 10
<strong>Exercises</strong>: 3</p>
<hr/>
<h2>4.1 Database Design Fundamentals</h2>
<h3>4.1.1 Schema Design Principles</h3>
<p>In materials database schema design, we define the following three main tables:</p>
<ol>
<li><strong>Materials</strong>: Basic material information</li>
<li><strong>Properties</strong>: Measured property data</li>
<li><strong>Experiments</strong>: Experimental conditions and metadata</li>
</ol>
<p><strong>ER Diagram (Entity-Relationship Diagram)</strong>:</p>
<div class="mermaid">
erDiagram
    Materials ||--o{ Properties : has
    Materials ||--o{ Experiments : tested_in
    Experiments ||--o{ Properties : produces

    Materials {
        int material_id PK
        string formula
        string structure_type
        float density
        datetime created_at
    }

    Properties {
        int property_id PK
        int material_id FK
        int experiment_id FK
        string property_type
        float value
        float uncertainty
        string unit
    }

    Experiments {
        int experiment_id PK
        string experiment_type
        datetime date
        string operator
        string conditions
    }
</div>
<h3>4.1.2 Normalization</h3>
<p><strong>First Normal Form (1NF)</strong>: Each column contains atomic values
<strong>Second Normal Form (2NF)</strong>: Eliminate partial functional dependencies
<strong>Third Normal Form (3NF)</strong>: Eliminate transitive functional dependencies</p>
<p><strong>Code Example 1: Schema Definition (SQLite)</strong></p>
<pre><code class="language-python">import sqlite3

# Database connection
conn = sqlite3.connect('materials.db')
cursor = conn.cursor()

# Materials table
cursor.execute('''
CREATE TABLE IF NOT EXISTS materials (
    material_id INTEGER PRIMARY KEY AUTOINCREMENT,
    formula TEXT NOT NULL,
    structure_type TEXT,
    density REAL,
    space_group INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    notes TEXT
)
''')

# Properties table
cursor.execute('''
CREATE TABLE IF NOT EXISTS properties (
    property_id INTEGER PRIMARY KEY AUTOINCREMENT,
    material_id INTEGER NOT NULL,
    experiment_id INTEGER,
    property_type TEXT NOT NULL,
    value REAL NOT NULL,
    uncertainty REAL,
    unit TEXT,
    measured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (material_id) REFERENCES materials(material_id),
    FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id)
)
''')

# Experiments table
cursor.execute('''
CREATE TABLE IF NOT EXISTS experiments (
    experiment_id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_type TEXT NOT NULL,
    date DATE,
    operator TEXT,
    temperature REAL,
    pressure REAL,
    conditions TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
''')

# Create indexes (for faster queries)
cursor.execute('''
CREATE INDEX IF NOT EXISTS idx_formula
ON materials(formula)
''')

cursor.execute('''
CREATE INDEX IF NOT EXISTS idx_property_type
ON properties(property_type)
''')

conn.commit()
print("Database schema created successfully")
</code></pre>
<hr/>
<h2>4.2 Local Database with SQLite</h2>
<h3>4.2.1 CRUD Operations</h3>
<p><strong>Code Example 2: Create (Data Insertion)</strong></p>
<pre><code class="language-python">import sqlite3
from datetime import datetime

conn = sqlite3.connect('materials.db')
cursor = conn.cursor()

def insert_material(formula, structure_type, density):
    """Insert material data"""
    cursor.execute('''
    INSERT INTO materials (formula, structure_type, density)
    VALUES (?, ?, ?)
    ''', (formula, structure_type, density))

    conn.commit()
    material_id = cursor.lastrowid
    print(f"Material added: ID={material_id}, {formula}")
    return material_id

def insert_property(
    material_id,
    property_type,
    value,
    uncertainty=None,
    unit=None
):
    """Insert property data"""
    cursor.execute('''
    INSERT INTO properties
    (material_id, property_type, value, uncertainty, unit)
    VALUES (?, ?, ?, ?, ?)
    ''', (material_id, property_type, value, uncertainty, unit))

    conn.commit()
    print(f"Property added: {property_type}={value} {unit}")

# Usage example
mat_id = insert_material("TiO2", "rutile", 4.23)
insert_property(mat_id, "band_gap", 3.0, 0.1, "eV")
insert_property(mat_id, "refractive_index", 2.61, 0.02, "")
</code></pre>
<p><strong>Code Example 3: Read (Data Retrieval)</strong></p>
<pre><code class="language-python">def query_materials(formula_pattern=None):
    """Search materials"""
    if formula_pattern:
        cursor.execute('''
        SELECT * FROM materials
        WHERE formula LIKE ?
        ''', (f"%{formula_pattern}%",))
    else:
        cursor.execute('SELECT * FROM materials')

    results = cursor.fetchall()

    print(f"Search results: {len(results)} items")
    for row in results:
        print(f"ID: {row[0]}, Formula: {row[1]}, "
              f"Type: {row[2]}, Density: {row[3]}")

    return results

def query_properties(material_id):
    """Retrieve property data"""
    cursor.execute('''
    SELECT p.property_type, p.value, p.uncertainty, p.unit
    FROM properties p
    WHERE p.material_id = ?
    ''', (material_id,))

    results = cursor.fetchall()

    print(f"\nProperties for material ID {material_id}:")
    for row in results:
        prop_type, value, unc, unit = row
        if unc:
            print(f"- {prop_type}: {value} ¬± {unc} {unit}")
        else:
            print(f"- {prop_type}: {value} {unit}")

    return results

# Usage example
query_materials("TiO2")
query_properties(1)
</code></pre>
<p><strong>Code Example 4: Update (Data Modification)</strong></p>
<pre><code class="language-python">def update_material(material_id, **kwargs):
    """Update material data"""
    set_clause = ", ".join(
        [f"{key} = ?" for key in kwargs.keys()]
    )
    values = list(kwargs.values()) + [material_id]

    cursor.execute(f'''
    UPDATE materials
    SET {set_clause}
    WHERE material_id = ?
    ''', values)

    conn.commit()
    print(f"Material ID {material_id} updated")

# Usage example
update_material(1, density=4.24, notes="Updated density")
</code></pre>
<p><strong>Code Example 5: Delete (Data Deletion)</strong></p>
<pre><code class="language-python">def delete_material(material_id):
    """Delete material data (cascade)"""
    # Delete associated property data
    cursor.execute('''
    DELETE FROM properties WHERE material_id = ?
    ''', (material_id,))

    cursor.execute('''
    DELETE FROM materials WHERE material_id = ?
    ''', (material_id,))

    conn.commit()
    print(f"Material ID {material_id} deleted")

# Usage example (use with caution)
# delete_material(1)
</code></pre>
<hr/>
<h2>4.3 Large-Scale Data Management with PostgreSQL/MySQL</h2>
<h3>4.3.1 PostgreSQL Connection</h3>
<p><strong>Code Example 6: PostgreSQL Connection and Schema Creation</strong></p>
<pre><code class="language-python">import psycopg2
from psycopg2 import sql

# PostgreSQL connection
conn = psycopg2.connect(
    host="localhost",
    database="materials_db",
    user="your_username",
    password="your_password"
)

cursor = conn.cursor()

# Create tables (PostgreSQL version)
cursor.execute('''
CREATE TABLE IF NOT EXISTS materials (
    material_id SERIAL PRIMARY KEY,
    formula VARCHAR(100) NOT NULL,
    structure_type VARCHAR(50),
    density REAL,
    space_group INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    notes TEXT,
    UNIQUE(formula)
)
''')

cursor.execute('''
CREATE TABLE IF NOT EXISTS properties (
    property_id SERIAL PRIMARY KEY,
    material_id INTEGER NOT NULL,
    experiment_id INTEGER,
    property_type VARCHAR(50) NOT NULL,
    value REAL NOT NULL,
    uncertainty REAL,
    unit VARCHAR(20),
    measured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (material_id)
        REFERENCES materials(material_id)
        ON DELETE CASCADE,
    FOREIGN KEY (experiment_id)
        REFERENCES experiments(experiment_id)
        ON DELETE SET NULL
)
''')

# GIN index (for full-text search)
cursor.execute('''
CREATE INDEX IF NOT EXISTS idx_formula_gin
ON materials USING GIN (to_tsvector('english', formula))
''')

conn.commit()
print("PostgreSQL schema created successfully")
</code></pre>
<h3>4.3.2 Bulk Insert</h3>
<p><strong>Code Example 7: Efficient Bulk Data Insertion</strong></p>
<pre><code class="language-python">import pandas as pd
from psycopg2.extras import execute_batch

def bulk_insert_materials(df):
    """Bulk insert from DataFrame"""
    data = [
        (
            row['formula'],
            row['structure_type'],
            row['density']
        )
        for _, row in df.iterrows()
    ]

    insert_query = '''
    INSERT INTO materials (formula, structure_type, density)
    VALUES (%s, %s, %s)
    ON CONFLICT (formula) DO NOTHING
    '''

    execute_batch(cursor, insert_query, data, page_size=1000)
    conn.commit()

    print(f"{len(data)} records inserted")

# Usage example
df = pd.DataFrame({
    'formula': ['TiO2', 'ZnO', 'GaN', 'SiC'],
    'structure_type': ['rutile', 'wurtzite', 'wurtzite', 'zincblende'],
    'density': [4.23, 5.61, 6.15, 3.21]
})

bulk_insert_materials(df)
</code></pre>
<hr/>
<h2>4.4 Backup Strategies</h2>
<h3>4.4.1 Periodic Backups</h3>
<p><strong>Code Example 8: Automated Backup Script</strong></p>
<pre><code class="language-python">import sqlite3
import shutil
from datetime import datetime
import os

class DatabaseBackup:
    """Database backup management"""

    def __init__(self, db_path, backup_dir="backups"):
        self.db_path = db_path
        self.backup_dir = backup_dir
        os.makedirs(backup_dir, exist_ok=True)

    def create_backup(self):
        """Create backup"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"materials_db_{timestamp}.db"
        backup_path = os.path.join(
            self.backup_dir, backup_name
        )

        # Copy database
        shutil.copy2(self.db_path, backup_path)

        # Compress (optional)
        import gzip
        with open(backup_path, 'rb') as f_in:
            with gzip.open(f"{backup_path}.gz", 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        os.remove(backup_path)  # Delete uncompressed version

        print(f"Backup created: {backup_path}.gz")
        return f"{backup_path}.gz"

    def restore_backup(self, backup_file):
        """Restore from backup"""
        import gzip

        # Decompress
        temp_db = "temp_restored.db"
        with gzip.open(backup_file, 'rb') as f_in:
            with open(temp_db, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        # Backup current database
        self.create_backup()

        # Restore
        shutil.move(temp_db, self.db_path)
        print(f"Database restored from: {backup_file}")

    def list_backups(self):
        """List backups"""
        backups = sorted(
            [f for f in os.listdir(self.backup_dir)
             if f.endswith('.gz')]
        )

        print("=== Backup List ===")
        for i, backup in enumerate(backups, 1):
            size = os.path.getsize(
                os.path.join(self.backup_dir, backup)
            ) / 1024  # KB
            print(f"{i}. {backup} ({size:.1f} KB)")

        return backups

# Usage example
backup_mgr = DatabaseBackup('materials.db')
backup_mgr.create_backup()
backup_mgr.list_backups()
</code></pre>
<h3>4.4.2 Version Control (Git LFS)</h3>
<p><strong>Code Example 9: Data Version Control with Git LFS</strong></p>
<pre><code class="language-bash"># Git LFS setup
git lfs install

# Track large files
git lfs track "*.db"
git lfs track "*.db.gz"

# Automatically added to .gitattributes
cat .gitattributes
# *.db filter=lfs diff=lfs merge=lfs -text
# *.db.gz filter=lfs diff=lfs merge=lfs -text

# Commit
git add materials.db .gitattributes
git commit -m "Add materials database"
git push origin main
</code></pre>
<hr/>
<h2>4.5 Data Publication and DOI Acquisition</h2>
<h3>4.5.1 Uploading to Zenodo</h3>
<p><strong>Code Example 10: Publishing Data via Zenodo API</strong></p>
<pre><code class="language-python">import requests
import json

ZENODO_TOKEN = "your_zenodo_token"
ZENODO_URL = "https://zenodo.org/api/deposit/depositions"

def create_zenodo_deposit(
    title,
    description,
    creators,
    keywords
):
    """Create Zenodo deposit"""

    headers = {
        "Content-Type": "application/json"
    }
    params = {"access_token": ZENODO_TOKEN}

    # Metadata
    data = {
        "metadata": {
            "title": title,
            "upload_type": "dataset",
            "description": description,
            "creators": creators,
            "keywords": keywords,
            "access_right": "open",
            "license": "cc-by-4.0"
        }
    }

    # Create deposit
    response = requests.post(
        ZENODO_URL,
        params=params,
        json=data,
        headers=headers
    )

    if response.status_code == 201:
        deposition = response.json()
        deposition_id = deposition['id']
        bucket_url = deposition['links']['bucket']

        print(f"Deposit created successfully: ID={deposition_id}")
        return deposition_id, bucket_url
    else:
        print(f"Error: {response.status_code}")
        print(response.text)
        return None, None

def upload_file_to_zenodo(bucket_url, file_path):
    """Upload file"""
    params = {"access_token": ZENODO_TOKEN}

    with open(file_path, 'rb') as f:
        response = requests.put(
            f"{bucket_url}/{os.path.basename(file_path)}",
            params=params,
            data=f
        )

    if response.status_code == 200:
        print(f"File uploaded successfully: {file_path}")
        return True
    else:
        print(f"Upload error: {response.status_code}")
        return False

def publish_zenodo_deposit(deposition_id):
    """Publish deposit (obtain DOI)"""
    params = {"access_token": ZENODO_TOKEN}
    url = f"{ZENODO_URL}/{deposition_id}/actions/publish"

    response = requests.post(url, params=params)

    if response.status_code == 202:
        result = response.json()
        doi = result['doi']
        print(f"Published successfully! DOI: {doi}")
        return doi
    else:
        print(f"Publication error: {response.status_code}")
        return None

# Usage example
deposition_id, bucket_url = create_zenodo_deposit(
    title="Materials Database - Experimental Data",
    description="Experimental materials properties dataset",
    creators=[
        {"name": "Hashimoto, Yusuke",
         "affiliation": "Tohoku University"}
    ],
    keywords=["materials science", "database",
              "properties"]
)

if deposition_id:
    # Upload file
    upload_file_to_zenodo(bucket_url, "materials.db.gz")

    # Publish (obtain DOI)
    doi = publish_zenodo_deposit(deposition_id)
    print(f"\nDOI: https://doi.org/{doi}")
</code></pre>
<hr/>
<h2>4.6 Practical Project</h2>
<h3>4.6.1 Structuring Experimental Data</h3>
<div class="mermaid">
flowchart LR
    A[Lab Notebook] --&gt; B[Excel Data]
    B --&gt; C[pandas DataFrame]
    C --&gt; D[Data Cleaning]
    D --&gt; E[Insert into SQLite]
    E --&gt; F[Quality Check]
    F --&gt; G[Migrate to PostgreSQL]
    G --&gt; H[Publish to Zenodo]
    H --&gt; I[Obtain DOI]

    style A fill:#e3f2fd
    style I fill:#e8f5e9
</div>
<hr/>
<h2>4.7 Chapter Summary</h2>
<h3>What You Learned</h3>
<ol>
<li>
<p><strong>Database Design</strong>
   - Schema design using ER diagrams
   - Normalization (1NF, 2NF, 3NF)
   - Index strategies</p>
</li>
<li>
<p><strong>SQLite Implementation</strong>
   - CRUD operations (Create, Read, Update, Delete)
   - Transaction management
   - Query optimization</p>
</li>
<li>
<p><strong>PostgreSQL</strong>
   - Large-scale data handling
   - Bulk insert operations
   - Full-text search indexing</p>
</li>
<li>
<p><strong>Backup</strong>
   - Automated periodic backups
   - Compressed storage
   - Version control with Git LFS</p>
</li>
<li>
<p><strong>Data Publication</strong>
   - Upload via Zenodo API
   - DOI acquisition
   - Open license selection</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>‚úÖ Schema design is critical from the start</li>
<li>‚úÖ SQLite is optimal for small to medium-scale data</li>
<li>‚úÖ PostgreSQL handles large-scale data</li>
<li>‚úÖ Backups are essential (3-2-1 rule)</li>
<li>‚úÖ Publish data with DOI for citability</li>
</ul>
<h3>Series Completed!</h3>
<p>Congratulations! You have completed the Materials Database Fundamentals series.</p>
<p><strong>Skills Acquired</strong>:
- Utilizing the four major materials databases
- Complete mastery of Materials Project API
- Integration and cleaning of multiple databases
- Building and publishing custom databases</p>
<p><strong>Next Steps</strong>:
- Learn machine learning with the <a href="../mi-introduction/index.html">MI Introduction Series</a>
- Efficient exploration with <a href="../bayesian-optimization-introduction/index.html">Bayesian Optimization Introduction</a>
- Apply to your own research projects</p>
<p><strong><a href="./index.html">‚Üê Back to Series Contents</a></strong></p>
<hr/>
<h2>Exercises</h2>
<h3>Exercise 1 (Difficulty: easy)</h3>
<p>Create a materials database using SQLite and insert the following data.</p>
<p><strong>Data</strong>:
- TiO2: rutile, density 4.23 g/cm¬≥, band gap 3.0 eV
- ZnO: wurtzite, density 5.61 g/cm¬≥, band gap 3.4 eV</p>
<p><strong>Requirements</strong>:
1. Create materials and properties tables
2. Insert data
3. Retrieve and display all data</p>
<details>
<summary>Solution Example</summary>
<pre><code class="language-python">import sqlite3

conn = sqlite3.connect('materials_practice.db')
cursor = conn.cursor()

# Create tables
cursor.execute('''
CREATE TABLE materials (
    material_id INTEGER PRIMARY KEY,
    formula TEXT,
    structure_type TEXT,
    density REAL
)
''')

cursor.execute('''
CREATE TABLE properties (
    property_id INTEGER PRIMARY KEY,
    material_id INTEGER,
    property_type TEXT,
    value REAL,
    unit TEXT,
    FOREIGN KEY (material_id) REFERENCES materials(material_id)
)
''')

# Insert data
materials_data = [
    ('TiO2', 'rutile', 4.23),
    ('ZnO', 'wurtzite', 5.61)
]

for formula, struct, density in materials_data:
    cursor.execute('''
    INSERT INTO materials (formula, structure_type, density)
    VALUES (?, ?, ?)
    ''', (formula, struct, density))

# Add band gaps
cursor.execute('''
INSERT INTO properties (material_id, property_type, value, unit)
VALUES (1, 'band_gap', 3.0, 'eV')
''')

cursor.execute('''
INSERT INTO properties (material_id, property_type, value, unit)
VALUES (2, 'band_gap', 3.4, 'eV')
''')

conn.commit()

# Retrieve data
cursor.execute('''
SELECT m.formula, m.structure_type, m.density, p.value
FROM materials m
JOIN properties p ON m.material_id = p.material_id
WHERE p.property_type = 'band_gap'
''')

print("=== Database Contents ===")
for row in cursor.fetchall():
    print(f"{row[0]}: {row[1]}, {row[2]} g/cm¬≥, "
          f"Eg={row[3]} eV")

conn.close()
</code></pre>
</details>
<hr/>
<h3>Exercise 2 (Difficulty: medium)</h3>
<p>Build a backup system.</p>
<p><strong>Requirements</strong>:
1. Daily backups (compressed)
2. Keep only the latest 5 generations
3. Backup list display function</p>
<details>
<summary>Solution Example</summary>
<pre><code class="language-python">import os
import shutil
import gzip
from datetime import datetime

class BackupManager:
    def __init__(self, db_path, backup_dir="backups",
                 max_backups=5):
        self.db_path = db_path
        self.backup_dir = backup_dir
        self.max_backups = max_backups
        os.makedirs(backup_dir, exist_ok=True)

    def create_backup(self):
        """Create backup"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}.db.gz"
        backup_path = os.path.join(self.backup_dir,
                                   backup_name)

        # Compressed backup
        with open(self.db_path, 'rb') as f_in:
            with gzip.open(backup_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        print(f"Backup created: {backup_name}")

        # Delete old backups
        self.cleanup_old_backups()

    def cleanup_old_backups(self):
        """Delete old backups"""
        backups = sorted(
            [f for f in os.listdir(self.backup_dir)
             if f.endswith('.db.gz')]
        )

        if len(backups) &gt; self.max_backups:
            to_delete = backups[:-self.max_backups]
            for backup in to_delete:
                os.remove(
                    os.path.join(self.backup_dir, backup)
                )
                print(f"Deleted: {backup}")

    def list_backups(self):
        """List backups"""
        backups = sorted(
            [f for f in os.listdir(self.backup_dir)
             if f.endswith('.db.gz')]
        )

        print("=== Backup List ===")
        for backup in backups:
            size = os.path.getsize(
                os.path.join(self.backup_dir, backup)
            ) / 1024
            print(f"{backup} ({size:.1f} KB)")

# Usage example
backup_mgr = BackupManager('materials_practice.db',
                           max_backups=5)
backup_mgr.create_backup()
backup_mgr.list_backups()
</code></pre>
</details>
<hr/>
<h3>Exercise 3 (Difficulty: hard)</h3>
<p>Build a complete workflow that stores experimental data in SQLite and publishes it to Zenodo.</p>
<p><strong>Requirements</strong>:
1. Load experimental data from Excel file
2. Store in SQLite
3. Data quality check
4. Create backup
5. Prepare Zenodo metadata (publishing optional)</p>
<details>
<summary>Solution Example</summary>
<pre><code class="language-python">import pandas as pd
import sqlite3
import json
from datetime import datetime

class ExperimentalDataPipeline:
    """Experimental data publication pipeline"""

    def __init__(self, db_path='experimental_data.db'):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.setup_database()

    def setup_database(self):
        """Database initialization"""
        cursor = self.conn.cursor()

        cursor.execute('''
        CREATE TABLE IF NOT EXISTS experiments (
            experiment_id INTEGER PRIMARY KEY,
            material_formula TEXT,
            experiment_type TEXT,
            date DATE,
            operator TEXT
        )
        ''')

        cursor.execute('''
        CREATE TABLE IF NOT EXISTS measurements (
            measurement_id INTEGER PRIMARY KEY,
            experiment_id INTEGER,
            property_type TEXT,
            value REAL,
            uncertainty REAL,
            unit TEXT,
            FOREIGN KEY (experiment_id)
                REFERENCES experiments(experiment_id)
        )
        ''')

        self.conn.commit()

    def load_excel_data(self, file_path):
        """Load data from Excel"""
        df = pd.read_excel(file_path)
        print(f"Data loaded: {len(df)} records")
        return df

    def insert_data(self, df):
        """Insert data"""
        cursor = self.conn.cursor()

        for _, row in df.iterrows():
            # Insert experiment data
            cursor.execute('''
            INSERT INTO experiments
            (material_formula, experiment_type, date, operator)
            VALUES (?, ?, ?, ?)
            ''', (
                row['formula'],
                row['experiment_type'],
                row['date'],
                row['operator']
            ))

            exp_id = cursor.lastrowid

            # Insert measurement values
            cursor.execute('''
            INSERT INTO measurements
            (experiment_id, property_type, value,
             uncertainty, unit)
            VALUES (?, ?, ?, ?, ?)
            ''', (
                exp_id,
                row['property'],
                row['value'],
                row.get('uncertainty'),
                row['unit']
            ))

        self.conn.commit()
        print(f"{len(df)} records inserted")

    def quality_check(self):
        """Quality check"""
        cursor = self.conn.cursor()

        # Data counts
        cursor.execute('SELECT COUNT(*) FROM experiments')
        exp_count = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM measurements')
        meas_count = cursor.fetchone()[0]

        # Check for missing values
        cursor.execute('''
        SELECT COUNT(*) FROM measurements
        WHERE uncertainty IS NULL
        ''')
        missing_unc = cursor.fetchone()[0]

        print("=== Quality Report ===")
        print(f"Experiments: {exp_count}")
        print(f"Measurements: {meas_count}")
        print(f"Missing uncertainty: {missing_unc}")

    def prepare_zenodo_metadata(self, output_file='zenodo_metadata.json'):
        """Prepare Zenodo metadata"""
        metadata = {
            "title": "Experimental Materials Database",
            "upload_type": "dataset",
            "description": "Experimental data from lab measurements",
            "creators": [
                {
                    "name": "Your Name",
                    "affiliation": "Your Institution"
                }
            ],
            "keywords": [
                "materials science",
                "experimental data",
                "properties"
            ],
            "access_right": "open",
            "license": "cc-by-4.0",
            "version": "1.0",
            "publication_date": datetime.now().strftime("%Y-%m-%d")
        }

        with open(output_file, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"Metadata saved: {output_file}")

    def run(self, excel_file):
        """Run pipeline"""
        print("=== Experimental Data Publication Pipeline ===")

        # Load data
        df = self.load_excel_data(excel_file)

        # Insert data
        self.insert_data(df)

        # Quality check
        self.quality_check()

        # Backup
        backup_mgr = BackupManager(self.db_path)
        backup_mgr.create_backup()

        # Prepare Zenodo metadata
        self.prepare_zenodo_metadata()

        print("\n=== Completed ===")
        print("Next steps:")
        print("1. Verify database")
        print("2. Upload to Zenodo")
        print("3. Obtain DOI")

# Usage example (requires Excel file)
# pipeline = ExperimentalDataPipeline()
# pipeline.run('experimental_data.xlsx')
</code></pre>
</details>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>Wilkinson, M. D. et al. (2016). "The FAIR Guiding Principles." <em>Scientific Data</em>, 3, 160018.
   DOI: <a href="https://doi.org/10.1038/sdata.2016.18">10.1038/sdata.2016.18</a></p>
</li>
<li>
<p>Zenodo Documentation. "Developers." URL: <a href="https://developers.zenodo.org">developers.zenodo.org</a></p>
</li>
<li>
<p>SQLite Documentation. URL: <a href="https://sqlite.org/docs.html">sqlite.org/docs</a></p>
</li>
<li>
<p>PostgreSQL Documentation. URL: <a href="https://www.postgresql.org/docs/">postgresql.org/docs</a></p>
</li>
</ol>
<hr/>
<h2>Navigation</h2>
<h3>Previous Chapter</h3>
<p><strong><a href="./chapter-3.html">‚Üê Chapter 3: Database Integration and Workflows</a></strong></p>
<h3>Series Contents</h3>
<p><strong><a href="./index.html">‚Üê Back to Series Contents</a></strong></p>
<hr/>
<h2>Author Information</h2>
<p><strong>Author</strong>: AI Terakoya Content Team
<strong>Created</strong>: 2025-10-17
<strong>Version</strong>: 1.0</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr/>
<p><strong>Congratulations on completing the Materials Database Fundamentals series!</strong></p>
<p><strong>Next Steps</strong>: Dive into machine learning with the <a href="../mi-introduction/index.html">MI Introduction Series</a></p><div class="navigation">
<a class="nav-button" href="chapter-3.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Contents</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical assurance, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, functionality, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are governed by the specified terms (e.g., CC BY 4.0). Such licenses typically include disclaimer clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
