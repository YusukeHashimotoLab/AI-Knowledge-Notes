<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 5: Integration of First-Principles Calculations and Machine Learning - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 5: Integration of First-Principles Calculations and Machine Learning - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="../../index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="../index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="index.html">Computational Materials Basics</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 5</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/computational-materials-basics-introduction/chapter-5.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>

<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 5: Integration of First-Principles Calculations and Machine Learning</h1>
<p class="subtitle">Machine Learning Potential and Active Learning</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Advanced</span>
<span class="meta-item">üíª Code examples: 6</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 5: Integration of First-Principles Calculations and Machine Learning</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">This chapter demonstrates the minimal execution path using VASP/Quantum ESPRESSO/LAMMPS. Standard pre- and post-processing tools are also overviewed in a comprehensive list.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> The most efficient approach is to first run the pipeline on a small system to familiarize yourself with input/output formats and unit systems.</p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:
- Understand the basic concepts of Machine Learning Potentials (MLP)
- Explain the differences and usage of Classical MD, AIMD, and MLP
- Train neural network potentials from DFT calculation data
- Understand efficient data generation strategies using Active Learning
- Grasp the latest trends in Universal MLP and Foundation Models</p>
<hr/>
<h2>5.1 Why Machine Learning Potentials Are Needed</h2>
<h3>Comparison of Three Computational Methods</h3>
<div class="mermaid">
flowchart LR
    A[Classical MD] --&gt;|Accuracy vs Speed| B[AIMD]
    B --&gt;|Accuracy vs Speed| C[Machine Learning Potential]
    C - Data-driven .-&gt; B

    style A fill:#ffcccc
    style B fill:#ccffcc
    style C fill:#ccccff
</div>
<table>
<thead>
<tr>
<th>Item</th>
<th>Classical MD</th>
<th>AIMD (DFT-MD)</th>
<th>MLP-MD</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Force Calculation</strong></td>
<td>Empirical force field</td>
<td>DFT (First-principles)</td>
<td>Machine learning model</td>
</tr>
<tr>
<td><strong>Accuracy</strong></td>
<td>Medium (force field dependent)</td>
<td>High (quantum mechanical)</td>
<td>High (DFT-equivalent)</td>
</tr>
<tr>
<td><strong>Computational Speed</strong></td>
<td>Very fast (ns/day)</td>
<td>Very slow (ps/day)</td>
<td>Fast (ns/day)</td>
</tr>
<tr>
<td><strong>System Size</strong></td>
<td>Millions of atoms</td>
<td>Hundreds of atoms</td>
<td>Thousands to tens of thousands of atoms</td>
</tr>
<tr>
<td><strong>Applicability</strong></td>
<td>Trained systems only</td>
<td>General-purpose</td>
<td>Within training data range</td>
</tr>
<tr>
<td><strong>Development Cost</strong></td>
<td>Low (using existing force fields)</td>
<td>None</td>
<td>High (training data generation)</td>
</tr>
</tbody>
</table>
<h3>Advantages of MLP</h3>
<p><strong>"DFT-level accuracy at Classical MD speed"</strong></p>
<ul>
<li>‚úÖ Accurately describes chemical reactions (bond breaking/formation)</li>
<li>‚úÖ Long-time simulations (ns-Œºs scale)</li>
<li>‚úÖ Large-scale systems (thousands to tens of thousands of atoms)</li>
<li>‚úÖ Applicable to novel materials without existing force fields</li>
</ul>
<p><strong>Challenges</strong>:
- ‚ùå Cost of generating training data (DFT calculations)
- ‚ùå Accuracy degrades outside training data range
- ‚ùå Model training requires computational resources and expertise</p>
<hr/>
<h2>5.2 Types of Machine Learning Potentials</h2>
<h3>1. Gaussian Approximation Potential (GAP)</h3>
<p><strong>Principle</strong>: Kernel method (Gaussian Process)</p>
<p>$$
E_{\text{GAP}}(\mathbf{R}) = \sum_{i=1}^N \alpha_i K(\mathbf{R}, \mathbf{R}_i)
$$</p>
<ul>
<li>$K$: Kernel function (measures similarity)</li>
<li>$\mathbf{R}_i$: Atomic configuration in training data</li>
<li>$\alpha_i$: Training parameters</li>
</ul>
<p><strong>Features</strong>:
- ‚úÖ Uncertainty estimation possible (advantageous for Active Learning)
- ‚úÖ Can learn from limited data
- ‚ùå Computational cost increases with number of training data points</p>
<h3>2. Neural Network Potential (NNP)</h3>
<p><strong>Behler-Parrinello type</strong>: Describes local environment of each atom</p>
<p>$$
E_{\text{NNP}} = \sum_{i=1}^{N_{\text{atoms}}} E_i^{\text{NN}}({\mathbf{G}_i})
$$</p>
<ul>
<li>$E_i^{\text{NN}}$: Neural network energy of atom $i$</li>
<li>$\mathbf{G}_i$: Symmetry Functions, describing the environment around atom $i$</li>
</ul>
<p><strong>Example of symmetry functions</strong> (radial component):</p>
<p>$$
G_i^{\text{rad}} = \sum_{j \neq i} e^{-\eta(r_{ij} - R_s)^2} f_c(r_{ij})
$$</p>
<ul>
<li>$r_{ij}$: Interatomic distance</li>
<li>$f_c(r)$: Cutoff function (ignores beyond a certain distance)</li>
</ul>
<p><strong>Features</strong>:
- ‚úÖ Fast even for large-scale systems
- ‚úÖ Computational cost constant regardless of training data size
- ‚ùå Uncertainty estimation difficult</p>
<h3>3. Message Passing Neural Network (MPNN)</h3>
<p>A type of Graph Neural Network (GNN):</p>
<p>$$
\mathbf{h}_i^{(k+1)} = \text{Update}\left(\mathbf{h}_i^{(k)}, \sum_{j \in \mathcal{N}(i)} \text{Message}(\mathbf{h}_i^{(k)}, \mathbf{h}_j^{(k)}, \mathbf{e}_{ij})\right)
$$</p>
<ul>
<li>$\mathbf{h}_i^{(k)}$: Hidden state of atom $i$ at layer $k$</li>
<li>$\mathcal{N}(i)$: Neighboring atoms of atom $i$</li>
<li>$\mathbf{e}_{ij}$: Bond information (distance, angle)</li>
</ul>
<p><strong>Representative models</strong>: SchNet, DimeNet, GemNet, MACE</p>
<p><strong>Features</strong>:
- ‚úÖ Naturally realizes rotation and translation invariance
- ‚úÖ Efficiently learns long-range interactions
- ‚úÖ State-of-the-art high-accuracy models</p>
<h3>4. Moment Tensor Potential (MTP)</h3>
<p><strong>Principle</strong>: Describes atomic environment with many-body expansion</p>
<p>$$
E_{\text{MTP}} = \sum_i \sum_{\alpha} c_{\alpha} B_{\alpha}(\mathbf{R}_i)
$$</p>
<p>$B_{\alpha}$ are moment tensor basis functions.</p>
<p><strong>Features</strong>:
- ‚úÖ Fast (linear model)
- ‚úÖ Easy to train
- ‚ùå Lower expressiveness than NNP</p>
<hr/>
<h2>5.3 Neural Network Potential Training (Practice)</h2>
<h3>Example 1: NNP Training Using AMP (Water Molecule)</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
from ase.build import molecule
from ase.calculators.emt import EMT
from gpaw import GPAW, PW
from amp import Amp
from amp.descriptor.gaussian import Gaussian
from amp.model.neuralnetwork import NeuralNetwork
import matplotlib.pyplot as plt

# Step 1: Generate training data (MD simulation + DFT)
def generate_training_data(n_samples=50):
    &quot;&quot;&quot;
    DFT calculations for various water molecule configurations
    &quot;&quot;&quot;
    from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
    from ase.md.verlet import VelocityVerlet
    from ase import units

    h2o = molecule('H2O')
    h2o.center(vacuum=5.0)

    # DFT calculator
    calc = GPAW(mode=PW(300), xc='PBE', txt=None)
    h2o.calc = calc

    # Initial velocity
    MaxwellBoltzmannDistribution(h2o, temperature_K=500)

    # MD simulation
    dyn = VelocityVerlet(h2o, timestep=1.0*units.fs)

    images = []
    for i in range(n_samples):
        dyn.run(10)  # Sample every 10 steps
        atoms_copy = h2o.copy()
        atoms_copy.calc = calc
        atoms_copy.get_potential_energy()  # Execute DFT calculation
        atoms_copy.get_forces()
        images.append(atoms_copy)
        print(f&quot;Sample {i+1}/{n_samples} collected&quot;)

    return images

print(&quot;Generating training data...&quot;)
train_images = generate_training_data(n_samples=50)

# Step 2: NNP training
print(&quot;Training Neural Network Potential...&quot;)

# Descriptor: Gaussian symmetry functions
descriptor = Gaussian()

# Model: Neural network
model = NeuralNetwork(hiddenlayers=(10, 10, 10))  # 3 layers, 10 nodes each

# AMP potential
calc_nnp = Amp(descriptor=descriptor,
               model=model,
               label='h2o_nnp',
               dblabel='h2o_nnp')

# Training
calc_nnp.train(images=train_images,
               energy_coefficient=1.0,
               force_coefficient=0.04)

print(&quot;Training complete!&quot;)

# Step 3: Accuracy evaluation on test data
print(&quot;\nGenerating test data...&quot;)
test_images = generate_training_data(n_samples=10)

E_dft = []
E_nnp = []
F_dft = []
F_nnp = []

for atoms in test_images:
    # DFT
    atoms.calc = GPAW(mode=PW(300), xc='PBE', txt=None)
    e_dft = atoms.get_potential_energy()
    f_dft = atoms.get_forces().flatten()

    # NNP
    atoms.calc = calc_nnp
    e_nnp = atoms.get_potential_energy()
    f_nnp = atoms.get_forces().flatten()

    E_dft.append(e_dft)
    E_nnp.append(e_nnp)
    F_dft.extend(f_dft)
    F_nnp.extend(f_nnp)

E_dft = np.array(E_dft)
E_nnp = np.array(E_nnp)
F_dft = np.array(F_dft)
F_nnp = np.array(F_nnp)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Energy
axes[0].scatter(E_dft, E_nnp, alpha=0.6)
axes[0].plot([E_dft.min(), E_dft.max()],
             [E_dft.min(), E_dft.max()], 'r--', label='Perfect')
axes[0].set_xlabel('DFT Energy (eV)', fontsize=12)
axes[0].set_ylabel('NNP Energy (eV)', fontsize=12)
axes[0].set_title('Energy Prediction', fontsize=14)
mae_e = np.mean(np.abs(E_dft - E_nnp))
axes[0].text(0.05, 0.95, f'MAE = {mae_e:.3f} eV',
            transform=axes[0].transAxes, va='top')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Force
axes[1].scatter(F_dft, F_nnp, alpha=0.3, s=10)
axes[1].plot([F_dft.min(), F_dft.max()],
             [F_dft.min(), F_dft.max()], 'r--', label='Perfect')
axes[1].set_xlabel('DFT Force (eV/√Ö)', fontsize=12)
axes[1].set_ylabel('NNP Force (eV/√Ö)', fontsize=12)
axes[1].set_title('Force Prediction', fontsize=14)
mae_f = np.mean(np.abs(F_dft - F_nnp))
axes[1].text(0.05, 0.95, f'MAE = {mae_f:.3f} eV/√Ö',
            transform=axes[1].transAxes, va='top')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('nnp_accuracy.png', dpi=150)
plt.show()

print(f&quot;\nNNP Accuracy:&quot;)
print(f&quot;Energy MAE: {mae_e:.4f} eV&quot;)
print(f&quot;Force MAE: {mae_f:.4f} eV/√Ö&quot;)
</code></pre>
<p><strong>Target accuracy</strong>:
- Energy: MAE &lt; 1 meV/atom
- Force: MAE &lt; 0.1 eV/√Ö</p>
<hr/>
<h2>5.4 Active Learning</h2>
<h3>Basic Concept</h3>
<p><strong>Problem</strong>: Performing DFT calculations for all configurations is impractical (high computational cost)</p>
<p><strong>Solution</strong>: <strong>Prioritize sampling configurations with the most information</strong></p>
<div class="mermaid">
flowchart TD
    A[Small initial dataset] --&gt; B[NNP training]
    B --&gt; C[MD simulation with NNP]
    C --&gt; D[Detect high-uncertainty configurations]
    D --&gt; E{Additional data needed?}
    E --&gt;|Yes| F[Additional DFT calculations]
    F --&gt; G[Add to dataset]
    G --&gt; B
    E --&gt;|No| H[Training complete]

    style A fill:#e3f2fd
    style H fill:#c8e6c9
</div>
<h3>Methods for Uncertainty Estimation</h3>
<p><strong>1. Ensemble Method</strong>:
- Train multiple NNPs (different initial values, data splits)
- Use prediction variance as uncertainty</p>
<p>$$
\sigma_E^2 = \frac{1}{M}\sum_{m=1}^M (E_m - \bar{E})^2
$$</p>
<p><strong>2. Dropout Method</strong>:
- Randomly disable nodes during training
- Apply dropout during inference as well, predict multiple times
- Use prediction variance as uncertainty</p>
<p><strong>3. Query-by-Committee</strong>:
- Use multiple models with different algorithms
- Sample configurations with low agreement in predictions</p>
<h3>Active Learning Implementation Example</h3>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

import numpy as np
from ase.md.langevin import Langevin
from ase import units

def active_learning_loop(initial_images, n_iterations=5, n_md_steps=1000):
    &quot;&quot;&quot;
    Efficient training data generation using Active Learning
    &quot;&quot;&quot;
    dataset = initial_images.copy()

    for iteration in range(n_iterations):
        print(f&quot;\n--- Iteration {iteration+1}/{n_iterations} ---&quot;)

        # Step 1: Train NNP
        print(&quot;Training NNP...&quot;)
        nnp = train_nnp(dataset)  # AMP training as described above

        # Step 2: MD simulation with NNP
        print(&quot;Running MD with NNP...&quot;)
        h2o = dataset[0].copy()
        h2o.calc = nnp

        # Langevin MD (with thermostat)
        dyn = Langevin(h2o, timestep=1.0*units.fs,
                       temperature_K=500, friction=0.01)

        # Collect high-uncertainty configurations
        uncertain_images = []
        uncertainties = []

        for step in range(n_md_steps):
            dyn.run(1)

            # Uncertainty estimation with Ensemble (simplified)
            # In practice, calculate variance with multiple NNPs
            uncertainty = estimate_uncertainty(h2o, nnp)  # Virtual function

            if uncertainty &gt; threshold:  # Add if above threshold
                atoms_copy = h2o.copy()
                uncertain_images.append(atoms_copy)
                uncertainties.append(uncertainty)

        print(f&quot;Found {len(uncertain_images)} uncertain configurations&quot;)

        # Step 3: DFT calculations for high-uncertainty configurations
        print(&quot;Running DFT for uncertain configurations...&quot;)
        for atoms in uncertain_images[:10]:  # Top 10
            atoms.calc = GPAW(mode=PW(300), xc='PBE', txt=None)
            atoms.get_potential_energy()
            atoms.get_forces()
            dataset.append(atoms)

        print(f&quot;Dataset size: {len(dataset)}&quot;)

    return dataset, nnp

# Execution
initial_data = generate_training_data(n_samples=20)
final_dataset, final_nnp = active_learning_loop(initial_data, n_iterations=5)

print(f&quot;\nFinal dataset size: {len(final_dataset)}&quot;)
print(f&quot;vs. random sampling: 50-100 samples would be needed&quot;)
print(f&quot;Efficiency gain: {100/len(final_dataset):.1f}x&quot;)
</code></pre>
<p><strong>Advantages of Active Learning</strong>:
- Can reduce training data by 50-90%
- Prioritizes important configurations (phase transitions, reaction pathways)
- Efficient use of computational resources</p>
<hr/>
<h2>5.5 Latest Trends</h2>
<h3>1. Universal Machine Learning Potential</h3>
<p><strong>Goal</strong>: One model covering diverse material systems</p>
<p><strong>Representative examples</strong>:
- <strong>CHGNet</strong> (2023): Trained on 1.4 million materials from Materials Project
  - Covers 89 elements
  - Includes magnetism
  - Open source</p>
<ul>
<li><strong>M3GNet</strong> (2022): Many-body graph network</li>
<li>Applicable to crystals, surfaces, molecules</li>
<li>
<p>Predicts forces, stresses, magnetic moments</p>
</li>
<li>
<p><strong>MACE</strong> (2023): Equivariant message passing</p>
</li>
<li>High accuracy (approximately twice DFT error)</li>
<li>Can train on small-scale data</li>
</ul>
<p><strong>Usage</strong>:</p>
<pre><code class="language-python">from chgnet.model import CHGNet
from pymatgen.core import Structure

# Load pre-trained model
model = CHGNet.load()

# Predict for arbitrary crystal structure
structure = Structure.from_file('POSCAR')
energy = model.predict_structure(structure)

print(f"Predicted energy: {energy} eV")
</code></pre>
<h3>2. Foundation Models for Materials</h3>
<p><strong>Materials science version of Large Language Models (LLM)</strong>:</p>
<ul>
<li><strong>MatGPT</strong>: Pre-trained on materials databases</li>
<li><strong>LLaMat</strong>: Crystal structure ‚Üí property prediction</li>
</ul>
<p><strong>Transfer learning</strong>:
- Pre-training on large-scale data
- Fine-tuning with limited data
- Practical accuracy with 10-100 samples</p>
<h3>3. Application to Autonomous Experiments</h3>
<p><strong>Closed-loop optimization</strong>:</p>
<pre><code>ML prediction ‚Üí Optimal candidate proposal ‚Üí Robot experiment ‚Üí Measurement ‚Üí Data accumulation ‚Üí ML retraining
</code></pre>
<p><strong>Real examples</strong>:
- <strong>A-Lab</strong> (Berkeley, 2023): Synthesized and evaluated 41 materials in 17 days
- <strong>Autonomous materials discovery</strong>: Catalysts, battery materials, quantum dots</p>
<hr/>
<h2>5.6 Practical Guidelines for MLP</h2>
<h3>When to Use MLP</h3>
<p><strong>Suitable cases</strong>:
- ‚úÖ Long-time MD (ns-Œºs) required
- ‚úÖ Large-scale systems (thousands of atoms or more)
- ‚úÖ Includes chemical reactions
- ‚úÖ Novel materials without existing force fields
- ‚úÖ Computational resources available for training data generation</p>
<p><strong>Unsuitable cases</strong>:
- ‚ùå One-time short MD (direct AIMD is simpler)
- ‚ùå Cannot ensure representativeness of training data
- ‚ùå Extrapolation beyond training data range required
- ‚ùå Existing high-accuracy force fields available (ReaxFF, COMB, etc.)</p>
<h3>Implementation Workflow</h3>
<div class="mermaid">
flowchart TD
    A[Problem definition] --&gt; B[Initial data generation 20-100 samples]
    B --&gt; C[NNP training]
    C --&gt; D[Accuracy evaluation on validation set]
    D --&gt; E{Accuracy OK?}
    E --&gt;|No| F[Active Learning]
    F --&gt; G[Additional DFT calculations]
    G --&gt; C
    E --&gt;|Yes| H[Production MD simulation]
    H --&gt; I[Property calculations]

    style A fill:#e3f2fd
    style I fill:#c8e6c9
</div>
<h3>Recommended Tools</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Method</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AMP</strong></td>
<td>NNP</td>
<td>Python native, ASE integration</td>
</tr>
<tr>
<td><strong>DeePMD</strong></td>
<td>NNP</td>
<td>Fast, parallelized, TensorFlow</td>
</tr>
<tr>
<td><strong>SchNetPack</strong></td>
<td>GNN</td>
<td>SchNet, research-oriented</td>
</tr>
<tr>
<td><strong>MACE</strong></td>
<td>Equivariant GNN</td>
<td>Latest, high accuracy</td>
</tr>
<tr>
<td><strong>GAP</strong></td>
<td>Gaussian Process</td>
<td>Uncertainty estimation</td>
</tr>
<tr>
<td><strong>MTP</strong></td>
<td>Moment Tensor</td>
<td>Fast training</td>
</tr>
<tr>
<td><strong>CHGNet</strong></td>
<td>Universal</td>
<td>Pre-trained</td>
</tr>
</tbody>
</table>
<hr/>
<h2>5.7 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Need for MLP</strong>
   - DFT-level accuracy + Classical MD speed
   - Long-time and large-scale system simulations</p>
</li>
<li>
<p><strong>Types of MLP</strong>
   - GAP (Gaussian Process)
   - NNP (Neural Network)
   - MPNN (Graph Neural Network)
   - MTP (Moment Tensor)</p>
</li>
<li>
<p><strong>NNP Training</strong>
   - DFT data generation
   - Implementation with AMP
   - Accuracy evaluation</p>
</li>
<li>
<p><strong>Active Learning</strong>
   - Uncertainty estimation
   - Efficient data generation
   - 50-90% computational reduction</p>
</li>
<li>
<p><strong>Latest Trends</strong>
   - Universal MLP (CHGNet, M3GNet)
   - Foundation Models
   - Autonomous experiments</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>MLP is a new paradigm in computational materials science</li>
<li>Active Learning is the key to training efficiency</li>
<li>Pre-trained models available through Universal MLP</li>
<li>Practical applications are advancing (autonomous experiments, materials discovery)</li>
</ul>
<h3>Next Steps</h3>
<ul>
<li>Try MLP on your own research topics</li>
<li>Follow latest papers (<em>npj Computational Materials</em>, <em>Nature Materials</em>)</li>
<li>Contribute to open-source tools</li>
<li>Collaborate with experimental researchers</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Summarize the differences between Classical MD, AIMD, and MLP-MD in a table.</p>
<details>
<summary>Sample Answer</summary>

| Item | Classical MD | AIMD (DFT-MD) | MLP-MD |
|-----|-------------|-------------|--------|
| **Force Calculation Method** | Empirical force field (analytical) | DFT (First-principles) | Machine learning model |
| **Accuracy** | Medium (depends on force field quality) | High (quantum mechanically accurate) | High (DFT-equivalent) |
| **Computational Speed** | Very fast (1 ns/day) | Very slow (10 ps/day) | Fast (1 ns/day) |
| **System Size** | Millions of atoms | Hundreds of atoms | Thousands to tens of thousands of atoms |
| **Chemical Reactions** | Not described (ReaxFF can) | Accurately described | Accurately described |
| **Applicability** | Only systems with trained force fields | General-purpose | Within training data range |
| **Development Cost** | Low (existing force fields) | None | High (training data generation) |
| **Applications** | Diffusion, phase transitions, large-scale | Chemical reactions, electronic states | Reactions + long-time MD |

**Guidelines for selection**:
- Existing force field available ‚Üí Classical MD
- Chemical reactions with short time ‚Üí AIMD
- Chemical reactions + long time ‚Üí MLP-MD
- Novel materials discovery ‚Üí AIMD ‚Üí MLP ‚Üí large-scale MD

</details>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Explain why Active Learning is efficient, with specific examples.</p>
<details>
<summary>Sample Answer</summary>

**Basic Principle of Active Learning**:

Traditional machine learning (Random Sampling):
- Randomly sample data
- Many data points are duplicates of "known regions"
- Inefficient

Active Learning (Uncertainty Sampling):
- Prioritize sampling configurations where model is "uncertain"
- Efficiently acquire new information
- High accuracy with less data

**Specific Example: NNP Training for Water Molecules**

**Random Sampling (Traditional method)**:
- Randomly sample 100 configurations from 300K equilibrium state
- 80% are near equilibrium structure (similar configurations)
- Remaining 20% are reaction pathways or high-energy configurations
- Result: 100 DFT calculations, accuracy MAE = 5 meV/atom

**Active Learning**:
- Train from initial 20 configurations
- Detect high-uncertainty configurations during MD simulation
  - Configurations with extended O-H bonds (dissociation process)
  - Configurations with highly distorted H-O-H angles
  - High-energy excited states
- DFT calculations for these configurations (20 additional configurations)
- Total 40 DFT calculations, accuracy MAE = 3 meV/atom

**Reasons for Efficiency**:

1. **Maximize information content**:
   - Avoid duplicates of similar configurations
   - Prioritize regions model "doesn't know"

2. **Balance exploration and exploitation**:
   - Stable predictions in known configurations (exploitation)
   - Acquire new information in unknown configurations (exploration)

3. **Adaptive sampling**:
   - Automatically detect important regions (reaction pathways, phase transitions)
   - Does not rely on human intuition

**Actual efficiency gains**:
- 50-90% DFT calculation reduction (literature values)
- Particularly effective for complex systems (multi-component, reactive systems)
- 10-50x efficiency gain in total training time

**Example: Li-ion battery electrolyte**:
- Random: 10,000 DFT calculations, 2 months
- Active Learning: 2,000 DFT calculations, 2 weeks
- Efficiency: 5x, equivalent accuracy

</details>
<h3>Problem 3 (Difficulty: hard)</h3>
<p>Discuss the advantages and limitations of Universal Machine Learning Potentials (CHGNet, M3GNet, etc.).</p>
<details>
<summary>Sample Answer</summary>

**Overview of Universal MLP (Example: CHGNet)**:

- **Training data**: Materials Project (1.4 million materials, 89 elements)
- **Model**: Graph Neural Network
- **Predictions**: Energy, forces, stresses, magnetic moments

**Advantages**:

1. **Immediately usable**:
   - Pre-trained ‚Üí No additional training needed
   - Predictable for arbitrary crystal structures
   - Screen thousands of materials in seconds

2. **Wide applicability**:
   - 89 elements (H to Am)
   - Oxides, alloys, semiconductors, insulators
   - Magnetic materials also supported

3. **Foundation for transfer learning**:
   - Fine-tuning with limited data (10-100 samples)
   - Efficiently create system-specific high-accuracy models

4. **Accelerate materials discovery**:
   - Large-scale candidate screening (1 million materials/day)
   - Narrow down experimental candidates
   - Combination with high-throughput calculations

**Limitations**:

1. **Accuracy limits**:
   - Approximately 2-5 times DFT error (CHGNet: MAE ~30 meV/atom)
   - Insufficient for precision calculations
   - Inferior to dedicated MLP for specific systems

2. **Extrapolation problems**:
   - Accuracy degrades for configurations not in training data (extreme temperature/pressure)
   - Uncertain for novel material systems (ultra-high pressure, new element combinations)

3. **Data bias**:
   - Depends on Materials Project calculation conditions (PBE functional)
   - Systematic deviations from experiments (band gap underestimation, etc.)
   - Over/under-representation of specific material classes

4. **Lack of physical constraints**:
   - No strict guarantee of energy conservation
   - Drift in long-time MD
   - Symmetry breaking (rare)

**Practical Strategies**:

**Scenario 1: Materials Screening**
- Narrow down from 1 million candidates to top 1000 with Universal MLP
- Precision calculations with DFT
- Efficiency: 1000x

**Scenario 2: Precision MD for Specific Systems**
- Transfer learning from Universal MLP
- Additional training with system-specific data (100 samples)
- Accuracy improvement: MAE 5 meV/atom (practical level)

**Scenario 3: Novel Material Classes**
- Universal MLP as reference only
- Build dedicated MLP from scratch (Active Learning)
- Training data: 500-1000 samples

**Future Outlook**:

1. **Dataset expansion**:
   - Integration of experimental data
   - Data from diverse computational methods (GW, DMFT)

2. **Evolution to Foundation Models**:
   - Equivalent to GPT in natural language processing
   - Few-shot learning (adapt with few samples)
   - Zero-shot transfer (new systems without training)

3. **Integration with experiments**:
   - Autonomous experiment loops
   - Real-time feedback

**Conclusion**:
Universal MLP is becoming "foundational infrastructure" for materials science, but is not omnipotent. Important to use dedicated MLP appropriately depending on application.

</details>
<hr/>
<h2>Data Licenses and Citations</h2>
<h3>Datasets Used</h3>
<ol>
<li>
<p><strong>Materials Project Database</strong> (CC BY 4.0)
   - DFT data for 1.4 million materials (CHGNet training)
   - URL: https://materialsproject.org
   - Citation: Jain, A., et al. (2013). <em>APL Materials</em>, 1, 011002.</p>
</li>
<li>
<p><strong>Open Catalyst Project</strong> (CC BY 4.0)
   - DFT dataset for catalyst surfaces
   - URL: https://opencatalystproject.org/</p>
</li>
<li>
<p><strong>QM9 Dataset</strong> (CC0)
   - DFT data for 134,000 small molecules
   - URL: http://quantum-machine.org/datasets/</p>
</li>
</ol>
<h3>Software Used</h3>
<ol>
<li>
<p><strong>AMP - Atomistic Machine-learning Package</strong> (GPL v3)
   - URL: https://amp.readthedocs.io/</p>
</li>
<li>
<p><strong>CHGNet</strong> (MIT License)
   - Universal ML Potential
   - URL: https://github.com/CederGroupHub/chgnet</p>
</li>
<li>
<p><strong>M3GNet</strong> (BSD 3-Clause)
   - Graph Neural Network Potential
   - URL: https://github.com/materialsvirtuallab/m3gnet</p>
</li>
<li>
<p><strong>MACE</strong> (MIT License)
   - Equivariant Message Passing
   - URL: https://github.com/ACEsuit/mace</p>
</li>
</ol>
<hr/>
<h2>Code Reproducibility Checklist</h2>
<h3>Environment Setup</h3>
<pre><code class="language-bash"># Basic MLP environment
conda create -n mlp python=3.11
conda activate mlp
conda install pytorch torchvision -c pytorch
conda install -c conda-forge ase gpaw

# Individual MLP tools
pip install amp-atomistics  # AMP
pip install chgnet  # CHGNet
pip install m3gnet  # M3GNet
pip install mace-torch  # MACE
</code></pre>
<h3>GPU Requirements (Recommended)</h3>
<table>
<thead>
<tr>
<th>Training Data Size</th>
<th>GPU Memory</th>
<th>Training Time</th>
<th>Recommended GPU</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 samples</td>
<td>~2 GB</td>
<td>~30 minutes</td>
<td>GTX 1060</td>
</tr>
<tr>
<td>1,000 samples</td>
<td>~8 GB</td>
<td>~3 hours</td>
<td>RTX 3070</td>
</tr>
<tr>
<td>10,000 samples</td>
<td>~16 GB</td>
<td>~1 day</td>
<td>RTX 3090/A100</td>
</tr>
</tbody>
</table>
<h3>Troubleshooting</h3>
<p><strong>Problem</strong>: CUDA out of memory
<strong>Solution</strong>:</p>
<pre><code class="language-python"># Reduce batch size
model.train(batch_size=8)  # 32 ‚Üí 8
</code></pre>
<p><strong>Problem</strong>: Training does not converge
<strong>Solution</strong>:</p>
<pre><code class="language-python"># Adjust learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # 1e-3 ‚Üí 1e-4
</code></pre>
<hr/>
<h2>Practical Pitfalls and Countermeasures</h2>
<h3>1. Training Data Bias</h3>
<pre><code class="language-python"># ‚ùå Wrong: Only equilibrium structures
train_data = [equilibrium_structures]  # Range too narrow

# ‚úÖ Correct: Sample diverse configurations
# - Equilibrium structures
# - MD trajectories (high temperature)
# - Mid-structure optimization
# - High-energy configurations
</code></pre>
<h3>2. Inappropriate Force Weights</h3>
<pre><code class="language-python"># ‚ùå Imbalanced: Energy-only emphasis
loss = energy_loss  # Ignores forces

# ‚úÖ Balanced: Forces also emphasized
loss = energy_loss + 0.1 * force_loss  # Force weight 0.1
</code></pre>
<h3>3. Use in Extrapolation Region</h3>
<pre><code class="language-python"># ‚ùå Dangerous: Prediction outside training range
# Training: 0-1000 K
# Usage: 2000 K ‚Üí Inaccurate

# ‚úÖ Safe: Use within training range
# Or warn with uncertainty estimation
</code></pre>
<h3>4. Active Learning Threshold Setting</h3>
<pre><code class="language-python"># ‚ùå Threshold too high ‚Üí Data shortage
uncertainty_threshold = 10.0  # Too loose

# ‚úÖ Appropriate threshold
uncertainty_threshold = 0.1  # Energy [eV/atom]
</code></pre>
<hr/>
<h2>Quality Assurance Checklist</h2>
<h3>MLP Training Validity</h3>
<ul>
<li>[ ] Training error: Energy MAE &lt; 10 meV/atom</li>
<li>[ ] Training error: Force MAE &lt; 0.1 eV/√Ö</li>
<li>[ ] Test error within twice training error (no overfitting)</li>
<li>[ ] Stable performance on validation set</li>
</ul>
<h3>Physical Validity</h3>
<ul>
<li>[ ] Energy conservation (verify with NVE MD)</li>
<li>[ ] Translation and rotation invariance</li>
<li>[ ] Symmetry conservation</li>
<li>[ ] No abnormal forces (&gt; 10 eV/√Ö)</li>
</ul>
<h3>Active Learning Efficiency</h3>
<ul>
<li>[ ] Training data reduction rate &gt; 50%</li>
<li>[ ] Iterations to convergence &lt; 10</li>
<li>[ ] Final accuracy equal to or better than random sampling</li>
</ul>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>Behler, J., &amp; Parrinello, M. (2007). "Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces." <em>Physical Review Letters</em>, 98, 146401.
   DOI: <a href="https://doi.org/10.1103/PhysRevLett.98.146401">10.1103/PhysRevLett.98.146401</a></p>
</li>
<li>
<p>Bart√≥k, A. P., et al. (2010). "Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons." <em>Physical Review Letters</em>, 104, 136403.
   DOI: <a href="https://doi.org/10.1103/PhysRevLett.104.136403">10.1103/PhysRevLett.104.136403</a></p>
</li>
<li>
<p>Sch√ºtt, K. T., et al. (2017). "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions." <em>NeurIPS</em>.</p>
</li>
<li>
<p>Chen, C., &amp; Ong, S. P. (2022). "A universal graph deep learning interatomic potential for the periodic table." <em>Nature Computational Science</em>, 2, 718-728.
   DOI: <a href="https://doi.org/10.1038/s43588-022-00349-3">10.1038/s43588-022-00349-3</a></p>
</li>
<li>
<p>Batatia, I., et al. (2022). "MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields." <em>NeurIPS</em>.</p>
</li>
<li>
<p>CHGNet: https://github.com/CederGroupHub/chgnet</p>
</li>
<li>M3GNet: https://github.com/materialsvirtuallab/m3gnet</li>
<li>MACE: https://github.com/ACEsuit/mace</li>
</ol>
<hr/>
<h2>Author Information</h2>
<p><strong>Created by</strong>: MI Knowledge Hub Content Team
<strong>Date Created</strong>: 2025-10-17
<strong>Version</strong>: 1.0
<strong>Series</strong>: Computational Materials Basics Introduction v1.0</p>
<p><strong>License</strong>: Creative Commons BY-NC-SA 4.0</p>
<hr/>
<p><strong>Congratulations! You have completed the Computational Materials Basics Introduction series!</strong></p>
<p>Next steps:
- Execute actual calculations on your own research topics
- Proceed to High-Throughput Computing Introduction series
- Deepen knowledge by reading latest papers
- Join the community (GitHub, conferences)</p>
<p><strong>Continuous learning opens the future of materials science!</strong></p>
<div class="navigation">
<a class="nav-button" href="chapter-4.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Contents</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, or operational safety.</li>
<li>The creators and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>The creators and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content, to the maximum extent permitted by applicable law.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Created by</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Date Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
