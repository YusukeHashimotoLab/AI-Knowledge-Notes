<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/transformer-introduction/chapter-3.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 3</h1>
<p class="subtitle">Pre-trained Models and Transfer Learning</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 0</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 3: Pre-trained Models and Transfer Learning</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Learn where to use domain-specific pre-trained models like MatBERT and ChemBERTa for materials science and chemistry applications. Master the key principles of fine-tuning with limited data.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Compare freezing, partial freezing, and full layer training strategies to find the optimal balance between computational resources and accuracy.</p>
<p><strong>Learning Time</strong>: 25-30 minutes | <strong>Difficulty</strong>: Intermediate to Advanced</p>
<h2>üìã What You'll Learn</h2>
<ul>
<li>The importance and principles of pre-training</li>
<li>Materials science-specific pre-trained models like MatBERT and MolBERT</li>
<li>Fine-tuning strategies</li>
<li>Few-shot learning and prompt engineering</li>
<li>Domain adaptation</li>
</ul>
<hr/>
<h2>3.1 Importance of Pre-training</h2>
<h3>Why Pre-training is Necessary</h3>
<p><strong>Challenges in Materials Science</strong>:
- ‚ùå Limited labeled data (experimental data is expensive)
- ‚ùå Domain-specific knowledge is required
- ‚ùå Training from scratch is time-consuming and costly</p>
<p><strong>Advantages of Pre-training</strong>:
- ‚úÖ Acquire general knowledge from large-scale <strong>unlabeled data</strong>
- ‚úÖ Achieve <strong>high accuracy</strong> with small amounts of labeled data
- ‚úÖ <strong>Significantly reduce</strong> development time (weeks ‚Üí hours)</p>
<div class="mermaid">
flowchart LR
    A[Large-scale Unlabeled Data] --&gt; B[Pre-training]
    B --&gt; C[General Representation Model]
    C --&gt; D[Fine-tuning]
    E[Small Labeled Data] --&gt; D
    D --&gt; F[Task-Specific Model]

    style B fill:#e1f5ff
    style D fill:#ffe1e1
</div>
<h3>Pre-training Tasks</h3>
<p><strong>Examples from NLP</strong>:
- <strong>Masked Language Model (MLM)</strong>: Mask some words and predict them
- <strong>Next Sentence Prediction (NSP)</strong>: Predict continuity of two sentences</p>
<p><strong>Applications in Materials Science</strong>:
- <strong>Masked Atom Prediction</strong>: Mask some atoms and predict them
- <strong>Property Prediction</strong>: Simultaneously predict multiple material properties
- <strong>Contrastive Learning</strong>: Place similar materials close together, different ones far apart</p>
<hr/>
<h2>3.2 MatBERT: Materials BERT</h2>
<h3>Overview</h3>
<p><strong>MatBERT</strong> is a model trained with BERT on material composition formulas.</p>
<p><strong>Features</strong>:
- Pre-trained on <strong>500k materials</strong> composition formulas
- <strong>Masked atom prediction</strong> task
- Applicable to various property predictions through transfer learning</p>
<h3>Tokenization of Composition Formulas</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class CompositionTokenizer:
    def __init__(self):
        # Custom vocabulary (periodic table elements)
        self.vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + [
            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
            # ... all elements
        ]
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}

    def tokenize(self, composition):
        """
        Tokenize composition formula

        Args:
            composition: Composition formula like 'Fe2O3'
        Returns:
            tokens: List of tokens
        """
        import re
        # Split elements and numbers
        pattern = r'([A-Z][a-z]?)(\d*\.?\d*)'
        matches = re.findall(pattern, composition)

        tokens = ['[CLS]']
        for element, count in matches:
            if element in self.vocab:
                # Add element
                tokens.append(element)
                # If count is greater than 1, repeat that many times (simplified)
                if count and float(count) &gt; 1:
                    for _ in range(int(float(count)) - 1):
                        tokens.append(element)
        tokens.append('[SEP]')

        return tokens

    def encode(self, compositions, max_length=32):
        """
        Convert composition formulas to IDs

        Args:
            compositions: List of composition formulas
            max_length: Maximum length
        Returns:
            input_ids: (batch_size, max_length)
            attention_mask: (batch_size, max_length)
        """
        batch_input_ids = []
        batch_attention_mask = []

        for comp in compositions:
            tokens = self.tokenize(comp)
            ids = [self.token_to_id.get(token, 0) for token in tokens]

            # Padding
            attention_mask = [1] * len(ids)
            while len(ids) &lt; max_length:
                ids.append(0)  # [PAD]
                attention_mask.append(0)

            # Truncation
            ids = ids[:max_length]
            attention_mask = attention_mask[:max_length]

            batch_input_ids.append(ids)
            batch_attention_mask.append(attention_mask)

        return torch.tensor(batch_input_ids), torch.tensor(batch_attention_mask)

# Usage example
tokenizer = CompositionTokenizer()

compositions = [
    'Fe2O3',     # Iron oxide
    'LiCoO2',    # Lithium cobalt oxide (battery material)
    'BaTiO3'     # Barium titanate (dielectric)
]

input_ids, attention_mask = tokenizer.encode(compositions)
print(f"Input IDs shape: {input_ids.shape}")
print(f"First composition tokens: {input_ids[0][:10]}")
</code></pre>
<h3>MatBERT Model</h3>
<pre><code class="language-python">class MatBERT(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8):
        super(MatBERT, self).__init__()

        # Embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(512, d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=2048,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        self.d_model = d_model

    def forward(self, input_ids, attention_mask):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, seq_len, d_model)
        """
        batch_size, seq_len = input_ids.shape

        # Token embedding
        token_embeddings = self.embedding(input_ids)

        # Positional embedding
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embedding(positions)

        # Sum
        embeddings = token_embeddings + position_embeddings

        # Transformer
        # Convert attention_mask for Transformer (0‚Üí-inf, 1‚Üí0)
        transformer_mask = (1 - attention_mask).bool()
        output = self.transformer_encoder(embeddings, src_key_padding_mask=transformer_mask)

        return output

# Usage example
vocab_size = len(tokenizer.vocab)
model = MatBERT(vocab_size, d_model=512, num_layers=6, num_heads=8)

embeddings = model(input_ids, attention_mask)
print(f"Embeddings shape: {embeddings.shape}")  # (3, 32, 512)
</code></pre>
<h3>Pre-training: Masked Atom Prediction</h3>
<pre><code class="language-python">def masked_atom_prediction_loss(model, input_ids, attention_mask, mask_prob=0.15):
    """
    Pre-training with masked atom prediction

    Args:
        model: MatBERT model
        input_ids: (batch_size, seq_len)
        attention_mask: (batch_size, seq_len)
        mask_prob: Probability of masking
    Returns:
        loss: Loss value
    """
    batch_size, seq_len = input_ids.shape

    # Random masking
    mask_token_id = tokenizer.token_to_id['[MASK]']
    mask = torch.rand(batch_size, seq_len) &lt; mask_prob
    mask = mask &amp; (attention_mask == 1)  # Exclude padding

    # Save original tokens
    original_input_ids = input_ids.clone()

    # Apply mask
    input_ids[mask] = mask_token_id

    # Forward
    embeddings = model(input_ids, attention_mask)

    # Prediction head
    prediction_head = nn.Linear(model.d_model, vocab_size)
    logits = prediction_head(embeddings)

    # Loss calculation (only for masked positions)
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    labels = original_input_ids.clone()
    labels[~mask] = -100  # Ignore non-masked positions

    loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

    return loss

# Pre-training loop (simplified version)
def pretrain_matbert(model, dataloader, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for input_ids, attention_mask in dataloader:
            loss = masked_atom_prediction_loss(model, input_ids, attention_mask)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Pretraining Loss: {avg_loss:.4f}")

    return model
</code></pre>
<hr/>
<h2>3.3 Fine-tuning Strategies</h2>
<h3>What is Fine-tuning?</h3>
<p><strong>Definition</strong>: Additional training to adapt a pre-trained model to a specific task</p>
<p><strong>Strategies</strong>:
1. <strong>Full Fine-tuning</strong>: Update all parameters
2. <strong>Feature Extraction</strong>: Use only embedding layers, train only prediction head
3. <strong>Partial Fine-tuning</strong>: Update only some layers</p>
<div class="mermaid">
flowchart TD
    A[Pre-trained MatBERT] --&gt; B{Fine-tuning Strategy}
    B --&gt; C[Full Fine-tuning]
    B --&gt; D[Feature Extraction]
    B --&gt; E[Partial Fine-tuning]

    C --&gt; F[Update all layers]
    D --&gt; G[Freeze embeddings, train only prediction head]
    E --&gt; H[Update only upper layers]

    style C fill:#ffe1e1
    style D fill:#e1f5ff
    style E fill:#f5ffe1
</div>
<h3>Implementation: Band Gap Prediction</h3>
<pre><code class="language-python">class MatBERTForBandgap(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(MatBERTForBandgap, self).__init__()
        self.matbert = matbert_model

        # Prediction head
        self.bandgap_predictor = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask):
        # MatBERT embedding
        embeddings = self.matbert(input_ids, attention_mask)

        # Use [CLS] token embedding
        cls_embedding = embeddings[:, 0, :]

        # Band gap prediction
        bandgap = self.bandgap_predictor(cls_embedding)
        return bandgap

# Fine-tuning
def finetune_for_bandgap(pretrained_model, train_loader, val_loader, strategy='full'):
    """
    Fine-tuning for band gap prediction

    Args:
        pretrained_model: Pre-trained MatBERT
        train_loader: Training data loader
        val_loader: Validation data loader
        strategy: 'full', 'feature', 'partial'
    """
    model = MatBERTForBandgap(pretrained_model)

    # Freeze parameters based on strategy
    if strategy == 'feature':
        # Freeze MatBERT
        for param in model.matbert.parameters():
            param.requires_grad = False
    elif strategy == 'partial':
        # Freeze lower layers, update only upper layers
        for i, layer in enumerate(model.matbert.transformer_encoder.layers):
            if i &lt; 3:  # Freeze lower 3 layers
                for param in layer.parameters():
                    param.requires_grad = False

    # Optimization
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)
    criterion = nn.MSELoss()

    # Training loop
    best_val_loss = float('inf')
    for epoch in range(20):
        model.train()
        train_loss = 0
        for input_ids, attention_mask, bandgaps in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, bandgaps)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for input_ids, attention_mask, bandgaps in val_loader:
                predictions = model(input_ids, attention_mask)
                loss = criterion(predictions, bandgaps)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_matbert_bandgap.pt')

    return model
</code></pre>
<hr/>
<h2>3.4 Few-shot Learning</h2>
<h3>Overview</h3>
<p><strong>Few-shot learning</strong>: Learning new tasks with a small number of samples (a few to dozens)</p>
<p><strong>Importance in Materials Science</strong>:
- Very limited data for new materials
- High cost of acquiring experimental data
- Need for rapid prototyping</p>
<h3>Prototypical Networks</h3>
<pre><code class="language-python">class PrototypicalNetwork(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PrototypicalNetwork, self).__init__()
        self.encoder = matbert_model

    def forward(self, support_ids, support_mask, query_ids, query_mask, support_labels):
        """
        Classification with Prototypical Networks

        Args:
            support_ids: Support set input (n_support, seq_len)
            support_mask: Support set mask
            query_ids: Query input (n_query, seq_len)
            query_mask: Query mask
            support_labels: Support set labels (n_support,)
        Returns:
            predictions: Predicted labels for queries
        """
        # Embeddings for support set and query
        support_embeddings = self.encoder(support_ids, support_mask)[:, 0, :]  # [CLS]
        query_embeddings = self.encoder(query_ids, query_mask)[:, 0, :]

        # Calculate prototype (mean embedding) for each class
        unique_labels = torch.unique(support_labels)
        prototypes = []
        for label in unique_labels:
            mask = (support_labels == label)
            prototype = support_embeddings[mask].mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # (num_classes, d_model)

        # Distance between queries and prototypes
        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, num_classes)

        # Predict the class of the closest prototype
        predictions = torch.argmin(distances, dim=1)

        return predictions

# Usage example: 3-way 5-shot classification
# 3 classes, 5 samples per class
n_classes = 3
n_support_per_class = 5
n_query = 10

support_ids = torch.randint(0, vocab_size, (n_classes * n_support_per_class, 32))
support_mask = torch.ones_like(support_ids)
support_labels = torch.arange(n_classes).repeat_interleave(n_support_per_class)

query_ids = torch.randint(0, vocab_size, (n_query, 32))
query_mask = torch.ones_like(query_ids)

proto_net = PrototypicalNetwork(model)
predictions = proto_net(support_ids, support_mask, query_ids, query_mask, support_labels)
print(f"Predictions: {predictions}")
</code></pre>
<hr/>
<h2>3.5 Prompt Engineering</h2>
<h3>Prompts in Materials Science</h3>
<p><strong>Prompt</strong>: Providing additional information to the model to improve performance</p>
<p><strong>Example</strong>:</p>
<pre><code class="language-python"># Normal: 'Fe2O3'
# With prompt: '[OXIDE] Fe2O3 [BANDGAP]'
</code></pre>
<h3>Implementation</h3>
<pre><code class="language-python">class PromptedMatBERT(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PromptedMatBERT, self).__init__()
        self.matbert = matbert_model

        # Task-specific prompt embeddings (learnable)
        self.task_prompts = nn.Parameter(torch.randn(10, d_model))  # 10 types of tasks

    def forward(self, input_ids, attention_mask, task_id=0):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            task_id: Task ID (0-9)
        """
        batch_size = input_ids.size(0)

        # Normal embedding
        embeddings = self.matbert(input_ids, attention_mask)

        # Add task prompt at the beginning
        task_prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([task_prompt, embeddings], dim=1)

        return embeddings

# Usage example
prompted_model = PromptedMatBERT(model)

# Task 0: Band gap prediction
embeddings_task0 = prompted_model(input_ids, attention_mask, task_id=0)

# Task 1: Formation energy prediction
embeddings_task1 = prompted_model(input_ids, attention_mask, task_id=1)

print(f"Embeddings with prompt shape: {embeddings_task0.shape}")
</code></pre>
<hr/>
<h2>3.6 Domain Adaptation</h2>
<h3>Overview</h3>
<p><strong>Domain Adaptation</strong>: Adapting a model trained on a source domain to a target domain</p>
<p><strong>Example</strong>:
- Source: Inorganic materials data
- Target: Organic molecule data</p>
<h3>Adversarial Domain Adaptation</h3>
<pre><code class="language-python">class DomainClassifier(nn.Module):
    def __init__(self, d_model=512):
        super(DomainClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # Source or Target
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

class DomainAdaptiveMatBERT(nn.Module):
    def __init__(self, matbert_model):
        super(DomainAdaptiveMatBERT, self).__init__()
        self.matbert = matbert_model
        self.domain_classifier = DomainClassifier()
        self.task_predictor = nn.Linear(512, 1)  # Example: band gap prediction

    def forward(self, input_ids, attention_mask, alpha=1.0):
        """
        Args:
            alpha: Strength of domain adaptation
        """
        embeddings = self.matbert(input_ids, attention_mask)[:, 0, :]

        # Task prediction
        task_output = self.task_predictor(embeddings)

        # Domain prediction (using gradient reversal layer)
        # Simplified here, omitting details
        domain_output = self.domain_classifier(embeddings)

        return task_output, domain_output

# Training loop (simplified)
def train_domain_adaptive(model, source_loader, target_loader, epochs=20):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    task_criterion = nn.MSELoss()
    domain_criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for (source_ids, source_mask, source_labels), (target_ids, target_mask, _) in zip(source_loader, target_loader):
            # Source domain
            source_task, source_domain = model(source_ids, source_mask)
            source_domain_labels = torch.zeros(source_ids.size(0), dtype=torch.long)  # Source = 0

            # Target domain
            target_task, target_domain = model(target_ids, target_mask)
            target_domain_labels = torch.ones(target_ids.size(0), dtype=torch.long)  # Target = 1

            # Loss
            task_loss = task_criterion(source_task, source_labels)
            domain_loss = domain_criterion(source_domain, source_domain_labels) + \
                          domain_criterion(target_domain, target_domain_labels)

            total_loss = task_loss + 0.1 * domain_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        print(f"Epoch {epoch+1}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}")
</code></pre>
<hr/>
<h2>3.7 Summary</h2>
<h3>Key Points</h3>
<ol>
<li><strong>Pre-training</strong>: Acquire general knowledge from large-scale unlabeled data</li>
<li><strong>Fine-tuning</strong>: Task specialization with small amounts of data</li>
<li><strong>Few-shot Learning</strong>: Learn new tasks with just a few samples</li>
<li><strong>Prompt Engineering</strong>: Express task information through embeddings</li>
<li><strong>Domain Adaptation</strong>: Transfer knowledge between different domains</li>
</ol>
<h3>Preparation for Next Chapter</h3>
<p>In Chapter 4, you will learn about molecule generation and inverse materials design using diffusion models.</p>
<hr/>
<h2>üìù Exercises</h2>
<h3>Problem 1: Conceptual Understanding</h3>
<p>Explain the three fine-tuning strategies (Full, Feature Extraction, Partial) and describe in which situations each is appropriate.</p>
<details>
<summary>Sample Answer</summary>

1. **Full Fine-tuning**:
   - **Application**: When the target domain has relatively abundant data (thousands or more samples)
   - **Advantage**: Can achieve the highest accuracy
   - **Disadvantage**: Risk of overfitting, high computational cost

2. **Feature Extraction**:
   - **Application**: When data is very limited (tens to hundreds of samples)
   - **Advantage**: Prevents overfitting easily, fast
   - **Disadvantage**: Lower accuracy when domain differs significantly

3. **Partial Fine-tuning**:
   - **Application**: Moderate amount of data, similar domains
   - **Advantage**: Balanced performance and cost
   - **Disadvantage**: Difficult to choose which layers to update
</details>
<h3>Problem 2: Implementation</h3>
<p>Fill in the blanks in the following code to complete a function that loads a pre-trained model and fine-tunes it.</p>
<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # Load pre-trained model
    matbert = MatBERT(vocab_size=______, d_model=512)
    matbert.load_state_dict(torch.load(______))

    # Build fine-tuning model
    model = MatBERTForBandgap(______)

    # Optimization
    optimizer = torch.optim.Adam(______.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # Training loop
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(______, ______)
            loss = ______(predictions, targets)

            optimizer.zero_grad()
            ______.backward()
            optimizer.step()

    return model
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):
    # Load pre-trained model
    matbert = MatBERT(vocab_size=len(tokenizer.vocab), d_model=512)
    matbert.load_state_dict(torch.load(pretrained_path))

    # Build fine-tuning model
    model = MatBERTForBandgap(matbert)

    # Optimization
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # Training loop
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return model
</code></pre>
</details>
<h3>Problem 3: Application</h3>
<p>Identify three scenarios in materials science where Few-shot learning is particularly useful and explain the reasons for each.</p>
<details>
<summary>Sample Answer</summary>

1. **Rapid Evaluation of Novel Materials**:
   - **Scenario**: New classes of materials (e.g., novel perovskites)
   - **Reason**: Experimental data is still scarce, need property predictions from a few samples

2. **Efficient Experimental Planning**:
   - **Scenario**: High-cost experiments (single crystal growth, high-pressure synthesis)
   - **Reason**: Propose next experimental conditions based on limited experimental results

3. **Proprietary Materials Development in Industry**:
   - **Scenario**: Proprietary materials that cannot be shared with competitors
   - **Reason**: Must learn from internal data only, cannot use external data
</details>
<hr/>
<h2>üöÄ Implementation Exercises: Transformer for Materials</h2>
<h3>Exercise 1: MatBERT Implementation (BERT for Materials)</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from transformers import BertConfig, BertModel

class MaterialsBERT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        """
        Materials BERT implementation

        Args:
            vocab_size: Number of atomic species + special tokens
            d_model: Hidden layer dimension
            num_layers: Number of Transformer layers
            num_heads: Number of attention heads
        """
        super().__init__()

        # BERT configuration
        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=d_model,
            num_hidden_layers=num_layers,
            num_attention_heads=num_heads,
            intermediate_size=d_model * 4,
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1,
            max_position_embeddings=512
        )

        self.bert = BertModel(config)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        """
        Args:
            input_ids: (batch_size, seq_len) Atomic number sequence
            attention_mask: (batch_size, seq_len)
            token_type_ids: (batch_size, seq_len)
        Returns:
            outputs: BERT outputs with pooler_output
        """
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        return outputs

# Usage example
mat_bert = MaterialsBERT(vocab_size=120, d_model=768)

# Dummy data: Fe2O3 (iron oxide)
# [CLS] Fe Fe O O O [SEP]
input_ids = torch.tensor([[101, 26, 26, 8, 8, 8, 102]])  # 101=[CLS], 102=[SEP]
attention_mask = torch.ones_like(input_ids)

outputs = mat_bert(input_ids, attention_mask)
print(f"Last hidden state shape: {outputs.last_hidden_state.shape}")  # (1, 7, 768)
print(f"Pooler output shape: {outputs.pooler_output.shape}")  # (1, 768)
</code></pre>
<h3>Exercise 2: MatGPT Implementation (GPT for Materials Generation)</h3>
<pre><code class="language-python">from transformers import GPT2Config, GPT2LMHeadModel

class MaterialsGPT(nn.Module):
    def __init__(self, vocab_size=120, d_model=768, num_layers=12, num_heads=12):
        """
        Materials GPT for generative tasks

        Args:
            vocab_size: Number of atomic species + special tokens
            d_model: Hidden layer dimension
            num_layers: Number of Transformer layers
            num_heads: Number of attention heads
        """
        super().__init__()

        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=d_model,
            n_layer=num_layers,
            n_head=num_heads,
            resid_pdrop=0.1,
            embd_pdrop=0.1,
            attn_pdrop=0.1
        )

        self.gpt = GPT2LMHeadModel(config)

    def forward(self, input_ids, labels=None):
        """
        Args:
            input_ids: (batch_size, seq_len)
            labels: (batch_size, seq_len) for training
        """
        outputs = self.gpt(input_ids=input_ids, labels=labels)
        return outputs

    def generate_composition(self, start_tokens, max_length=50, temperature=1.0):
        """
        Generate composition formula

        Args:
            start_tokens: (1, start_len) Starting tokens
            max_length: Maximum generation length
            temperature: Sampling temperature
        """
        self.eval()
        with torch.no_grad():
            for _ in range(max_length - start_tokens.size(1)):
                outputs = self.gpt(start_tokens)
                logits = outputs.logits[:, -1, :] / temperature

                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                start_tokens = torch.cat([start_tokens, next_token], dim=1)

                # Stop at [SEP] token
                if next_token.item() == 102:
                    break

        return start_tokens

# Usage example
mat_gpt = MaterialsGPT(vocab_size=120, d_model=768)

# Generate: [CLS] Fe ... (generate an oxide)
start = torch.tensor([[101, 26]])  # [CLS] Fe
generated = mat_gpt.generate_composition(start, max_length=20)
print(f"Generated sequence: {generated}")
</code></pre>
<h3>Exercise 3: MatT5 Implementation (T5 for Materials Seq2Seq)</h3>
<pre><code class="language-python">from transformers import T5Config, T5ForConditionalGeneration

class MaterialsT5(nn.Module):
    def __init__(self, vocab_size=120, d_model=512, num_layers=6):
        """
        Materials T5 for sequence-to-sequence tasks
        (e.g., composition ‚Üí properties description)

        Args:
            vocab_size: Vocabulary size
            d_model: Model dimension
            num_layers: Number of encoder/decoder layers
        """
        super().__init__()

        config = T5Config(
            vocab_size=vocab_size,
            d_model=d_model,
            d_kv=64,
            d_ff=d_model * 4,
            num_layers=num_layers,
            num_decoder_layers=num_layers,
            num_heads=8,
            dropout_rate=0.1
        )

        self.t5 = T5ForConditionalGeneration(config)

    def forward(self, input_ids, attention_mask=None, labels=None):
        """
        Args:
            input_ids: (batch_size, src_len) Input sequence
            labels: (batch_size, tgt_len) Target sequence
        """
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs

    def predict_properties(self, composition_ids, max_length=50):
        """
        Generate property description from composition formula

        Args:
            composition_ids: (batch_size, seq_len) Composition formula
            max_length: Maximum generation length
        """
        self.eval()
        with torch.no_grad():
            outputs = self.t5.generate(
                composition_ids,
                max_length=max_length,
                num_beams=4,
                early_stopping=True
            )
        return outputs

# Usage example
mat_t5 = MaterialsT5(vocab_size=120, d_model=512)

# Input: Fe2O3 ‚Üí Output: "semiconductor bandgap 2.0 eV"
input_ids = torch.tensor([[26, 26, 8, 8, 8]])  # Fe Fe O O O
outputs = mat_t5.predict_properties(input_ids, max_length=20)
print(f"Predicted properties: {outputs}")
</code></pre>
<hr/>
<h2>üß™ SMILES/SELFIES Tokenization Implementation</h2>
<h3>SMILES Tokenizer</h3>
<pre><code class="language-python">import re
from typing import List, Dict

class SMILESTokenizer:
    """
    Complete SMILES string tokenization

    Supports:
    - Aromaticity (c, n, o, s)
    - Stereochemistry (@, @@, /, \\)
    - Branching ((, ))
    - Bonds (-, =, #, :)
    - Rings (numbers)
    """

    def __init__(self):
        # Regular expression pattern (in order of priority)
        self.pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'

        # Special tokens
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3,
            '[UNK]': 4
        }

        # Build vocabulary
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        """Build vocabulary"""
        vocab = list(self.special_tokens.keys())

        # Element symbols
        elements = ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I',
                   'c', 'n', 'o', 's', 'p']  # Aromatic

        # Symbols
        symbols = ['(', ')', '[', ']', '=', '#', '-', '+', '\\', '/',
                  ':', '.', '@', '@@']

        # Numbers
        numbers = [str(i) for i in range(10)]

        vocab.extend(elements + symbols + numbers)

        return vocab

    def tokenize(self, smiles: str) -&gt; List[str]:
        """
        Tokenize SMILES string

        Args:
            smiles: SMILES string

        Returns:
            tokens: List of tokens

        Examples:
            &gt;&gt;&gt; tokenizer = SMILESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize("CC(C)Cc1ccc(cc1)C(C)C(=O)O")
            ['C', 'C', '(', 'C', ')', 'C', 'c', '1', 'c', 'c', 'c', '(', ...]
        """
        tokens = re.findall(self.pattern, smiles)
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, smiles: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        """
        Convert SMILES string to IDs

        Args:
            smiles: SMILES string
            max_length: Maximum length

        Returns:
            encoding: input_ids, attention_mask
        """
        tokens = self.tokenize(smiles)

        # Convert tokens to IDs
        ids = [self.token_to_id.get(token, self.token_to_id['[UNK]'])
               for token in tokens]

        # Padding
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # Truncation
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

    def decode(self, ids: List[int]) -&gt; str:
        """Decode IDs back to SMILES string"""
        tokens = [self.id_to_token.get(id, '[UNK]') for id in ids]
        # Remove special tokens
        tokens = [t for t in tokens if t not in self.special_tokens]
        return ''.join(tokens)

# Usage example
tokenizer = SMILESTokenizer()

# Ibuprofen
smiles = "CC(C)Cc1ccc(cc1)C(C)C(=O)O"
tokens = tokenizer.tokenize(smiles)
print(f"Tokens: {tokens[:10]}...")

encoding = tokenizer.encode(smiles)
print(f"Input IDs shape: {encoding['input_ids'].shape}")
print(f"First 10 IDs: {encoding['input_ids'][0][:10]}")

# Decode
decoded = tokenizer.decode(encoding['input_ids'][0].tolist())
print(f"Decoded: {decoded}")
</code></pre>
<h3>SELFIES Tokenizer</h3>
<pre><code class="language-python">try:
    import selfies as sf
except ImportError:
    print("Install selfies: pip install selfies")

class SELFIESTokenizer:
    """
    SELFIES (SELF-referencIng Embedded Strings) Tokenizer

    Advantages:
    - Generates 100% valid molecules
    - Grammatically correct
    - More robust than SMILES
    """

    def __init__(self):
        self.special_tokens = {
            '[PAD]': 0,
            '[CLS]': 1,
            '[SEP]': 2,
            '[MASK]': 3
        }

        # Common SELFIES tokens
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}

    def _build_vocab(self) -&gt; List[str]:
        """
        Build SELFIES vocabulary

        Common tokens:
        [C], [N], [O], [=C], [=N], [Ring1], [Branch1], etc.
        """
        vocab = list(self.special_tokens.keys())

        # Basic tokens
        common_tokens = [
            '[C]', '[N]', '[O]', '[S]', '[P]', '[F]', '[Cl]', '[Br]', '[I]',
            '[=C]', '[=N]', '[=O]', '[#C]', '[#N]',
            '[Ring1]', '[Ring2]', '[Branch1]', '[Branch2]',
            '[O-1]', '[N+1]', '[nop]'
        ]

        vocab.extend(common_tokens)
        return vocab

    def smiles_to_selfies(self, smiles: str) -&gt; str:
        """Convert SMILES to SELFIES"""
        try:
            selfies = sf.encoder(smiles)
            return selfies
        except Exception as e:
            print(f"Encoding error: {e}")
            return ""

    def selfies_to_smiles(self, selfies: str) -&gt; str:
        """Convert SELFIES to SMILES"""
        try:
            smiles = sf.decoder(selfies)
            return smiles
        except Exception as e:
            print(f"Decoding error: {e}")
            return ""

    def tokenize(self, selfies: str) -&gt; List[str]:
        """
        Tokenize SELFIES string

        Args:
            selfies: SELFIES string

        Returns:
            tokens: List of tokens

        Examples:
            &gt;&gt;&gt; tokenizer = SELFIESTokenizer()
            &gt;&gt;&gt; tokenizer.tokenize("[C][C][Branch1][C][C][C]")
            ['[CLS]', '[C]', '[C]', '[Branch1]', '[C]', '[C]', '[C]', '[SEP]']
        """
        tokens = list(sf.split_selfies(selfies))
        return ['[CLS]'] + tokens + ['[SEP]']

    def encode(self, selfies: str, max_length: int = 128) -&gt; Dict[str, torch.Tensor]:
        """Convert SELFIES string to IDs"""
        tokens = self.tokenize(selfies)

        # Convert tokens to IDs (dynamically add unknown tokens)
        ids = []
        for token in tokens:
            if token not in self.token_to_id:
                new_id = len(self.vocab)
                self.vocab.append(token)
                self.token_to_id[token] = new_id
                self.id_to_token[new_id] = token
            ids.append(self.token_to_id[token])

        # Padding
        attention_mask = [1] * len(ids)
        while len(ids) &lt; max_length:
            ids.append(self.token_to_id['[PAD]'])
            attention_mask.append(0)

        # Truncation
        ids = ids[:max_length]
        attention_mask = attention_mask[:max_length]

        return {
            'input_ids': torch.tensor([ids]),
            'attention_mask': torch.tensor([attention_mask])
        }

# Usage example
if 'sf' in dir():
    tokenizer_selfies = SELFIESTokenizer()

    # Convert from SMILES to SELFIES
    smiles = "CC(C)Cc1ccc(cc1)C(C)C(=O)O"
    selfies = tokenizer_selfies.smiles_to_selfies(smiles)
    print(f"SELFIES: {selfies}")

    # Tokenize
    tokens = tokenizer_selfies.tokenize(selfies)
    print(f"Tokens: {tokens[:10]}...")

    # Encode
    encoding = tokenizer_selfies.encode(selfies)
    print(f"Encoded shape: {encoding['input_ids'].shape}")
</code></pre>
<hr/>
<h2>‚ö†Ô∏è Practical Pitfalls and Solutions</h2>
<h3>1. Overfitting in Fine-tuning</h3>
<p><strong>Problem</strong>: Validation loss diverges when training with small amounts of data</p>
<pre><code class="language-python"># ‚ùå PROBLEM: Updating all parameters with large learning rate
def wrong_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Too large!

    for epoch in range(100):  # Too many epochs
        for batch in train_loader:
            loss = compute_loss(batch)
            loss.backward()
            optimizer.step()

# ‚úÖ SOLUTION: Layer-wise learning rate decay + Early stopping
def correct_finetuning():
    model = MatBERTForBandgap(pretrained_matbert)

    # Layer-wise learning rate
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': 0.01,
            'lr': 2e-5  # Small for pre-trained part
        },
        {
            'params': [p for n, p in model.matbert.named_parameters()
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': 2e-5
        },
        {
            'params': model.bandgap_predictor.parameters(),
            'lr': 1e-4  # Large for prediction head
        }
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    # Early stopping
    best_val_loss = float('inf')
    patience = 5
    patience_counter = 0

    for epoch in range(100):
        train_loss = train_epoch(model, train_loader, optimizer)
        val_loss = validate(model, val_loader)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pt')
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter &gt;= patience:
            print(f"Early stopping at epoch {epoch}")
            break

    # Restore best model
    model.load_state_dict(torch.load('best_model.pt'))
    return model
</code></pre>
<h3>2. Domain Shift Problem</h3>
<p><strong>Problem</strong>: Applying a model pre-trained on inorganic materials to organic molecules</p>
<pre><code class="language-python"># ‚ùå PROBLEM: Direct application despite different domains
def wrong_domain_adaptation():
    # Pre-trained on inorganic materials
    matbert = pretrained_on_inorganic_materials()

    # Direct fine-tuning on organic molecule data
    # ‚Üí Low performance!
    finetune_on_organic_molecules(matbert)

# ‚úÖ SOLUTION: Intermediate task transfer
def correct_domain_adaptation():
    # Step 1: Pre-train on inorganic materials
    matbert = pretrained_on_inorganic_materials()

    # Step 2: Continual learning on intermediate task (between inorganic and organic)
    # Example: Metal-organic frameworks (MOF) data
    matbert = continual_pretrain_on_mof(matbert)

    # Step 3: Fine-tune on organic molecule data
    model = finetune_on_organic_molecules(matbert)

    return model

# Alternative: Domain-adversarial training
class DomainAdversarialTraining:
    def train(self, source_data, target_data):
        for source_batch, target_batch in zip(source_data, target_data):
            # Source domain: Task loss
            source_output = model(source_batch)
            task_loss = compute_task_loss(source_output, source_batch.labels)

            # Both domains: Domain classification loss (reverse gradient)
            source_domain_pred = domain_classifier(source_output, reverse_gradient=True)
            target_domain_pred = domain_classifier(target_output, reverse_gradient=True)

            domain_loss = compute_domain_loss(source_domain_pred, target_domain_pred)

            total_loss = task_loss + 0.1 * domain_loss
            total_loss.backward()
            optimizer.step()
</code></pre>
<h3>3. Masked Language Modeling Masking Strategy Mistakes</h3>
<p><strong>Problem</strong>: Biased masking patterns</p>
<pre><code class="language-python"># ‚ùå PROBLEM: Random masking (chemically meaningless)
def wrong_masking(composition_ids):
    mask_prob = 0.15
    mask = torch.rand(composition_ids.shape) &lt; mask_prob
    composition_ids[mask] = MASK_TOKEN_ID
    return composition_ids

# ‚úÖ SOLUTION: Chemically aware masking
def chemically_aware_masking(composition_ids, element_groups):
    """
    Masking considering element groups

    Args:
        composition_ids: (batch, seq_len)
        element_groups: {group_id: [element_ids]}
            Example: {0: [26, 27, 28], 1: [8, 16]}  # Transition metals, Chalcogens
    """
    mask_prob = 0.15
    masked_ids = composition_ids.clone()

    for i in range(composition_ids.size(0)):
        # Mask by chemical group
        for group_id, element_ids in element_groups.items():
            group_positions = torch.isin(composition_ids[i], torch.tensor(element_ids))
            if group_positions.sum() &gt; 0:
                # Mask some within the group
                mask_within_group = torch.rand(group_positions.sum()) &lt; mask_prob
                group_indices = torch.where(group_positions)[0]
                masked_positions = group_indices[mask_within_group]
                masked_ids[i, masked_positions] = MASK_TOKEN_ID

    return masked_ids

# Usage example
element_groups = {
    0: [26, 27, 28, 29],  # Fe, Co, Ni, Cu (transition metals)
    1: [8, 16, 34],       # O, S, Se (chalcogens)
    2: [3, 11, 19]        # Li, Na, K (alkali metals)
}

masked_composition = chemically_aware_masking(composition_ids, element_groups)
</code></pre>
<h3>4. Few-shot Learning Support Set Selection Mistakes</h3>
<p><strong>Problem</strong>: Biased support set</p>
<pre><code class="language-python"># ‚ùå PROBLEM: Random support set selection
def wrong_support_selection(dataset, k=5):
    indices = torch.randperm(len(dataset))[:k]
    return dataset[indices]

# ‚úÖ SOLUTION: Diverse support set selection considering diversity
def diverse_support_selection(dataset, embeddings, k=5):
    """
    Select diverse samples using K-means

    Args:
        dataset: Dataset
        embeddings: (N, d) Sample embeddings
        k: Support set size
    """
    from sklearn.cluster import KMeans

    # Clustering with K-means
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(embeddings.numpy())

    # Select sample closest to each cluster center
    support_indices = []
    for i in range(k):
        cluster_indices = torch.where(torch.tensor(labels) == i)[0]
        cluster_embeddings = embeddings[cluster_indices]
        cluster_center = kmeans.cluster_centers_[i]

        # Sample closest to center
        distances = torch.norm(cluster_embeddings - torch.tensor(cluster_center), dim=1)
        closest_idx = cluster_indices[torch.argmin(distances)]
        support_indices.append(closest_idx.item())

    return dataset[support_indices]

# Usage example
# Pre-compute dataset embeddings
embeddings = compute_embeddings(dataset, matbert)
support_set = diverse_support_selection(dataset, embeddings, k=10)
</code></pre>
<h3>5. Insufficient Prompt Engineering Optimization</h3>
<p><strong>Problem</strong>: Low performance with fixed prompts</p>
<pre><code class="language-python"># ‚ùå PROBLEM: Manually designed fixed prompt
class FixedPromptModel(nn.Module):
    def __init__(self, matbert):
        super().__init__()
        self.matbert = matbert
        # Fixed prompt
        self.prompt = nn.Parameter(torch.randn(1, 10, 768), requires_grad=False)

# ‚úÖ SOLUTION: Learnable prompt (Prefix-Tuning)
class LearnablePromptModel(nn.Module):
    def __init__(self, matbert, prompt_length=10, num_tasks=5):
        super().__init__()
        self.matbert = matbert
        self.prompt_length = prompt_length

        # Task-specific learnable prompts
        self.task_prompts = nn.Parameter(torch.randn(num_tasks, prompt_length, 768))

        # Freeze MatBERT parameters
        for param in self.matbert.parameters():
            param.requires_grad = False

    def forward(self, input_ids, task_id=0):
        batch_size = input_ids.size(0)

        # Input embedding
        input_embeddings = self.matbert.embeddings(input_ids)

        # Add task-specific prompt
        prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([prompt, input_embeddings], dim=1)

        # Pass through Transformer
        outputs = self.matbert.encoder(embeddings)

        return outputs

# Training
model = LearnablePromptModel(pretrained_matbert, prompt_length=10, num_tasks=5)

# Optimize only prompts (significantly reduce parameter count)
optimizer = torch.optim.Adam([model.task_prompts], lr=1e-3)
</code></pre>
<hr/>
<h2>‚úÖ Chapter 3 Completion Checklist</h2>
<h3>Conceptual Understanding (10 items)</h3>
<ul>
<li>[ ] Can explain the importance and advantages of pre-training</li>
<li>[ ] Understand the principle of Masked Language Modeling</li>
<li>[ ] Can explain the differences between Full/Feature Extraction/Partial Fine-tuning</li>
<li>[ ] Understand the principle of Few-shot learning (Prototypical Networks)</li>
<li>[ ] Understand the concept of prompt engineering</li>
<li>[ ] Can explain the necessity of domain adaptation</li>
<li>[ ] Understand the relationship between pre-training tasks and downstream tasks</li>
<li>[ ] Can quantitatively evaluate the effect of Transfer Learning</li>
<li>[ ] Understand the features of materials-specific models like MatBERT and MolBERT</li>
<li>[ ] Can explain the differences between BERT/GPT/T5 and their application scenarios</li>
</ul>
<h3>Implementation Skills (15 items)</h3>
<ul>
<li>[ ] Can implement <code>MatBERT</code></li>
<li>[ ] Can implement <code>MatGPT</code></li>
<li>[ ] Can implement <code>MatT5</code></li>
<li>[ ] Can implement SMILES tokenizer</li>
<li>[ ] Can implement SELFIES tokenizer</li>
<li>[ ] Can implement Masked Atom Prediction</li>
<li>[ ] Can implement fine-tuning strategies (Full/Feature/Partial)</li>
<li>[ ] Can implement Prototypical Networks</li>
<li>[ ] Can implement learnable prompts</li>
<li>[ ] Can implement domain-adversarial training</li>
<li>[ ] Can implement early stopping</li>
<li>[ ] Can configure layer-wise learning rates</li>
<li>[ ] Can save and load pre-trained models</li>
<li>[ ] Can utilize the Hugging Face Transformers library</li>
<li>[ ] Can integrate custom tokenizers into Transformers</li>
</ul>
<h3>Debugging Skills (5 items)</h3>
<ul>
<li>[ ] Can detect overfitting and address it with regularization</li>
<li>[ ] Can detect domain shift and apply adaptation methods</li>
<li>[ ] Can evaluate the validity of masking strategies</li>
<li>[ ] Can evaluate Few-shot support set quality</li>
<li>[ ] Can visualize and analyze prompt effects</li>
</ul>
<h3>Application Ability (5 items)</h3>
<ul>
<li>[ ] Can apply pre-trained models to new materials property prediction tasks</li>
<li>[ ] Can combine multiple pre-training tasks to improve performance</li>
<li>[ ] Can design domain adaptation strategies</li>
<li>[ ] Can combine Few-shot learning with data augmentation</li>
<li>[ ] Can optimize performance through prompt engineering</li>
</ul>
<h3>Data Processing (5 items)</h3>
<ul>
<li>[ ] Can preprocess SMILES data</li>
<li>[ ] Can convert to SELFIES</li>
<li>[ ] Can implement data augmentation (SMILES enumeration)</li>
<li>[ ] Can split data by domain</li>
<li>[ ] Can generate episodes for Few-shot</li>
</ul>
<h3>Evaluation Skills (5 items)</h3>
<ul>
<li>[ ] Can quantitatively evaluate the effect of pre-training (vs from scratch)</li>
<li>[ ] Can comparatively evaluate fine-tuning strategies</li>
<li>[ ] Can properly evaluate Few-shot performance (N-way K-shot)</li>
<li>[ ] Can measure the effect of domain adaptation</li>
<li>[ ] Can analyze the impact of prompts</li>
</ul>
<h3>Theoretical Background (5 items)</h3>
<ul>
<li>[ ] Have read MatBERT/MolBERT papers</li>
<li>[ ] Have read BERT paper (Devlin et al., 2019)</li>
<li>[ ] Have read GPT paper</li>
<li>[ ] Have read at least one Few-shot learning paper</li>
<li>[ ] Understand Transfer Learning theory</li>
</ul>
<h3>Completion Criteria</h3>
<ul>
<li><strong>Minimum Standard</strong>: Achieve 40+ items (80%)</li>
<li><strong>Recommended Standard</strong>: Achieve 45+ items (90%)</li>
<li><strong>Excellent Standard</strong>: Achieve all 50 items (100%)</li>
</ul>
<hr/>
<p><strong>Next Chapter</strong>: <strong><a href="chapter-4.html">Chapter 4: Generative Models and Inverse Design</a></strong></p>
<hr/>
<p><strong>Author</strong>: Yusuke Hashimoto (Tohoku University)
<strong>Last Updated</strong>: October 19, 2025</p><div class="navigation">
<a class="nav-button" href="chapter-2.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Contents</a>
<a class="nav-button" href="chapter-4.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranties of any kind, whether express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content shall be governed by the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
