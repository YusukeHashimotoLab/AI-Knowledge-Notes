<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }



        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Reading Time: 20-25 minutes</span>
                <span class="meta-item">üìä Difficulty: Beginner</span>
                <span class="meta-item">üíª Code Examples: 0</span>
                <span class="meta-item">üìù Exercises: 0</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 4: Generative Models and Inverse Design</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Understand the fundamental concepts and key considerations of inverse design using diffusion models and VAEs. Grasp evaluation metrics and operational challenges.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> There is a trade-off between diversity and feasibility of generated results. Embedding physical and chemical constraints is the key.</p>




<p><strong>Learning Time</strong>: 20-25 minutes | <strong>Difficulty</strong>: Advanced</p>
<h2>üìã What You'll Learn in This Chapter</h2>
<ul>
<li>Principles of Diffusion Models</li>
<li>Conditional Generation</li>
<li>Molecular Generation and SMILES Generation</li>
<li>Materials Inverse Design</li>
<li>Industrial Applications and Career Paths</li>
</ul>
<hr />
<h2>4.1 What Are Generative Models?</h2>
<h3>Importance of Generative Models in Materials Science</h3>
<p><strong>Traditional Approach (Forward Problem)</strong>:</p>
<pre><code>Material Structure ‚Üí Property Prediction
</code></pre>
<p><strong>Inverse Design (Inverse Problem)</strong>:</p>
<pre><code>Desired Properties ‚Üí Material Structure Generation
</code></pre>
<p><strong>Advantages of Generative Models</strong>:
- ‚úÖ Automatic generation of candidates from vast search spaces
- ‚úÖ Multi-objective optimization (simultaneously satisfying multiple properties)
- ‚úÖ Generation considering synthesizability
- ‚úÖ Discovery of novel structures beyond human intuition</p>
<div class="mermaid">
flowchart LR
    A[Target Properties] --> B[Generative Model]
    C[Constraints] --> B
    B --> D[Candidate Materials]
    D --> E[Property Prediction]
    E --> F{Goal Achieved?}
    F -->|No| B
    F -->|Yes| G[Experimental Validation]

    style B fill:#e1f5ff
    style G fill:#ffe1e1
</div>

<hr />
<h2>4.2 Principles of Diffusion Models</h2>
<h3>What Are Diffusion Models?</h3>
<p><strong>Basic Idea</strong>: Reverse the noise addition process to generate data from noise</p>
<p><strong>Forward Process (Noise Addition)</strong>:
$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$</p>
<p><strong>Reverse Process (Noise Removal)</strong>:
$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$</p>
<h3>Visual Understanding</h3>
<div class="mermaid">
flowchart LR
    X0[Original Data x‚ÇÄ] -->|Add Noise| X1[x‚ÇÅ]
    X1 -->|Add Noise| X2[x‚ÇÇ]
    X2 -->|...| XT[Pure Noise x‚Çú]

    XT -->|Remove Noise| X2R[x‚ÇÇ]
    X2R -->|Remove Noise| X1R[x‚ÇÅ]
    X1R -->|Remove Noise| X0R[Generated Data x‚ÇÄ]

    style X0 fill:#e1f5ff
    style XT fill:#ffe1e1
    style X0R fill:#e1ffe1
</div>

<h3>Simple Implementation</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class SimpleDiffusionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, num_timesteps=1000):
        super(SimpleDiffusionModel, self).__init__()
        self.num_timesteps = num_timesteps

        # Noise schedule
        self.betas = torch.linspace(1e-4, 0.02, num_timesteps)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)

        # Noise prediction network
        self.noise_predictor = nn.Sequential(
            nn.Linear(input_dim + 1, hidden_dim),  # +1 for timestep
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )

    def forward_process(self, x0, t):
        &quot;&quot;&quot;
        Forward process: Add noise

        Args:
            x0: Original data (batch_size, input_dim)
            t: Timestep (batch_size,)
        Returns:
            xt: Noised data
            noise: Added noise
        &quot;&quot;&quot;
        batch_size = x0.size(0)

        # Noise level for each timestep
        alpha_t = self.alphas_cumprod[t].view(-1, 1)
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)

        # Sample noise
        noise = torch.randn_like(x0)

        # Add noise
        xt = sqrt_alpha_t * x0 + sqrt_one_minus_alpha_t * noise

        return xt, noise

    def predict_noise(self, xt, t):
        &quot;&quot;&quot;
        Predict noise

        Args:
            xt: Noised data
            t: Timestep
        Returns:
            predicted_noise: Predicted noise
        &quot;&quot;&quot;
        # Embed timestep
        t_embed = t.float().unsqueeze(1) / self.num_timesteps

        # Predict noise
        x_with_t = torch.cat([xt, t_embed], dim=1)
        predicted_noise = self.noise_predictor(x_with_t)

        return predicted_noise

    def reverse_process(self, xt, t):
        &quot;&quot;&quot;
        Reverse process: Remove noise (1 step)

        Args:
            xt: Current data
            t: Timestep
        Returns:
            x_prev: Previous step data
        &quot;&quot;&quot;
        # Predict noise
        predicted_noise = self.predict_noise(xt, t)

        # Parameters
        alpha_t = self.alphas[t].view(-1, 1)
        alpha_t_cumprod = self.alphas_cumprod[t].view(-1, 1)
        beta_t = self.betas[t].view(-1, 1)

        # Calculate previous step
        x_prev = (1 / torch.sqrt(alpha_t)) * (
            xt - (beta_t / torch.sqrt(1 - alpha_t_cumprod)) * predicted_noise
        )

        # Add noise (if t > 0)
        if t[0] &gt; 0:
            noise = torch.randn_like(xt)
            x_prev = x_prev + torch.sqrt(beta_t) * noise

        return x_prev

    def generate(self, batch_size, input_dim):
        &quot;&quot;&quot;
        Generate data

        Args:
            batch_size: Batch size
            input_dim: Data dimension
        Returns:
            x0: Generated data
        &quot;&quot;&quot;
        # Start from pure noise
        xt = torch.randn(batch_size, input_dim)

        # Execute reverse process
        for t in reversed(range(self.num_timesteps)):
            t_batch = torch.full((batch_size,), t, dtype=torch.long)
            xt = self.reverse_process(xt, t_batch)

        return xt

# Usage example: Generate molecular descriptors
input_dim = 128  # Descriptor dimension
diffusion_model = SimpleDiffusionModel(input_dim, hidden_dim=256, num_timesteps=100)

# Training data (dummy)
x0 = torch.randn(64, input_dim)  # Descriptors for 64 molecules

# Forward process (add noise)
t = torch.randint(0, 100, (64,))
xt, noise = diffusion_model.forward_process(x0, t)

# Noise prediction
predicted_noise = diffusion_model.predict_noise(xt, t)

# Loss
loss = F.mse_loss(predicted_noise, noise)
print(f&quot;Training loss: {loss.item():.4f}&quot;)

# Generation
generated_data = diffusion_model.generate(batch_size=10, input_dim=input_dim)
print(f&quot;Generated data shape: {generated_data.shape}&quot;)
</code></pre>
<hr />
<h2>4.3 Conditional Generation</h2>
<h3>Overview</h3>
<p><strong>Conditional Generation</strong>: Generate by providing target properties as conditions</p>
<p><strong>Example</strong>:</p>
<pre><code class="language-python"># Condition: Bandgap = 2.0 eV, Formation energy &lt; 0
# Generation: Material structure satisfying the conditions
</code></pre>
<h3>Implementation: Conditional Diffusion</h3>
<pre><code class="language-python">class ConditionalDiffusionModel(nn.Module):
    def __init__(self, input_dim, condition_dim, hidden_dim=256, num_timesteps=1000):
        super(ConditionalDiffusionModel, self).__init__()
        self.num_timesteps = num_timesteps

        # Noise schedule
        self.betas = torch.linspace(1e-4, 0.02, num_timesteps)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)

        # Condition encoder
        self.condition_encoder = nn.Sequential(
            nn.Linear(condition_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Noise prediction network (conditional)
        self.noise_predictor = nn.Sequential(
            nn.Linear(input_dim + hidden_dim + 1, hidden_dim),  # +1 for timestep
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )

    def predict_noise(self, xt, t, condition):
        &quot;&quot;&quot;
        Conditional noise prediction

        Args:
            xt: Noised data (batch_size, input_dim)
            t: Timestep (batch_size,)
            condition: Condition (target properties) (batch_size, condition_dim)
        Returns:
            predicted_noise: Predicted noise
        &quot;&quot;&quot;
        # Embed condition
        condition_embed = self.condition_encoder(condition)

        # Embed timestep
        t_embed = t.float().unsqueeze(1) / self.num_timesteps

        # Concatenate
        x_with_condition = torch.cat([xt, condition_embed, t_embed], dim=1)

        # Predict noise
        predicted_noise = self.noise_predictor(x_with_condition)

        return predicted_noise

    def generate_conditional(self, condition, input_dim):
        &quot;&quot;&quot;
        Conditional data generation

        Args:
            condition: Condition (batch_size, condition_dim)
            input_dim: Data dimension
        Returns:
            x0: Generated data
        &quot;&quot;&quot;
        batch_size = condition.size(0)

        # Start from pure noise
        xt = torch.randn(batch_size, input_dim)

        # Reverse process
        for t in reversed(range(self.num_timesteps)):
            t_batch = torch.full((batch_size,), t, dtype=torch.long)

            # Predict noise
            predicted_noise = self.predict_noise(xt, t_batch, condition)

            # Parameters
            alpha_t = self.alphas[t]
            alpha_t_cumprod = self.alphas_cumprod[t]
            beta_t = self.betas[t]

            # Calculate previous step
            xt = (1 / torch.sqrt(alpha_t)) * (
                xt - (beta_t / torch.sqrt(1 - alpha_t_cumprod)) * predicted_noise
            )

            # Add noise (if t > 0)
            if t &gt; 0:
                noise = torch.randn_like(xt)
                xt = xt + torch.sqrt(beta_t) * noise

        return xt

# Usage example
input_dim = 128
condition_dim = 3  # Bandgap, formation energy, magnetic moment

conditional_model = ConditionalDiffusionModel(input_dim, condition_dim, hidden_dim=256, num_timesteps=100)

# Target properties
target_properties = torch.tensor([
    [2.0, -0.5, 0.0],  # Bandgap 2.0eV, formation energy -0.5eV, non-magnetic
    [3.5, -1.0, 2.0],  # Bandgap 3.5eV, formation energy -1.0eV, magnetic
])

# Conditional generation
generated_materials = conditional_model.generate_conditional(target_properties, input_dim)
print(f&quot;Generated materials shape: {generated_materials.shape}&quot;)  # (2, 128)
</code></pre>
<hr />
<h2>4.4 Molecular Generation: SMILES Generation</h2>
<h3>Overview</h3>
<p><strong>SMILES (Simplified Molecular Input Line Entry System)</strong>: Molecular representation as a string</p>
<p><strong>Examples</strong>:
- Ethanol: <code>CCO</code>
- Benzene: <code>c1ccccc1</code>
- Aspirin: <code>CC(=O)Oc1ccccc1C(=O)O</code></p>
<h3>Transformer-based SMILES Generation</h3>
<pre><code class="language-python">from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer

class SMILESGenerator(nn.Module):
    def __init__(self, vocab_size=1000, d_model=512, num_layers=6):
        super(SMILESGenerator, self).__init__()

        # GPT-2 config
        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=d_model,
            n_layer=num_layers,
            n_head=8
        )

        self.gpt = GPT2LMHeadModel(config)

    def forward(self, input_ids, labels=None):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            labels: (batch_size, seq_len) Target for next token prediction
        &quot;&quot;&quot;
        outputs = self.gpt(input_ids, labels=labels)
        return outputs

    def generate_smiles(self, start_token_id, max_length=100, temperature=1.0):
        &quot;&quot;&quot;
        Generate SMILES string

        Args:
            start_token_id: Start token ID
            max_length: Maximum length
            temperature: Sampling temperature (higher = more random)
        Returns:
            generated_ids: Generated token IDs
        &quot;&quot;&quot;
        generated = [start_token_id]

        for _ in range(max_length):
            input_ids = torch.tensor([generated])
            outputs = self.gpt(input_ids)
            logits = outputs.logits[:, -1, :] / temperature

            # Sampling
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()

            generated.append(next_token)

            # Stop if end token
            if next_token == 2:  # [EOS]
                break

        return generated

# Conditional SMILES generation
class ConditionalSMILESGenerator(nn.Module):
    def __init__(self, vocab_size=1000, condition_dim=10, d_model=512):
        super(ConditionalSMILESGenerator, self).__init__()

        # Condition encoder
        self.condition_encoder = nn.Linear(condition_dim, d_model)

        # GPT-2 config
        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=512,
            n_embd=d_model,
            n_layer=6,
            n_head=8
        )
        self.gpt = GPT2LMHeadModel(config)

    def forward(self, input_ids, condition):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch_size, seq_len)
            condition: (batch_size, condition_dim) Target properties
        &quot;&quot;&quot;
        batch_size, seq_len = input_ids.shape

        # Embed condition
        condition_embed = self.condition_encoder(condition).unsqueeze(1)  # (batch, 1, d_model)

        # Token embeddings
        token_embeddings = self.gpt.transformer.wte(input_ids)

        # Add condition at the beginning
        embeddings = torch.cat([condition_embed, token_embeddings], dim=1)

        # GPT-2 forward (directly from embeddings)
        outputs = self.gpt(inputs_embeds=embeddings)

        return outputs

# Usage example: Generate molecules with high solubility
condition_dim = 5  # logP, solubility, molecular weight, HB donors, HB acceptors
target_properties = torch.tensor([[1.5, 10.0, 250.0, 2.0, 3.0]])  # High solubility

conditional_smiles_gen = ConditionalSMILESGenerator(vocab_size=1000, condition_dim=condition_dim)
</code></pre>
<hr />
<h2>4.5 Materials Inverse Design Workflow</h2>
<h3>Complete Workflow</h3>
<div class="mermaid">
flowchart TB
    A[Define Target Properties] --> B[Conditional Generative Model]
    B --> C[Generate Candidate Materials]
    C --> D[Property Prediction Model]
    D --> E{Goal Achieved?}
    E -->|No| F[Exclude Candidate]
    F --> B
    E -->|Yes| G[Synthesizability Check]
    G --> H{Synthesizable?}
    H -->|No| F
    H -->|Yes| I[Stability Calculation]
    I --> J{Stable?}
    J -->|No| F
    J -->|Yes| K[Experimental Candidate List]

    style A fill:#e1f5ff
    style K fill:#e1ffe1
</div>

<h3>Implementation Example</h3>
<pre><code class="language-python">class MaterialsInverseDesign:
    def __init__(self, generator, predictor, synthesizability_checker):
        &quot;&quot;&quot;
        Materials inverse design system

        Args:
            generator: Conditional generative model
            predictor: Property prediction model
            synthesizability_checker: Synthesizability checker
        &quot;&quot;&quot;
        self.generator = generator
        self.predictor = predictor
        self.synthesizability_checker = synthesizability_checker

    def design_materials(self, target_properties, num_candidates=100, threshold=0.1):
        &quot;&quot;&quot;
        Inverse design materials

        Args:
            target_properties: Target properties (condition_dim,)
            num_candidates: Number of candidates to generate
            threshold: Tolerance error
        Returns:
            valid_materials: List of validated materials
        &quot;&quot;&quot;
        valid_materials = []

        for i in range(num_candidates):
            # 1. Generate candidate
            candidate = self.generator.generate_conditional(
                target_properties.unsqueeze(0),
                input_dim=128
            )

            # 2. Predict properties
            predicted_properties = self.predictor(candidate)

            # 3. Compare with target
            error = torch.abs(predicted_properties - target_properties).mean()
            if error &gt; threshold:
                continue

            # 4. Synthesizability check
            if not self.synthesizability_checker(candidate):
                continue

            # 5. Stability check (omitted)

            # Passed
            valid_materials.append({
                'structure': candidate,
                'predicted_properties': predicted_properties,
                'error': error.item()
            })

        # Sort by error
        valid_materials.sort(key=lambda x: x['error'])

        return valid_materials

# Usage example
def simple_synthesizability_checker(structure):
    &quot;&quot;&quot;
    Simple synthesizability check (actual implementation would be more complex)
    &quot;&quot;&quot;
    # Always returns True here (in practice, use Retrosyn etc.)
    return True

# Build system
inverse_design_system = MaterialsInverseDesign(
    generator=conditional_model,
    predictor=lambda x: torch.randn(x.size(0), 3),  # Dummy predictor
    synthesizability_checker=simple_synthesizability_checker
)

# Target properties
target = torch.tensor([2.5, -0.8, 0.0])  # Bandgap, formation energy, magnetic moment

# Execute inverse design
designed_materials = inverse_design_system.design_materials(target, num_candidates=50)
print(f&quot;Found {len(designed_materials)} valid materials&quot;)

# Display top 3
for i, material in enumerate(designed_materials[:3]):
    print(f&quot;\nMaterial {i+1}:&quot;)
    print(f&quot;  Predicted properties: {material['predicted_properties']}&quot;)
    print(f&quot;  Error: {material['error']:.4f}&quot;)
</code></pre>
<hr />
<h2>4.6 Industrial Applications and Careers</h2>
<h3>Real-World Success Stories</h3>
<h4>1. Drug Discovery: Novel Antibiotic Discovery</h4>
<p><strong>MIT (2020)</strong>:
- <strong>Method</strong>: Molecular generation with diffusion models
- <strong>Result</strong>: Discovery of halicin (novel antibiotic)
- <strong>Impact</strong>: 100x faster than conventional methods</p>
<h4>2. Battery Materials: High Energy Density Electrolytes</h4>
<p><strong>Stanford/Toyota (2022)</strong>:
- <strong>Method</strong>: Transformer + Reinforcement learning
- <strong>Result</strong>: Solid electrolyte with 1.5x lithium conductivity
- <strong>Impact</strong>: Accelerated commercialization of all-solid-state batteries</p>
<h4>3. Catalysts: CO‚ÇÇ Reduction Catalysts</h4>
<p><strong>CMU (2023)</strong>:
- <strong>Method</strong>: Conditional generation + DFT calculations
- <strong>Result</strong>: Discovery of catalyst with 10x efficiency
- <strong>Impact</strong>: Contribution to carbon neutrality</p>
<h3>Career Paths</h3>
<p><strong>AI Materials Design Engineer</strong>:
- <strong>Position</strong>: R&amp;D in pharmaceutical, chemical, materials companies
- <strong>Salary</strong>: 8-15M JPY (Japan), $120k-$250k (US)
- <strong>Required Skills</strong>: Transformer, generative models, materials science</p>
<p><strong>Researcher (Academia)</strong>:
- <strong>Position</strong>: PI at universities/research institutions
- <strong>Research Area</strong>: AI materials science, computational materials science
- <strong>Competition</strong>: Nature/Science-level publications required</p>
<p><strong>Startup Founder</strong>:
- <strong>Examples</strong>: Insilico Medicine (drug discovery AI), Citrine Informatics (materials AI)
- <strong>Funding</strong>: Series A-C, hundreds of millions to billions of yen
- <strong>Success Stories</strong>: IPO, acquisition by major companies</p>
<hr />
<h2>4.7 Summary</h2>
<h3>Key Points</h3>
<ol>
<li><strong>Diffusion Models</strong>: Generate high-quality data from noise</li>
<li><strong>Conditional Generation</strong>: Design materials by specifying target properties</li>
<li><strong>SMILES Generation</strong>: Generate molecular structures with Transformer</li>
<li><strong>Inverse Design</strong>: Reverse search from properties to structure</li>
<li><strong>Industrial Applications</strong>: Practical implementation advancing in drug discovery, batteries, catalysts</li>
</ol>
<h3>Series Summary</h3>
<p><strong>Chapter 1</strong>: Transformer basics, Attention mechanism
<strong>Chapter 2</strong>: Materials-specific architectures (Matformer, ChemBERTa)
<strong>Chapter 3</strong>: Pre-trained models, transfer learning
<strong>Chapter 4</strong>: Generative models, inverse design</p>
<p><strong>Next Steps</strong>:
1. Gain experience through practical projects
2. Update knowledge by reading latest papers
3. Test your skills by participating in Kaggle competitions
4. Join communities for information exchange</p>
<hr />
<h2>üìù Exercises</h2>
<h3>Exercise 1: Concept Understanding</h3>
<p>List three advantages of diffusion models compared to conventional generative models (VAE, GAN).</p>
<details>
<summary>Sample Answer</summary>

1. **Training Stability**: Less prone to mode collapse unlike GANs
2. **Sample Quality**: Can generate high-quality and diverse samples
3. **Flexible Conditioning**: Easy to incorporate various conditions (properties, constraints)

Additional:
- **Interpretability**: Generation process is stepwise and easy to understand
- **Scalability**: Efficient training even with large-scale data
</details>

<h3>Exercise 2: Implementation</h3>
<p>Write code to generate materials that simultaneously satisfy multiple target properties (bandgap, formation energy) using conditional generation.</p>
<pre><code class="language-python">def multi_objective_generation(generator, target_bandgap, target_formation_energy, num_samples=10):
    &quot;&quot;&quot;
    Generate materials with multi-objective optimization

    Args:
        generator: Conditional generative model
        target_bandgap: Target bandgap (eV)
        target_formation_energy: Target formation energy (eV/atom)
        num_samples: Number of samples to generate
    Returns:
        generated_materials: List of generated materials
    &quot;&quot;&quot;
    # Implement here
    pass
</code></pre>
<details>
<summary>Sample Answer</summary>


<pre><code class="language-python">def multi_objective_generation(generator, target_bandgap, target_formation_energy, num_samples=10):
    # Create condition
    condition = torch.tensor([[target_bandgap, target_formation_energy]])
    condition = condition.repeat(num_samples, 1)

    # Generate
    generated_materials = generator.generate_conditional(condition, input_dim=128)

    return generated_materials

# Usage example
target_bg = 2.0  # 2.0 eV
target_fe = -0.5  # -0.5 eV/atom

materials = multi_objective_generation(conditional_model, target_bg, target_fe, num_samples=20)
print(f&quot;Generated {materials.shape[0]} materials&quot;)
</code></pre>

</details>

<h3>Exercise 3: Application</h3>
<p>List five important criteria for evaluating generated candidate materials in materials inverse design, and explain each one.</p>
<details>
<summary>Sample Answer</summary>

1. **Target Property Achievement**:
   - How close predicted properties are to target values
   - For multiple properties, Pareto optimality

2. **Synthesizability**:
   - Can be fabricated using known synthesis methods
   - Availability of precursors
   - Feasibility of synthesis conditions (temperature, pressure)

3. **Thermodynamic Stability**:
   - Formation energy is negative (stable phase)
   - Most stable compared to other crystal structures
   - Stability against decomposition reactions

4. **Chemical Validity**:
   - Satisfies valence rules
   - Reasonable bond distances and angles
   - Consistent with known chemical systems

5. **Cost and Environmental Impact**:
   - Price and reserves of constituent elements
   - Use of hazardous elements (Cd, Pb, etc.)
   - Recyclability
</details>

<hr />
<h2>üéì Congratulations on Completing the Series!</h2>
<p>By completing this series, you have mastered the fundamentals to applications of Transformers and generative models, and their utilization in materials science.</p>
<h3>Next Steps</h3>
<ol>
<li>
<p><strong>Practical Projects</strong>:
   - Materials property prediction with Materials Project data
   - Molecular generation with QM9 dataset
   - Fine-tuning with your own data</p>
</li>
<li>
<p><strong>Paper Implementation</strong>:
   - Read and implement Matformer paper
   - Challenge latest generative model papers</p>
</li>
<li>
<p><strong>Competitions</strong>:
   - Open Catalyst Challenge
   - Kaggle molecular prediction competitions</p>
</li>
<li>
<p><strong>Community Participation</strong>:
   - Hugging Face Forum
   - Materials Project Community
   - Materials science conferences (MRS, APS)</p>
</li>
</ol>
<hr />
<h2>üéØ Materials-Specific Transformers in Detail</h2>
<h3>ChemBERTa: Chemical BERT</h3>
<pre><code class="language-python">from transformers import RobertaTokenizer, RobertaModel, RobertaConfig

class ChemBERTa(nn.Module):
    &quot;&quot;&quot;
    ChemBERTa: RoBERTa trained on 10M SMILES strings

    Features:
    - Pre-trained on PubChem, ZINC, ChEMBL
    - SMILES-specific tokenizer
    - Optimized for molecular property prediction
    &quot;&quot;&quot;

    def __init__(self, pretrained_model=&quot;seyonec/ChemBERTa-zinc-base-v1&quot;):
        super().__init__()
        self.tokenizer = RobertaTokenizer.from_pretrained(pretrained_model)
        self.model = RobertaModel.from_pretrained(pretrained_model)

    def forward(self, smiles_list):
        &quot;&quot;&quot;
        Args:
            smiles_list: List of SMILES strings

        Returns:
            embeddings: (batch_size, 768) molecular embeddings
        &quot;&quot;&quot;
        # Tokenize
        encoded = self.tokenizer(
            smiles_list,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )

        # Forward
        outputs = self.model(**encoded)

        # [CLS] token embedding
        embeddings = outputs.last_hidden_state[:, 0, :]

        return embeddings

# Usage example
chemberta = ChemBERTa()

smiles_list = [
    &quot;CC(C)Cc1ccc(cc1)C(C)C(=O)O&quot;,  # Ibuprofen
    &quot;CN1C=NC2=C1C(=O)N(C(=O)N2C)C&quot;  # Caffeine
]

embeddings = chemberta(smiles_list)
print(f&quot;Molecular embeddings: {embeddings.shape}&quot;)  # (2, 768)
</code></pre>
<h3>MatBERT: Materials Composition BERT</h3>
<pre><code class="language-python">class MatBERT(nn.Module):
    &quot;&quot;&quot;
    MatBERT: BERT for materials composition

    Pre-training:
    - Materials Project (500k+ compositions)
    - OQMD, AFLOW datasets
    - Masked composition prediction
    &quot;&quot;&quot;

    def __init__(self, vocab_size=120, d_model=768, num_layers=12):
        super().__init__()

        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=d_model,
            num_hidden_layers=num_layers,
            num_attention_heads=12,
            intermediate_size=3072,
            max_position_embeddings=50  # Maximum atoms in material
        )

        self.bert = BertModel(config)

    def forward(self, composition_ids, attention_mask=None):
        &quot;&quot;&quot;
        Args:
            composition_ids: (batch, seq_len) Atomic number sequence
                             Example: [CLS] Fe Fe O O O [SEP]

        Returns:
            outputs: BERT outputs
        &quot;&quot;&quot;
        outputs = self.bert(
            input_ids=composition_ids,
            attention_mask=attention_mask
        )

        return outputs

# Fine-tuning example: Bandgap prediction
class MatBERTForBandgap(nn.Module):
    def __init__(self, matbert):
        super().__init__()
        self.matbert = matbert

        # Prediction head
        self.regressor = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1)
        )

    def forward(self, composition_ids, attention_mask=None):
        outputs = self.matbert(composition_ids, attention_mask)
        cls_embedding = outputs.pooler_output

        bandgap = self.regressor(cls_embedding)
        return bandgap
</code></pre>
<h3>MatGPT: Materials Generation GPT</h3>
<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Config

class MatGPT(nn.Module):
    &quot;&quot;&quot;
    MatGPT: GPT for materials composition generation

    Applications:
    - Generation of novel material compositions
    - Conditional generation (target properties ‚Üí composition)
    - Automation of materials design
    &quot;&quot;&quot;

    def __init__(self, vocab_size=120, d_model=768, num_layers=12):
        super().__init__()

        config = GPT2Config(
            vocab_size=vocab_size,
            n_positions=50,
            n_embd=d_model,
            n_layer=num_layers,
            n_head=12
        )

        self.gpt = GPT2LMHeadModel(config)

    def generate_composition(self, start_tokens, max_length=30, temperature=1.0, top_k=50):
        &quot;&quot;&quot;
        Composition generation

        Args:
            start_tokens: (1, start_len) Start tokens
                         Example: [CLS] Li
            max_length: Maximum generation length
            temperature: Sampling temperature (low‚Üídeterministic, high‚Üírandom)
            top_k: Top-k sampling

        Returns:
            generated: (1, gen_len) Generated composition
        &quot;&quot;&quot;
        self.eval()

        with torch.no_grad():
            generated = self.gpt.generate(
                start_tokens,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                do_sample=True,
                pad_token_id=0
            )

        return generated

# Conditional generation
class ConditionalMatGPT(nn.Module):
    &quot;&quot;&quot;
    Conditional materials generation

    Conditions: Bandgap, formation energy, magnetic moment
    &quot;&quot;&quot;

    def __init__(self, matgpt, condition_dim=3):
        super().__init__()
        self.matgpt = matgpt

        # Condition encoder
        self.condition_encoder = nn.Sequential(
            nn.Linear(condition_dim, 768),
            nn.ReLU(),
            nn.Linear(768, 768)
        )

    def forward(self, input_ids, conditions):
        &quot;&quot;&quot;
        Args:
            input_ids: (batch, seq_len)
            conditions: (batch, condition_dim) Target properties

        Returns:
            logits: (batch, seq_len, vocab_size)
        &quot;&quot;&quot;
        # Embed conditions
        condition_embed = self.condition_encoder(conditions)
        condition_embed = condition_embed.unsqueeze(1)  # (batch, 1, 768)

        # Input embeddings
        input_embeddings = self.matgpt.gpt.transformer.wte(input_ids)

        # Add condition at the beginning
        embeddings = torch.cat([condition_embed, input_embeddings], dim=1)

        # GPT forward
        outputs = self.matgpt.gpt(inputs_embeds=embeddings)

        return outputs.logits

# Usage example
matgpt = MatGPT(vocab_size=120)
cond_matgpt = ConditionalMatGPT(matgpt, condition_dim=3)

# Target: Bandgap 2.5 eV, formation energy -1.0 eV, non-magnetic
target_conditions = torch.tensor([[2.5, -1.0, 0.0]])

# Start token for generation
start = torch.tensor([[101]])  # [CLS]

# Generate
with torch.no_grad():
    logits = cond_matgpt(start, target_conditions)
    # Sample next token
    probs = torch.softmax(logits[:, -1, :], dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)

print(f&quot;Next token: {next_token}&quot;)
</code></pre>
<hr />
<h2>üî¨ Detailed Transfer Learning Strategies</h2>
<h3>Strategy 1: Full Fine-tuning</h3>
<pre><code class="language-python">def full_finetuning(pretrained_model, train_loader, val_loader):
    &quot;&quot;&quot;
    Update all parameters

    Application scenarios:
    - Sufficient target data (thousands of samples or more)
    - Similar domains
    - Aiming for highest accuracy
    &quot;&quot;&quot;
    model = pretrained_model

    # Update all parameters
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

    # Learning rate scheduler
    num_training_steps = len(train_loader) * epochs
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_training_steps)

    best_val_loss = float('inf')

    for epoch in range(epochs):
        model.train()
        for batch in train_loader:
            optimizer.zero_grad()

            outputs = model(**batch)
            loss = outputs.loss

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

        # Validation
        model.eval()
        val_loss = evaluate(model, val_loader)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_full_finetuned.pt')

    return model
</code></pre>
<h3>Strategy 2: Adapter Tuning</h3>
<pre><code class="language-python">class AdapterLayer(nn.Module):
    &quot;&quot;&quot;
    Adapter: High performance with few parameters

    Idea: Insert Adapter (small bottleneck NN) in each Transformer layer
    &quot;&quot;&quot;

    def __init__(self, d_model, adapter_size=64):
        super().__init__()

        self.adapter = nn.Sequential(
            nn.Linear(d_model, adapter_size),  # Down-project
            nn.ReLU(),
            nn.Linear(adapter_size, d_model)   # Up-project
        )

        # Residual connection
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x):
        &quot;&quot;&quot;
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            x + adapter(x): Residual connection
        &quot;&quot;&quot;
        residual = x
        x = self.layer_norm(x)
        x = self.adapter(x)
        return residual + x

class MatBERTWithAdapters(nn.Module):
    &quot;&quot;&quot;
    MatBERT + Adapters

    Advantages:
    - Updated parameters: 1-2% of full model
    - Performance: 95-98% of Full fine-tuning
    - Can switch adapters for multiple tasks
    &quot;&quot;&quot;

    def __init__(self, pretrained_matbert, adapter_size=64):
        super().__init__()
        self.matbert = pretrained_matbert

        # Freeze MatBERT parameters
        for param in self.matbert.parameters():
            param.requires_grad = False

        # Add adapter to each Transformer layer
        self.adapters = nn.ModuleList([
            AdapterLayer(768, adapter_size)
            for _ in range(12)  # 12 layers
        ])

    def forward(self, input_ids, attention_mask=None):
        # MatBERT forward (frozen)
        outputs = self.matbert(input_ids, attention_mask, output_hidden_states=True)

        hidden_states = outputs.hidden_states

        # Apply adapter to each layer
        for i, adapter in enumerate(self.adapters):
            hidden_states[i+1] = adapter(hidden_states[i+1])

        # Final layer output
        final_hidden = hidden_states[-1]

        return final_hidden

# Usage example
pretrained = MatBERT(vocab_size=120)
model_with_adapters = MatBERTWithAdapters(pretrained, adapter_size=64)

# Train only adapters
trainable_params = sum(p.numel() for p in model_with_adapters.adapters.parameters())
total_params = sum(p.numel() for p in model_with_adapters.parameters())

print(f&quot;Trainable params: {trainable_params} ({trainable_params/total_params*100:.2f}%)&quot;)
</code></pre>
<h3>Strategy 3: LoRA (Low-Rank Adaptation)</h3>
<pre><code class="language-python">class LoRALayer(nn.Module):
    &quot;&quot;&quot;
    LoRA: Low-Rank Adaptation of Large Language Models

    Idea: Low-rank decomposition of weight matrix updates
    W_new = W_frozen + BA (B: m√ór, A: r√ón, r &lt;&lt; m,n)
    &quot;&quot;&quot;

    def __init__(self, in_features, out_features, rank=8):
        super().__init__()

        self.rank = rank

        # Low-rank matrices (trainable)
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) / rank)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

    def forward(self, x, frozen_weight):
        &quot;&quot;&quot;
        Args:
            x: (batch, seq_len, in_features)
            frozen_weight: (out_features, in_features) Frozen weights

        Returns:
            output: (batch, seq_len, out_features)
        &quot;&quot;&quot;
        # Frozen part
        output = torch.matmul(x, frozen_weight.T)

        # LoRA part
        lora_output = torch.matmul(x, self.lora_A.T)
        lora_output = torch.matmul(lora_output, self.lora_B.T)

        return output + lora_output

class MatBERTWithLoRA(nn.Module):
    &quot;&quot;&quot;
    MatBERT + LoRA

    Advantages:
    - Updated parameters: 0.1-1% of full model
    - Performance: Equivalent to Full fine-tuning
    - Can merge LoRA during inference (no speed degradation)
    &quot;&quot;&quot;

    def __init__(self, pretrained_matbert, rank=8):
        super().__init__()
        self.matbert = pretrained_matbert

        # Freeze MatBERT parameters
        for param in self.matbert.parameters():
            param.requires_grad = False

        # Add LoRA to Attention QKV
        self.lora_layers = nn.ModuleDict()
        for layer_idx in range(12):
            self.lora_layers[f'layer_{layer_idx}_q'] = LoRALayer(768, 768, rank)
            self.lora_layers[f'layer_{layer_idx}_v'] = LoRALayer(768, 768, rank)

    def forward(self, input_ids, attention_mask=None):
        # Omitted: Integrate LoRA into Attention calculation
        pass

# Usage example
model_with_lora = MatBERTWithLoRA(pretrained, rank=8)

trainable_params = sum(p.numel() for p in model_with_lora.lora_layers.parameters())
total_params = sum(p.numel() for p in model_with_lora.parameters())

print(f&quot;Trainable params: {trainable_params} ({trainable_params/total_params*100:.3f}%)&quot;)
</code></pre>
<hr />
<h2>üéì Implementation of Materials-Oriented Pre-training</h2>
<h3>Pre-training Task 1: Masked Atom Prediction</h3>
<pre><code class="language-python">def pretrain_masked_atom_prediction(model, dataloader, epochs=100):
    &quot;&quot;&quot;
    Masked Atom Prediction (MAP)

    Task: Predict masked atoms
    Example: Fe [MASK] O ‚Üí Fe Fe O (Fe2O3)
    &quot;&quot;&quot;
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Pad token

    model.train()

    for epoch in range(epochs):
        total_loss = 0

        for batch in dataloader:
            composition_ids = batch['composition_ids']  # (batch, seq_len)

            # Mask 15% of atoms
            mask_prob = 0.15
            masked_composition, labels = mask_atoms(composition_ids, mask_prob)

            # Forward
            outputs = model(masked_composition)
            logits = outputs.logits  # (batch, seq_len, vocab_size)

            # Loss
            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f&quot;Epoch {epoch+1}, MAP Loss: {avg_loss:.4f}&quot;)

    return model

def mask_atoms(composition_ids, mask_prob=0.15):
    &quot;&quot;&quot;
    Mask atoms

    Strategy:
    - 80%: Replace with [MASK]
    - 10%: Replace with random atom
    - 10%: No change
    &quot;&quot;&quot;
    labels = composition_ids.clone()
    masked_composition = composition_ids.clone()

    # Select mask targets
    mask = torch.rand(composition_ids.shape) &lt; mask_prob
    mask[:, 0] = False  # Exclude [CLS]
    mask[:, -1] = False  # Exclude [SEP]

    # 80% to [MASK]
    mask_token_mask = torch.rand(composition_ids.shape) &lt; 0.8
    masked_composition[mask &amp; mask_token_mask] = MASK_TOKEN_ID

    # 10% to random atom
    random_mask = torch.rand(composition_ids.shape) &lt; 0.1
    random_atoms = torch.randint(1, 119, composition_ids.shape)
    masked_composition[mask &amp; random_mask] = random_atoms[mask &amp; random_mask]

    # 10% unchanged

    # Ignore labels at non-masked positions
    labels[~mask] = -100

    return masked_composition, labels
</code></pre>
<h3>Pre-training Task 2: Contrastive Learning</h3>
<pre><code class="language-python">class ContrastiveLearning(nn.Module):
    &quot;&quot;&quot;
    Contrastive Learning for Materials

    Idea: Place similar materials close, dissimilar materials far apart
    &quot;&quot;&quot;

    def __init__(self, matbert, temperature=0.07):
        super().__init__()
        self.matbert = matbert
        self.temperature = temperature

    def forward(self, compositions1, compositions2, labels):
        &quot;&quot;&quot;
        Args:
            compositions1: (batch, seq_len) Augmented sample 1
            compositions2: (batch, seq_len) Augmented sample 2
            labels: (batch,) 1 if similar, 0 if dissimilar

        Returns:
            loss: Contrastive loss
        &quot;&quot;&quot;
        # Embeddings
        emb1 = self.matbert(compositions1).pooler_output  # (batch, 768)
        emb2 = self.matbert(compositions2).pooler_output

        # Normalize
        emb1 = F.normalize(emb1, dim=-1)
        emb2 = F.normalize(emb2, dim=-1)

        # Cosine similarity
        similarity = torch.matmul(emb1, emb2.T) / self.temperature  # (batch, batch)

        # Loss: InfoNCE
        loss = F.cross_entropy(similarity, torch.arange(emb1.size(0), device=emb1.device))

        return loss

# Data augmentation
def augment_composition(composition_ids):
    &quot;&quot;&quot;
    Composition data augmentation

    Methods:
    - Shuffle atom order (Fe2O3 ‚Üí O3Fe2)
    - Substitute with same-group elements (LiCoO2 ‚Üí NaCoO2)
    &quot;&quot;&quot;
    # Implementation omitted
    pass
</code></pre>
<hr />
<h2>‚úÖ Chapter 4 Completion Checklist</h2>
<h3>Concept Understanding (10 items)</h3>
<ul>
<li>[ ] Understand principles of diffusion models (forward/reverse process)</li>
<li>[ ] Understand mechanism of conditional generation</li>
<li>[ ] Can explain differences and advantages of SMILES and SELFIES</li>
<li>[ ] Understand materials inverse design workflow</li>
<li>[ ] Can explain differences between ChemBERTa and MatBERT</li>
<li>[ ] Understand differences and application scenarios of Full fine-tuning/Adapter/LoRA</li>
<li>[ ] Understand principles of Masked Atom Prediction</li>
<li>[ ] Understand application of Contrastive Learning to materials science</li>
<li>[ ] Can explain evaluation metrics for generative models (validity, diversity, novelty)</li>
<li>[ ] Understand constraints in materials inverse design (synthesizability, stability)</li>
</ul>
<h3>Implementation Skills (15 items)</h3>
<ul>
<li>[ ] Can implement SimpleDiffusionModel</li>
<li>[ ] Can implement ConditionalDiffusionModel</li>
<li>[ ] Can implement SMILESGenerator</li>
<li>[ ] Can implement ConditionalSMILESGenerator</li>
<li>[ ] Can use ChemBERTa</li>
<li>[ ] Can implement MatBERT</li>
<li>[ ] Can implement MatGPT (including conditional generation)</li>
<li>[ ] Can implement AdapterLayer</li>
<li>[ ] Can implement LoRALayer</li>
<li>[ ] Can implement Masked Atom Prediction</li>
<li>[ ] Can implement Contrastive Learning</li>
<li>[ ] Can build materials inverse design system</li>
<li>[ ] Can implement synthesizability check</li>
<li>[ ] Can build validation pipeline for generated materials</li>
<li>[ ] Can implement beam search</li>
</ul>
<h3>Debugging Skills (5 items)</h3>
<ul>
<li>[ ] Can evaluate sample quality of diffusion models</li>
<li>[ ] Can verify validity of generated SMILES</li>
<li>[ ] Can evaluate condition satisfaction in conditional generation</li>
<li>[ ] Can compare performance of LoRA/Adapter with full fine-tuning</li>
<li>[ ] Can visualize and analyze effects of pre-training</li>
</ul>
<h3>Application Skills (5 items)</h3>
<ul>
<li>[ ] Can apply generative models to novel materials discovery</li>
<li>[ ] Can implement multi-objective optimization (multiple properties)</li>
<li>[ ] Can build loop combining generative and prediction models</li>
<li>[ ] Can incorporate domain knowledge (chemical rules, crystallography) into generation</li>
<li>[ ] Can prioritize experimental candidates</li>
</ul>
<h3>Data Processing (5 items)</h3>
<ul>
<li>[ ] Can convert between SMILES/SELFIES</li>
<li>[ ] Can perform molecular validity check (RDKit)</li>
<li>[ ] Can normalize composition formulas</li>
<li>[ ] Can implement data augmentation</li>
<li>[ ] Can post-process generated data (filtering)</li>
</ul>
<h3>Evaluation Skills (5 items)</h3>
<ul>
<li>[ ] Can measure validity of generative models</li>
<li>[ ] Can quantitatively evaluate diversity</li>
<li>[ ] Can evaluate novelty</li>
<li>[ ] Can measure condition satisfaction</li>
<li>[ ] Can calculate synthesizability score</li>
</ul>
<h3>Theoretical Background (5 items)</h3>
<ul>
<li>[ ] Read diffusion model paper (Ho et al., 2020)</li>
<li>[ ] Read ChemBERTa/MatBERT papers</li>
<li>[ ] Read LoRA paper (Hu et al., 2021)</li>
<li>[ ] Read at least one paper on materials inverse design</li>
<li>[ ] Understand theory of generative models (VAE, GAN, Diffusion)</li>
</ul>
<h3>Completion Criteria</h3>
<ul>
<li><strong>Minimum Standard</strong>: 40+ items achieved (80%)</li>
<li><strong>Recommended Standard</strong>: 45+ items achieved (90%)</li>
<li><strong>Excellent Standard</strong>: All 50 items achieved (100%)</li>
</ul>
<hr />
<h2>üîó References</h2>
<h3>Papers</h3>
<ul>
<li>Ho et al. (2020) "Denoising Diffusion Probabilistic Models" <a href="https://arxiv.org/abs/2006.11239">arXiv:2006.11239</a></li>
<li>Chen et al. (2022) "Matformer: Nested Transformer for Elastic Inference"</li>
<li>Xie et al. (2021) "Crystal Diffusion Variational Autoencoder" <a href="https://arxiv.org/abs/2110.06197">arXiv:2110.06197</a></li>
<li>Stokes et al. (2020) "A Deep Learning Approach to Antibiotic Discovery" Nature</li>
<li>Hu et al. (2021) "LoRA: Low-Rank Adaptation of Large Language Models" <a href="https://arxiv.org/abs/2106.09685">arXiv:2106.09685</a></li>
<li>Chithrananda et al. (2020) "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction" <a href="https://arxiv.org/abs/2010.09885">arXiv:2010.09885</a></li>
</ul>
<h3>Tools</h3>
<ul>
<li><a href="https://github.com/huggingface/diffusers">Hugging Face Diffusers</a></li>
<li><a href="https://www.rdkit.org/">RDKit</a> - Molecular processing</li>
<li><a href="https://materialsproject.org/">Materials Project API</a></li>
<li><a href="https://github.com/aspuru-guzik-group/selfies">SELFIES</a> - Molecular representation</li>
<li><a href="https://pymatgen.org/">PyMatGen</a> - Materials science</li>
</ul>
<h3>Next Series</h3>
<ul>
<li><strong>Introduction to Reinforcement Learning</strong>: Application of reinforcement learning to materials discovery</li>
<li><strong>Introduction to GNN</strong>: Molecular and materials representation with Graph Neural Networks</li>
<li><strong>Introduction to Foundation Models</strong>: LLaMA, GPT-4, Claude for Materials</li>
</ul>
<hr />
<p><strong>Author</strong>: Yusuke Hashimoto (Tohoku University)
<strong>Last Updated</strong>: October 19, 2025
<strong>Series</strong>: Introduction to Transformer & Foundation Models (4 chapters complete)</p>
<p><strong>License</strong>: CC BY 4.0</p><div class="navigation">
    <a href="chapter-3.html" class="nav-button">‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Return to Series Index</a>
</div>
    </main>


    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
            <li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
            <li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
            <li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
            <li>The content of this material may be changed, updated, or discontinued without notice.</li>
            <li>The copyright and license of this content are subject to the stated conditions (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>