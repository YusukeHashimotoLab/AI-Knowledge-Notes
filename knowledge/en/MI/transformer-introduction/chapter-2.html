<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 2: Materials-Specific Transformer Architectures - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 2</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/transformer-introduction/chapter-2.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 2: Materials-Specific Transformer Architectures</h1>
<p class="subtitle">Specialized Transformer Architectures for Materials Science</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-25 minutes</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code examples: 0</span>
<span class="meta-item">üìù Exercises: 0</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 2: Materials-Specific Transformer Architectures</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">We overview representative models such as Matformer and their differences and application ranges. We also cover connections to crystals and graphs.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Performance significantly varies based on input representation (structure/composition/text) and pre-training task design.</p>
<p><strong>Learning time</strong>: 30-35 minutes | <strong>Difficulty</strong>: Intermediate to Advanced</p>
<h2>üìã What You'll Learn in This Chapter</h2>
<ul>
<li>Design principles of materials science-specific Transformer architectures</li>
<li>Matformer: Materials Transformer for Property Prediction</li>
<li>CrystalFormer: Crystal Structure Representation</li>
<li>ChemBERTa: Molecular SMILES Representation Learning</li>
<li>Perceiver IO: Diverse Data Integration</li>
<li>Implementation exercise: Material property prediction with Matformer</li>
</ul>
<hr/>
<h2>2.1 The Need for Materials Science-Specific Transformers</h2>
<h3>Limitations of General-Purpose Transformers</h3>
<p><strong>Problems with directly using NLP Transformers</strong>:
- ‚ùå 3D structural information of molecules/materials is lost
- ‚ùå Cannot consider chemical bonds or interatomic distances
- ‚ùå Cannot handle periodic boundary conditions (crystals)
- ‚ùå Ignores physical constraints (conservation laws, symmetry)</p>
<h3>Features of Materials-Specific Transformers</h3>
<p><strong>Required extensions</strong>:
- ‚úÖ <strong>3D structure embedding</strong>: atomic coordinates, distances, angles
- ‚úÖ <strong>Periodic boundary conditions</strong>: crystal lattice repetition
- ‚úÖ <strong>Physical constraints</strong>: symmetry, equivariance
- ‚úÖ <strong>Diverse data integration</strong>: structure + composition + experimental data</p>
<div class="mermaid">
flowchart TD
    A[General-Purpose Transformer] --&gt; B[Materials-Specific Transformer]
    B --&gt; C[3D Structure Embedding]
    B --&gt; D[Periodic Boundary Conditions]
    B --&gt; E[Physical Constraints]
    B --&gt; F[Diverse Data Integration]

    C --&gt; G[Matformer]
    D --&gt; G
    E --&gt; H[CrystalFormer]
    F --&gt; I[Perceiver IO]

    style G fill:#e1f5ff
    style H fill:#ffe1f5
    style I fill:#f5ffe1
</div>
<hr/>
<h2>2.2 Matformer: Materials Transformer</h2>
<h3>Overview</h3>
<p><strong>Matformer</strong> (Chen et al., 2022) is a Transformer model that predicts properties from material crystal structures.</p>
<p><strong>Features</strong>:
- <strong>Nested Transformer</strong>: hierarchical processing at atom and crystal levels
- <strong>Distance-aware Attention</strong>: considers interatomic distances
- <strong>Elastic Inference</strong>: dynamically adjusts computation and memory</p>
<h3>Architecture</h3>
<div class="mermaid">
flowchart TB
    subgraph Input
        A1[Atomic Coordinates] --&gt; B[Atom Embedding]
        A2[Atomic Numbers] --&gt; B
        A3[Lattice Constants] --&gt; B
    end

    B --&gt; C[Positional Encoding]
    C --&gt; D[Distance Matrix]

    subgraph "Nested Transformer"
        D --&gt; E1[Atom-level Attention]
        E1 --&gt; E2[Structure-level Attention]
    end

    E2 --&gt; F[Pooling]
    F --&gt; G[Prediction Head]
    G --&gt; H[Band Gap/Formation Energy]

    style E1 fill:#e1f5ff
    style E2 fill:#ffe1e1
</div>
<h3>Atom Embedding</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

class AtomEmbedding(nn.Module):
    def __init__(self, num_atoms=118, d_model=256):
        """
        Atom embedding layer

        Args:
            num_atoms: Number of atom types (periodic table, 118 elements)
            d_model: Embedding dimension
        """
        super(AtomEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_atoms, d_model)

    def forward(self, atomic_numbers):
        """
        Args:
            atomic_numbers: (batch_size, num_atoms) atomic numbers
        Returns:
            embeddings: (batch_size, num_atoms, d_model)
        """
        return self.embedding(atomic_numbers)

# Usage example: NaCl crystal
batch_size = 2
num_atoms = 8  # Number of atoms in unit cell

# Atomic numbers: Na(11), Cl(17)
atomic_numbers = torch.tensor([
    [11, 17, 11, 17, 11, 17, 11, 17],  # Sample 1
    [11, 17, 11, 17, 11, 17, 11, 17]   # Sample 2
])

atom_emb = AtomEmbedding(num_atoms=118, d_model=256)
embeddings = atom_emb(atomic_numbers)
print(f"Atom embeddings shape: {embeddings.shape}")  # (2, 8, 256)
</code></pre>
<h3>Distance-aware Attention</h3>
<p><strong>Attention considering interatomic distances</strong>:</p>
<pre><code class="language-python">class DistanceAwareAttention(nn.Module):
    def __init__(self, d_model, num_heads, max_distance=10.0):
        """
        Distance-aware Attention

        Args:
            d_model: Model dimension
            num_heads: Number of attention heads
            max_distance: Maximum distance (√Ö)
        """
        super(DistanceAwareAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_distance = max_distance

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        # Distance embedding
        self.distance_embedding = nn.Linear(1, num_heads)

    def forward(self, x, distance_matrix):
        """
        Args:
            x: (batch_size, num_atoms, d_model)
            distance_matrix: (batch_size, num_atoms, num_atoms) interatomic distances (√Ö)
        """
        batch_size = x.size(0)

        # Q, K, V
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)

        # Distance bias
        # Larger value for closer distances, smaller for farther distances
        distance_bias = self.distance_embedding(distance_matrix.unsqueeze(-1))  # (batch, num_atoms, num_atoms, num_heads)
        distance_bias = distance_bias.permute(0, 3, 1, 2)  # (batch, num_heads, num_atoms, num_atoms)

        # Convert distance with Gaussian function (higher score for closer atoms)
        distance_factor = torch.exp(-distance_matrix.unsqueeze(1) / 2.0)  # (batch, 1, num_atoms, num_atoms)

        scores = scores + distance_bias * distance_factor

        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)

        # Apply attention
        output = torch.matmul(attention_weights, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_o(output)

        return output, attention_weights

# Usage example
d_model = 256
num_heads = 8
num_atoms = 8

dist_attn = DistanceAwareAttention(d_model, num_heads)

x = torch.randn(2, num_atoms, d_model)
# NaCl crystal interatomic distances (simplified)
distance_matrix = torch.tensor([
    [[0.0, 2.8, 3.9, 4.8, 3.9, 5.5, 4.8, 6.7],  # Distances from atom 1
     [2.8, 0.0, 2.8, 3.9, 5.5, 3.9, 6.7, 4.8],
     # ... omitted
     [6.7, 4.8, 5.5, 3.9, 4.8, 3.9, 2.8, 0.0]]
]).repeat(2, 1, 1)  # Replicate for batch_size

output, attn_weights = dist_attn(x, distance_matrix)
print(f"Output shape: {output.shape}")  # (2, 8, 256)
</code></pre>
<h3>Matformer Block</h3>
<pre><code class="language-python">class MatformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff=1024, dropout=0.1):
        """
        Basic Matformer block

        Args:
            d_model: Model dimension
            num_heads: Number of attention heads
            d_ff: Feed-forward layer intermediate dimension
            dropout: Dropout rate
        """
        super(MatformerBlock, self).__init__()

        self.distance_attention = DistanceAwareAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, distance_matrix):
        # Distance-aware Attention + Residual
        attn_output, _ = self.distance_attention(x, distance_matrix)
        x = self.norm1(x + self.dropout1(attn_output))

        # Feed-Forward + Residual
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x
</code></pre>
<hr/>
<h2>2.3 CrystalFormer: Crystal Structure Transformer</h2>
<h3>Overview</h3>
<p><strong>CrystalFormer</strong> is a Transformer that considers periodic boundary conditions of crystals.</p>
<p><strong>Features</strong>:
- <strong>Wyckoff position embedding</strong>: considers crystal symmetry
- <strong>Fractional Coordinates</strong>: representation in fractional coordinates
- <strong>Space Group Encoding</strong>: embedding space group information</p>
<h3>Fractional Coordinate Embedding</h3>
<pre><code class="language-python">class FractionalCoordinateEncoding(nn.Module):
    def __init__(self, d_model):
        super(FractionalCoordinateEncoding, self).__init__()
        self.coord_linear = nn.Linear(3, d_model)

    def forward(self, fractional_coords):
        """
        Args:
            fractional_coords: (batch_size, num_atoms, 3) fractional coordinates [0, 1)
        Returns:
            encoding: (batch_size, num_atoms, d_model)
        """
        # Trigonometric embedding
        freqs = torch.arange(1, d_model // 6 + 1, dtype=torch.float32)
        coords_expanded = fractional_coords.unsqueeze(-1) * freqs

        encoding = torch.cat([
            torch.sin(2 * np.pi * coords_expanded),
            torch.cos(2 * np.pi * coords_expanded)
        ], dim=-1)

        # Linear transformation for dimension adjustment
        encoding = encoding.flatten(start_dim=2)
        encoding = self.coord_linear(encoding)

        return encoding
</code></pre>
<h3>Considering Periodic Boundary Conditions</h3>
<pre><code class="language-python">def compute_periodic_distance(coords1, coords2, lattice_matrix):
    """
    Distance calculation with periodic boundary conditions

    Args:
        coords1: (num_atoms1, 3) fractional coordinates
        coords2: (num_atoms2, 3) fractional coordinates
        lattice_matrix: (3, 3) lattice vector matrix
    Returns:
        distances: (num_atoms1, num_atoms2) minimum distances (√Ö)
    """
    # Convert to Cartesian coordinates
    cart1 = torch.matmul(coords1, lattice_matrix)
    cart2 = torch.matmul(coords2, lattice_matrix)

    # Consider all periodic images (range -1, 0, 1)
    offsets = torch.tensor([
        [i, j, k] for i in [-1, 0, 1]
                  for j in [-1, 0, 1]
                  for k in [-1, 0, 1]
    ], dtype=torch.float32)  # 27 combinations

    min_distances = []
    for offset in offsets:
        offset_cart = torch.matmul(offset, lattice_matrix)
        shifted_cart2 = cart2 + offset_cart

        # Distance calculation
        diff = cart1.unsqueeze(1) - shifted_cart2.unsqueeze(0)
        distances = torch.norm(diff, dim=-1)
        min_distances.append(distances)

    # Select minimum distance
    min_distances = torch.stack(min_distances, dim=-1)
    min_distances, _ = torch.min(min_distances, dim=-1)

    return min_distances

# Usage example: simple cubic lattice
fractional_coords = torch.tensor([
    [0.0, 0.0, 0.0],  # Atom 1
    [0.5, 0.5, 0.5]   # Atom 2
])

lattice_matrix = torch.tensor([
    [5.0, 0.0, 0.0],
    [0.0, 5.0, 0.0],
    [0.0, 0.0, 5.0]
])  # 5√Ö cubic lattice

distances = compute_periodic_distance(fractional_coords, fractional_coords, lattice_matrix)
print("Distance matrix (√Ö):")
print(distances)
</code></pre>
<hr/>
<h2>2.4 ChemBERTa: Molecular SMILES Representation Learning</h2>
<h3>Overview</h3>
<p><strong>ChemBERTa</strong> is a model that learns molecular SMILES strings with BERT.</p>
<p><strong>Features</strong>:
- <strong>RoBERTa</strong> base (improved BERT version)
- <strong>10M molecules</strong> pre-trained
- <strong>Transfer learning</strong> achieves high accuracy even with small data</p>
<h3>SMILES Tokenization</h3>
<pre><code class="language-python">from transformers import RobertaTokenizer

class SMILESTokenizer:
    def __init__(self):
        # ChemBERTa tokenizer
        self.tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")

    def encode(self, smiles_list):
        """
        Tokenize SMILES strings

        Args:
            smiles_list: List of SMILES
        Returns:
            input_ids: Token IDs
            attention_mask: Mask
        """
        encoded = self.tokenizer(
            smiles_list,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors='pt'
        )
        return encoded['input_ids'], encoded['attention_mask']

# Usage example
smiles_list = [
    'CCO',  # Ethanol
    'CC(C)Cc1ccc(cc1)C(C)C(=O)O',  # Ibuprofen
    'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'  # Caffeine
]

tokenizer = SMILESTokenizer()
input_ids, attention_mask = tokenizer.encode(smiles_list)

print(f"Input IDs shape: {input_ids.shape}")
print(f"Attention mask shape: {attention_mask.shape}")
print(f"First molecule tokens: {input_ids[0][:10]}")
</code></pre>
<h3>Using ChemBERTa Model</h3>
<pre><code class="language-python">from transformers import RobertaModel

class ChemBERTaEmbedding(nn.Module):
    def __init__(self, pretrained_model="seyonec/ChemBERTa-zinc-base-v1"):
        super(ChemBERTaEmbedding, self).__init__()
        self.bert = RobertaModel.from_pretrained(pretrained_model)

    def forward(self, input_ids, attention_mask):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, hidden_size)
        """
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        # Use [CLS] token embedding
        cls_embedding = outputs.last_hidden_state[:, 0, :]

        return cls_embedding

# Molecular property prediction model
class MoleculePropertyPredictor(nn.Module):
    def __init__(self, hidden_size=768, num_properties=1):
        super(MoleculePropertyPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()
        self.predictor = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_properties)
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        predictions = self.predictor(embeddings)
        return predictions

# Usage example
model = MoleculePropertyPredictor(num_properties=1)  # e.g., logP prediction
predictions = model(input_ids, attention_mask)
print(f"Predictions shape: {predictions.shape}")  # (3, 1)
</code></pre>
<hr/>
<h2>2.5 Perceiver IO: Diverse Data Integration</h2>
<h3>Overview</h3>
<p><strong>Perceiver IO</strong> is a Transformer that can integrate and process different types of data.</p>
<p><strong>Applications in materials science</strong>:
- Structural data + compositional data
- Experimental data + computational data
- Images + text + numerical values</p>
<h3>Architecture</h3>
<div class="mermaid">
flowchart TB
    A1[Structural Data] --&gt; C[Cross-Attention]
    A2[Compositional Data] --&gt; C
    A3[Experimental Data] --&gt; C

    B[Latent Array] --&gt; C
    C --&gt; D[Latent Transformer]
    D --&gt; E[Cross-Attention Decoder]
    E --&gt; F[Prediction Results]

    style C fill:#e1f5ff
    style D fill:#ffe1e1
</div>
<h3>Simple Implementation</h3>
<pre><code class="language-python">class PerceiverBlock(nn.Module):
    def __init__(self, latent_dim, input_dim, num_latents=64):
        super(PerceiverBlock, self).__init__()
        self.num_latents = num_latents
        self.latent_dim = latent_dim

        # Latent array (learnable)
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        # Cross-Attention: Latent ‚Üí Input
        self.cross_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # Self-Attention: Latent ‚Üí Latent
        self.self_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # Embed input
        self.input_projection = nn.Linear(input_dim, latent_dim)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, input_dim) input data
        Returns:
            latents: (batch_size, num_latents, latent_dim)
        """
        batch_size = x.size(0)

        # Embed input
        x_embed = self.input_projection(x)

        # Replicate latents
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # Cross-Attention: Latent (Query) ‚Üê Input (Key, Value)
        latents, _ = self.cross_attn(latents, x_embed, x_embed)

        # Self-Attention: within Latent
        latents, _ = self.self_attn(latents, latents, latents)

        return latents

# Usage example: integrate structural and compositional data
batch_size = 2
seq_len = 20
input_dim = 128
latent_dim = 256

perceiver = PerceiverBlock(latent_dim, input_dim, num_latents=32)

# Structural data (e.g., atomic coordinates)
structure_data = torch.randn(batch_size, seq_len, input_dim)

latents = perceiver(structure_data)
print(f"Latent representation shape: {latents.shape}")  # (2, 32, 256)
</code></pre>
<hr/>
<h2>2.6 Implementation Exercise: Material Property Prediction with Matformer</h2>
<h3>Complete Implementation Example</h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Dataset
class MaterialsDataset(Dataset):
    def __init__(self, num_samples=100):
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # Dummy data (in practice, get from Materials Project, etc.)
        num_atoms = 8
        atomic_numbers = torch.randint(1, 30, (num_atoms,))  # Atomic numbers
        positions = torch.randn(num_atoms, 3)  # Atomic coordinates (√Ö)
        distance_matrix = torch.cdist(positions, positions)  # Distance matrix

        # Target: band gap (eV)
        target = torch.randn(1)

        return atomic_numbers, distance_matrix, target

# Matformer model (simplified)
class SimpleMatformer(nn.Module):
    def __init__(self, d_model=256, num_heads=8, num_layers=4):
        super(SimpleMatformer, self).__init__()

        self.atom_embedding = AtomEmbedding(num_atoms=118, d_model=d_model)

        self.layers = nn.ModuleList([
            MatformerBlock(d_model, num_heads)
            for _ in range(num_layers)
        ])

        self.pooling = nn.AdaptiveAvgPool1d(1)
        self.predictor = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, atomic_numbers, distance_matrix):
        # Atom embedding
        x = self.atom_embedding(atomic_numbers)

        # Matformer blocks
        for layer in self.layers:
            x = layer(x, distance_matrix)

        # Global pooling
        x = x.transpose(1, 2)  # (batch, d_model, num_atoms)
        x = self.pooling(x).squeeze(-1)  # (batch, d_model)

        # Prediction
        output = self.predictor(x)
        return output

# Training
def train_matformer():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Data
    dataset = MaterialsDataset(num_samples=100)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    # Model
    model = SimpleMatformer(d_model=256, num_heads=8, num_layers=4).to(device)

    # Optimization
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # Training loop
    model.train()
    for epoch in range(5):
        total_loss = 0
        for atomic_numbers, distance_matrix, target in dataloader:
            atomic_numbers = atomic_numbers.to(device)
            distance_matrix = distance_matrix.to(device)
            target = target.to(device)

            # Forward
            predictions = model(atomic_numbers, distance_matrix)
            loss = criterion(predictions, target)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

    return model

# Execute
trained_model = train_matformer()
</code></pre>
<hr/>
<h2>2.7 Summary</h2>
<h3>Key Points</h3>
<ol>
<li><strong>Matformer</strong>: Distance-aware attention, hierarchical structure</li>
<li><strong>CrystalFormer</strong>: Periodic boundary conditions, fractional coordinates, space groups</li>
<li><strong>ChemBERTa</strong>: SMILES representation learning, transfer learning</li>
<li><strong>Perceiver IO</strong>: Diverse data integration</li>
</ol>
<h3>Preparation for Next Chapter</h3>
<p>In Chapter 3, we'll learn about pre-trained models (MatBERT, MolBERT) and fine-tuning.</p>
<hr/>
<h2>üìù Exercises</h2>
<h3>Problem 1: Conceptual Understanding</h3>
<p>List three reasons why Distance-aware Attention is superior to regular Attention in materials science.</p>
<details>
<summary>Sample Answer</summary>

1. **Considering chemical bonds**: Reflects the physical law that interactions are stronger for closer interatomic distances
2. **Suppressing long-range interactions**: Reduces unnecessary attention to distant atoms, improving computational efficiency
3. **Improved interpretability**: Attention weights correspond to chemically meaningful bond strengths
</details>
<h3>Problem 2: Implementation</h3>
<p>Implement a simple function that calculates distances without considering periodic boundary conditions.</p>
<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    """
    Simple distance calculation (no periodic boundary conditions)

    Args:
        coords1: (num_atoms1, 3)
        coords2: (num_atoms2, 3)
    Returns:
        distances: (num_atoms1, num_atoms2)
    """
    # Implement here
    pass
</code></pre>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    diff = coords1.unsqueeze(1) - coords2.unsqueeze(0)
    distances = torch.norm(diff, dim=-1)
    return distances
</code></pre>
</details>
<h3>Problem 3: Application</h3>
<p>Design a model using ChemBERTa to predict molecular water solubility. Explain the necessary layers and structure.</p>
<details>
<summary>Sample Answer</summary>
<pre><code class="language-python">class SolubilityPredictor(nn.Module):
    def __init__(self):
        super(SolubilityPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()  # 768 dimensions

        self.predictor = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)  # Solubility (continuous value)
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        solubility = self.predictor(embeddings)
        return solubility
</code></pre>


**Design rationale**:
- Extract general molecular features with ChemBERTa
- Transform to solubility-specific representation with 3-layer fully connected network
- Prevent overfitting with Dropout
- Output is continuous value (e.g., log10(mol/L))
</details>
<hr/>
<h2>üìä Data Licenses and Terms of Use</h2>
<h3>Materials Datasets</h3>
<ul>
<li><strong>Materials Project</strong>: <a href="https://materialsproject.org/about/terms">CC BY 4.0</a></li>
<li>Computational data for 690,000+ materials</li>
<li>Citation: <code>Jain, A. et al. APL Materials 1, 011002 (2013)</code></li>
<li><strong>OQMD (Open Quantum Materials Database)</strong>: Academic use permitted</li>
<li>1M+ DFT calculation data</li>
<li>Citation: <code>Saal, J. E. et al. JOM 65, 1501-1509 (2013)</code></li>
<li><strong>AFLOW</strong>: <a href="http://aflowlib.org/">CC BY 4.0</a></li>
<li>3.5M+ materials data</li>
</ul>
<h3>Molecular/Chemical Datasets</h3>
<ul>
<li><strong>ZINC15</strong>: Free for academic use, contact required for commercial use</li>
<li>1B+ commercially available compounds</li>
<li><strong>ChEMBL</strong>: <a href="https://chembl.gitbook.io/chembl-interface-documentation/about#data-licensing">CC BY-SA 3.0</a></li>
<li>2M+ bioactive compounds</li>
<li><strong>QM9</strong>: <a href="https://figshare.com/collections/Quantum_chemistry_structures_and_properties_of_134_kilo_molecules/978904">CC0 1.0</a></li>
<li>130K molecular quantum chemistry calculation data</li>
</ul>
<h3>Crystal Structure Datasets</h3>
<ul>
<li><strong>COD (Crystallography Open Database)</strong>: Public domain</li>
<li>500K+ crystal structures</li>
<li><strong>ICSD (Inorganic Crystal Structure Database)</strong>: License purchase required</li>
<li>250K+ inorganic crystal structures</li>
</ul>
<hr/>
<h2>üîß Code Reproducibility Guidelines</h2>
<h3>Environment Setup (for materials science)</h3>
<pre><code class="language-python"># requirements.txt
torch==2.0.1
transformers==4.30.2
pymatgen==2023.8.10  # Materials science library
rdkit==2023.3.2  # Chemical computation
ase==3.22.1  # Atomic Simulation Environment
numpy==1.24.3
scipy==1.11.1
matplotlib==3.7.1
</code></pre>
<h3>Reproducible Distance Matrix Calculation</h3>
<pre><code class="language-python">import numpy as np
from pymatgen.core import Structure
from pymatgen.analysis.local_env import CrystalNN

def reproducible_distance_matrix(structure, cutoff=10.0, seed=42):
    """
    Reproducible distance matrix calculation

    Args:
        structure: pymatgen Structure object
        cutoff: Distance cutoff (√Ö)
        seed: Random seed
    """
    np.random.seed(seed)

    # Get atomic coordinates
    positions = structure.cart_coords

    # Calculate distance matrix (considering periodic boundary conditions)
    distance_matrix = structure.distance_matrix

    # Apply cutoff
    distance_matrix[distance_matrix &gt; cutoff] = cutoff

    return distance_matrix

# Usage example
from pymatgen.core import Lattice, Structure

# NaCl structure
lattice = Lattice.cubic(5.64)
species = ["Na", "Cl"]
coords = [[0, 0, 0], [0.5, 0.5, 0.5]]
structure = Structure(lattice, species, coords)

dist_matrix = reproducible_distance_matrix(structure)
print(f"Distance matrix shape: {dist_matrix.shape}")
</code></pre>
<h3>Matformer Configuration Parameters</h3>
<pre><code class="language-python"># Matformer configuration
matformer_config = {
    'model': {
        'd_model': 256,
        'num_heads': 8,
        'num_layers': 4,
        'd_ff': 1024,
        'dropout': 0.1,
        'max_atoms': 50,  # Maximum number of atoms
        'max_distance': 10.0  # Distance cutoff (√Ö)
    },
    'distance_embedding': {
        'num_gaussians': 50,
        'gaussian_start': 0.0,
        'gaussian_end': 10.0
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'scheduler': 'cosine_with_warmup',
        'warmup_epochs': 10
    },
    'seed': 42
}
</code></pre>
<hr/>
<h2>‚ö†Ô∏è Practical Pitfalls and Solutions</h2>
<h3>1. Periodic Boundary Condition Calculation Errors</h3>
<p><strong>Problem</strong>: Interatomic distances across lattice boundaries are not calculated correctly</p>
<pre><code class="language-python"># ‚ùå Wrong: simple Euclidean distance
def wrong_periodic_distance(pos1, pos2, lattice):
    return np.linalg.norm(pos1 - pos2)

# ‚úÖ Correct: minimum image convention
def correct_periodic_distance(frac1, frac2, lattice_matrix):
    """
    Distance calculation with periodic boundary conditions

    Args:
        frac1, frac2: Fractional coordinates (3,)
        lattice_matrix: Lattice vectors (3, 3)
    """
    # Minimum image convention: -0.5 &lt;= delta &lt; 0.5
    delta_frac = frac1 - frac2
    delta_frac = delta_frac - np.floor(delta_frac + 0.5)

    # Convert to Cartesian coordinates
    delta_cart = np.dot(delta_frac, lattice_matrix)

    return np.linalg.norm(delta_cart)

# Debugging method
print("Fractional coordinates:", frac1, frac2)
print("Delta (before wrapping):", frac1 - frac2)
print("Delta (after wrapping):", delta_frac)
print("Minimum distance:", correct_periodic_distance(frac1, frac2, lattice_matrix))
</code></pre>
<h3>2. Handling Missing Values in Atomic Number Embedding</h3>
<p><strong>Problem</strong>: Handling unknown atomic species or vacant sites</p>
<pre><code class="language-python"># ‚ùå Problem: error for unknown atoms
class NaiveAtomEmbedding(nn.Module):
    def __init__(self):
        self.embedding = nn.Embedding(118, 256)  # 118 elements

    def forward(self, atomic_numbers):
        return self.embedding(atomic_numbers)  # Error for 118+!

# ‚úÖ Solution: add unknown token
class RobustAtomEmbedding(nn.Module):
    def __init__(self, num_atoms=118, d_model=256):
        super().__init__()
        # +1 for unknown atoms
        self.embedding = nn.Embedding(num_atoms + 1, d_model, padding_idx=0)
        self.num_atoms = num_atoms

    def forward(self, atomic_numbers):
        # Clip out-of-range atomic numbers
        atomic_numbers = torch.clamp(atomic_numbers, 0, self.num_atoms)
        return self.embedding(atomic_numbers)

# Test
embedding = RobustAtomEmbedding()
test_atoms = torch.tensor([1, 6, 8, 200])  # 200 is out of range
output = embedding(test_atoms)
print(f"Output shape: {output.shape}")  # No error
</code></pre>
<h3>3. Memory Efficiency of Distance-aware Attention</h3>
<p><strong>Problem</strong>: Distance matrix is too large causing OOM</p>
<pre><code class="language-python"># ‚ùå Problem: hold distances for all atom pairs
def memory_intensive_distance_attention(x, all_distances):
    # all_distances: (batch, max_atoms, max_atoms)
    # Memory: batch * max_atoms^2 * 4 bytes
    pass

# ‚úÖ Solution: sparse distance matrix
def sparse_distance_attention(x, positions, cutoff=8.0):
    """
    Calculate only within cutoff distance

    Args:
        x: Atomic features (batch, num_atoms, d_model)
        positions: Atomic coordinates (batch, num_atoms, 3)
        cutoff: Distance cutoff (√Ö)
    """
    batch_size, num_atoms, _ = positions.shape

    # Calculate distance matrix
    diff = positions.unsqueeze(2) - positions.unsqueeze(1)
    distances = torch.norm(diff, dim=-1)

    # Cutoff mask
    mask = distances &lt; cutoff

    # Attention scores (outside cutoff is -inf)
    scores = compute_attention_scores(x)
    scores = scores.masked_fill(~mask, float('-inf'))

    return scores
</code></pre>
<h3>4. Special Character Handling in SMILES Tokenization</h3>
<p><strong>Problem</strong>: Aromaticity and stereochemistry symbols are lost</p>
<pre><code class="language-python"># ‚ùå Wrong: only alphanumeric
def wrong_smiles_tokenize(smiles):
    return list(filter(str.isalnum, smiles))

# ‚úÖ Correct: preserve special characters
def correct_smiles_tokenize(smiles):
    """
    Complete SMILES tokenization

    Aromaticity: c, n, o, s (lowercase)
    Stereochemistry: @, @@
    Branching: (), []
    Bonds: -, =, #, :, /,
    """
    import re
    pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'
    tokens = re.findall(pattern, smiles)

    return tokens

# Test: complex SMILES
smiles = "C[C@H](N)C(=O)O"  # L-alanine (with stereochemistry)
tokens = correct_smiles_tokenize(smiles)
print(f"Tokens: {tokens}")
# ['C', '[C@H]', '(', 'N', ')', 'C', '(', '=', 'O', ')', 'O']
</code></pre>
<h3>5. Space Group Encoding in CrystalFormer</h3>
<p><strong>Problem</strong>: Efficient representation of 230 space groups</p>
<pre><code class="language-python"># ‚ùå Inefficient: One-hot encoding (230 dimensions)
def inefficient_space_group_encoding(space_group_number):
    encoding = torch.zeros(230)
    encoding[space_group_number - 1] = 1
    return encoding

# ‚úÖ Efficient: Learnable embedding
class SpaceGroupEmbedding(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        # 230 space groups + unknown
        self.embedding = nn.Embedding(231, d_model)

    def forward(self, space_group_numbers):
        """
        Args:
            space_group_numbers: (batch,) space group numbers 1-230
        """
        return self.embedding(space_group_numbers)

# Further: use hierarchical structure of space groups
class HierarchicalSpaceGroupEmbedding(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        # Crystal system (7 types)
        self.crystal_system_emb = nn.Embedding(8, d_model // 2)
        # Point group (32 types)
        self.point_group_emb = nn.Embedding(33, d_model // 2)

    def forward(self, crystal_system, point_group):
        cs_emb = self.crystal_system_emb(crystal_system)
        pg_emb = self.point_group_emb(point_group)
        return torch.cat([cs_emb, pg_emb], dim=-1)
</code></pre>
<hr/>
<h2>‚úÖ Chapter 2 Completion Checklist</h2>
<h3>Conceptual Understanding (10 items)</h3>
<ul>
<li>[ ] Can explain differences between general-purpose and materials-specific Transformers</li>
<li>[ ] Understands Matformer's hierarchical architecture</li>
<li>[ ] Understands principles of Distance-aware Attention</li>
<li>[ ] Understands importance of periodic boundary conditions</li>
<li>[ ] Understands conversion between fractional and Cartesian coordinates</li>
<li>[ ] Understands CrystalFormer's space group encoding</li>
<li>[ ] Understands relationship between ChemBERTa and RoBERTa</li>
<li>[ ] Understands Perceiver IO's cross-attention mechanism</li>
<li>[ ] Can list 3+ reasons why materials-specific Transformers are necessary</li>
<li>[ ] Can explain application scenarios for each model (Matformer, CrystalFormer, ChemBERTa)</li>
</ul>
<h3>Mathematical/Physical Understanding (5 items)</h3>
<ul>
<li>[ ] Can derive distance calculation formula under periodic boundary conditions</li>
<li>[ ] Can write transformation matrix from fractional to Cartesian coordinates</li>
<li>[ ] Understands mathematical formulation of distance embedding with Gaussian basis functions</li>
<li>[ ] Understands relationship between lattice vectors and reciprocal lattice vectors</li>
<li>[ ] Understands concept of Wyckoff positions</li>
</ul>
<h3>Implementation Skills (15 items)</h3>
<ul>
<li>[ ] Can implement <code>AtomEmbedding</code> class</li>
<li>[ ] Can implement <code>DistanceAwareAttention</code></li>
<li>[ ] Can implement <code>MatformerBlock</code></li>
<li>[ ] Can implement distance calculation considering periodic boundary conditions</li>
<li>[ ] Can implement fractional coordinate encoding</li>
<li>[ ] Can use <code>ChemBERTaEmbedding</code></li>
<li>[ ] Can implement/use SMILES tokenizer</li>
<li>[ ] Can implement <code>PerceiverBlock</code></li>
<li>[ ] Can read crystal structures using PyMatGen</li>
<li>[ ] Can retrieve Materials Project data</li>
<li>[ ] Can visualize distance matrices</li>
<li>[ ] Can interpret attention weights from materials science perspective</li>
<li>[ ] Can handle different numbers of atoms in batch processing (padding)</li>
<li>[ ] Can implement model training loop</li>
<li>[ ] Can evaluate prediction results (MAE, RMSE)</li>
</ul>
<h3>Debugging Skills (5 items)</h3>
<ul>
<li>[ ] Can detect periodic boundary condition calculation errors</li>
<li>[ ] Can handle atomic number out-of-range errors</li>
<li>[ ] Can identify and solve memory efficiency problems</li>
<li>[ ] Can debug SMILES tokenization errors</li>
<li>[ ] Can verify distance matrix symmetry</li>
</ul>
<h3>Application Ability (5 items)</h3>
<ul>
<li>[ ] Can apply Matformer to new material property prediction tasks</li>
<li>[ ] Can apply ChemBERTa to molecular property prediction</li>
<li>[ ] Can integrate multiple data sources (structure+composition)</li>
<li>[ ] Can extend existing models with new features</li>
<li>[ ] Can propose how to apply prediction results to experimental design</li>
</ul>
<h3>Data Processing (5 items)</h3>
<ul>
<li>[ ] Can preprocess Materials Project data</li>
<li>[ ] Can canonicalize SMILES</li>
<li>[ ] Can standardize crystal structures (primitive cell)</li>
<li>[ ] Can implement data augmentation</li>
<li>[ ] Can properly split train/validation/test data</li>
</ul>
<h3>Theoretical Background (5 items)</h3>
<ul>
<li>[ ] Read Matformer paper (Chen et al., 2022)</li>
<li>[ ] Read ChemBERTa paper</li>
<li>[ ] Read Perceiver paper</li>
<li>[ ] Understands crystallography basics (Bravais lattices, space groups)</li>
<li>[ ] Understands materials science basics (bonding, lattice defects)</li>
</ul>
<h3>Completion Criteria</h3>
<ul>
<li><strong>Minimum standard</strong>: 40+ items achieved (80%)</li>
<li><strong>Recommended standard</strong>: 45+ items achieved (90%)</li>
<li><strong>Excellent standard</strong>: All 50 items achieved (100%)</li>
</ul>
<hr/>
<p><strong>Next Chapter</strong>: <strong><a href="chapter-3.html">Chapter 3: Pre-trained Models and Transfer Learning</a></strong></p>
<hr/>
<p><strong>Author</strong>: Yusuke Hashimoto (Tohoku University)
<strong>Last Updated</strong>: October 19, 2025</p><div class="navigation">
<a class="nav-button" href="chapter-1.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Back to Series Index</a>
<a class="nav-button" href="chapter-3.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantee, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content shall follow the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
