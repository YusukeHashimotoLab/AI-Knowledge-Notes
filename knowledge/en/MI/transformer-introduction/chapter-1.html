<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1 - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/transformer-introduction/index.html">Transformer</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/transformer-introduction/chapter-1.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 1: Transformer Revolution and Materials Science</h1>
<p class="subtitle">Understanding Self-Attention Fundamentals and Applications in Materials</p>
<div class="meta">
<span class="meta-item">üìñ Reading time: 20-25 min</span>
<span class="meta-item">üìä Level: Beginner</span>
<span class="meta-item">üíª Code examples: 6</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 1: Transformer Revolution and Materials Science</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Gain high-level understanding of Self-Attention fundamentals and why it's applicable to materials. Learn transfer patterns from existing domains.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Supplement:</strong> Attention is a mechanism to "spotlight" essential parts. It's applicable to sequences and graph information in materials.</p>
<p><strong>Study time</strong>: 20-30 minutes | <strong>Level</strong>: Intermediate</p>
<h2>üìã What You'll Learn in This Chapter</h2>
<ul>
<li>Principles and mathematical understanding of Attention mechanism</li>
<li>How Self-Attention and Multi-Head Attention work</li>
<li>Why Transformers outperform RNN/CNN</li>
<li>Basic structure and differences of BERT and GPT</li>
<li>Success stories in materials science</li>
</ul>
<hr/>
<h2>1.1 Why Transformers Caused a Revolution</h2>
<h3>Limitations of Traditional RNN/CNN</h3>
<p><strong>Problems with RNN (Recurrent Neural Network)</strong>:
- Vanishing/exploding gradients in long sequences
- Difficult to parallelize (requires sequential processing)
- Hard to capture long-range dependencies</p>
<p><strong>Problems with CNN (Convolutional Neural Network)</strong>:
- Only captures local features
- Requires deep layers to capture long-range relationships
- Unsuitable for irregular structures like molecules and materials</p>
<h3>Transformer's Innovations</h3>
<p><strong>Introduced in 2017 with "Attention Is All You Need" paper</strong>:
- ‚úÖ <strong>Directly models relationships between all elements</strong> (Attention mechanism)
- ‚úÖ <strong>Fully parallelizable</strong> (maximizes GPU utilization)
- ‚úÖ <strong>Efficiently captures long-range dependencies</strong>
- ‚úÖ <strong>Interpretability</strong> (visualize important parts via Attention weights)</p>
<div class="mermaid">
flowchart LR
    A[Input Sequence] --&gt; B[Self-Attention]
    B --&gt; C[Feed Forward]
    C --&gt; D[Output]

    B -.-&gt; E[Computes relationships between all elements]
    E -.-&gt; B

    style B fill:#e1f5ff
</div>
<hr/>
<h2>1.2 Principles of Attention Mechanism</h2>
<h3>What is Attention?</h3>
<p><strong>Basic concept</strong>: A mechanism to learn "where to focus" in the input</p>
<p><strong>Formula</strong>:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<ul>
<li><strong>Q (Query)</strong>: "What are you looking for"</li>
<li><strong>K (Key)</strong>: "What do you have"</li>
<li><strong>V (Value)</strong>: "Actual content"</li>
<li>$d_k$: Key dimension (scaling factor)</li>
</ul>
<h3>Intuitive Understanding</h3>
<p><strong>Library analogy</strong>:
- <strong>Query</strong>: "Looking for books on machine learning"
- <strong>Key</strong>: Table of contents/title of each book
- <strong>Value</strong>: Actual content of the book
- <strong>Attention</strong>: "Focus" on and read highly relevant books</p>
<h3>Python Implementation: Basic Attention</h3>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)
        mask: Optional mask
    """
    d_k = Q.size(-1)

    # 1. Compute dot product of Q and K (similarity)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    # scores shape: (batch_size, seq_len_q, seq_len_k)

    # 2. Apply mask (if needed)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 3. Normalize with Softmax (Attention weights)
    attention_weights = F.softmax(scores, dim=-1)

    # 4. Weighted sum of Value with Attention weights
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

# Usage example
batch_size, seq_len, d_model = 2, 5, 64
Q = torch.randn(batch_size, seq_len, d_model)
K = torch.randn(batch_size, seq_len, d_model)
V = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = scaled_dot_product_attention(Q, K, V)
print(f"Output shape: {output.shape}")  # (2, 5, 64)
print(f"Attention weights shape: {attn_weights.shape}")  # (2, 5, 5)
</code></pre>
<h3>Visualizing Attention Weights</h3>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens=None):
    """
    Visualize Attention weights as a heatmap

    Args:
        attention_weights: (seq_len, seq_len) Attention weights
        tokens: List of tokens (optional)
    """
    plt.figure(figsize=(8, 6))

    # Get Attention weights from first sample, first head
    attn = attention_weights[0].detach().numpy()

    sns.heatmap(attn, cmap='YlOrRd', cbar=True, square=True,
                xticklabels=tokens if tokens else range(attn.shape[0]),
                yticklabels=tokens if tokens else range(attn.shape[0]))

    plt.xlabel('Key (reference)')
    plt.ylabel('Query (focus)')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.show()

# Usage example
tokens = ['H', 'C', 'C', 'O', 'H']
visualize_attention(attn_weights, tokens)
</code></pre>
<hr/>
<h2>1.3 Self-Attention: Self-Attention Mechanism</h2>
<h3>What is Self-Attention?</h3>
<p><strong>Definition</strong>: Applying Attention to the input sequence itself</p>
<p><strong>Features</strong>:
- Query, Key, and Value all generated from the same input
- Directly models relationships between any two elements in the sequence
- Focuses on highly related elements regardless of position</p>
<h3>Self-Attention Example in Molecules</h3>
<p><strong>Example of Methanol (CH‚ÇÉOH)</strong>:</p>
<pre><code class="language-python"># Atoms: C, H, H, H, O, H
# Self-Attention learns relationships between each atom and others
# Example: O atom has strong relationship with C atom
</code></pre>
<h3>Self-Attention Implementation</h3>
<pre><code class="language-python">import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()
        self.d_model = d_model

        # Linear transformations to Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        # Generate Q, K, V
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        # Scaled Dot-Product Attention
        output, attn_weights = scaled_dot_product_attention(Q, K, V)

        return output, attn_weights

# Usage example
d_model = 128
seq_len = 10
batch_size = 4

self_attn = SelfAttention(d_model)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = self_attn(x)

print(f"Input shape: {x.shape}")          # (4, 10, 128)
print(f"Output shape: {output.shape}")    # (4, 10, 128)
print(f"Attention shape: {attn_weights.shape}")  # (4, 10, 10)
</code></pre>
<hr/>
<h2>1.4 Multi-Head Attention: Multi-Head Attention Mechanism</h2>
<h3>Why Multi-Head is Needed</h3>
<p><strong>Limitations of Single Attention Head</strong>:
- Can only view relationships from one perspective
- Cannot capture complex relationships (chemical bonds, conformations, etc.)</p>
<p><strong>Advantages of Multi-Head Attention</strong>:
- Learns relationships from multiple different perspectives
- Each head captures different features (bonds, distances, angles, etc.)
- Enables richer representations</p>
<h3>Formula</h3>
<p>$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$</p>
<p>where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>
<h3>Implementation</h3>
<pre><code class="language-python">class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Q, K, V transformations
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Output transformation
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size = x.size(0)

        # 1. Generate Q, K, V and split by heads
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Shape: (batch_size, num_heads, seq_len, d_k)

        # 2. Scaled Dot-Product Attention for each head
        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        # output: (batch_size, num_heads, seq_len, d_k)

        # 3. Concatenate heads
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # Shape: (batch_size, seq_len, d_model)

        # 4. Output transformation
        output = self.W_o(output)

        return output, attn_weights

# Usage example
d_model = 512
num_heads = 8
seq_len = 20
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)
output, attn_weights = mha(x)

print(f"Input shape: {x.shape}")          # (2, 20, 512)
print(f"Output shape: {output.shape}")    # (2, 20, 512)
print(f"Attention shape: {attn_weights.shape}")  # (2, 8, 20, 20)
</code></pre>
<hr/>
<h2>1.5 Positional Encoding: Embedding Positional Information</h2>
<h3>Why is it Needed?</h3>
<p><strong>Problem</strong>: Self-Attention has no notion of order
- Cannot distinguish "H-C-O" from "O-C-H"
- In molecules and materials, atomic arrangement order is important</p>
<p><strong>Solution</strong>: Add positional information via Positional Encoding</p>
<h3>Formula</h3>
<p>$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>
<p>$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>
<h3>Implementation</h3>
<pre><code class="language-python">class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x

# Usage example and visualization
d_model = 128
max_len = 100

pos_enc = PositionalEncoding(d_model, max_len)

# Dummy input
x = torch.zeros(1, 50, d_model)
output = pos_enc(x)

# Visualization
plt.figure(figsize=(12, 4))
plt.plot(pos_enc.pe[0, :50, :8].numpy())
plt.xlabel('Position')
plt.ylabel('Encoding Value')
plt.title('Positional Encoding (first 8 dimensions)')
plt.legend([f'dim {i}' for i in range(8)])
plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>1.6 Transformer and BERT/GPT</h2>
<h3>Complete Transformer Architecture</h3>
<div class="mermaid">
flowchart TB
    subgraph Encoder
        E1[Input Embedding] --&gt; E2[Positional Encoding]
        E2 --&gt; E3[Multi-Head Attention]
        E3 --&gt; E4[Add and Norm]
        E4 --&gt; E5[Feed Forward]
        E5 --&gt; E6[Add and Norm]
    end

    subgraph Decoder
        D1[Output Embedding] --&gt; D2[Positional Encoding]
        D2 --&gt; D3[Masked Multi-Head Attention]
        D3 --&gt; D4[Add and Norm]
        D4 --&gt; D5[Multi-Head Attention]
        D5 --&gt; D6[Add and Norm]
        D6 --&gt; D7[Feed Forward]
        D7 --&gt; D8[Add and Norm]
    end

    E6 -.-&gt; D5
    D8 --&gt; O[Output]

    style E3 fill:#e1f5ff
    style D3 fill:#ffe1e1
    style D5 fill:#e1ffe1
</div>
<h3>BERT (Bidirectional Encoder Representations from Transformers)</h3>
<p><strong>Features</strong>:
- Uses <strong>Encoder only</strong>
- Understands context <strong>bidirectionally</strong>
- <strong>Pre-training tasks</strong>: Masked Language Model (MLM) + Next Sentence Prediction (NSP)
- <strong>Applications</strong>: Classification, feature extraction, question answering</p>
<p><strong>Applications in materials science</strong>:
- MatBERT: Property prediction from materials composition
- ChemBERTa: Molecular SMILES representation learning</p>
<h3>GPT (Generative Pre-trained Transformer)</h3>
<p><strong>Features</strong>:
- Uses <strong>Decoder only</strong>
- Generates text <strong>unidirectionally</strong> (left to right)
- <strong>Pre-training task</strong>: Next word prediction
- <strong>Applications</strong>: Text generation, dialogue, creative tasks</p>
<p><strong>Applications in materials science</strong>:
- Molecular generation (SMILES string generation)
- Automated generation of materials descriptions
- Synthesis route proposals</p>
<hr/>
<h2>1.7 Success Stories in Materials Science</h2>
<h3>1. ChemBERTa: Molecular Representation Learning</h3>
<p><strong>Overview</strong>: Learning SMILES with BERT</p>
<pre><code class="language-python"># Molecular SMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O (Ibuprofen)
# Convert to embedding vector with ChemBERTa ‚Üí Property prediction
</code></pre>
<p><strong>Achievements</strong>:
- High-accuracy prediction with small-scale data
- Shortened development time through transfer learning
- Interpretability (visualize important parts via Attention)</p>
<h3>2. Matformer: Materials Property Prediction</h3>
<p><strong>Overview</strong>: Processing crystal structures with Transformer</p>
<pre><code class="language-python"># Input: Atomic coordinates, atomic numbers, lattice constants
# Output: Bandgap, formation energy
</code></pre>
<p><strong>Achievements</strong>:
- High accuracy on Materials Project data
- Performance equal to or better than GNN
- Good computational efficiency</p>
<h3>3. Molecular Generation via Diffusion Models</h3>
<p><strong>Overview</strong>: Novel molecular generation with conditional diffusion models</p>
<pre><code class="language-python"># Conditions: Solubility &gt; 5 mg/mL, LogP &lt; 3
# Generation: Molecular SMILES satisfying conditions
</code></pre>
<p><strong>Achievements</strong>:
- Discovery of promising candidate molecules in drug discovery
- Higher diversity than conventional methods
- Considers synthesizability</p>
<hr/>
<h2>1.8 Summary</h2>
<h3>Key Points</h3>
<ol>
<li><strong>Attention mechanism</strong>: Directly models relationships between arbitrary elements in a sequence</li>
<li><strong>Self-Attention</strong>: Attention on the input sequence itself</li>
<li><strong>Multi-Head Attention</strong>: Learns relationships from multiple perspectives</li>
<li><strong>Positional Encoding</strong>: Embeds positional information</li>
<li><strong>BERT/GPT</strong>: Representative Transformer-based pre-trained models</li>
<li><strong>Materials science applications</strong>: Molecular/materials representation learning, property prediction, generative models</li>
</ol>
<h3>Preparing for the Next Chapter</h3>
<p>In Chapter 2, we'll learn in detail about Transformer architectures specialized for materials science (Matformer, CrystalFormer, ChemBERTa).</p>
<hr/>
<h2>üìù Exercises</h2>
<h3>Problem 1: Conceptual Understanding</h3>
<p>Explain the roles of Query, Key, and Value in the Attention mechanism using an analogy other than the library example.</p>
<details>
<summary>Sample Solution</summary>

**Search Engine Analogy**:
- **Query**: Search keywords entered by user
- **Key**: Metadata of each webpage (title, summary)
- **Value**: Actual content of webpages
- **Attention**: Display highly relevant pages at top of results

**Molecular Analogy**:
- **Query**: "Which atoms does a certain atom want to interact with"
- **Key**: Features of each atom (atomic number, charge, position)
- **Value**: Detailed information of each atom
- **Attention**: Represents strength of chemical bonds or interactions
</details>
<h3>Problem 2: Implementation (Coding)</h3>
<p>Fill in the blanks to implement Simple Attention (without scaling, without mask).</p>
<pre><code class="language-python">def simple_attention(Q, K, V):
    """
    Simple Attention mechanism

    Args:
        Q: Query (batch_size, seq_len, d_k)
        K: Key (batch_size, seq_len, d_k)
        V: Value (batch_size, seq_len, d_v)

    Returns:
        output: (batch_size, seq_len, d_v)
        attention_weights: (batch_size, seq_len, seq_len)
    """
    # 1. Compute dot product of Q and K
    scores = torch.matmul(______, ______.transpose(-2, -1))

    # 2. Normalize with Softmax
    attention_weights = F.softmax(______, dim=-1)

    # 3. Weighted sum of Value with Attention weights
    output = torch.matmul(______, ______)

    return output, attention_weights
</code></pre>
<details>
<summary>Sample Solution</summary>
<pre><code class="language-python">def simple_attention(Q, K, V):
    # 1. Compute dot product of Q and K
    scores = torch.matmul(Q, K.transpose(-2, -1))

    # 2. Normalize with Softmax
    attention_weights = F.softmax(scores, dim=-1)

    # 3. Weighted sum of Value with Attention weights
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
</code></pre>
</details>
<h3>Problem 3: Application (Reflection)</h3>
<p>Consider Self-Attention for molecule "CCO" (Ethanol). Answer the following questions:</p>
<ol>
<li>Which atomic pair do you expect to have the highest Attention weight?</li>
<li>Explain the reason from a chemical perspective.</li>
<li>In Multi-Head Attention, what different types of information might each head capture?</li>
</ol>
<details>
<summary>Sample Solution</summary>

1. **Highest Attention weight**: C-C bond, C-O bond

2. **Chemical reasons**:
   - Strong interaction due to covalent bonds
   - High electron density from electron sharing
   - O atom forms polar bond with C atom

3. **Examples of information captured by each head**:
   - **Head 1**: Chemical bonds (primary bonds)
   - **Head 2**: Secondary bonds (C-C-O angle)
   - **Head 3**: Electron density distribution
   - **Head 4**: Atom types (C vs O vs H)
   - **Head 5**: Conformational information
   - **Head 6**: Polar interactions

   Each head understands the molecule from different perspectives, enabling richer representations.
</details>
<hr/>
<h2>üìä Data Licenses and Terms of Use</h2>
<h3>Language Datasets</h3>
<ul>
<li><strong>WikiText-103</strong>: <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a></li>
<li><strong>BookCorpus</strong>: Research purposes only, no redistribution</li>
<li><strong>Common Crawl</strong>: <a href="https://commoncrawl.org/terms-of-use/">Common Crawl Terms of Use</a></li>
</ul>
<h3>Materials Science Datasets</h3>
<ul>
<li><strong>Materials Project</strong>: <a href="https://materialsproject.org/about/terms">CC BY 4.0</a></li>
<li>Paper citation: <code>Jain, A. et al. APL Materials 1, 011002 (2013)</code></li>
<li><strong>SMILES Molecular Data</strong>:</li>
<li><strong>ZINC</strong>: Free for academic use, check for commercial</li>
<li><strong>ChEMBL</strong>: <a href="https://chembl.gitbook.io/chembl-interface-documentation/about#data-licensing">CC BY-SA 3.0</a></li>
<li><strong>PubChem</strong>: Public domain</li>
<li><strong>Crystal Structure Data</strong>:</li>
<li><strong>ICSD</strong>: License purchase required</li>
<li><strong>COD (Crystallography Open Database)</strong>: Public domain</li>
</ul>
<h3>Best Practices for License Compliance</h3>
<pre><code class="language-python"># Example citation when using dataset
"""
This work uses data from Materials Project (materialsproject.org),
which is released under CC BY 4.0 license.

Citation:
Jain, A., Ong, S. P., Hautier, G., Chen, W., Richards, W. D.,
Dacek, S., ... &amp; Persson, K. A. (2013).
Commentary: The Materials Project: A materials genome approach
to accelerating materials innovation. APL materials, 1(1).
"""
</code></pre>
<hr/>
<h2>üîß Code Reproducibility Guidelines</h2>
<h3>Environment Setup</h3>
<pre><code class="language-python"># requirements.txt
torch==2.0.1
transformers==4.30.2
numpy==1.24.3
matplotlib==3.7.1
seaborn==0.12.2

# Recommended: Complete environment reproduction
# conda env export &gt; environment.yml
</code></pre>
<h3>Settings for Reproducibility</h3>
<pre><code class="language-python">import torch
import numpy as np
import random

def set_seed(seed=42):
    """
    Ensure complete reproducibility

    Args:
        seed: Random seed
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # Make CuDNN behavior deterministic (reduces speed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Usage example
set_seed(42)

# Record version information
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
</code></pre>
<h3>Explicit Transformer Parameters</h3>
<pre><code class="language-python"># Manage experiment settings in dictionary
config = {
    'model': {
        'd_model': 512,
        'num_heads': 8,
        'num_layers': 6,
        'd_ff': 2048,
        'dropout': 0.1,
        'max_seq_len': 512
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'warmup_steps': 4000,
        'optimizer': 'Adam',
        'weight_decay': 0.01
    },
    'data': {
        'train_split': 0.8,
        'val_split': 0.1,
        'test_split': 0.1,
        'tokenizer': 'BPE',
        'vocab_size': 50000
    },
    'seed': 42
}

# Save configuration
import json
with open('experiment_config.json', 'w') as f:
    json.dump(config, f, indent=2)
</code></pre>
<h3>Detailed Attention Parameter Settings</h3>
<pre><code class="language-python">class ReproducibleMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1, bias=True):
        """
        Reproducibility-focused Multi-Head Attention

        Args:
            d_model: Model dimension (512 recommended)
            num_heads: Number of heads (8 recommended, must divide d_model)
            dropout: Dropout rate (0.1 recommended)
            bias: Whether to use bias in linear layers
        """
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Explicit initialization method
        self.W_q = nn.Linear(d_model, d_model, bias=bias)
        self.W_k = nn.Linear(d_model, d_model, bias=bias)
        self.W_v = nn.Linear(d_model, d_model, bias=bias)
        self.W_o = nn.Linear(d_model, d_model, bias=bias)

        # Xavier initialization
        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:
            nn.init.xavier_uniform_(module.weight)
            if bias:
                nn.init.zeros_(module.bias)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Implementation same as previous code
        pass
</code></pre>
<hr/>
<h2>‚ö†Ô∏è Practical Pitfalls and Solutions</h2>
<h3>1. Attention Mask Errors</h3>
<p><strong>Problem</strong>: Future information leakage due to incorrect mask application</p>
<pre><code class="language-python"># ‚ùå Wrong: Mask incorrectly applied
def wrong_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1))
    # Mask values reversed
    scores = scores.masked_fill(mask == 1, -1e9)  # Wrong!
    return F.softmax(scores, dim=-1)

# ‚úÖ Correct: Mask ignores positions with 0
def correct_attention(Q, K, V, mask):
    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(Q.size(-1))
    if mask is not None:
        # Set positions with mask==0 to -inf
        scores = scores.masked_fill(mask == 0, float('-inf'))
    return F.softmax(scores, dim=-1)

# Debugging method
print("Attention scores before mask:", scores)
print("Mask:", mask)
print("Attention scores after mask:", scores.masked_fill(mask == 0, float('-inf')))
</code></pre>
<h3>2. Positional Encoding Implementation Errors</h3>
<p><strong>Problem</strong>: Wrong dimension assignment for sin and cos</p>
<pre><code class="language-python"># ‚ùå Wrong: Dimension assignment reversed
class WrongPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 1::2] = torch.sin(position * div_term)  # Wrong!
        pe[:, 0::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

# ‚úÖ Correct: sin for even dimensions, cos for odd dimensions
class CorrectPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # Correct
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
</code></pre>
<h3>3. Memory Overflow</h3>
<p><strong>Problem</strong>: OOM (Out of Memory) with long sequences</p>
<pre><code class="language-python"># ‚ùå Problem: Process entire sequence at once
def memory_intensive_attention(x):
    # x: (batch=64, seq_len=10000, d_model=512)
    # Attention matrix: (64, 10000, 10000) = approximately 24GB!
    return multi_head_attention(x)

# ‚úÖ Solution 1: Gradient checkpointing
from torch.utils.checkpoint import checkpoint

def memory_efficient_attention(x):
    return checkpoint(multi_head_attention, x)

# ‚úÖ Solution 2: Split sequence
def chunked_attention(x, chunk_size=512):
    batch, seq_len, d_model = x.shape
    outputs = []

    for i in range(0, seq_len, chunk_size):
        chunk = x[:, i:i+chunk_size, :]
        output = multi_head_attention(chunk)
        outputs.append(output)

    return torch.cat(outputs, dim=1)

# ‚úÖ Solution 3: Sparse Attention (for long-range tasks)
# Use libraries like Longformer, BigBird
</code></pre>
<h3>4. Tokenization Issues (Materials Science Specific)</h3>
<p><strong>Problem</strong>: SMILES brackets and branches not processed correctly</p>
<pre><code class="language-python"># ‚ùå Wrong: Simple character splitting
def wrong_smiles_tokenize(smiles):
    return list(smiles)  # "C(C)O" ‚Üí ['C', '(', 'C', ')', 'O']

# ‚úÖ Correct: Use SMILES tokenizer
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")

# Or regex-based
import re
def correct_smiles_tokenize(smiles):
    pattern = r'(\[[^\]]+\]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;|\*|\$|\%[0-9]{2}|[0-9])'
    return re.findall(pattern, smiles)

# Test
smiles = "CC(C)Cc1ccc(cc1)C(C)C(=O)O"  # Ibuprofen
tokens = correct_smiles_tokenize(smiles)
print(f"Tokens: {tokens}")
</code></pre>
<h3>5. Numerical Instability</h3>
<p><strong>Problem</strong>: Softmax overflow/underflow</p>
<pre><code class="language-python"># ‚ùå Problem: Large scores cause exp() overflow
def unstable_softmax(x):
    return torch.exp(x) / torch.sum(torch.exp(x), dim=-1, keepdim=True)

# ‚úÖ Solution: Numerically stable softmax (PyTorch implements internally)
def stable_softmax(x):
    # Subtract maximum value for numerical stability
    x_max = torch.max(x, dim=-1, keepdim=True)[0]
    exp_x = torch.exp(x - x_max)
    return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)

# Using PyTorch's F.softmax() is safest
import torch.nn.functional as F
safe_output = F.softmax(x, dim=-1)
</code></pre>
<hr/>
<h2>‚úÖ Chapter 1 Completion Checklist</h2>
<h3>Conceptual Understanding (10 items)</h3>
<ul>
<li>[ ] Can explain roles of Query, Key, Value in Attention mechanism</li>
<li>[ ] Understand difference between Self-Attention and regular Attention</li>
<li>[ ] Can explain why Multi-Head Attention needs multiple heads</li>
<li>[ ] Understand necessity of Positional Encoding</li>
<li>[ ] Can explain why Transformer is parallelizable</li>
<li>[ ] Can list 3+ advantages of Transformer over RNN/CNN</li>
<li>[ ] Understand difference between BERT and GPT (Encoder vs Decoder)</li>
<li>[ ] Know meaning of scaling factor (‚àöd_k) in Scaled Dot-Product Attention</li>
<li>[ ] Understand what can be learned from visualizing Attention weights</li>
<li>[ ] Can explain why Transformers are effective for materials science</li>
</ul>
<h3>Mathematical Understanding (5 items)</h3>
<ul>
<li>[ ] Can write Attention(Q,K,V) formula</li>
<li>[ ] Understand Positional Encoding formula</li>
<li>[ ] Understand Multi-Head Attention formula</li>
<li>[ ] Understand Softmax formula and meaning</li>
<li>[ ] Can correctly calculate matrix dimensions (shape)</li>
</ul>
<h3>Implementation Skills (15 items)</h3>
<ul>
<li>[ ] Can implement <code>scaled_dot_product_attention</code></li>
<li>[ ] Can implement <code>SelfAttention</code> class</li>
<li>[ ] Can implement <code>MultiHeadAttention</code> class</li>
<li>[ ] Can implement <code>PositionalEncoding</code> class</li>
<li>[ ] Can visualize Attention weights as heatmap</li>
<li>[ ] Can correctly apply masks (padding mask, causal mask)</li>
<li>[ ] Understand PyTorch tensor operations (view, transpose, matmul)</li>
<li>[ ] Understand usage of <code>nn.Linear</code>, <code>nn.Embedding</code></li>
<li>[ ] Can correctly implement batch processing</li>
<li>[ ] Can properly manage devices (CPU/GPU)</li>
<li>[ ] Can save/load models</li>
<li>[ ] Understand gradient calculation and backpropagation</li>
<li>[ ] Understand where to apply dropout</li>
<li>[ ] Understand role of Layer Normalization</li>
<li>[ ] Can select initialization methods (Xavier, Kaiming, etc.)</li>
</ul>
<h3>Debugging Skills (5 items)</h3>
<ul>
<li>[ ] Can debug tensor shape errors</li>
<li>[ ] Can detect and fix Attention mask errors</li>
<li>[ ] Can identify causes of memory errors (OOM)</li>
<li>[ ] Can detect and handle numerical instability (NaN, inf)</li>
<li>[ ] Can discover bugs by visualizing intermediate outputs</li>
</ul>
<h3>Application Skills (5 items)</h3>
<ul>
<li>[ ] Can think about how to apply Attention to molecular data (SMILES)</li>
<li>[ ] Can think about how to apply Transformer to materials composition</li>
<li>[ ] Can formulate strategy to adapt existing Transformer models (BERT, GPT) to materials science</li>
<li>[ ] Can extract chemically meaningful information from Attention weights</li>
<li>[ ] Can design model modifications needed for new tasks</li>
</ul>
<h3>Theoretical Background (5 items)</h3>
<ul>
<li>[ ] Read original Transformer paper ("Attention Is All You Need")</li>
<li>[ ] Read BERT paper</li>
<li>[ ] Read at least one paper on Transformer applications in materials science</li>
<li>[ ] Understand meaning of computational complexity order (O(n¬≤))</li>
<li>[ ] Understand concept of inductive bias</li>
</ul>
<h3>Reproducibility (5 items)</h3>
<ul>
<li>[ ] Can ensure reproducibility by setting random seed</li>
<li>[ ] Can save/load experiment settings in JSON</li>
<li>[ ] Have checked dataset licenses</li>
<li>[ ] Record version information</li>
<li>[ ] Write documentation (docstrings) in code</li>
</ul>
<h3>Completion Criteria</h3>
<ul>
<li><strong>Minimum</strong>: 40+ items achieved (80%)</li>
<li><strong>Recommended</strong>: 45+ items achieved (90%)</li>
<li><strong>Excellent</strong>: All 50 items achieved (100%)</li>
</ul>
<hr/>
<h2>üîó References</h2>
<h3>Papers</h3>
<ul>
<li>Vaswani et al. (2017) "Attention Is All You Need" <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></li>
<li>Devlin et al. (2019) "BERT: Pre-training of Deep Bidirectional Transformers" <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></li>
</ul>
<h3>Tutorials</h3>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">PyTorch Transformer Tutorial</a></li>
</ul>
<h3>Next Chapter</h3>
<p><strong><a href="chapter-2.html">Chapter 2: Materials-Specific Transformer Architectures</a></strong> will cover models specialized for materials science like Matformer and ChemBERTa.</p>
<hr/>
<p><strong>Author</strong>: Yusuke Hashimoto (Tohoku University)
<strong>Last Updated</strong>: October 19, 2025</p><div class="navigation">
<a class="nav-button" href="index.html">Return to Series Index</a>
<a class="nav-button" href="chapter-2.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided for educational, research, and informational purposes only and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranty, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content follow the specified terms (e.g., CC BY 4.0). Such licenses typically include warranty disclaimers.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
