<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Chapter 4: Time Series Data and Integrated Analysis - AI Terakoya" name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 4: Time Series Data and Integrated Analysis - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/experimental-data-analysis-introduction/index.html">Experimental Data Analysis</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 4</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a href="../../../jp/MI/experimental-data-analysis-introduction/chapter-4.html" class="locale-link">üáØüáµ JP</a>
<span class="locale-separator">|</span>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>
<header>
<div class="header-content">
<h1>Chapter 4: Time Series Data and Integrated Analysis</h1>
<p class="subtitle">Sensor Data Analysis and PCA - Integrated Understanding of Multivariate Data</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 min</span>
<span class="meta-item">üìä Difficulty: Intermediate</span>
<span class="meta-item">üíª Code Examples: 5</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 4: Time Series Data and Integrated Analysis</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Learn comprehensive data processing and visualization for UV-Vis/IR/Raman/TGA/DSC data. Master how to interpret key metrics.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Supplement:</strong> Perform baseline correction and normalization first. Compare three aspects consistently: peak position, area, and width.</p>
<p><strong>Sensor Data Analysis and PCA - Integrated Understanding of Multivariate Data</strong></p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will master:</p>
<ul>
<li>‚úÖ Preprocessing temperature/pressure sensor data and anomaly detection</li>
<li>‚úÖ Implementing sliding window analysis</li>
<li>‚úÖ Dimensionality reduction and visualization using PCA (Principal Component Analysis)</li>
<li>‚úÖ Building integrated analysis pipelines with sklearn Pipeline</li>
<li>‚úÖ Integrating data from multiple measurement techniques</li>
</ul>
<p><strong>Reading Time</strong>: 20-25 min
<strong>Code Examples</strong>: 5
<strong>Exercises</strong>: 3</p>
<hr/>
<h2>4.1 Characteristics and Preprocessing of Time Series Data</h2>
<h3>Time Series Data in Materials Synthesis and Process Monitoring</h3>
<p>In materials synthesis processes, physical quantities such as temperature, pressure, flow rate, and composition are measured over time.</p>
<table>
<thead>
<tr>
<th>Measurement Item</th>
<th>Typical Sampling Rate</th>
<th>Main Application</th>
<th>Data Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Temperature</strong></td>
<td>0.1-10 Hz</td>
<td>Reaction control, heat treatment</td>
<td>Trends, periodicity</td>
</tr>
<tr>
<td><strong>Pressure</strong></td>
<td>1-100 Hz</td>
<td>CVD, sputtering</td>
<td>High noise, steep changes</td>
</tr>
<tr>
<td><strong>Flow Rate</strong></td>
<td>0.1-1 Hz</td>
<td>Gas supply control</td>
<td>Drift, step changes</td>
</tr>
<tr>
<td><strong>Composition</strong></td>
<td>0.01-1 Hz</td>
<td>In-situ analysis</td>
<td>Delay, integration effects</td>
</tr>
</tbody>
</table>
<h3>Time Series Data Analysis Workflow</h3>
<div class="mermaid">
flowchart TD
    A[Data Collection] --&gt; B[Preprocessing]
    B --&gt; C[Feature Extraction]
    C --&gt; D[Anomaly Detection]
    D --&gt; E[Integrated Analysis]
    E --&gt; F[Process Optimization]

    B --&gt; B1[Resampling]
    B --&gt; B2[Detrending]
    B --&gt; B3[Noise Removal]

    C --&gt; C1[Statistics]
    C --&gt; C2[Rolling Window Analysis]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
</div>
<hr/>
<h2>4.2 Data Licensing and Reproducibility</h2>
<h3>Time Series Data Repositories and Licenses</h3>
<p>Public repositories for sensor data and time series measurement data are limited, but the following resources are available.</p>
<h4>Major Data Sources</h4>
<table>
<thead>
<tr>
<th>Data Source</th>
<th>Content</th>
<th>License</th>
<th>Access</th>
<th>Citation Requirements</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Kaggle Time Series</strong></td>
<td>Industrial process data</td>
<td>Mixed</td>
<td>Free</td>
<td>Check required</td>
</tr>
<tr>
<td><strong>UCI Machine Learning</strong></td>
<td>Sensor datasets</td>
<td>CC BY 4.0</td>
<td>Free</td>
<td>Recommended</td>
</tr>
<tr>
<td><strong>NIST Measurement Data</strong></td>
<td>Standard measurement data</td>
<td>Public Domain</td>
<td>Free</td>
<td>Recommended</td>
</tr>
<tr>
<td><strong>PhysioNet</strong></td>
<td>Physiological signal data</td>
<td>Mixed</td>
<td>Free</td>
<td>Required</td>
</tr>
<tr>
<td><strong>In-house Experimental Data</strong></td>
<td>Process monitoring</td>
<td>Follow internal regulations</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<h4>Precautions When Using Data</h4>
<p><strong>Example of Public Data Usage</strong>:</p>
<pre><code class="language-python">"""
Process sensor data from UCI ML Repository
Reference: UCI ML Repository - Gas Sensor Array Dataset
Citation: Vergara, A. et al. (2012) Sensors and Actuators B
License: CC BY 4.0
URL: https://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array
Sampling rate: 100 Hz
Measurement period: 3600 seconds
"""
</code></pre>
<p><strong>Recording Measurement Metadata</strong>:</p>
<pre><code class="language-python">SENSOR_METADATA = {
    'instrument': 'Thermocouple Type-K',
    'sampling_rate_hz': 10,
    'measurement_start': '2025-10-15T10:00:00',
    'measurement_duration_s': 1000,
    'calibration_date': '2025-10-01',
    'sensor_accuracy': '¬±0.5¬∞C',
    'data_logger': 'Agilent 34970A'
}

import json
with open('sensor_metadata.json', 'w') as f:
    json.dump(SENSOR_METADATA, f, indent=2)
</code></pre>
<h3>Best Practices for Code Reproducibility</h3>
<h4>Recording Environment Information</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scikit-learn&gt;=1.3.0, &lt;1.5.0
# - scipy&gt;=1.11.0

"""
Example: Recording Environment Information

Purpose: Demonstrate core concepts and implementation patterns
Target: Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import sys
import numpy as np
import pandas as pd
import scipy
from sklearn import __version__ as sklearn_version

print("=== Time Series Analysis Environment ===")
print(f"Python: {sys.version}")
print(f"NumPy: {np.__version__}")
print(f"pandas: {pd.__version__}")
print(f"SciPy: {scipy.__version__}")
print(f"scikit-learn: {sklearn_version}")

# Recommended versions (as of October 2025):
# - Python: 3.10 or higher
# - NumPy: 1.24 or higher
# - pandas: 2.0 or higher
# - SciPy: 1.10 or higher
# - scikit-learn: 1.3 or higher
</code></pre>
<h4>Documenting Parameters</h4>
<p><strong>Bad Example</strong> (not reproducible):</p>
<pre><code class="language-python">rolling_mean = df['temperature'].rolling(window=50).mean()  # Why 50?
</code></pre>
<p><strong>Good Example</strong> (reproducible):</p>
<pre><code class="language-python"># Rolling window parameter settings
SAMPLING_RATE_HZ = 10  # Sensor sampling frequency
WINDOW_DURATION_S = 5  # Window duration (seconds)
WINDOW_SIZE = SAMPLING_RATE_HZ * WINDOW_DURATION_S  # 50 points

WINDOW_DESCRIPTION = """
Window size: 50 points = 5 seconds
Rationale: Adopted approximately half of the characteristic time constant (~10 sec) of the process.
      Too short: Sensitive to noise
      Too long: Cannot capture steep changes
"""

rolling_mean = df['temperature'].rolling(
    window=WINDOW_SIZE, center=True
).mean()
</code></pre>
<h4>Fixing Random Seeds</h4>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Fixing Random Seeds

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
from sklearn.model_selection import train_test_split

# Fix random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# Specify seed when splitting data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_SEED
)
</code></pre>
<hr/>
<h2>4.3 Preprocessing Time Series Data and Rolling Window Analysis</h2>
<h3>Sample Data Generation and Basic Visualization</h3>
<p><strong>Code Example 1: Generating Sensor Data for Synthesis Process</strong></p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0
# - scipy&gt;=1.11.0

"""
Example: Code Example 1: Generating Sensor Data for Synthesis Process

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 5-15 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal

# Simulation of materials synthesis process (1000 seconds, 10 Hz)
np.random.seed(42)
time = np.linspace(0, 1000, 10000)

# Temperature profile (ramp heating ‚Üí holding ‚Üí cooling)
temperature = np.piecewise(
    time,
    [time &lt; 300, (time &gt;= 300) &amp; (time &lt; 700), time &gt;= 700],
    [lambda t: 25 + 0.25 * t,  # Heating
     lambda t: 100,             # Holding
     lambda t: 100 - 0.1 * (t - 700)]  # Cooling
)
temperature += np.random.normal(0, 2, len(time))  # Noise

# Pressure (vacuum level, with step changes)
pressure = np.piecewise(
    time,
    [time &lt; 200, (time &gt;= 200) &amp; (time &lt; 800), time &gt;= 800],
    [100, 1, 100]  # Torr
)
pressure += np.random.normal(0, 0.5, len(time))

# Gas flow rate (periodic variation)
flow_rate = 50 + 10 * np.sin(2 * np.pi * time / 100) + \
            np.random.normal(0, 2, len(time))

# Intentionally insert anomalies (simulating sensor malfunction)
temperature[5000:5010] = 200  # Spike noise
pressure[3000] = -50          # Physically impossible value

# Store in DataFrame
df_sensor = pd.DataFrame({
    'time': time,
    'temperature': temperature,
    'pressure': pressure,
    'flow_rate': flow_rate
})

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)

axes[0].plot(df_sensor['time'], df_sensor['temperature'],
             linewidth=0.8, alpha=0.8)
axes[0].set_ylabel('Temperature (¬∞C)')
axes[0].set_title('Process Sensor Data')
axes[0].grid(True, alpha=0.3)

axes[1].plot(df_sensor['time'], df_sensor['pressure'],
             linewidth=0.8, alpha=0.8, color='orange')
axes[1].set_ylabel('Pressure (Torr)')
axes[1].grid(True, alpha=0.3)

axes[2].plot(df_sensor['time'], df_sensor['flow_rate'],
             linewidth=0.8, alpha=0.8, color='green')
axes[2].set_xlabel('Time (s)')
axes[2].set_ylabel('Flow Rate (sccm)')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Sensor Data Statistics ===")
print(df_sensor.describe())
</code></pre>
<h3>Rolling Window Analysis (Rolling Statistics)</h3>
<p><strong>Code Example 2: Rolling Mean and Rolling Standard Deviation</strong></p>
<pre><code class="language-python"># Calculate rolling statistics
window_size = 100  # 10-second window (10 Hz √ó 10s)

df_sensor['temp_rolling_mean'] = df_sensor['temperature'].rolling(
    window=window_size, center=True
).mean()

df_sensor['temp_rolling_std'] = df_sensor['temperature'].rolling(
    window=window_size, center=True
).std()

df_sensor['pressure_rolling_mean'] = df_sensor['pressure'].rolling(
    window=window_size, center=True
).mean()

# Anomaly detection (3œÉ method)
df_sensor['temp_anomaly'] = np.abs(
    df_sensor['temperature'] - df_sensor['temp_rolling_mean']
) &gt; 3 * df_sensor['temp_rolling_std']

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)

# Temperature and rolling mean
axes[0].plot(df_sensor['time'], df_sensor['temperature'],
             label='Raw', alpha=0.5, linewidth=0.8)
axes[0].plot(df_sensor['time'], df_sensor['temp_rolling_mean'],
             label=f'Rolling mean (window={window_size})',
             linewidth=2, color='red')
axes[0].fill_between(
    df_sensor['time'],
    df_sensor['temp_rolling_mean'] - 3 * df_sensor['temp_rolling_std'],
    df_sensor['temp_rolling_mean'] + 3 * df_sensor['temp_rolling_std'],
    alpha=0.2, color='red', label='¬±3œÉ'
)
axes[0].scatter(df_sensor.loc[df_sensor['temp_anomaly'], 'time'],
                df_sensor.loc[df_sensor['temp_anomaly'], 'temperature'],
                color='black', s=50, zorder=5, label='Anomalies')
axes[0].set_ylabel('Temperature (¬∞C)')
axes[0].set_title('Rolling Statistics and Anomaly Detection')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Rolling standard deviation (magnitude of variation)
axes[1].plot(df_sensor['time'], df_sensor['temp_rolling_std'],
             linewidth=1.5, color='purple')
axes[1].set_xlabel('Time (s)')
axes[1].set_ylabel('Temperature Std (¬∞C)')
axes[1].set_title('Rolling Standard Deviation')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"=== Anomaly Detection Results ===")
print(f"Number of anomalies: {df_sensor['temp_anomaly'].sum()}")
anomaly_times = df_sensor.loc[df_sensor['temp_anomaly'], 'time'].values
print(f"Anomaly occurrence times: {anomaly_times[:5]}... (first 5 points)")
</code></pre>
<h3>Detrending and Stationarization</h3>
<p><strong>Code Example 3: Differencing and Detrending</strong></p>
<pre><code class="language-python">from scipy.signal import detrend

# Differencing (first-order difference = rate of change)
df_sensor['temp_diff'] = df_sensor['temperature'].diff()

# Detrending (linear trend removal)
df_sensor['temp_detrended'] = detrend(
    df_sensor['temperature'].fillna(method='bfill')
)

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)

# Original data
axes[0].plot(df_sensor['time'], df_sensor['temperature'],
             linewidth=1, alpha=0.8)
axes[0].set_ylabel('Temperature (¬∞C)')
axes[0].set_title('Original Time Series')
axes[0].grid(True, alpha=0.3)

# First-order difference
axes[1].plot(df_sensor['time'], df_sensor['temp_diff'],
             linewidth=1, alpha=0.8, color='orange')
axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)
axes[1].set_ylabel('Temperature Diff (¬∞C/0.1s)')
axes[1].set_title('First Difference (Change Rate)')
axes[1].grid(True, alpha=0.3)

# Detrended
axes[2].plot(df_sensor['time'], df_sensor['temp_detrended'],
             linewidth=1, alpha=0.8, color='green')
axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
axes[2].set_xlabel('Time (s)')
axes[2].set_ylabel('Temperature (¬∞C)')
axes[2].set_title('Detrended (Linear Trend Removed)')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Stationarity assessment (stability of variation)
print("=== Stationarity Assessment ===")
print(f"Standard deviation of original data: {df_sensor['temperature'].std():.2f}")
print(f"Standard deviation of differenced data: {df_sensor['temp_diff'].std():.2f}")
print(f"Standard deviation of detrended data: {df_sensor['temp_detrended'].std():.2f}")
</code></pre>
<hr/>
<h2>4.4 Dimensionality Reduction Using PCA (Principal Component Analysis)</h2>
<h3>Visualization of Multivariate Data</h3>
<p><strong>Code Example 4: Dimensionality Reduction and Visualization Using PCA</strong></p>
<pre><code class="language-python">from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Data preparation (after removing anomalies)
df_clean = df_sensor[~df_sensor['temp_anomaly']].copy()

# Feature matrix (temperature, pressure, flow rate)
X = df_clean[['temperature', 'pressure', 'flow_rate']].dropna().values

# Standardization (essential before PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Execute PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Results to DataFrame
df_pca = pd.DataFrame(
    X_pca,
    columns=['PC1', 'PC2', 'PC3']
)
df_pca['time'] = df_clean['time'].values[:len(df_pca)]

# Visualization
fig = plt.figure(figsize=(16, 12))

# 2D scatter plot (PC1 vs PC2)
ax1 = plt.subplot(2, 2, 1)
scatter = ax1.scatter(df_pca['PC1'], df_pca['PC2'],
                     c=df_pca['time'], cmap='viridis',
                     s=10, alpha=0.6)
ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
ax1.set_title('PCA: PC1 vs PC2 (colored by time)')
plt.colorbar(scatter, ax=ax1, label='Time (s)')
ax1.grid(True, alpha=0.3)

# Explained variance ratio (Scree plot)
ax2 = plt.subplot(2, 2, 2)
cumsum_variance = np.cumsum(pca.explained_variance_ratio_)
ax2.bar(range(1, 4), pca.explained_variance_ratio_, alpha=0.7,
        label='Individual')
ax2.plot(range(1, 4), cumsum_variance, 'ro-', linewidth=2,
         markersize=8, label='Cumulative')
ax2.set_xlabel('Principal Component')
ax2.set_ylabel('Explained Variance Ratio')
ax2.set_title('Scree Plot')
ax2.set_xticks(range(1, 4))
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# 3D scatter plot
ax3 = plt.subplot(2, 2, 3, projection='3d')
scatter_3d = ax3.scatter(df_pca['PC1'], df_pca['PC2'], df_pca['PC3'],
                         c=df_pca['time'], cmap='viridis',
                         s=10, alpha=0.5)
ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
ax3.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.1f}%)')
ax3.set_title('PCA: 3D Visualization')

# Loading plot (interpretation of principal components)
ax4 = plt.subplot(2, 2, 4)
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
features = ['Temperature', 'Pressure', 'Flow Rate']

for i, feature in enumerate(features):
    ax4.arrow(0, 0, loadings[i, 0], loadings[i, 1],
             head_width=0.05, head_length=0.05, fc='blue', ec='blue')
    ax4.text(loadings[i, 0] * 1.15, loadings[i, 1] * 1.15,
            feature, fontsize=12, ha='center')

ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
ax4.set_title('Loading Plot (Feature Contribution)')
ax4.axhline(y=0, color='k', linewidth=0.5)
ax4.axvline(x=0, color='k', linewidth=0.5)
ax4.grid(True, alpha=0.3)
ax4.set_xlim(-1, 1)
ax4.set_ylim(-1, 1)

plt.tight_layout()
plt.show()

# PCA statistics
print("=== PCA Results ===")
print(f"Cumulative explained variance ratio:")
for i, var in enumerate(cumsum_variance, 1):
    print(f"  Up to PC{i}: {var*100:.2f}%")

print(f"\nPrincipal component loadings:")
loading_df = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(3)],
    index=features
)
print(loading_df)
</code></pre>
<p><strong>Interpretation of PCA</strong>:
- <strong>PC1 (First Principal Component)</strong>: Direction with the largest variance (typically, overall process progression)
- <strong>PC2 (Second Principal Component)</strong>: Direction orthogonal to PC1 with the next largest variance
- <strong>Loading values</strong>: Impact of each variable on the principal component (larger absolute value = more important)</p>
<hr/>
<h2>4.5 Integrated Analysis Pipeline (sklearn Pipeline)</h2>
<h3>Integrating Data from Multiple Measurement Techniques</h3>
<p><strong>Code Example 5: Automated Integrated Analysis Pipeline</strong></p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - joblib&gt;=1.3.0

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import joblib

class IntegratedAnalysisPipeline:
    """Integrated analysis pipeline"""

    def __init__(self, n_clusters=3):
        """
        Parameters:
        -----------
        n_clusters : int
            Number of clusters (number of process states)
        """
        self.pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),  # Imputation of missing values
            ('scaler', RobustScaler()),  # Robust standardization against outliers
            ('pca', PCA(n_components=0.95)),  # Up to 95% cumulative variance
            ('clustering', KMeans(n_clusters=n_clusters, random_state=42))
        ])
        self.n_clusters = n_clusters

    def fit(self, X):
        """Train pipeline"""
        self.pipeline.fit(X)
        return self

    def transform(self, X):
        """Dimensionality reduction only"""
        # Execute steps up to clustering
        X_transformed = X.copy()
        for step_name, step in self.pipeline.steps[:-1]:
            X_transformed = step.transform(X_transformed)
        return X_transformed

    def predict(self, X):
        """Cluster prediction"""
        return self.pipeline.predict(X)

    def get_feature_importance(self, feature_names):
        """Feature importance in principal components"""
        pca = self.pipeline.named_steps['pca']
        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

        importance_df = pd.DataFrame(
            loadings,
            columns=[f'PC{i+1}' for i in range(pca.n_components_)],
            index=feature_names
        )
        return importance_df

    def save(self, filename):
        """Save model"""
        joblib.dump(self.pipeline, filename)

    @staticmethod
    def load(filename):
        """Load model"""
        return joblib.load(filename)

# Usage example: Execute integrated analysis
# Prepare feature matrix
X_integrated = df_clean[['temperature', 'pressure',
                         'flow_rate']].dropna().values

# Execute pipeline
pipeline = IntegratedAnalysisPipeline(n_clusters=3)
pipeline.fit(X_integrated)

# Dimensionality reduction results
X_reduced = pipeline.transform(X_integrated)

# Cluster prediction
clusters = pipeline.predict(X_integrated)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Cluster visualization in time series
time_clean = df_clean['time'].values[:len(clusters)]
axes[0, 0].scatter(time_clean, clusters, c=clusters,
                  cmap='viridis', s=5, alpha=0.6)
axes[0, 0].set_xlabel('Time (s)')
axes[0, 0].set_ylabel('Cluster ID')
axes[0, 0].set_title('Process State Clustering (Time Series)')
axes[0, 0].grid(True, alpha=0.3)

# Clusters in PCA space
axes[0, 1].scatter(X_reduced[:, 0], X_reduced[:, 1],
                  c=clusters, cmap='viridis', s=10, alpha=0.6)
axes[0, 1].set_xlabel('PC1')
axes[0, 1].set_ylabel('PC2')
axes[0, 1].set_title('Clusters in PCA Space')
axes[0, 1].grid(True, alpha=0.3)

# Temperature profile for each cluster
temp_clean = df_clean['temperature'].values[:len(clusters)]
for cluster_id in range(pipeline.n_clusters):
    mask = clusters == cluster_id
    axes[1, 0].scatter(time_clean[mask], temp_clean[mask],
                      label=f'Cluster {cluster_id}', s=5, alpha=0.6)
axes[1, 0].set_xlabel('Time (s)')
axes[1, 0].set_ylabel('Temperature (¬∞C)')
axes[1, 0].set_title('Temperature Profile by Cluster')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Feature importance
importance = pipeline.get_feature_importance(
    ['Temperature', 'Pressure', 'Flow Rate']
)
importance.plot(kind='bar', ax=axes[1, 1])
axes[1, 1].set_title('Feature Importance in PCA')
axes[1, 1].set_ylabel('Loading')
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Cluster statistics
print("=== Cluster Statistics ===")
for cluster_id in range(pipeline.n_clusters):
    mask = clusters == cluster_id
    cluster_temp = temp_clean[mask]
    print(f"Cluster {cluster_id}:")
    print(f"  Sample count: {mask.sum()}")
    print(f"  Average temperature: {cluster_temp.mean():.2f}¬∞C")
    print(f"  Temperature range: {cluster_temp.min():.2f} - {cluster_temp.max():.2f}¬∞C")

# Save pipeline
pipeline.save('process_analysis_pipeline.pkl')
print("\nPipeline saved to 'process_analysis_pipeline.pkl'")
</code></pre>
<p><strong>Benefits of sklearn Pipeline</strong>:
1. <strong>Reproducibility</strong>: All processing steps stored in a single object
2. <strong>Maintainability</strong>: Easy to add/change steps
3. <strong>Deployment</strong>: Can be saved/loaded as <code>.pkl</code> file
4. <strong>Automation</strong>: Application to new data completed with one <code>predict()</code> line</p>
<hr/>
<h2>4.6 Practical Pitfalls and Countermeasures</h2>
<h3>Common Mistakes and Best Practices</h3>
<h4>Mistake 1: Applying PCA to Non-Stationary Time Series</h4>
<p><strong>Symptom</strong>: PCA results do not align with physical meaning of process</p>
<p><strong>Cause</strong>: Applying PCA to non-stationary time series data (with trends) as is</p>
<p><strong>Countermeasure</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad example: PCA on non-stationary data as is
X_raw = df[['temperature', 'pressure', 'flow_rate']].values
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_raw)
# Result: PC1 is dominated by trend, making physical interpretation difficult

# ‚úÖ Good example: PCA after stationarization (differencing)
# Step 1: First-order difference of each variable
X_diff = df[['temperature', 'pressure', 'flow_rate']].diff().dropna().values

# Step 2: Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_diff)

# Step 3: PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Validation: Stationarity test
from statsmodels.tsa.stattools import adfuller
for col in ['temperature', 'pressure', 'flow_rate']:
    result = adfuller(df[col].dropna())
    print(f"{col}: ADF statistic = {result[0]:.3f}, p-value = {result[1]:.3f}")
    if result[1] &gt; 0.05:
        print(f"  Warning: {col} is non-stationary (p &gt; 0.05). Please take difference")
</code></pre>
<h4>Mistake 2: PCA Without Standardization</h4>
<p><strong>Symptom</strong>: Large-scale variables (pressure: ~100 Torr) dominate PC1, and small-scale variables (flow rate: ~50 sccm) are ignored</p>
<p><strong>Cause</strong>: PCA is based on variance and is sensitive to scale</p>
<p><strong>Countermeasure</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad example: Without standardization
X = df[['temperature', 'pressure', 'flow_rate']].values
pca = PCA()
pca.fit(X)
# Result: Pressure contribution to PC1 is overestimated

# ‚úÖ Good example: Always standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA()
pca.fit(X_scaled)

# Confirm effect of standardization
print("=== Variance Before Standardization ===")
print(f"Temperature: {df['temperature'].var():.2f}")
print(f"Pressure: {df['pressure'].var():.2f}")
print(f"Flow: {df['flow_rate'].var():.2f}")

print("\n=== Variance After Standardization (all become 1.0) ===")
X_scaled_df = pd.DataFrame(X_scaled, columns=['temperature', 'pressure', 'flow_rate'])
print(X_scaled_df.var())
</code></pre>
<h4>Mistake 3: Data Leakage in Time Series</h4>
<p><strong>Symptom</strong>: High accuracy in cross-validation, but performance degradation in actual operation</p>
<p><strong>Cause</strong>: Using future data to predict the past (ignoring temporal order)</p>
<p><strong>Countermeasure</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad example: Random split (ignoring time series)
from sklearn.model_selection import cross_val_score, KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf)
# Result: Predicting past with future data

# ‚úÖ Good example: Time series cross-validation
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X, y, cv=tscv)

# Visualization: Fold structure
fig, ax = plt.subplots(figsize=(12, 6))
for i, (train_idx, test_idx) in enumerate(tscv.split(X)):
    ax.plot(train_idx, [i]*len(train_idx), 'b-', linewidth=10, label='Train' if i == 0 else '')
    ax.plot(test_idx, [i]*len(test_idx), 'r-', linewidth=10, label='Test' if i == 0 else '')
ax.set_xlabel('Time Index')
ax.set_ylabel('Fold')
ax.set_title('TimeSeriesSplit: Train/Test Splits')
ax.legend()
plt.show()

print("TimeSeriesSplit: Training data is always before test data")
</code></pre>
<h4>Mistake 4: Inappropriate Rolling Window Size Setting</h4>
<p><strong>Symptom</strong>: Rolling average is too smooth (missing important changes), or full of noise</p>
<p><strong>Cause</strong>: Window size does not match the time constant of the process</p>
<p><strong>Countermeasure</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad example: Empirically determined window size
window_size = 100  # Arbitrary value

# ‚úÖ Good example: Determine window size from process characteristics
# Step 1: Autocorrelation analysis
from statsmodels.graphics.tsaplots import plot_acf
fig, ax = plt.subplots(figsize=(12, 4))
plot_acf(df['temperature'].dropna(), lags=200, ax=ax)
ax.set_title('Autocorrelation: Temperature')
plt.show()

# Step 2: Identify the point where autocorrelation first falls below threshold (e.g., 0.5)
from statsmodels.tsa.stattools import acf
acf_values = acf(df['temperature'].dropna(), nlags=200)
threshold = 0.5
lag_threshold = np.where(acf_values &lt; threshold)[0][0]
print(f"Lag where autocorrelation falls below {threshold}: {lag_threshold} points")

# Step 3: Determine window size (about half of characteristic time constant)
optimal_window = lag_threshold // 2
print(f"Recommended window size: {optimal_window} points")

rolling_mean = df['temperature'].rolling(window=optimal_window).mean()
</code></pre>
<h4>Mistake 5: Excessive Imputation (Missing Value Imputation)</h4>
<p><strong>Symptom</strong>: Imputed missing values greatly deviate from actual data</p>
<p><strong>Cause</strong>: Imputing large missing regions with simple methods (mean, forward fill)</p>
<p><strong>Countermeasure</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad example: Unconditionally impute large missing regions
df_filled = df['temperature'].fillna(method='ffill')  # Forward fill
# Result: Same value continues during long sensor downtime (unrealistic)

# ‚úÖ Good example: Check missing range and select appropriate imputation method
# Step 1: Evaluate length of consecutive missing values
is_missing = df['temperature'].isnull()
missing_groups = is_missing.ne(is_missing.shift()).cumsum()
max_consecutive_missing = is_missing.groupby(missing_groups).sum().max()

print(f"Maximum consecutive missing values: {max_consecutive_missing} points")

# Step 2: Threshold judgment
MAX_ALLOWED_GAP = 10  # Maximum allowed missing length (e.g., 10 times sampling interval)

if max_consecutive_missing &gt; MAX_ALLOWED_GAP:
    print(f"Warning: Large missing regions exist ({max_consecutive_missing} points)")
    print("Consider excluding data instead of imputation")

    # Exclude intervals containing large missing regions
    consecutive_missing = is_missing.groupby(missing_groups).transform('sum')
    df_clean = df[consecutive_missing &lt;= MAX_ALLOWED_GAP].copy()
else:
    # Impute only small missing values (linear interpolation)
    df_clean = df.copy()
    df_clean['temperature'] = df_clean['temperature'].interpolate(method='linear')

print(f"Number of data points after imputation: {len(df_clean)} / {len(df)}")
</code></pre>
<hr/>
<h2>4.7 Time Series Data Skills Checklist</h2>
<h3>Time Series Preprocessing Skills</h3>
<h4>Basic Level</h4>
<ul>
<li>[ ] Can handle time series data with pandas DataFrame</li>
<li>[ ] Can calculate rolling average</li>
<li>[ ] Can execute downsampling/upsampling</li>
<li>[ ] Can detect missing values</li>
<li>[ ] Can visually confirm presence of trends</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can distinguish between first-order and second-order differencing</li>
<li>[ ] Can implement detrending</li>
<li>[ ] Can estimate time constant by autocorrelation analysis</li>
<li>[ ] Can verify stationarity with ADF test</li>
<li>[ ] Can determine adaptive rolling window size</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can execute seasonal decomposition (STL)</li>
<li>[ ] Can build ARIMA/SARIMA models</li>
<li>[ ] Can estimate states with Kalman filter</li>
<li>[ ] Can quantitatively evaluate missing value imputation accuracy</li>
</ul>
<h3>Anomaly Detection Skills</h3>
<h4>Basic Level</h4>
<ul>
<li>[ ] Can detect statistical anomalies using 3œÉ method</li>
<li>[ ] Can implement threshold-based anomaly detection</li>
<li>[ ] Can visualize anomaly points</li>
<li>[ ] Understand anomaly detection accuracy (True Positive, False Positive)</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can implement rolling statistics-based anomaly detection</li>
<li>[ ] Can perform robust anomaly detection using IQR method</li>
<li>[ ] Can calculate anomaly scores</li>
<li>[ ] Can evaluate detection performance with ROC curve</li>
<li>[ ] Can combine domain knowledge with statistical methods</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can implement anomaly detection using Isolation Forest</li>
<li>[ ] Can detect anomalies using LSTM autoencoder</li>
<li>[ ] Can execute change point detection</li>
<li>[ ] Can build real-time anomaly detection systems</li>
</ul>
<h3>PCA and Dimensionality Reduction Skills</h3>
<h4>Basic Level</h4>
<ul>
<li>[ ] Can standardize data</li>
<li>[ ] Can execute PCA and obtain principal component scores</li>
<li>[ ] Can calculate and visualize cumulative explained variance ratio</li>
<li>[ ] Can visualize PCA results with 2D/3D scatter plots</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can interpret principal components with loading plots</li>
<li>[ ] Can select appropriate number of principal components (cumulative variance, scree plot)</li>
<li>[ ] Can preprocess non-stationary data (differencing, detrending)</li>
<li>[ ] Understand PCA assumptions (linearity, normality)</li>
<li>[ ] Can explain influence of original variables from principal component scores</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can perform nonlinear dimensionality reduction with kernel PCA</li>
<li>[ ] Can visualize with t-SNE/UMAP</li>
<li>[ ] Can improve interpretability with sparse PCA</li>
<li>[ ] Can implement principal component regression (PCR)</li>
</ul>
<h3>Integrated Analysis and Pipeline Building Skills</h3>
<h4>Basic Level</h4>
<ul>
<li>[ ] Understand basic structure of sklearn Pipeline</li>
<li>[ ] Can build basic pipeline: Imputer‚ÜíScaler‚ÜíPCA</li>
<li>[ ] Can execute training/prediction with pipeline</li>
<li>[ ] Can save/load pipeline</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can create custom Transformers</li>
<li>[ ] Can apply different preprocessing per column with ColumnTransformer</li>
<li>[ ] Can optimize hyperparameters with GridSearchCV</li>
<li>[ ] Can execute time series CV with TimeSeriesSplit</li>
<li>[ ] Can diagram entire pipeline processing flow</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can ensemble multiple models</li>
<li>[ ] Can implement custom CV strategies (Blocked Time Series Split)</li>
<li>[ ] Can optimize computation time for each pipeline step</li>
<li>[ ] Can build deployment pipeline for production environment</li>
</ul>
<h3>Integrated Skills: Comprehensive Experimental Data Analysis</h3>
<h4>Basic Level</h4>
<ul>
<li>[ ] Can process XRD, XPS, SEM, and sensor data individually</li>
<li>[ ] Can extract features from each data type</li>
<li>[ ] Can combine features into tabular data</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can appropriately standardize data with different scales</li>
<li>[ ] Can integrate data considering time delays</li>
<li>[ ] Can extract main patterns from integrated data using PCA</li>
<li>[ ] Can classify process states using clustering</li>
<li>[ ] Can predict material properties using regression models</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can perform multimodal data integration (Early/Late Fusion)</li>
<li>[ ] Can identify causal relationships between variables using causal inference</li>
<li>[ ] Can optimize process conditions using Bayesian optimization</li>
<li>[ ] Can apply integrated analysis to digital twin construction</li>
</ul>
<hr/>
<h2>4.8 Comprehensive Skills Assessment</h2>
<p>Please self-assess using the following criteria.</p>
<h3>Level 1: Beginner (60% or more of basic skills)</h3>
<ul>
<li>Can perform basic time series preprocessing (rolling average, differencing)</li>
<li>Can detect anomalies using 3œÉ method</li>
<li>Can reduce dimensions with PCA and visualize in 2D</li>
<li>Understand basics of sklearn Pipeline</li>
</ul>
<p><strong>Next Steps</strong>:
- Master stationarity test (ADF)
- Deepen loading interpretation of PCA
- Master TimeSeriesSplit</p>
<h3>Level 2: Intermediate (100% basic + 60% or more applied)</h3>
<ul>
<li>Can appropriately stationarize non-stationary time series</li>
<li>Can implement rolling statistics-based anomaly detection</li>
<li>Can physically interpret PCA results</li>
<li>Can create custom Transformers</li>
</ul>
<p><strong>Next Steps</strong>:
- Challenge ARIMA/SARIMA models
- Master advanced anomaly detection methods like Isolation Forest
- Experience pipeline optimization and deployment</p>
<h3>Level 3: Advanced (100% basic + 100% applied + 60% or more advanced)</h3>
<ul>
<li>Can build seasonal decomposition and ARIMA models</li>
<li>Can perform advanced anomaly detection with LSTM/Isolation Forest</li>
<li>Can perform nonlinear dimensionality reduction with kernel PCA/t-SNE</li>
<li>Can deploy pipelines to production environment</li>
</ul>
<p><strong>Next Steps</strong>:
- Advance to causal inference and Bayesian optimization
- Build real-time analysis systems
- Participate in digital twin development</p>
<h3>Level 4: Expert (90% or more of all items)</h3>
<ul>
<li>Can lead the field of time series and multivariate analysis</li>
<li>Can develop and implement new anomaly detection methods</li>
<li>Can design integrated analysis of multiple measurement techniques</li>
<li>Contributing to standardization of experimental data analysis</li>
</ul>
<p><strong>Activity Examples</strong>:
- In-house training and tutorial instructor
- Paper writing and conference presentations
- Open source contributions</p>
<hr/>
<h2>4.9 Action Plan Template</h2>
<h3>Current Level: <strong><em>_</em></strong>____</h3>
<h3>Target Level (after 3 months): <strong><em>_</em></strong>____</h3>
<h3>Priority Skills to Strengthen (select 3):</h3>
<ol>
<li>
<hr/>
</li>
<li>
<hr/>
</li>
<li>
<hr/>
</li>
</ol>
<h3>Specific Action Plan:</h3>
<p><strong>Week 1-2</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 3-4</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 5-8</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 9-12</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<h3>Evaluation Metrics:</h3>
<ul>
<li>[ ] Apply time series analysis to real project</li>
<li>[ ] Build anomaly detection system</li>
<li>[ ] Report PCA results to team</li>
</ul>
<hr/>
<h2>4.10 Integrated Analysis Workflow Diagram</h2>
<h3>Integration of Multiple Measurement Techniques</h3>
<div class="mermaid">
flowchart LR
    A[XRD Data] --&gt; D[Feature Extraction]
    B[XPS Data] --&gt; E[Feature Extraction]
    C[SEM Data] --&gt; F[Feature Extraction]
    G[Sensor Data] --&gt; H[Feature Extraction]

    D --&gt; I[Integrated Feature Matrix]
    E --&gt; I
    F --&gt; I
    H --&gt; I

    I --&gt; J[Standardization]
    J --&gt; K[PCA]
    K --&gt; L[Clustering\nor\nRegression Model]
    L --&gt; M[Result Interpretation]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style G fill:#e8f5e9
    style I fill:#fce4ec
    style M fill:#fff9c4
</div>
<hr/>
<h2>4.11 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Time Series Data Analysis</strong>
   - Rolling window statistics (mean, standard deviation)
   - Anomaly detection (3œÉ method)
   - Detrending and stationarization (differencing, detrending)</p>
</li>
<li>
<p><strong>PCA (Principal Component Analysis)</strong>
   - Dimensionality reduction of multivariate data
   - Component selection by explained variance ratio
   - Interpretation using loading plots</p>
</li>
<li>
<p><strong>Integrated Analysis Pipeline</strong>
   - Automation with sklearn Pipeline
   - Imputation‚ÜíStandardization‚ÜíPCA‚ÜíClustering
   - Model saving and reuse</p>
</li>
<li>
<p><strong>Practical Applications</strong>
   - Clustering of process states
   - Integration of multiple measurement techniques
   - Anomaly detection and quality control</p>
</li>
<li>
<p><strong>Data Licensing and Reproducibility</strong> (added in v1.1)
   - Proper citation of public data sources (UCI ML, NIST, PhysioNet)
   - Recording sensor metadata
   - Documentation of environment information and parameters</p>
</li>
<li>
<p><strong>Avoiding Practical Pitfalls</strong> (added in v1.1)
   - Precautions when applying PCA to non-stationary time series
   - Importance of standardization
   - Prevention of time series data leakage
   - Scientific determination of rolling window size
   - Appropriate limits on missing value imputation</p>
</li>
<li>
<p><strong>Skills Assessment and Career Path</strong> (added in v1.1)
   - Checklist for 5 skill categories
   - 3-level assessment (basic/applied/advanced)
   - 4-level comprehensive assessment (beginner‚Üíexpert)
   - Individual action plan template</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>‚úÖ Preprocessing (stationarization) is crucial for time series data</li>
<li>‚úÖ PCA is dimensionality reduction considering correlations between variables</li>
<li>‚úÖ sklearn Pipeline enables highly reproducible analysis</li>
<li>‚úÖ Integrated analysis reveals insights not obtainable from single measurements</li>
<li>‚úÖ <strong>Proper citation of data sources and ensuring reproducibility are the foundation of research quality</strong> (v1.1)</li>
<li>‚úÖ <strong>Knowing practical pitfalls in advance greatly reduces trial-and-error time</strong> (v1.1)</li>
<li>‚úÖ <strong>Systematic skills assessment enables efficient learning planning</strong> (v1.1)</li>
</ul>
<h3>Series Summary</h3>
<p>In this series, we learned from basics to applications of experimental data analysis in materials science:</p>
<ul>
<li><strong>Chapter 1</strong>: Basics of data preprocessing (noise removal, outlier detection, standardization)</li>
<li><strong>Chapter 2</strong>: Spectral data analysis (XRD, XPS, IR, Raman)</li>
<li><strong>Chapter 3</strong>: Image data analysis (SEM, TEM, particle detection, CNN classification)</li>
<li><strong>Chapter 4</strong>: Time series data and integrated analysis (sensor data, PCA, Pipeline)</li>
</ul>
<p><strong>Common additions in v1.1 across all chapters</strong>:
- Establishment of data licensing and citation standards
- Best practices for code reproducibility
- Practical pitfalls and countermeasures (5+ examples per chapter)
- Comprehensive skills assessment framework</p>
<p>By combining these technologies, accelerated and more accurate materials development can be realized.</p>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: easy)</h3>
<p>Judge whether the following statements are true or false.</p>
<ol>
<li>Rolling average filter removes trends from time series data</li>
<li>The first principal component in PCA is the direction with the largest variance</li>
<li>In sklearn Pipeline, all processing steps require the same data type</li>
</ol>
<details>
<summary>Hints</summary>

1. Rolling average is smoothing, different from detrending
2. Check the definition of PCA (variance maximization)
3. Consider the input/output types of each Pipeline step

</details>
<details>
<summary>Answer Example</summary>

**Answer**:
1. **False** - Rolling average is noise removal (smoothing); use differencing or detrending for trend removal
2. **True** - PCA defines the first principal component as the direction that maximizes variance
3. **False** - Each step can handle different data types with appropriate transformations (e.g., Imputer‚ÜíScaler)

**Explanation**:
"Smoothing," "detrending," and "stationarization" of time series data are different concepts. Rolling average removes high-frequency noise but preserves low-frequency trends. PCA is an unsupervised learning dimensionality reduction method that maximally preserves data variance.

</details>
<hr/>
<h3>Problem 2 (Difficulty: medium)</h3>
<p>Execute rolling window analysis and anomaly detection on the following sensor data.</p>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - numpy&gt;=1.24.0, &lt;2.0.0

"""
Example: Execute rolling window analysis and anomaly detection on the

Purpose: Demonstrate core concepts and implementation patterns
Target: Beginner to Intermediate
Execution time: ~5 seconds
Dependencies: None
"""

import numpy as np

# Sample sensor data
np.random.seed(200)
time = np.linspace(0, 500, 5000)
signal = 50 + 10 * np.sin(2 * np.pi * time / 50) + \
         np.random.normal(0, 3, len(time))

# Insert anomalies
signal[2000:2005] = 100
signal[3500] = -20
</code></pre>
<p><strong>Requirements</strong>:
1. Calculate rolling mean (window size 50)
2. Calculate rolling standard deviation
3. Detect anomalies using 3œÉ method
4. Output times of anomalies
5. Visualize results</p>
<details>
<summary>Hints</summary>

**Processing Flow**:
1. `pandas.Series.rolling(window=50).mean()`
2. `pandas.Series.rolling(window=50).std()`
3. `np.abs(signal - rolling_mean) &gt; 3 * rolling_std`
4. `time[anomaly_mask]`
5. Plot original signal, rolling mean, ¬±3œÉ range, and anomaly points using `matplotlib`

</details>
<details>
<summary>Answer Example</summary>
<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Requirements:
1. Calculate rolling mean (window size 50)
2. 

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 2-5 seconds
Dependencies: None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Sample data
np.random.seed(200)
time = np.linspace(0, 500, 5000)
signal = 50 + 10 * np.sin(2 * np.pi * time / 50) + \
         np.random.normal(0, 3, len(time))
signal[2000:2005] = 100
signal[3500] = -20

# Convert to DataFrame
df = pd.DataFrame({'time': time, 'signal': signal})

# Rolling statistics
window_size = 50
df['rolling_mean'] = df['signal'].rolling(
    window=window_size, center=True
).mean()
df['rolling_std'] = df['signal'].rolling(
    window=window_size, center=True
).std()

# Anomaly detection
df['anomaly'] = np.abs(
    df['signal'] - df['rolling_mean']
) &gt; 3 * df['rolling_std']

# Anomaly times
anomaly_times = df.loc[df['anomaly'], 'time'].values
print("=== Anomaly Detection Results ===")
print(f"Number of anomalies: {df['anomaly'].sum()}")
print(f"Anomaly occurrence times: {anomaly_times}")

# Visualization
plt.figure(figsize=(14, 6))

plt.plot(df['time'], df['signal'], label='Raw Signal',
         alpha=0.6, linewidth=0.8)
plt.plot(df['time'], df['rolling_mean'], label='Rolling Mean',
         linewidth=2, color='red')
plt.fill_between(
    df['time'],
    df['rolling_mean'] - 3 * df['rolling_std'],
    df['rolling_mean'] + 3 * df['rolling_std'],
    alpha=0.2, color='red', label='¬±3œÉ'
)
plt.scatter(df.loc[df['anomaly'], 'time'],
           df.loc[df['anomaly'], 'signal'],
           color='black', s=50, zorder=5, label='Anomalies')

plt.xlabel('Time (s)')
plt.ylabel('Signal')
plt.title('Rolling Window Analysis and Anomaly Detection')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>


**Example Output**:

<pre><code>=== Anomaly Detection Results ===
Number of anomalies: 6
Anomaly occurrence times: [200.04  200.14  200.24  200.34  200.44  350.07]
</code></pre>


**Explanation**:
Rolling window statistics capture the local behavior (mean and standard deviation) of the signal, and the 3œÉ rule detects statistical anomalies. In this example, the spike near 200 seconds and the negative spike near 350 seconds were correctly detected.

</details>
<hr/>
<h3>Problem 3 (Difficulty: hard)</h3>
<p>Build a pipeline to integrate data from multiple measurement techniques (XRD, XPS, SEM, sensors) and predict material quality.</p>
<p><strong>Background</strong>:
In materials synthesis experiments, the following data were acquired for each sample:
- XRD peak intensities (3 main peaks)
- XPS elemental composition (C, O, Fe atomic %)
- SEM particle size statistics (mean diameter, standard deviation)
- Process sensor statistics (maximum temperature, average pressure)</p>
<p>Build a model to predict material quality score (0-100) from these 11 variables.</p>
<p><strong>Tasks</strong>:
1. Build pipeline including missing value imputation and standardization
2. Dimensionality reduction with PCA (90% cumulative variance)
3. Quality prediction with regression model (Ridge regression)
4. Performance evaluation with cross-validation
5. Visualization of feature importance</p>
<p><strong>Constraints</strong>:
- 100 samples (80 training, 20 test)
- Some data has missing values (5-10%)
- Scales differ greatly (XRD is thousands, composition is 0-100%)</p>
<details>
<summary>Hints</summary>

**Design Policy**:
1. Integrate with `sklearn.pipeline.Pipeline`
2. `SimpleImputer` ‚Üí `StandardScaler` ‚Üí `PCA` ‚Üí `Ridge`
3. Evaluate with `cross_val_score`
4. Calculate feature importance from PCA loadings

**Pipeline Example**:

<pre><code class="language-python">from sklearn.linear_model import Ridge
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.9)),
    ('regressor', Ridge(alpha=1.0))
])
</code></pre>
</details>
<details>
<summary>Answer Example</summary>

**Answer Overview**:
Realize quality prediction with an integrated pipeline of missing value imputation, standardization, PCA, and regression.

**Implementation Code**:


<pre><code class="language-python"># Requirements:
# - Python 3.9+
# - matplotlib&gt;=3.7.0
# - numpy&gt;=1.24.0, &lt;2.0.0
# - pandas&gt;=2.0.0, &lt;2.2.0

"""
Example: Constraints:
- 100 samples (80 training, 20 test)
- Some dat

Purpose: Demonstrate data visualization techniques
Target: Intermediate
Execution time: 1-5 minutes
Dependencies: None
"""

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
n_samples = 100

# Generate features (11 variables)
data = {
    # XRD peak intensities
    'xrd_peak1': np.random.normal(1000, 200, n_samples),
    'xrd_peak2': np.random.normal(1500, 300, n_samples),
    'xrd_peak3': np.random.normal(800, 150, n_samples),

    # XPS composition
    'xps_C': np.random.normal(20, 5, n_samples),
    'xps_O': np.random.normal(50, 10, n_samples),
    'xps_Fe': np.random.normal(30, 8, n_samples),

    # SEM statistics
    'sem_mean_diameter': np.random.normal(50, 10, n_samples),
    'sem_std_diameter': np.random.normal(8, 2, n_samples),

    # Sensor statistics
    'max_temperature': np.random.normal(300, 50, n_samples),
    'avg_pressure': np.random.normal(10, 3, n_samples),
    'total_flow': np.random.normal(100, 20, n_samples)
}

df = pd.DataFrame(data)

# Quality score (linear combination of multiple variables + noise)
quality_score = (
    0.02 * df['xrd_peak2'] +
    0.5 * df['xps_Fe'] +
    0.3 * df['sem_mean_diameter'] +
    0.1 * df['max_temperature'] +
    np.random.normal(0, 5, n_samples)
)
quality_score = np.clip(quality_score, 0, 100)

# Intentionally insert missing values (5%)
mask = np.random.rand(n_samples, 11) &lt; 0.05
df_with_missing = df.copy()
df_with_missing[mask] = np.nan

# Data split
X = df_with_missing.values
y = quality_score.values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build pipeline
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.9)),  # 90% cumulative variance
    ('regressor', Ridge(alpha=1.0))
])

# Training
pipeline.fit(X_train, y_train)

# Prediction
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Performance evaluation
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)

# Cross-validation
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5,
                            scoring='r2')

print("=== Model Performance ===")
print(f"Training R¬≤: {train_r2:.3f}")
print(f"Test R¬≤: {test_r2:.3f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"CV R¬≤ (mean ¬± std): {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

# Number of PCA components
n_components = pipeline.named_steps['pca'].n_components_
print(f"\nNumber of PCA principal components: {n_components}")
print(f"Cumulative explained variance: {pipeline.named_steps['pca'].explained_variance_ratio_.sum()*100:.1f}%")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Prediction vs Actual (training data)
axes[0, 0].scatter(y_train, y_pred_train, alpha=0.6, s=30)
axes[0, 0].plot([0, 100], [0, 100], 'r--', linewidth=2)
axes[0, 0].set_xlabel('True Quality Score')
axes[0, 0].set_ylabel('Predicted Quality Score')
axes[0, 0].set_title(f'Training Set (R¬≤={train_r2:.3f})')
axes[0, 0].grid(True, alpha=0.3)

# Prediction vs Actual (test data)
axes[0, 1].scatter(y_test, y_pred_test, alpha=0.6, s=30, color='orange')
axes[0, 1].plot([0, 100], [0, 100], 'r--', linewidth=2)
axes[0, 1].set_xlabel('True Quality Score')
axes[0, 1].set_ylabel('Predicted Quality Score')
axes[0, 1].set_title(f'Test Set (R¬≤={test_r2:.3f})')
axes[0, 1].grid(True, alpha=0.3)

# Residual plot
residuals = y_test - y_pred_test
axes[1, 0].scatter(y_pred_test, residuals, alpha=0.6, s=30, color='green')
axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Predicted Quality Score')
axes[1, 0].set_ylabel('Residuals')
axes[1, 0].set_title('Residual Plot')
axes[1, 0].grid(True, alpha=0.3)

# Feature importance (PCA loadings)
pca = pipeline.named_steps['pca']
loadings = np.abs(pca.components_).sum(axis=0)
feature_importance = loadings / loadings.sum()

feature_names = list(data.keys())
axes[1, 1].barh(feature_names, feature_importance, alpha=0.7)
axes[1, 1].set_xlabel('Importance (normalized)')
axes[1, 1].set_title('Feature Importance (PCA Loadings)')
axes[1, 1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Feature importance ranking
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

print("\n=== Feature Importance Ranking ===")
print(importance_df.to_string(index=False))
</code></pre>


**Example Results**:

<pre><code>=== Model Performance ===
Training R¬≤: 0.892
Test R¬≤: 0.867
Test MAE: 4.23
CV R¬≤ (mean ¬± std): 0.875 ¬± 0.032

Number of PCA principal components: 6
Cumulative explained variance: 91.2%

=== Feature Importance Ranking ===
          Feature  Importance
      xrd_peak2      0.1456
        xps_Fe      0.1289
sem_mean_diameter   0.1142
max_temperature     0.1078
      xrd_peak1     0.0987
      avg_pressure  0.0921
      ...
</code></pre>


**Detailed Explanation**:
1. **Missing value handling**: Median imputation with `SimpleImputer` (robust to outliers)
2. **Standardization**: Unify variables with different scales (essential for PCA)
3. **PCA**: Reduced 11 variables ‚Üí 6 principal components (information loss &lt; 10%)
4. **Ridge regression**: Suppress overfitting with L2 regularization
5. **Feature importance**: XRD peak 2, Fe composition, and particle size found to be important

**Additional Considerations**:
- Hyperparameter optimization (GridSearchCV)
- Consider nonlinear models (RandomForest, XGBoost)
- Improve prediction interpretability with SHAP
- Integration with design of experiments (DOE)

</details>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>Hyndman, R. J., &amp; Athanasopoulos, G. (2018). "Forecasting: Principles and Practice." OTexts. URL: <a href="https://otexts.com/fpp2/">https://otexts.com/fpp2/</a></p>
</li>
<li>
<p>Jolliffe, I. T., &amp; Cadima, J. (2016). "Principal component analysis: a review and recent developments." <em>Philosophical Transactions of the Royal Society A</em>, 374(2065). DOI: <a href="https://doi.org/10.1098/rsta.2015.0202">10.1098/rsta.2015.0202</a></p>
</li>
<li>
<p>Pedregosa, F. et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</p>
</li>
<li>
<p>sklearn Documentation: Pipeline. URL: <a href="https://scikit-learn.org/stable/modules/compose.html">https://scikit-learn.org/stable/modules/compose.html</a></p>
</li>
<li>
<p>Chandola, V. et al. (2009). "Anomaly detection: A survey." <em>ACM Computing Surveys</em>, 41(3), 1-58. DOI: <a href="https://doi.org/10.1145/1541880.1541882">10.1145/1541880.1541882</a></p>
</li>
</ol>
<hr/>
<h2>Navigation</h2>
<h3>Previous Chapter</h3>
<p><strong><a href="chapter-3.html">Chapter 3: Image Data Analysis ‚Üê</a></strong></p>
<h3>Series Index</h3>
<p><strong><a href="./index.html">‚Üê Return to Series Index</a></strong></p>
<hr/>
<h2>Author Information</h2>
<p><strong>Author</strong>: AI Terakoya Content Team
<strong>Created</strong>: 2025-10-17
<strong>Version</strong>: 1.1</p>
<p><strong>Update History</strong>:
- 2025-10-19: v1.1 Quality improvement version released
  - Added data licensing and reproducibility section (Section 4.2)
  - Added 5 practical pitfall examples (Section 4.6)
  - Added time series data skills checklist (Section 4.7)
  - Added comprehensive skills assessment and action plan template (Sections 4.8-4.9)
- 2025-10-17: v1.0 Initial version released</p>
<p><strong>Feedback</strong>:
- GitHub Issues: [Repository URL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr/>
<h2>Series Complete</h2>
<p><strong>Congratulations! You have completed the Experimental Data Analysis Introduction series!</strong></p>
<p>Skills acquired in this series:
- ‚úÖ Data preprocessing (noise removal, outlier detection, standardization)
- ‚úÖ Spectral analysis (XRD, XPS, IR, Raman)
- ‚úÖ Image analysis (SEM, TEM, particle detection, CNN)
- ‚úÖ Time series analysis (sensor data, PCA, Pipeline)</p>
<p><strong>Next Steps</strong>:
- Machine learning applications (regression, classification, clustering)
- Deep learning introduction (PyTorch, TensorFlow)
- Materials exploration using Bayesian optimization
- Integration with design of experiments (DOE)</p>
<p>Continue deepening your learning at AI Terakoya!</p>
<div class="navigation">
<a class="nav-button" href="chapter-3.html">‚Üê Previous Chapter</a>
<a class="nav-button" href="index.html">Return to Series Index</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical guarantees, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without warranty of any kind, express or implied, including but not limited to merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, libraries, etc.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content of this material may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.1 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
