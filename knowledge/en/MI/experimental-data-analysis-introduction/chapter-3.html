<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Image Data Analysis - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }
        .disclaimer {
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #6c757d;
            border-radius: 4px;
        }

        .disclaimer h3 {
            color: #495057;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .disclaimer ul {
            list-style: none;
            padding-left: 0;
        }

        .disclaimer li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.9rem;
            color: #6c757d;
            line-height: 1.6;
        }

        .disclaimer li::before {
            content: "‚ö†Ô∏è";
            position: absolute;
            left: 0;
        }


        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    
        
    
        /* Breadcrumb styles */
        .breadcrumb {
            background: #f7fafc;
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            font-size: 0.9rem;
        }

        .breadcrumb-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }

        .breadcrumb a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .breadcrumb-separator {
            color: #a0aec0;
            margin: 0 0.25rem;
        }

        .breadcrumb-current {
            color: #4a5568;
            font-weight: 500;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
            <nav class="breadcrumb">
        <div class="breadcrumb-content">
            <a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Home</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/experimental-data-analysis-introduction/index.html">Experimental Data Analysis</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 3</span>
        </div>
    </nav>

        <header>
        <div class="header-content">
            <h1>Chapter 3: Image Data Analysis</h1>
            <p class="subtitle">Automated Analysis of SEM/TEM Images - From Particle Detection to Deep Learning</p>
            <div class="meta">
                <span class="meta-item">üìñ Reading time: 30-35 minutes</span>
                <span class="meta-item">üìä Level: Intermediate</span>
                <span class="meta-item">üíª Code examples: 13</span>
                <span class="meta-item">üìù Exercises: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: Image Data Analysis</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">Implement a basic pipeline for extracting particle information from SEM/TEM images. Tips for robustness against noise and overlapping are also introduced.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> The workflow is fixed in the order: preprocessing (smoothing, binarization) ‚Üí feature extraction ‚Üí post-processing. The quality of annotations determines performance.</p>





<p><strong>Automated Analysis of SEM/TEM Images - From Particle Detection to Deep Learning</strong></p>
<h2>Learning Objectives</h2>
<p>By completing this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Perform preprocessing of SEM/TEM images (noise removal, contrast adjustment)</li>
<li>‚úÖ Implement particle detection using the Watershed method</li>
<li>‚úÖ Quantify particle size distribution and shape parameters (circularity, aspect ratio)</li>
<li>‚úÖ Perform material image classification using CNN (transfer learning)</li>
<li>‚úÖ Build an image analysis pipeline using OpenCV and scikit-image</li>
</ul>
<p><strong>Reading time</strong>: 30-35minutes
<strong>Code examples</strong>: 13
<strong>Exercises</strong>: 3</p>
<hr />
<h2>3.1 Characteristics of Image Data and Preprocessing Strategy</h2>
<h3>Characteristics of SEM/TEM Images</h3>
<p>Electron microscopy images are powerful tools for visualizing nano- to micro-scale structures of materials.</p>
<table>
<thead>
<tr>
<th>Measurement Technique</th>
<th>Á©∫ÈñìminutesËß£ËÉΩ</th>
<th>Typical Field of View</th>
<th>Main Information</th>
<th>Image Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SEM</strong></td>
<td>Several nm to several Œºm</td>
<td>10Œºm to 1mm</td>
<td>Surface morphology, microstructure</td>
<td>Deep depth of field, shadow effects</td>
</tr>
<tr>
<td><strong>TEM</strong></td>
<td>Atomic level</td>
<td>Tens of nm to several Œºm</td>
<td>Internal structure, crystallinity</td>
<td>High contrast, diffraction patterns</td>
</tr>
<tr>
<td><strong>STEM</strong></td>
<td>Sub-nm</td>
<td>Tens to hundreds of nm</td>
<td>ÂéüÂ≠êÈÖçÂàó„ÄÅÂÖÉÁ¥†minutesÂ∏É</td>
<td>ÂéüÂ≠êminutesËß£ËÉΩ</td>
</tr>
</tbody>
</table>
<h3>Typical Workflow for Image Analysis</h3>
<div class="mermaid">
flowchart TD
    A[Image Acquisition] --> B[Preprocessing]
    B --> C[Segmentation]
    C --> D[Feature Extraction]
    D --> E[Quantitative Analysis]
    E --> F[Statistical Processing]
    F --> G[Visualization & Reporting]

    B --> B1[Noise Removal]
    B --> B2[Contrast Adjustment]
    B --> B3[Binarization]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style G fill:#fff9c4
</div>

<hr />
<h2>3.2 Data Licensing and Reproducibility</h2>
<h3>Image Data Repositories and Licensing</h3>
<p>Utilization of microscopy image databases is essential for algorithm development and validation.</p>
<h4>Major Image Databases</h4>
<table>
<thead>
<tr>
<th>Database</th>
<th>Content</th>
<th>License</th>
<th>Access</th>
<th>Citation Requirements</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EMPIAR (Electron Microscopy)</strong></td>
<td>TEM/cryo-EM images</td>
<td>CC BY 4.0</td>
<td>Free</td>
<td>Required</td>
</tr>
<tr>
<td><strong>NanoMine</strong></td>
<td>Nanocomposite SEM</td>
<td>CC BY 4.0</td>
<td>Free</td>
<td>Recommended</td>
</tr>
<tr>
<td><strong>Materials Data Facility</strong></td>
<td>Materials image datasets</td>
<td>Mixed</td>
<td>Free</td>
<td>Required</td>
</tr>
<tr>
<td><strong>Kaggle Datasets (Microscopy)</strong></td>
<td>Various microscopy images</td>
<td>Mixed</td>
<td>Free</td>
<td>Verification required</td>
</tr>
<tr>
<td><strong>NIST SRD</strong></td>
<td>Standard reference images</td>
<td>Public Domain</td>
<td>Free</td>
<td>Recommended</td>
</tr>
</tbody>
</table>
<h4>Notes on Data Usage</h4>
<p><strong>Example of Using Public Data</strong>:</p>
<pre><code class="language-python">&quot;&quot;&quot;
SEM image from NanoMine database.
Reference: NanoMine Dataset L123 - Polymer Nanocomposite
Citation: Zhao, H. et al. (2016) Computational Materials Science
License: CC BY 4.0
URL: https://materialsmine.org/nm/L123
Scale bar: 500 nm (calibration: 2.5 nm/pixel)
&quot;&quot;&quot;
</code></pre>
<p><strong>Recording Image Metadata</strong>:</p>
<pre><code class="language-python">IMAGE_METADATA = {
    'instrument': 'FEI Quanta 200',
    'accelerating_voltage': '20 kV',
    'magnification': '10000x',
    'working_distance': '10 mm',
    'pixel_size': 2.5,  # nm/pixel
    'scale_bar': 500,   # nm
    'detector': 'SE detector',
    'acquisition_date': '2025-10-15'
}

# Save metadata to JSON (ensuring reproducibility)
import json
with open('image_metadata.json', 'w') as f:
    json.dump(IMAGE_METADATA, f, indent=2)
</code></pre>
<h3>Best Practices for Code Reproducibility</h3>
<h4>Recording Environment Information</h4>
<pre><code class="language-python">import sys
import cv2
import numpy as np
from skimage import __version__ as skimage_version
import tensorflow as tf

print(&quot;=== Image Analysis Environment ===&quot;)
print(f&quot;Python: {sys.version}&quot;)
print(f&quot;OpenCV: {cv2.__version__}&quot;)
print(f&quot;NumPy: {np.__version__}&quot;)
print(f&quot;scikit-image: {skimage_version}&quot;)
print(f&quot;TensorFlow: {tf.__version__}&quot;)

# RecommendedVersionÔºà2025Âπ¥10ÊúàÊôÇÁÇπÔºâ:
# - Python: 3.10‰ª•‰∏ä
# - OpenCV: 4.8‰ª•‰∏ä
# - NumPy: 1.24‰ª•‰∏ä
# - scikit-image: 0.21‰ª•‰∏ä
# - TensorFlow: 2.13‰ª•‰∏äÔºàGPUÁâàRecommendedÔºâ
</code></pre>
<h4>Parameter Documentation</h4>
<p><strong>Bad Example</strong>Ôºànot reproducibleÔºâ:</p>
<pre><code class="language-python">denoised = cv2.fastNlMeansDenoising(image, None, 10, 7, 21)  # Why these values?
</code></pre>
<p><strong>Good Example</strong>ÔºàreproducibleÔºâ:</p>
<pre><code class="language-python"># Non-Local MeansParameter Settings
NLM_H = 10  # Filter strength (corresponding to noise level, typical SEM values: 5-15)
NLM_TEMPLATE_WINDOW = 7  # Template window size (odd number, recommended: 7)
NLM_SEARCH_WINDOW = 21   # Search window size (odd number, recommended: 21)
NLM_DESCRIPTION = &quot;&quot;&quot;
h: Adjust according to noise level. Low noise: 5-7, high noise: 10-15
templateWindowSize: 7„ÅåÊ®ôÊ∫ñÔºàSmaller is faster but leaves residual noiseÔºâ
searchWindowSize: 21„ÅåÊ®ôÊ∫ñÔºàLarger is higher quality but slowerÔºâ
&quot;&quot;&quot;
denoised = cv2.fastNlMeansDenoising(
    image, None, NLM_H, NLM_TEMPLATE_WINDOW, NLM_SEARCH_WINDOW
)
</code></pre>
<h4>Recording Image Calibration</h4>
<pre><code class="language-python"># Pixel-to-physical size conversion parameters
CALIBRATION_PARAMS = {
    'pixel_size_nm': 2.5,  # nm/pixelÔºàCalibrated from scale barÔºâ
    'scale_bar_length_nm': 500,  # Physical size of scale bar
    'scale_bar_pixels': 200,  # Number of pixels in scale bar
    'calibration_date': '2025-10-15',
    'calibration_method': 'Scale bar measurement',
    'uncertainty': 0.1  # nm/pixelÔºàCalibration uncertaintyÔºâ
}

# Convert pixels to physical size
def pixels_to_nm(pixels, calib_params=CALIBRATION_PARAMS):
    &quot;&quot;&quot;Convert pixel count to physical size (nm)&quot;&quot;&quot;
    return pixels * calib_params['pixel_size_nm']

# Usage Example
diameter_pixels = 50
diameter_nm = pixels_to_nm(diameter_pixels)
print(f&quot;Particle diameter: {diameter_nm:.1f} ¬± {CALIBRATION_PARAMS['uncertainty']*diameter_pixels:.1f} nm&quot;)
</code></pre>
<hr />
<h2>3.2 ÁîªÂÉèPreprocessing</h2>
<h3>Noise Removal</h3>
<p><strong>Code examples1: Comparison of Various Noise Removal Filters</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage import filters, io
from scipy import ndimage

# Fixed random seed (ensuring reproducibility)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# Generate synthetic SEM image (simulating particles)
def generate_synthetic_sem(size=512, num_particles=30):
    &quot;&quot;&quot;Generate synthetic SEM image&quot;&quot;&quot;
    image = np.zeros((size, size), dtype=np.float32)

    # Random particle placement
    for _ in range(num_particles):
        x = np.random.randint(50, size - 50)
        y = np.random.randint(50, size - 50)
        radius = np.random.randint(15, 35)

        # Circular particles
        Y, X = np.ogrid[:size, :size]
        mask = (X - x)**2 + (Y - y)**2 &lt;= radius**2
        image[mask] = 200

    # Add Gaussian noise
    noise = np.random.normal(0, 25, image.shape)
    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)

    return noisy_image

# Generate image
noisy_image = generate_synthetic_sem()

# Various noise removal filters
gaussian_blur = cv2.GaussianBlur(noisy_image, (5, 5), 1.0)
median_filter = cv2.medianBlur(noisy_image, 5)
bilateral_filter = cv2.bilateralFilter(noisy_image, 9, 75, 75)
nlm_filter = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

# Visualization
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

axes[0, 0].imshow(noisy_image, cmap='gray')
axes[0, 0].set_title('Noisy SEM Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(gaussian_blur, cmap='gray')
axes[0, 1].set_title('Gaussian Blur')
axes[0, 1].axis('off')

axes[0, 2].imshow(median_filter, cmap='gray')
axes[0, 2].set_title('Median Filter')
axes[0, 2].axis('off')

axes[1, 0].imshow(bilateral_filter, cmap='gray')
axes[1, 0].set_title('Bilateral Filter')
axes[1, 0].axis('off')

axes[1, 1].imshow(nlm_filter, cmap='gray')
axes[1, 1].set_title('Non-Local Means')
axes[1, 1].axis('off')

# Noise Level Comparison
axes[1, 2].bar(['Original', 'Gaussian', 'Median', 'Bilateral', 'NLM'],
               [np.std(noisy_image),
                np.std(gaussian_blur),
                np.std(median_filter),
                np.std(bilateral_filter),
                np.std(nlm_filter)])
axes[1, 2].set_ylabel('Noise Level (std)')
axes[1, 2].set_title('Denoising Performance')
axes[1, 2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;=== Noise Level (Standard Deviation) ===&quot;)
print(f&quot;Original image: {np.std(noisy_image):.2f}&quot;)
print(f&quot;Gaussian: {np.std(gaussian_blur):.2f}&quot;)
print(f&quot;Median: {np.std(median_filter):.2f}&quot;)
print(f&quot;Bilateral: {np.std(bilateral_filter):.2f}&quot;)
print(f&quot;NLM: {np.std(nlm_filter):.2f}&quot;)
</code></pre>
<p><strong>Filter Selection Guidelines</strong>:
- <strong>Gaussian</strong>: Fast, smooth edges ‚Üí General preprocessing
- <strong>Median</strong>: Edge-preserving, robust to salt & pepper noise
- <strong>Bilateral</strong>: Excellent edge preservation, moderate computational cost
- <strong>Non-Local Means</strong>: Highest quality, high computational cost</p>
<h3>Contrast Adjustment</h3>
<p><strong>Code examples2: HistogramÂùáÁ≠âÂåñ„Å®CLAHE</strong></p>
<pre><code class="language-python">from skimage import exposure

# Contrast Adjustment
hist_eq = exposure.equalize_hist(noisy_image)
clahe = exposure.equalize_adapthist(noisy_image, clip_limit=0.03)

# Histogram computation
hist_original = np.histogram(noisy_image, bins=256, range=(0, 256))[0]
hist_eq_vals = np.histogram(
    (hist_eq * 255).astype(np.uint8), bins=256, range=(0, 256))[0]
hist_clahe_vals = np.histogram(
    (clahe * 255).astype(np.uint8), bins=256, range=(0, 256))[0]

# Visualization
fig = plt.figure(figsize=(16, 10))

# ÁîªÂÉè
ax1 = plt.subplot(2, 3, 1)
ax1.imshow(noisy_image, cmap='gray')
ax1.set_title('Original')
ax1.axis('off')

ax2 = plt.subplot(2, 3, 2)
ax2.imshow(hist_eq, cmap='gray')
ax2.set_title('Histogram Equalization')
ax2.axis('off')

ax3 = plt.subplot(2, 3, 3)
ax3.imshow(clahe, cmap='gray')
ax3.set_title('CLAHE (Adaptive)')
ax3.axis('off')

# Histogram
ax4 = plt.subplot(2, 3, 4)
ax4.hist(noisy_image.ravel(), bins=256, range=(0, 256), alpha=0.7)
ax4.set_xlabel('Pixel Value')
ax4.set_ylabel('Frequency')
ax4.set_title('Original Histogram')
ax4.grid(True, alpha=0.3)

ax5 = plt.subplot(2, 3, 5)
ax5.hist((hist_eq * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='orange')
ax5.set_xlabel('Pixel Value')
ax5.set_ylabel('Frequency')
ax5.set_title('Histogram Eq. Histogram')
ax5.grid(True, alpha=0.3)

ax6 = plt.subplot(2, 3, 6)
ax6.hist((clahe * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='green')
ax6.set_xlabel('Pixel Value')
ax6.set_ylabel('Frequency')
ax6.set_title('CLAHE Histogram')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;=== Contrast Metrics ===&quot;)
print(f&quot;Original image contrast: {noisy_image.max() - noisy_image.min()}&quot;)
print(f&quot;Histogram Eq.: {(hist_eq * 255).max() - (hist_eq * 255).min():.1f}&quot;)
print(f&quot;CLAHE: {(clahe * 255).max() - (clahe * 255).min():.1f}&quot;)
</code></pre>
<p><strong>CLAHEÔºàContrast Limited Adaptive Histogram EqualizationÔºâAdvantages</strong>:
- Local contrast enhancement
- Suppress excessive enhancement (clip_limit parameter)
- Details visible in both dark and bright areas of SEM images</p>
<hr />
<h2>3.3 Particle Detection (Watershed Method)</h2>
<h3>Binarization„Å®Distance transform</h3>
<p><strong>Code examples3: Otsu method for automaticBinarization</strong></p>
<pre><code class="language-python">from skimage import morphology, measure
from scipy.ndimage import distance_transform_edt

# Noise RemovalÂæå„ÅÆÁîªÂÉè„Çí‰ΩøÁî®
denoised = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

# OtsuÊ≥ï„Å´„Çà„ÇãBinarization
threshold = filters.threshold_otsu(denoised)
binary = denoised &gt; threshold

# Morphological operationsÔºàÂ∞è„Åï„Å™Noise RemovalÔºâ
binary_cleaned = morphology.remove_small_objects(binary, min_size=50)
binary_cleaned = morphology.remove_small_holes(binary_cleaned, area_threshold=50)

# Distance transform
distance = distance_transform_edt(binary_cleaned)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(12, 12))

axes[0, 0].imshow(denoised, cmap='gray')
axes[0, 0].set_title('Denoised Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu threshold={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(binary_cleaned, cmap='gray')
axes[1, 0].set_title('After Morphology')
axes[1, 0].axis('off')

axes[1, 1].imshow(distance, cmap='jet')
axes[1, 1].set_title('Distance Transform')
axes[1, 1].axis('off')
axes[1, 1].colorbar = plt.colorbar(axes[1, 1].imshow(distance, cmap='jet'),
                                   ax=axes[1, 1])

plt.tight_layout()
plt.show()

print(f&quot;=== BinarizationÁµêÊûú ===&quot;)
print(f&quot;Otsu threshold: {threshold:.1f}&quot;)
print(f&quot;White pixel percentage: {binary_cleaned.sum() / binary_cleaned.size * 100:.1f}%&quot;)
</code></pre>
<h3>WatershedÊ≥ï„Å´„Çà„ÇãparticlesminutesÈõ¢</h3>
<p><strong>Code examples4: Watershed Segmentation</strong></p>
<pre><code class="language-python">from skimage.feature import peak_local_max
from skimage.segmentation import watershed

# Detect local maxima (estimate particle centers)
local_max = peak_local_max(
    distance,
    min_distance=20,
    threshold_abs=5,
    labels=binary_cleaned
)

# Create markers
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

# Execute Watershed
labels = watershed(-distance, markers, mask=binary_cleaned)

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Display markers
axes[0].imshow(denoised, cmap='gray')
axes[0].plot(local_max[:, 1], local_max[:, 0], 'r+',
             markersize=12, markeredgewidth=2)
axes[0].set_title(f'Detected Centers ({len(local_max)} particles)')
axes[0].axis('off')

# Watershed labels
axes[1].imshow(labels, cmap='nipy_spectral')
axes[1].set_title('Watershed Segmentation')
axes[1].axis('off')

# Overlay contours
overlay = denoised.copy()
overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)
for region in measure.regionprops(labels):
    minr, minc, maxr, maxc = region.bbox
    cv2.rectangle(overlay_rgb, (minc, minr), (maxc, maxr),
                  (255, 0, 0), 2)

axes[2].imshow(overlay_rgb)
axes[2].set_title('Detected Particles')
axes[2].axis('off')

plt.tight_layout()
plt.show()

print(f&quot;=== Watershed Results ===&quot;)
print(f&quot;Number of detected particles: {len(local_max)}&quot;)
print(f&quot;Number of labels: {labels.max()}&quot;)
</code></pre>
<hr />
<h2>3.4 Particle diameterminutesÂ∏ÉËß£Êûê</h2>
<h3>particlesÁâπÂæ¥Èáè„ÅÆÊäΩÂá∫</h3>
<p><strong>Code examples5: Particle diameter„ÉªÂΩ¢Áä∂„Éë„É©„É°„Éº„Çø„ÅÆË®àÁÆó</strong></p>
<pre><code class="language-python"># ÂêÑparticles„ÅÆFeature extraction
particle_data = []

for region in measure.regionprops(labels):
    # Calculate equivalent circular diameter from area
    area = region.area
    equivalent_diameter = np.sqrt(4 * area / np.pi)

    # Aspect ratio
    major_axis = region.major_axis_length
    minor_axis = region.minor_axis_length
    aspect_ratio = major_axis / (minor_axis + 1e-10)

    # Circularity (4œÄ √ó area / perimeter¬≤)
    perimeter = region.perimeter
    circularity = 4 * np.pi * area / (perimeter ** 2 + 1e-10)

    particle_data.append({
        'label': region.label,
        'area': area,
        'diameter': equivalent_diameter,
        'aspect_ratio': aspect_ratio,
        'circularity': circularity,
        'centroid': region.centroid
    })

# DataFrameConvert to
import pandas as pd
df_particles = pd.DataFrame(particle_data)

print(&quot;=== particlesÁâπÂæ¥ÈáèÁµ±Ë®à ===&quot;)
print(df_particles[['diameter', 'aspect_ratio', 'circularity']].describe())

# Particle diameterminutesÂ∏É„Éó„É≠„ÉÉ„Éà
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Histogram
axes[0, 0].hist(df_particles['diameter'], bins=20, alpha=0.7,
                edgecolor='black')
axes[0, 0].set_xlabel('Diameter (pixels)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_title('Particle Size Distribution')
axes[0, 0].axvline(df_particles['diameter'].mean(), color='red',
                   linestyle='--', label=f'Mean: {df_particles[&quot;diameter&quot;].mean():.1f}')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')

# Á¥ØÁ©çminutesÂ∏É
sorted_diameters = np.sort(df_particles['diameter'])
cumulative = np.arange(1, len(sorted_diameters) + 1) / len(sorted_diameters) * 100
axes[0, 1].plot(sorted_diameters, cumulative, linewidth=2)
axes[0, 1].set_xlabel('Diameter (pixels)')
axes[0, 1].set_ylabel('Cumulative Percentage (%)')
axes[0, 1].set_title('Cumulative Size Distribution')
axes[0, 1].axhline(50, color='red', linestyle='--',
                   label=f'D50: {np.median(df_particles[&quot;diameter&quot;]):.1f}')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Êï£Â∏ÉÂõ≥ÔºàÁõ¥ÂæÑ vs Aspect ratioÔºâ
axes[1, 0].scatter(df_particles['diameter'],
                   df_particles['aspect_ratio'],
                   alpha=0.6, s=50)
axes[1, 0].set_xlabel('Diameter (pixels)')
axes[1, 0].set_ylabel('Aspect Ratio')
axes[1, 0].set_title('Diameter vs Aspect Ratio')
axes[1, 0].grid(True, alpha=0.3)

# Scatter plot (diameter vs circularity)
axes[1, 1].scatter(df_particles['diameter'],
                   df_particles['circularity'],
                   alpha=0.6, s=50, color='green')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Circularity')
axes[1, 1].set_title('Diameter vs Circularity')
axes[1, 1].axhline(0.8, color='red', linestyle='--',
                   label='Spherical threshold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Particle diameterÁµ±Ë®à
print(&quot;\n=== Particle diameterÁµ±Ë®à ===&quot;)
print(f&quot;Mean diameter: {df_particles['diameter'].mean():.2f} pixels&quot;)
print(f&quot;Median(D50): {df_particles['diameter'].median():.2f} pixels&quot;)
print(f&quot;Standard deviation: {df_particles['diameter'].std():.2f} pixels&quot;)
print(f&quot;Minimum diameter: {df_particles['diameter'].min():.2f} pixels&quot;)
print(f&quot;Maximum diameter: {df_particles['diameter'].max():.2f} pixels&quot;)
</code></pre>
<h3>Particle diameterminutesÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÔºàÂØæÊï∞TrueË¶èminutesÂ∏ÉÔºâ</h3>
<p><strong>Code examples6: ÂØæÊï∞TrueË¶èminutesÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞</strong></p>
<pre><code class="language-python">from scipy.stats import lognorm

# ÂØæÊï∞TrueË¶èminutesÂ∏É„ÅÆ„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞
diameters = df_particles['diameter'].values
shape, loc, scale = lognorm.fit(diameters, floc=0)

# Fitting results
x = np.linspace(diameters.min(), diameters.max(), 200)
pdf_fitted = lognorm.pdf(x, shape, loc, scale)

# Visualization
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(diameters, bins=20, density=True, alpha=0.6,
         label='Observed', edgecolor='black')
plt.plot(x, pdf_fitted, 'r-', linewidth=2,
         label=f'Log-normal fit (œÉ={shape:.2f})')
plt.xlabel('Diameter (pixels)')
plt.ylabel('Probability Density')
plt.title('Particle Size Distribution Fitting')
plt.legend()
plt.grid(True, alpha=0.3)

# Q-Q„Éó„É≠„ÉÉ„ÉàÔºàCheck goodness of fitÔºâ
plt.subplot(1, 2, 2)
theoretical_quantiles = lognorm.ppf(np.linspace(0.01, 0.99, 100),
                                    shape, loc, scale)
observed_quantiles = np.percentile(diameters, np.linspace(1, 99, 100))
plt.scatter(theoretical_quantiles, observed_quantiles, alpha=0.6)
plt.plot([diameters.min(), diameters.max()],
         [diameters.min(), diameters.max()],
         'r--', linewidth=2, label='Perfect fit')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Observed Quantiles')
plt.title('Q-Q Plot (Log-normal)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;=== ÂØæÊï∞TrueË¶èminutesÂ∏É„Éë„É©„É°„Éº„Çø ===&quot;)
print(f&quot;Shape (œÉ): {shape:.3f}&quot;)
print(f&quot;Scale (median): {scale:.2f} pixels&quot;)
</code></pre>
<hr />
<h2>3.5 Ê∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁîªÂÉèminutesÈ°û</h2>
<h3>Transfer learning (VGG16)„Å´„Çà„ÇãÊùêÊñôÁîªÂÉèminutesÈ°û</h3>
<p><strong>Code examples7: CNN„Å´„Çà„ÇãÊùêÊñôÁõ∏minutesÈ°û</strong></p>
<pre><code class="language-python"># TensorFlow/KerasImport
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    TENSORFLOW_AVAILABLE = True

    # TensorFlowRecord versions (reproducibility)
    print(f&quot;TensorFlow version: {tf.__version__}&quot;)
    print(f&quot;Keras version: {keras.__version__}&quot;)

    # Fix random seed
    RANDOM_SEED = 42
    tf.random.set_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print(&quot;TensorFlow not available. Skipping this example.&quot;)

if TENSORFLOW_AVAILABLE:
    # „Çµ„É≥„Éó„É´ÁîªÂÉèGenerate dataÔºà3„ÇØ„É©„ÇπminutesÈ°ûÔºâ
    def generate_material_images(num_samples=100, img_size=128):
        &quot;&quot;&quot;
        Generate material image samples
        „ÇØ„É©„Çπ0: Sphericalparticles
        „ÇØ„É©„Çπ1: Rod-shapedparticles
        „ÇØ„É©„Çπ2: Irregularparticles
        &quot;&quot;&quot;
        images = []
        labels = []

        for class_id in range(3):
            for _ in range(num_samples):
                img = np.zeros((img_size, img_size), dtype=np.uint8)

                if class_id == 0:  # Spherical
                    num_particles = np.random.randint(5, 15)
                    for _ in range(num_particles):
                        x = np.random.randint(20, img_size - 20)
                        y = np.random.randint(20, img_size - 20)
                        r = np.random.randint(8, 15)
                        cv2.circle(img, (x, y), r, 200, -1)

                elif class_id == 1:  # Rod-shaped
                    num_rods = np.random.randint(3, 8)
                    for _ in range(num_rods):
                        x1 = np.random.randint(10, img_size - 10)
                        y1 = np.random.randint(10, img_size - 10)
                        length = np.random.randint(30, 60)
                        angle = np.random.rand() * 2 * np.pi
                        x2 = int(x1 + length * np.cos(angle))
                        y2 = int(y1 + length * np.sin(angle))
                        cv2.line(img, (x1, y1), (x2, y2), 200, 3)

                else:  # Irregular
                    num_shapes = np.random.randint(5, 12)
                    for _ in range(num_shapes):
                        pts = np.random.randint(10, img_size - 10,
                                                size=(6, 2))
                        cv2.fillPoly(img, [pts], 200)

                # Add noise
                noise = np.random.normal(0, 20, img.shape)
                img = np.clip(img + noise, 0, 255).astype(np.uint8)

                # RGBConvert
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
                images.append(img_rgb)
                labels.append(class_id)

        return np.array(images), np.array(labels)

    # Generate data
    X_data, y_data = generate_material_images(num_samples=150)

    # Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„Éà„Éá„Éº„ÇøminutesÂâ≤
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=RANDOM_SEED
    )

    # Data normalization
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    # VGG16Load model (ImageNet weights)
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 128, 3)
    )

    # Freeze base model weights
    base_model.trainable = False

    # Êñ∞„Åó„ÅÑminutesÈ°ûÂ±§„ÇíËøΩÂä†
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    predictions = Dense(3, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # Compile model
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Train model
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=10,
        batch_size=16,
        verbose=0
    )

    # Evaluate on test data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    print(&quot;=== CNNminutesÈ°ûÁµêÊûú ===&quot;)
    print(f&quot;Test accuracy: {test_acc * 100:.2f}%&quot;)

    # Plot learning curves
    plt.figure(figsize=(14, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Confusion Matrix
    from sklearn.metrics import confusion_matrix, classification_report
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)

    cm = confusion_matrix(y_test, y_pred_classes)

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(3)
    plt.xticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])
    plt.yticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])

    # Êï∞ÂÄ§Ë°®Á§∫
    for i in range(3):
        for j in range(3):
            plt.text(j, i, cm[i, j], ha='center', va='center',
                    color='white' if cm[i, j] &gt; cm.max() / 2 else 'black')

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

    print(&quot;\n=== minutesÈ°û„É¨„Éù„Éº„Éà ===&quot;)
    print(classification_report(
        y_test, y_pred_classes,
        target_names=['Spherical', 'Rod', 'Irregular']
    ))
</code></pre>
<hr />
<h2>3.6 Integrated Image Analysis Pipeline</h2>
<h3>Building an Automated Analysis System</h3>
<p><strong>Code examples8: ÁîªÂÉèËß£Êûê„Éë„Ç§„Éó„É©„Ç§„É≥„ÇØ„É©„Çπ</strong></p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Dict
import json

@dataclass
class ParticleAnalysisResult:
    &quot;&quot;&quot;particlesAnalysis Results&quot;&quot;&quot;
    num_particles: int
    mean_diameter: float
    std_diameter: float
    mean_circularity: float
    particle_data: List[Dict]

class SEMImageAnalyzer:
    &quot;&quot;&quot;SEM Image Automated Analysis System&quot;&quot;&quot;

    def __init__(self, img_size=(512, 512)):
        self.img_size = img_size
        self.image = None
        self.binary = None
        self.labels = None
        self.particles = []

    def load_image(self, image: np.ndarray):
        &quot;&quot;&quot;Load image&quot;&quot;&quot;
        if len(image.shape) == 3:
            self.image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            self.image = image

        # Resize
        self.image = cv2.resize(self.image, self.img_size)

    def preprocess(self, denoise_strength=10):
        &quot;&quot;&quot;PreprocessingÔºàNoise Removal„ÉªContrast AdjustmentÔºâ&quot;&quot;&quot;
        # Noise Removal
        denoised = cv2.fastNlMeansDenoising(
            self.image, None, denoise_strength, 7, 21
        )

        # CLAHE
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)

        self.image = enhanced

    def segment_particles(self, min_size=50):
        &quot;&quot;&quot;particlesSegmentation&quot;&quot;&quot;
        # OtsuBinarization
        threshold = filters.threshold_otsu(self.image)
        binary = self.image &gt; threshold

        # Morphology
        binary_cleaned = morphology.remove_small_objects(
            binary, min_size=min_size
        )
        binary_cleaned = morphology.remove_small_holes(
            binary_cleaned, area_threshold=min_size
        )

        # Distance transform
        distance = distance_transform_edt(binary_cleaned)

        # Watershed
        local_max = peak_local_max(
            distance,
            min_distance=20,
            threshold_abs=5,
            labels=binary_cleaned
        )

        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

        self.labels = watershed(-distance, markers, mask=binary_cleaned)
        self.binary = binary_cleaned

    def extract_features(self):
        &quot;&quot;&quot;Feature extraction&quot;&quot;&quot;
        self.particles = []

        for region in measure.regionprops(self.labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            aspect_ratio = region.major_axis_length / \
                          (region.minor_axis_length + 1e-10)
            circularity = 4 * np.pi * area / \
                         (region.perimeter ** 2 + 1e-10)

            self.particles.append({
                'label': region.label,
                'area': area,
                'diameter': diameter,
                'aspect_ratio': aspect_ratio,
                'circularity': circularity,
                'centroid': region.centroid
            })

    def get_results(self) -&gt; ParticleAnalysisResult:
        &quot;&quot;&quot;Get results&quot;&quot;&quot;
        df = pd.DataFrame(self.particles)

        return ParticleAnalysisResult(
            num_particles=len(self.particles),
            mean_diameter=df['diameter'].mean(),
            std_diameter=df['diameter'].std(),
            mean_circularity=df['circularity'].mean(),
            particle_data=self.particles
        )

    def visualize(self):
        &quot;&quot;&quot;ÁµêÊûúVisualization&quot;&quot;&quot;
        fig, axes = plt.subplots(2, 2, figsize=(14, 14))

        # Original image
        axes[0, 0].imshow(self.image, cmap='gray')
        axes[0, 0].set_title('Preprocessed Image')
        axes[0, 0].axis('off')

        # Binarization
        axes[0, 1].imshow(self.binary, cmap='gray')
        axes[0, 1].set_title('Binary Segmentation')
        axes[0, 1].axis('off')

        # Watershed labels
        axes[1, 0].imshow(self.labels, cmap='nipy_spectral')
        axes[1, 0].set_title(f'Particles ({len(self.particles)})')
        axes[1, 0].axis('off')

        # Particle diameterminutesÂ∏É
        df = pd.DataFrame(self.particles)
        axes[1, 1].hist(df['diameter'], bins=20, alpha=0.7,
                       edgecolor='black')
        axes[1, 1].set_xlabel('Diameter (pixels)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Particle Size Distribution')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.show()

    def save_results(self, filename='analysis_results.json'):
        &quot;&quot;&quot;Save results to JSON&quot;&quot;&quot;
        results = self.get_results()
        output = {
            'num_particles': results.num_particles,
            'mean_diameter': results.mean_diameter,
            'std_diameter': results.std_diameter,
            'mean_circularity': results.mean_circularity,
            'particles': results.particle_data
        }

        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)

        print(f&quot;Results saved to {filename}&quot;)

# Usage Example
analyzer = SEMImageAnalyzer()
analyzer.load_image(noisy_image)
analyzer.preprocess(denoise_strength=10)
analyzer.segment_particles(min_size=50)
analyzer.extract_features()

results = analyzer.get_results()
print(&quot;=== Analysis Results ===&quot;)
print(f&quot;Ê§úÂá∫particlesÊï∞: {results.num_particles}&quot;)
print(f&quot;Mean diameter: {results.mean_diameter:.2f} ¬± {results.std_diameter:.2f} pixels&quot;)
print(f&quot;MeanÂÜÜÂΩ¢Â∫¶: {results.mean_circularity:.3f}&quot;)

analyzer.visualize()
analyzer.save_results('sem_analysis.json')
</code></pre>
<hr />
<h2>3.7 Practical Pitfalls and Solutions</h2>
<h3>Common Failure Examples and Best Practices</h3>
<h4>Â§±Êïó1: ÈÅéÂ∫¶„Å™SegmentationÔºàOver-segmentationÔºâ</h4>
<p><strong>Symptom</strong>: 1„Å§„ÅÆparticles„ÅåË§áÊï∞„Å´minutesÂâ≤„Åï„Çå„Çã</p>
<p><strong>Cause</strong>: Watershedmin_distance is too small, or residual noise remains</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad ExampleÔºöÈÅéÂ∫¶„Å™Segmentation
local_max = peak_local_max(distance, min_distance=5)  # Too small
# ÁµêÊûú: „Éé„Ç§„Ç∫„ÇÑparticlesÂÜÖ„ÅÆÂæÆÂ∞è„Å™ÂáπÂá∏„ÇíÂà•particles„Å®Ë™çË≠ò

# ‚úÖ Good ExampleÔºöÈÅ©Âàá„Å™min_distanceË®≠ÂÆö
# particles„ÅÆÂÖ∏ÂûãÁöÑ„Å™Áõ¥ÂæÑ„Åã„ÇâË®≠ÂÆö
expected_diameter_pixels = 30
min_distance = int(expected_diameter_pixels * 0.6)  # About 60% of diameter

local_max = peak_local_max(
    distance,
    min_distance=min_distance,
    threshold_abs=5,  # Distance transformÂÄ§„ÅÆÈñæÂÄ§
    labels=binary_cleaned
)
print(f&quot;min_distance: {min_distance} pixels&quot;)
print(f&quot;Number of detected particles: {len(local_max)}&quot;)
</code></pre>
<h4>Pitfall 2: Incorrect Threshold Selection</h4>
<p><strong>Symptom</strong>: particles„ÅÆ‰∏ÄÈÉ®„ÅåÊ¨†Êêç„ÄÅ„Åæ„Åü„ÅØBackground„Ååparticles„Å®„Åó„Å¶Ê§úÂá∫</p>
<p><strong>Cause</strong>: OtsuÊ≥ï„Åå‰∏çÈÅ©ÂàáÔºà„Éê„Ç§„É¢„Éº„ÉÄ„É´„Åß„Å™„ÅÑHistogramÔºâ</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad ExampleÔºöÂÖ®ÁîªÂÉè„Å´ÁÑ°Êù°‰ª∂„Å´OtsuÈÅ©Áî®
threshold = filters.threshold_otsu(image)
binary = image &gt; threshold

# ‚úÖ Good ExampleÔºöHistogramÊ§úË®º„Å®„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
threshold_otsu = filters.threshold_otsu(image)

# Histogram„ÅÆÂΩ¢Áä∂Á¢∫Ë™ç
hist, bin_edges = np.histogram(image, bins=256, range=(0, 256))

# Simple check for bimodality (two peaks)
from scipy.signal import find_peaks
peaks, _ = find_peaks(hist, prominence=100)

if len(peaks) &gt;= 2:
    # Bimodal ‚Üí Otsu applicable
    threshold = threshold_otsu
    print(f&quot;Otsu threshold: {threshold:.1f} (bimodal histogram)&quot;)
else:
    # ÂçòÂ≥∞ÊÄß ‚Üí ‰ª£ÊõøÊâãÊ≥ïÔºàMean+Standard deviation„Å™„Å©Ôºâ
    threshold = image.mean() + image.std()
    print(f&quot;Mean+Std threshold: {threshold:.1f} (unimodal histogram)&quot;)
    print(&quot;Ë≠¶Âëä: Histogram„ÅåÂçòÂ≥∞ÊÄß„Åß„Åô„ÄÇOtsuÊ≥ï„ÅØ‰∏çÈÅ©Âàá„Å™ÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô&quot;)

binary = image &gt; threshold
</code></pre>
<h4>Pitfall 3: Lack of Pixel Calibration</h4>
<p><strong>Symptom</strong>: Physical size (nm, Œºm) is unknown, data at different magnifications cannot be compared</p>
<p><strong>Cause</strong>: Scale bar information not utilized</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad ExampleÔºö„Éî„ÇØ„Çª„É´Âçò‰Ωç„ÅÆ„Åæ„ÅæÂ†±Âëä
print(f&quot;MeanParticle diameter: {mean_diameter:.1f} pixels&quot;)  # Physical size unknown

# ‚úÖ Good ExampleÔºö„Çπ„Ç±„Éº„É´„Éê„ÉºCalibration
# Step 1: Measure scale bar length (manual or OCR)
SCALE_BAR_NM = 500  # Physical size of scale barÔºànmÔºâ
SCALE_BAR_PIXELS = 200  # Number of pixels in scale bar

# Step 2: Calculate calibration coefficient
nm_per_pixel = SCALE_BAR_NM / SCALE_BAR_PIXELS
print(f&quot;Calibration: {nm_per_pixel:.2f} nm/pixel&quot;)

# Step3: Áâ©ÁêÜ„Çµ„Ç§„Ç∫Convert to
mean_diameter_nm = mean_diameter * nm_per_pixel
std_diameter_nm = std_diameter * nm_per_pixel

print(f&quot;MeanParticle diameter: {mean_diameter_nm:.1f} ¬± {std_diameter_nm:.1f} nm&quot;)

# Step4: CalibrationÊÉÖÂ†±„ÇíÁµêÊûú„Å´‰øùÂ≠ò
calibration_info = {
    'scale_bar_nm': SCALE_BAR_NM,
    'scale_bar_pixels': SCALE_BAR_PIXELS,
    'nm_per_pixel': nm_per_pixel,
    'calibration_date': '2025-10-19'
}
</code></pre>
<h4>Pitfall 4: CNN Overfitting (Biased Training Data)</h4>
<p><strong>Symptom</strong>: High training accuracy but poor performance on test data</p>
<p><strong>Cause</strong>: Dataset bias, insufficient data augmentation</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad ExampleÔºö„Éá„Éº„ÇøÊã°Âºµ„Å™„Åó„ÄÅÂ∞èË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà
model.fit(X_train, y_train, epochs=50, batch_size=16)
# Result: Overfitting to training data

# ‚úÖ Good ExampleÔºö„Éá„Éº„ÇøÊã°Âºµ„Å®Early Stopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# Data augmentation settings
datagen = ImageDataGenerator(
    rotation_range=20,  # Random rotation
    width_shift_range=0.1,  # Horizontal shift
    height_shift_range=0.1,  # Vertical shift
    horizontal_flip=True,  # Horizontal flip
    zoom_range=0.1,  # Zoom
    fill_mode='nearest'
)

# Early StoppingÔºàStop training when validation loss does not improveÔºâ
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Ë®ìÁ∑¥
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=16),
    validation_data=(X_test, y_test),
    epochs=50,
    callbacks=[early_stop],
    verbose=1
)

print(f&quot;Training stopped at epoch: {len(history.history['loss'])}&quot;)
</code></pre>
<h4>Pitfall 5: Batch Effect in Batch Processing</h4>
<p><strong>Symptom</strong>: Âêå‰∏ÄÊù°‰ª∂„ÅÆ„Çµ„É≥„Éó„É´„Åß„ÇÇÊ∏¨ÂÆöÊó•„Å´„Çà„Å£„Å¶Particle diameter„ÅåÁ≥ªÁµ±ÁöÑ„Å´Áï∞„Å™„Çã</p>
<p><strong>Cause</strong>: Ë£ÖÁΩÆ„ÅÆÁµåÊôÇÂ§âÂåñ„ÄÅCalibration„Åö„Çå</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># ‚ùå Bad ExampleÔºö„Éê„ÉÉ„ÉÅÈñì„ÅÆË£úTrue„Å™„Åó
results = []
for image_file in image_files:
    analyzer.load_image(image_file)
    analyzer.preprocess()
    results.append(analyzer.get_results())

# ‚úÖ Good ExampleÔºöÊ®ôÊ∫ñË©¶Êñô„Å´„Çà„ÇãË£úTrue
from datetime import datetime

# Ê®ôÊ∫ñË©¶ÊñôÔºàÊó¢Áü•Particle diameterÔºâ„ÅÆÊ∏¨ÂÆö
STANDARD_DIAMETER_NM = 100.0  # Known physical size

def calibrate_with_standard(standard_image, expected_diameter_nm):
    &quot;&quot;&quot;Ê®ôÊ∫ñË©¶Êñô„Åã„ÇâCalibration‰øÇÊï∞„ÇíÂèñÂæó&quot;&quot;&quot;
    analyzer_std = SEMImageAnalyzer()
    analyzer_std.load_image(standard_image)
    analyzer_std.preprocess()
    analyzer_std.segment_particles()
    analyzer_std.extract_features()

    results_std = analyzer_std.get_results()
    measured_diameter_pixels = results_std.mean_diameter

    nm_per_pixel = expected_diameter_nm / measured_diameter_pixels

    calibration = {
        'date': datetime.now().isoformat(),
        'nm_per_pixel': nm_per_pixel,
        'standard_diameter_nm': expected_diameter_nm,
        'measured_pixels': measured_diameter_pixels
    }

    return calibration

# Measure standard sample at the beginning of each batch
batch_calibrations = {}
for batch_id, batch_images in batches.items():
    # Standard sample measurement
    standard_image = load_standard_image(batch_id)
    calib = calibrate_with_standard(standard_image, STANDARD_DIAMETER_NM)
    batch_calibrations[batch_id] = calib

    print(f&quot;Batch {batch_id}: {calib['nm_per_pixel']:.3f} nm/pixel&quot;)

    # „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆ„Çµ„É≥„Éó„É´Ëß£ÊûêÔºàCalibration‰øÇÊï∞„ÇíÈÅ©Áî®Ôºâ
    for image_file in batch_images:
        analyzer.load_image(image_file)
        analyzer.preprocess()
        analyzer.segment_particles()
        analyzer.extract_features()

        results = analyzer.get_results()

        # Calibration‰øÇÊï∞„ÅßÁâ©ÁêÜ„Çµ„Ç§„Ç∫Convert to
        diameter_nm = results.mean_diameter * calib['nm_per_pixel']
        print(f&quot;  Sample: {diameter_nm:.1f} nm&quot;)
</code></pre>
<hr />
<h2>3.8 Image Analysis Skills Checklist</h2>
<h3>ÁîªÂÉèPreprocessing„Çπ„Ç≠„É´</h3>
<h4>Foundational Level</h4>
<ul>
<li>[ ] Can explain characteristics of SEM/TEM images (contrast, noise)</li>
<li>[ ] OpenCV„ÅßÁîªÂÉè„ÇíË™≠„ÅøËæº„Åø„ÄÅ„Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´Convert„Åß„Åç„Çã</li>
<li>[ ] HistogramÂùáÁ≠âÂåñ„Å®CLAHE„ÅÆÈÅï„ÅÑ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Gaussian„Éï„Ç£„É´„Çø„ÅßNoise Removal„Åß„Åç„Çã</li>
<li>[ ] OtsuÊ≥ï„ÅßBinarization„Åß„Åç„Çã</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can appropriately set parameters for Non-Local Means filter</li>
<li>[ ] ÁîªÂÉè contrast‰∏çËâØ„ÇíË®∫Êñ≠„Åó„ÄÅÈÅ©Âàá„Å™Preprocessing„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>[ ] Morphological operationsÔºàopening„ÄÅclosingÔºâ„Çí‰Ωø„ÅÑminutes„Åë„Çâ„Çå„Çã</li>
<li>[ ] Histogram„ÅÆÂΩ¢Áä∂„Åã„ÇâÈÅ©Âàá„Å™ÈñæÂÄ§Ë®≠ÂÆöÊ≥ï„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>[ ] Ë§áÊï∞„ÅÆ„Éï„Ç£„É´„Çø„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶ÊúÄÈÅ©„Å™Preprocessing„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅßÁîªÂÉèÂìÅË≥™„ÅÆ„Å∞„Çâ„Å§„Åç„ÇíËá™ÂãïÊ§úÂá∫„Åó„ÄÅÈÅ©ÂøúÁöÑ„Å´Preprocessing„Åß„Åç„Çã</li>
<li>[ ] Can design and implement custom filters (frequency domain filters)</li>
<li>[ ] Can implement deep learning denoising (DnCNN)</li>
<li>[ ] ÁîªÂÉèPreprocessing„ÅåÂæåÁ∂öËß£Êûê„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÂÆöÈáèË©ï‰æ°„Åß„Åç„Çã</li>
</ul>
<h3>particlesÊ§úÂá∫„ÉªSegmentation„Çπ„Ç≠„É´</h3>
<h4>Foundational Level</h4>
<ul>
<li>[ ] Distance transform„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çã</li>
<li>[ ] Can explain the basic principles of Watershed method</li>
<li>[ ] particles„ÅÆÊï∞„Çí„Ç´„Ç¶„É≥„Éà„Åß„Åç„Çã</li>
<li>[ ] Particle diameterÔºàÂÜÜÁõ∏ÂΩìÁõ¥ÂæÑÔºâ„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] Ê§úÂá∫ÁµêÊûú„ÇíVisualization„Åß„Åç„Çã</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Watershed„ÅÆmin_distance„Éë„É©„É°„Éº„Çø„ÇíParticle diameter„Å´Âøú„Åò„Å¶Ë®≠ÂÆö„Åß„Åç„Çã</li>
<li>[ ] Can diagnose over-detection and missed detection, and adjust parameters</li>
<li>[ ] Èáç„Å™„Å£„Åüparticles„ÇíminutesÈõ¢„Åß„Åç„Çã</li>
<li>[ ] ÂΩ¢Áä∂„Éë„É©„É°„Éº„ÇøÔºàÂÜÜÂΩ¢Â∫¶„ÄÅAspect ratioÔºâ„ÇíË®àÁÆó„ÉªËß£Èáà„Åß„Åç„Çã</li>
<li>[ ] Particle diameterminutesÂ∏É„ÇíÁµ±Ë®à„É¢„Éá„É´ÔºàÂØæÊï∞TrueË¶èminutesÂ∏ÉÔºâ„Åß„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„Åß„Åç„Çã</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Ë§áÈõë„Å™ÂΩ¢Áä∂particles„Å´ÂØæ„Åó„Å¶Active Contour„É¢„Éá„É´„ÇíÈÅ©Áî®„Åß„Åç„Çã</li>
<li>[ ] Ê©üÊ¢∞Â≠¶ÁøíÔºàU-NetÔºâ„Å´„Çà„Çã„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØSegmentation„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] 3DÁîªÂÉèÔºàXÁ∑öCTÔºâ„ÅÆ3Ê¨°ÂÖÉSegmentation„Åå„Åß„Åç„Çã</li>
<li>[ ] SegmentationÁ≤æÂ∫¶„ÇíÂÆöÈáèË©ï‰æ°ÔºàDice‰øÇÊï∞„ÄÅIoUÔºâ„Åß„Åç„Çã</li>
</ul>
<h3>ÁîªÂÉèÊ∏¨ÂÆö„ÉªCalibration„Çπ„Ç≠„É´</h3>
<h4>Foundational Level</h4>
<ul>
<li>[ ] Can locate scale bar position</li>
<li>[ ] ÊâãÂãï„Åß„Éî„ÇØ„Çª„É´-Áâ©ÁêÜ„Çµ„Ç§„Ç∫Convert„Åå„Åß„Åç„Çã</li>
<li>[ ] Particle diameter„Çínm/ŒºmÂçò‰Ωç„ÅßÂ†±Âëä„Åß„Åç„Çã</li>
<li>[ ] Recognize the existence of measurement error</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] „Çπ„Ç±„Éº„É´„Éê„Éº„Åã„ÇâËá™Âãï„ÅßCalibration‰øÇÊï∞„ÇíË®àÁÆó„Åß„Åç„Çã</li>
<li>[ ] Can analyze images at different magnifications uniformly</li>
<li>[ ] Calibration uncertainty„ÇíÊé®ÂÆö„Åó„ÄÅÁµêÊûú„Å´‰∏çÁ¢∫„Åã„Åï„ÇíÊòéÁ§∫„Åß„Åç„Çã</li>
<li>[ ] Can evaluate measurement repeatability (reproducibility)</li>
<li>[ ] Ê®ôÊ∫ñË©¶Êñô„Å´„Çà„ÇãË£ÖÁΩÆCalibration„ÇíÂÆüÊñΩ„Åß„Åç„Çã</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can correct for batch effects (periodic measurement of standard samples)</li>
<li>[ ] Can propagate measurement uncertainty (ISO GUM)</li>
<li>[ ] Can verify measurement value compatibility between different instruments</li>
<li>[ ] „Éà„É¨„Éº„Çµ„Éñ„É´„Å™Calibration„ÉÅ„Çß„Éº„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã</li>
</ul>
<h3>Ê∑±Â±§Â≠¶Áøí„ÉªÁîªÂÉèminutesÈ°û„Çπ„Ç≠„É´</h3>
<h4>Foundational Level</h4>
<ul>
<li>[ ] Understand basic CNN structure (convolutional layers, pooling layers)</li>
<li>[ ] Can explain the concept of transfer learning</li>
<li>[ ] Ë®ìÁ∑¥„Éá„Éº„Çø„Å®„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÇíminutesÂâ≤„Åß„Åç„Çã</li>
<li>[ ] Can evaluate model accuracy</li>
<li>[ ] Confusion Matrix„ÇíËß£Èáà„Åß„Åç„Çã</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can apply data augmentation</li>
<li>[ ] Can prevent overfitting with Early Stopping</li>
<li>[ ] Can adjust hyperparameters (learning rate, batch size)</li>
<li>[ ] Can compare different architectures (VGG, ResNet, EfficientNet)</li>
<li>[ ] Grad-CAM„ÅßÂà§Êñ≠Ê†πÊã†„ÇíVisualization„Åß„Åç„Çã</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can design custom CNN architectures</li>
<li>[ ] Can handle imbalanced data (class weights, SMOTE)</li>
<li>[ ] „Éû„É´„ÉÅ„Çø„Çπ„ÇØÂ≠¶ÁøíÔºàminutesÈ°ûÔºãÂõûÂ∏∞Ôºâ„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>[ ] Can quantify model uncertainty (Bayesian Deep Learning)</li>
<li>[ ] Can implement few-shot learning with limited data</li>
</ul>
<h3>Integrated Skills: Overall Workflow</h3>
<h4>Foundational Level</h4>
<ul>
<li>[ ] Load image‚ÜíPreprocessing‚ÜíSegmentation‚ÜíFeature Extraction„ÅÆÊµÅ„Çå„ÇíÂÆüË°å„Åß„Åç„Çã</li>
<li>[ ] Can summarize and report results in tables</li>
<li>[ ] „Ç∞„É©„ÉïÔºàHistogram„ÄÅÊï£Â∏ÉÂõ≥Ôºâ„ÅßVisualization„Åß„Åç„Çã</li>
</ul>
<h4>Applied Level</h4>
<ul>
<li>[ ] Can automate batch processing of multiple images</li>
<li>[ ] Can design processing pipeline as classes</li>
<li>[ ] Can save results in JSON/CSV format</li>
<li>[ ] Can implement error handling and logging</li>
<li>[ ] Can manage parameters in YAML/JSON files</li>
</ul>
<h4>Advanced Level</h4>
<ul>
<li>[ ] Can publish analysis tool with web interface (Streamlit, Gradio)</li>
<li>[ ] Can build scalable batch processing on cloud (AWS, GCP)</li>
<li>[ ] DatabaseÔºàMongoDBÔºâ„Å´ÁµêÊûú„Çí‰øùÂ≠ò„ÉªÊ§úÁ¥¢„Åß„Åç„Çã</li>
<li>[ ] Can automate testing with CI/CD pipeline</li>
<li>[ ] Can automatically generate publication-quality figures and reports</li>
</ul>
<hr />
<h2>3.9 Comprehensive Skills Assessment</h2>
<p>Please conduct self-assessment based on the following criteria.</p>
<h3>Level 1: Beginner (60%+ of foundational skills)</h3>
<ul>
<li>Âü∫Êú¨ÁöÑ„Å™ÁîªÂÉèPreprocessing„Å®particlesÊ§úÂá∫„Åå„Åß„Åç„Çã</li>
<li>Can understand existing code and adjust parameters</li>
<li>Can complete analysis on simple datasets</li>
</ul>
<p><strong>Next Steps</strong>:
- Applied Level„ÅÆÊäÄË°ìÔºàWatershed„ÄÅ„Éá„Éº„ÇøÊã°ÂºµÔºâ„ÇíÁøíÂæó
- Challenge more complex real data
- Deepen understanding of parameter meanings</p>
<h3>Level 2: Intermediate (100% foundational + 60%+ applied)</h3>
<ul>
<li>Ë§áÈõë„Å™ÁîªÂÉè„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶ÈÅ©Âàá„Å™Preprocessing„ÉªSegmentation„ÇíÈÅ∏Êäû„Åß„Åç„Çã</li>
<li>CNN„Å´„Çà„ÇãÁîªÂÉèminutesÈ°û„ÇíÂÆüË£Ö„Åß„Åç„Çã</li>
<li>Can perform batch processing and error handling</li>
</ul>
<p><strong>Next Steps</strong>:
- Challenge advanced techniques (custom CNN, 3D analysis)
- Practice in research projects
- Optimize code and improve scalability</p>
<h3>Level 3: Advanced (100% foundational + 100% applied + 60%+ advanced)</h3>
<ul>
<li>Can design and implement new algorithms</li>
<li>Can execute research paper-level analysis</li>
<li>Can provide tools to other researchers</li>
</ul>
<p><strong>Next Steps</strong>:
- Develop original methods
- Paper writing and conference presentations
- Contribute to open source</p>
<h3>Level 4: Expert (90%+ of all items)</h3>
<ul>
<li>ÊùêÊñôÁîªÂÉèËß£Êûê„ÅÆminutesÈáé„Çí„É™„Éº„Éâ„Åß„Åç„Çã</li>
<li>Êñ∞„Åó„ÅÑMeasurement Technique„Å´ÂØæÂøú„Åó„ÅüËß£ÊûêÊâãÊ≥ï„ÇíÈñãÁô∫„Åß„Åç„Çã</li>
<li>Contributing to international community</li>
</ul>
<p><strong>Activity Examples</strong>:
- Invited talks and tutorials
- Leading collaborative research
- Participating in standardization activities</p>
<hr />
<h2>3.10 Action Plan Template</h2>
<h3>Current Level: <strong><em>_</em></strong>____</h3>
<h3>Target Level (in 3 months): <strong><em>_</em></strong>____</h3>
<h3>Priority Skills to Strengthen (select 3):</h3>
<ol>
<li>
<hr />
</li>
<li>
<hr />
</li>
<li>
<hr />
</li>
</ol>
<h3>Specific Action Plan:</h3>
<p><strong>Week 1-2</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 3-4</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 5-8</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<p><strong>Week 9-12</strong>:
- [ ] Action 1: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_
- [ ] Action 2: </strong><strong><em>_</em></strong><strong><em>_</em></strong>____</p>
<h3>Evaluation Metrics:</h3>
<ul>
<li>[ ] Complete analysis on custom dataset</li>
<li>[ ] Present at research meeting/seminar</li>
<li>[ ] Publish results on GitHub/paper</li>
</ul>
<hr />
<h2>3.11 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>„Éá„Éº„ÇøLicense„Å®ÂÜçÁèæÊÄß</strong>
   - Utilization of image data repositories
   - ÁîªÂÉè„É°„Çø„Éá„Éº„Çø„Å®CalibrationÊÉÖÂ†±„ÅÆË®òÈå≤
   - Best Practices for Code Reproducibility</p>
</li>
<li>
<p><strong>ÁîªÂÉèPreprocessing</strong>
   - Noise RemovalÔºàGaussian„ÄÅMedian„ÄÅBilateral„ÄÅNLMÔºâ
   - Contrast AdjustmentÔºàHistogram Equalization„ÄÅCLAHEÔºâ
   - BinarizationÔºàOtsuÊ≥ïÔºâ</p>
</li>
<li>
<p><strong>particlesÊ§úÂá∫</strong>
   - WatershedÊ≥ï„Å´„Çà„ÇãSegmentation
   - Distance transform„Å®Â±ÄÊâÄÊúÄÂ§ßÂÄ§Ê§úÂá∫
   - Morphological operations</p>
</li>
<li>
<p><strong>Quantitative Analysis</strong>
   - Particle diameterminutesÂ∏ÉÔºàHistogram„ÄÅÁ¥ØÁ©çminutesÂ∏ÉÔºâ
   - ÂΩ¢Áä∂„Éë„É©„É°„Éº„ÇøÔºàÂÜÜÂΩ¢Â∫¶„ÄÅAspect ratioÔºâ
   - ÂØæÊï∞TrueË¶èminutesÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞</p>
</li>
<li>
<p><strong>Ê∑±Â±§Â≠¶Áøí</strong>
   - Transfer learning (VGG16)
   - ÊùêÊñôÁîªÂÉèminutesÈ°û
   - Confusion Matrix„Å´„Çà„ÇãÊÄßËÉΩË©ï‰æ°</p>
</li>
<li>
<p><strong>ÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥</strong>
   - ÈÅéÂ∫¶„Å™Segmentation„ÅÆÂõûÈÅø
   - „Éî„ÇØ„Çª„É´Calibration„ÅÆÈáçË¶ÅÊÄß
   - Correction of batch effects
   - CNN„ÅÆÈÅéÂ≠¶ÁøíSolution</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>‚úÖ Always record image metadata (magnification, scale bar)</li>
<li>‚úÖ Preprocessing„ÅÆË≥™„ÅåSegmentationÁ≤æÂ∫¶„ÇíÊ±∫ÂÆö„Åô„Çã</li>
<li>‚úÖ Parameter tuning is important for Watershed method (min_distance, threshold_abs)</li>
<li>‚úÖ Particle diameter„ÅØÁâ©ÁêÜÂçò‰ΩçÔºànm„ÄÅŒºmÔºâ„ÅßÂ†±Âëä„Åô„Çã</li>
<li>‚úÖ „Éî„ÇØ„Çª„É´Calibration„ÅØÊ®ôÊ∫ñË©¶Êñô„ÅßÂÆöÊúüÁöÑ„Å´Ê§úË®º</li>
<li>‚úÖ Ëª¢ÁßªÂ≠¶Áøí„Å´„Çà„ÇäÂ∞ëÈáè„Éá„Éº„Çø„Åß„ÇÇÈ´òÁ≤æÂ∫¶minutesÈ°û„ÅåÂÆüÁèæ</li>
<li>‚úÖ Prevent overfitting with data augmentation and Early Stopping</li>
</ul>
<h3>Next Chapter</h3>
<p>In Chapter 4, we will learn about time-series data and integrated analysis:
- Ê∏©Â∫¶„ÉªÂúßÂäõ„Çª„É≥„Çµ„Éº„Éá„Éº„Çø„ÅÆPreprocessing
- Moving window analysis
- Anomaly detection
- Dimensionality reduction using PCA
- Automation using sklearn Pipeline</p>
<p><strong><a href="./chapter-4.html">Chapter 4: Time-Series Data and Integrated Analysis ‚Üí</a></strong></p>
<hr />
<h2>Exercises</h2>
<h3>È°å1ÔºàÈõ£ÊòìÂ∫¶ÔºöeasyÔºâ</h3>
<p>Determine whether the following statements are true or false.</p>
<ol>
<li>Bilateral Filter can preserve edges better than Gaussian Filter</li>
<li>CLAHE„ÅØÁîªÂÉèÂÖ®‰Ωì„Å´Âêå‰∏Ä„ÅÆHistogramÂùáÁ≠âÂåñ„ÇíÈÅ©Áî®„Åô„Çã</li>
<li>WatershedÊ≥ï„Åß„ÅØDistance transform„ÅÆÂ±ÄÊâÄÊúÄÂ§ßÂÄ§„Çíparticles‰∏≠ÂøÉ„Å®„Åø„Å™„Åô</li>
</ol>
<details>
<summary>Hint</summary>

1. Operating principle of Bilateral Filter (considers both spatial distance and intensity difference)
2. CLAHE"Adaptive" meaning of
3. Watershed„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÊµÅ„ÇåÔºàDistance transform‚Üí„Éû„Éº„Ç´„Éº‚ÜíwatershedÔºâ

</details>

<details>
<summary>Solution Example</summary>

**Answer**:
1. **True** - Bilateral Filter considers intensity differences, so smoothing is suppressed near edges
2. **False** - CLAHE„ÅØÁîªÂÉè„ÇíÂ∞èÈ†òÂüüÔºà„Çø„Ç§„É´Ôºâ„Å´minutesÂâ≤„Åó„ÄÅÂêÑÈ†òÂüü„ÅßÈÅ©ÂøúÁöÑ„Å´HistogramÂùáÁ≠âÂåñ„ÇíË°å„ÅÜ
3. **True** - Distance transform„ÅÆÂ±ÄÊâÄÊúÄÂ§ßÂÄ§„Ååparticles‰∏≠ÂøÉ„Å´ÂØæÂøú„Åó„ÄÅWatershed„ÅÆ„Éû„Éº„Ç´„Éº„Å®„Åó„Å¶‰ΩøÁî®„Åï„Çå„Çã

**Explanation**:
ÁîªÂÉèPreprocessing„Åß„ÅØ„ÄÅÂá¶ÁêÜ„ÅÆÁõÆÁöÑÔºàNoise Removal vs „Ç®„ÉÉ„Ç∏‰øùÊåÅÔºâ„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™ÊâãÊ≥ï„ÇíÈÅ∏Êäû„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô„ÄÇWatershed„ÅØDistance transform„ÇíÊ¥ªÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅÊé•Ëß¶„Åó„Å¶„ÅÑ„Çãparticles„ÇÇminutesÈõ¢„Åß„Åç„ÇãÂº∑Âäõ„Å™ÊâãÊ≥ï„Åß„Åô„ÄÇ

</details>

<hr />
<h3>È°å2ÔºàÈõ£ÊòìÂ∫¶ÔºömediumÔºâ</h3>
<p>‰ª•‰∏ã„ÅÆSEMÁîªÂÉè„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶„ÄÅparticlesÊ§úÂá∫„Å®Particle diameterminutesÂ∏ÉËß£Êûê„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<pre><code class="language-python">import numpy as np

# „Çµ„É≥„Éó„É´SEMGenerate image
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()
</code></pre>
<p><strong>Requirements</strong>:
1. Non-Local Means„ÅßNoise Removal
2. OtsuÊ≥ï„ÅßBinarization
3. WatershedÊ≥ï„ÅßparticlesÊ§úÂá∫
4. Particle diameterminutesÂ∏É„ÇíHistogram„Åß„Éó„É≠„ÉÉ„Éà
5. MeanParticle diameter„ÉªStandard deviation„ÇíÂá∫Âäõ</p>
<details>
<summary>Hint</summary>

**Processing Flow**:
1. `cv2.fastNlMeansDenoising`„ÅßNoise Removal
2. `filters.threshold_otsu`for threshold calculation‚ÜíBinarization
3. `distance_transform_edt` + `peak_local_max` + `watershed`
4. `measure.regionprops`„ÅßparticlesFeature extraction
5. `matplotlib.pyplot.hist`„ÅßVisualization

</details>

<details>
<summary>Solution Example</summary>


<pre><code class="language-python">import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage import filters, morphology, measure
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from scipy.ndimage import distance_transform_edt

# „Çµ„É≥„Éó„É´Generate image
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()

# Step1: Noise Removal
denoised = cv2.fastNlMeansDenoising(sample_image, None, 10, 7, 21)

# Step2: OtsuBinarization
threshold = filters.threshold_otsu(denoised)
binary = denoised &gt; threshold
binary = morphology.remove_small_objects(binary, min_size=30)

# Step 3: Watershed
distance = distance_transform_edt(binary)
local_max = peak_local_max(distance, min_distance=15,
                           threshold_abs=3, labels=binary)
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
labels = watershed(-distance, markers, mask=binary)

# Step4: Particle diameterË®àÁÆó
diameters = []
for region in measure.regionprops(labels):
    area = region.area
    diameter = np.sqrt(4 * area / np.pi)
    diameters.append(diameter)

diameters = np.array(diameters)

# Step5: Áµ±Ë®à„ÉªVisualization
print(&quot;=== Particle diameterÁµ±Ë®à ===&quot;)
print(f&quot;Ê§úÂá∫particlesÊï∞: {len(diameters)}&quot;)
print(f&quot;MeanParticle diameter: {diameters.mean():.2f} pixels&quot;)
print(f&quot;Standard deviation: {diameters.std():.2f} pixels&quot;)

fig, axes = plt.subplots(2, 2, figsize=(14, 14))

axes[0, 0].imshow(sample_image, cmap='gray')
axes[0, 0].set_title('Original Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(labels, cmap='nipy_spectral')
axes[1, 0].set_title(f'Detected Particles ({len(diameters)})')
axes[1, 0].axis('off')

axes[1, 1].hist(diameters, bins=15, alpha=0.7, edgecolor='black')
axes[1, 1].axvline(diameters.mean(), color='red', linestyle='--',
                  label=f'Mean: {diameters.mean():.1f}')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Particle Size Distribution')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
</code></pre>


**Output Example**:

<pre><code>=== Particle diameterÁµ±Ë®à ===
Ê§úÂá∫particlesÊï∞: 38
MeanParticle diameter: 30.45 pixels
Standard deviation: 8.23 pixels
</code></pre>


**Explanation**:
„Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅWatershed„ÅÆmin_distance=15„Å´„Çà„Çä„ÄÅËøëÊé•particles„ÅÆÈÅéÊ§úÂá∫„ÇíÊäëÂà∂„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇParticle diameter„ÅÆ„Å∞„Çâ„Å§„ÅçÔºàStandard deviationÔºâ„ÅØÂêàÊàêÈÅéÁ®ã„Å´Ëµ∑Âõ†„Åó„Åæ„Åô„ÄÇÂÆü„Éá„Éº„Çø„Åß„ÅØ„ÄÅÊ∏¨ÂÆöÊù°‰ª∂„ÇÑÊùêÊñô„ÅÆ‰∏çÂùá‰∏ÄÊÄß„ÇíÂèçÊò†„Åó„Åæ„Åô„ÄÇ

</details>

<hr />
<h3>È°å3ÔºàÈõ£ÊòìÂ∫¶ÔºöhardÔºâ</h3>
<p>Ë§áÊï∞„ÅÆSEMÁîªÂÉè„ÇíËá™ÂãïÂá¶ÁêÜ„Åó„ÄÅParticle diameterminutesÂ∏É„ÅÆStatistical comparison„ÇíË°å„ÅÜ„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ</p>
<p><strong>Background</strong>:
Áï∞„Å™„ÇãÂêàÊàêÊù°‰ª∂„Åß‰ΩúË£Ω„Åï„Çå„ÅüÊùêÊñôA„ÄÅB„ÄÅC„ÅÆ„Çµ„É≥„Éó„É´„Å´„Å§„ÅÑ„Å¶„ÄÅÂêÑ10Êûö„ÅÆSEMÁîªÂÉè„ÅåÊíÆÂΩ±„Åï„Çå„Åæ„Åó„Åü„ÄÇÂêÑ„Çµ„É≥„Éó„É´„ÅÆParticle diameterminutesÂ∏É„ÇíËá™ÂãïËß£Êûê„Åó„ÄÅÁµ±Ë®àÁöÑ„Å´ÊØîËºÉ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</p>
<p><strong>Task</strong>:
1. Automatically analyze 30 images through batch processing
2. „Çµ„É≥„Éó„É´„Åî„Å®„ÅÆParticle diameterminutesÂ∏É„ÇíVisualization
3. minutesÊï£minutesÊûêÔºàANOVAÔºâ„Å´„Çà„ÇãÁµ±Ë®àÁöÑÊúâÊÑèÂ∑ÆÊ§úÂÆö
4. Output results as PDF report</p>
<p><strong>Constraints</strong>:
- Measurement conditions (magnification, exposure) may differ for each image
- Some images have poor contrast
- Processing time: within 5 seconds/image</p>
<details>
<summary>Hint</summary>

**Design Guidelines**:
1. `SEMImageAnalyzer`Extend class
2. Adaptive preprocessingÔºàHistogramËß£Êûê„Åß„Ç≥„É≥„Éà„É©„Çπ„ÉàÂà§ÂÆöÔºâ
3. Save results in structured format (JSON/CSV)
4. `scipy.stats.f_oneway`for ANOVA
5. `matplotlib.backends.backend_pdf`for PDF generation

</details>

<details>
<summary>Solution Example</summary>

**Answer„ÅÆÊ¶ÇË¶Å**:
Build an integrated system including batch processing, statistical analysis, and report generation.

**Implementation Code**:


<pre><code class="language-python">from scipy.stats import f_oneway
from matplotlib.backends.backend_pdf import PdfPages

class BatchSEMAnalyzer:
    &quot;&quot;&quot;Batch SEM image analysis system&quot;&quot;&quot;

    def __init__(self):
        self.results = {}

    def adaptive_preprocess(self, image):
        &quot;&quot;&quot;Adaptive preprocessing&quot;&quot;&quot;
        # Contrast evaluation
        contrast = image.max() - image.min()

        if contrast &lt; 100:  # Low contrast
            # CLAHEEnhancement
            clahe = cv2.createCLAHE(clipLimit=3.0,
                                    tileGridSize=(8, 8))
            image = clahe.apply(image)

        # Noise RemovalÔºàÈÅ©ÂøúÁöÑÂº∑Â∫¶Ôºâ
        noise_std = np.std(np.diff(image, axis=0))
        h = 10 if noise_std &lt; 20 else 15

        denoised = cv2.fastNlMeansDenoising(image, None, h, 7, 21)
        return denoised

    def analyze_single(self, image, sample_id):
        &quot;&quot;&quot;Single image analysis&quot;&quot;&quot;
        # Preprocessing
        preprocessed = self.adaptive_preprocess(image)

        # OtsuBinarization
        threshold = filters.threshold_otsu(preprocessed)
        binary = preprocessed &gt; threshold
        binary = morphology.remove_small_objects(binary, min_size=30)

        # Watershed
        distance = distance_transform_edt(binary)
        local_max = peak_local_max(distance, min_distance=15,
                                   labels=binary)
        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
        labels = watershed(-distance, markers, mask=binary)

        # Particle diameterÊäΩÂá∫
        diameters = []
        for region in measure.regionprops(labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            diameters.append(diameter)

        return np.array(diameters)

    def batch_analyze(self, image_dict):
        &quot;&quot;&quot;
        Batch analysis

        Parameters:
        -----------
        image_dict : dict
            {'sample_A': [img1, img2, ...], 'sample_B': [...]}
        &quot;&quot;&quot;
        for sample_id, images in image_dict.items():
            all_diameters = []

            for img in images:
                diameters = self.analyze_single(img, sample_id)
                all_diameters.extend(diameters)

            self.results[sample_id] = np.array(all_diameters)

    def statistical_comparison(self):
        &quot;&quot;&quot;Statistical comparison (ANOVA)&quot;&quot;&quot;
        groups = list(self.results.values())
        f_stat, p_value = f_oneway(*groups)

        print(&quot;=== minutesÊï£minutesÊûêÔºàANOVAÔºâ===&quot;)
        print(f&quot;F-statistic: {f_stat:.3f}&quot;)
        print(f&quot;p-value: {p_value:.4f}&quot;)

        if p_value &lt; 0.05:
            print(&quot;ÁµêË´ñ: „Çµ„É≥„Éó„É´Èñì„Å´ÊúâÊÑèÂ∑Æ„ÅÇ„ÇäÔºàp &lt; 0.05Ôºâ&quot;)
        else:
            print(&quot;Conclusion: No significant difference between samples (p ‚â• 0.05)&quot;)

        return f_stat, p_value

    def generate_report(self, filename='sem_report.pdf'):
        &quot;&quot;&quot;Generate PDF report&quot;&quot;&quot;
        with PdfPages(filename) as pdf:
            # „Éö„Éº„Ç∏1: Particle diameterminutesÂ∏ÉÊØîËºÉ
            fig, axes = plt.subplots(2, 2, figsize=(11, 8.5))

            for i, (sample_id, diameters) in enumerate(self.results.items()):
                ax = axes.ravel()[i]
                ax.hist(diameters, bins=20, alpha=0.7, edgecolor='black')
                ax.axvline(diameters.mean(), color='red',
                          linestyle='--',
                          label=f'Mean: {diameters.mean():.1f}')
                ax.set_xlabel('Diameter (pixels)')
                ax.set_ylabel('Frequency')
                ax.set_title(f'{sample_id} (n={len(diameters)})')
                ax.legend()
                ax.grid(True, alpha=0.3, axis='y')

            # Statistical Summary
            ax = axes.ravel()[3]
            ax.axis('off')
            summary_text = &quot;=== Statistical Summary ===\n\n&quot;
            for sample_id, diameters in self.results.items():
                summary_text += f&quot;{sample_id}:\n&quot;
                summary_text += f&quot;  Mean: {diameters.mean():.2f}\n&quot;
                summary_text += f&quot;  Std: {diameters.std():.2f}\n&quot;
                summary_text += f&quot;  n: {len(diameters)}\n\n&quot;

            ax.text(0.1, 0.5, summary_text, fontsize=12,
                   verticalalignment='center', family='monospace')

            plt.tight_layout()
            pdf.savefig(fig)
            plt.close()

            # Page 2: Box plot comparison
            fig, ax = plt.subplots(figsize=(11, 8.5))
            data = [self.results[key] for key in self.results.keys()]
            ax.boxplot(data, labels=list(self.results.keys()))
            ax.set_ylabel('Diameter (pixels)')
            ax.set_title('Particle Size Distribution Comparison')
            ax.grid(True, alpha=0.3, axis='y')

            pdf.savefig(fig)
            plt.close()

        print(f&quot;Report saved to {filename}&quot;)

# Demo execution
if __name__ == &quot;__main__&quot;:
    # „Çµ„É≥„Éó„É´Generate data
    np.random.seed(42)

    image_dict = {}
    for sample_id, mean_size in [('Sample_A', 25),
                                   ('Sample_B', 35),
                                   ('Sample_C', 30)]:
        images = []
        for _ in range(10):
            img = np.zeros((512, 512), dtype=np.uint8)
            num_particles = np.random.randint(30, 50)

            for _ in range(num_particles):
                x = np.random.randint(30, 482)
                y = np.random.randint(30, 482)
                r = int(np.random.normal(mean_size, 5))
                r = max(10, min(40, r))
                cv2.circle(img, (x, y), r, 200, -1)

            noise = np.random.normal(0, 25, img.shape)
            img = np.clip(img + noise, 0, 255).astype(np.uint8)
            images.append(img)

        image_dict[sample_id] = images

    # Batch analysis
    analyzer = BatchSEMAnalyzer()
    analyzer.batch_analyze(image_dict)

    # Statistical comparison
    analyzer.statistical_comparison()

    # Generate report
    analyzer.generate_report('sem_comparison_report.pdf')

    print(&quot;\n=== Statistics by Sample ===&quot;)
    for sample_id, diameters in analyzer.results.items():
        print(f&quot;{sample_id}:&quot;)
        print(f&quot;  particlesÊï∞: {len(diameters)}&quot;)
        print(f&quot;  Mean: {diameters.mean():.2f} ¬± {diameters.std():.2f}&quot;)
</code></pre>


**Result Example**:

<pre><code>=== minutesÊï£minutesÊûêÔºàANOVAÔºâ===
F-statistic: 124.567
p-value: 0.0001
ÁµêË´ñ: „Çµ„É≥„Éó„É´Èñì„Å´ÊúâÊÑèÂ∑Æ„ÅÇ„ÇäÔºàp &lt; 0.05Ôºâ

=== Statistics by Sample ===
Sample_A:
  particlesÊï∞: 423
  Mean: 25.12 ¬± 4.89
Sample_B:
  particlesÊï∞: 398
  Mean: 35.34 ¬± 5.23
Sample_C:
  particlesÊï∞: 415
  Mean: 30.05 ¬± 4.76

Report saved to sem_comparison_report.pdf
</code></pre>


**Ë©≥Á¥∞„Å™Explanation**:
1. **Adaptive preprocessing**: Evaluate contrast and noise level of each image and automatically adjust parameters
2. **Statistical testing**: ANOVA„Å´„Çà„Çä3Áæ§Èñì„ÅÆParticle diameterÂ∑Æ„ÇíÂÆöÈáèË©ï‰æ°
3. **PDF output**: Automatically generate multi-page report (directly usable for papers and reports)

**Additional Considerations**:
- Multiple comparison using Tukey HSD test (which pairs have significant differences)
- Particle diameterminutesÂ∏É„ÅÆÂΩ¢Áä∂ÊØîËºÉÔºàÊ≠™Â∫¶„ÄÅÂ∞ñÂ∫¶Ôºâ
- Image quality evaluation using machine learning (automatic exclusion of poor images)

</details>

<hr />
<h2>References</h2>
<ol>
<li>
<p>Bradski, G., &amp; Kaehler, A. (2008). "Learning OpenCV: Computer Vision with the OpenCV Library." O'Reilly Media. ISBN: 978-0596516130</p>
</li>
<li>
<p>van der Walt, S. et al. (2014). "scikit-image: image processing in Python." <em>PeerJ</em>, 2, e453. DOI: <a href="https://doi.org/10.7717/peerj.453">10.7717/peerj.453</a></p>
</li>
<li>
<p>Beucher, S., &amp; Meyer, F. (1993). "The morphological approach to segmentation: the watershed transformation." <em>Mathematical Morphology in Image Processing</em>, 433-481.</p>
</li>
<li>
<p>Simonyan, K., &amp; Zisserman, A. (2015). "Very Deep Convolutional Networks for Large-Scale Image Recognition." <em>ICLR 2015</em>. arXiv: <a href="https://arxiv.org/abs/1409.1556">1409.1556</a></p>
</li>
<li>
<p>OpenCV Documentation: Image Processing. URL: <a href="https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html">https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html</a></p>
</li>
<li>
<p>EMPIAR (Electron Microscopy Public Image Archive). URL: <a href="https://www.ebi.ac.uk/empiar/">https://www.ebi.ac.uk/empiar/</a></p>
</li>
<li>
<p>NanoMine Database. URL: <a href="https://materialsmine.org/">https://materialsmine.org/</a></p>
</li>
</ol>
<hr />
<h2>Navigation</h2>
<h3>‚Üê Previous Chapter</h3>
<p><strong><a href="./chapter-2.html">Chapter 2: Spectral Data Analysis ‚Üê</a></strong></p>
<h3>Ê¨°„ÅÆÁ´†</h3>
<p><strong><a href="./chapter-4.html">Chapter 4: Time-Series Data and Integrated Analysis ‚Üí</a></strong></p>
<h3>Series Index</h3>
<p><strong><a href="./index.html">‚Üê Series Index„Å´Êàª„Çã</a></strong></p>
<hr />
<h2>Author Information</h2>
<p><strong>Author</strong>: AI Terakoya Content Team
<strong>Created</strong>: 2025-10-17
<strong>Updated</strong>: 2025-10-19
<strong>Version</strong>: 1.1</p>
<p><strong>Update History</strong>:
- 2025-10-19: v1.1 „Éá„Éº„ÇøLicense„ÄÅÂÆüË∑µÁöÑ„Å™ËêΩ„Å®„ÅóÁ©¥„ÄÅ„Çπ„Ç≠„É´„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàËøΩÂä†
- 2025-10-17: v1.0 Initial release</p>
<p><strong>Feedback</strong>:
- GitHub Issues: [Repository URL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr />
<p><strong>Continue learning in the next chapter!</strong></p><div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê ‚Üê Previous Chapter</a>
    <a href="index.html" class="nav-button">Series Index„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Next Chapter ‚Üí</a>
</div>
    </main>

    
    <section class="disclaimer">
        <h3>Disclaimer</h3>
        <ul>
            <li>This content is intended solely for educational, research, and informational purposes and does not provide professional advice (legal, accounting, technical warranties, etc.).</li>
            <li>Êú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Åä„Çà„Å≥‰ªòÈöè„Åô„ÇãCode examples„ÅØ„ÄåÁèæÁä∂ÊúâÂßø(AS IS)„Äç„ÅßÊèê‰æõ„Åï„Çå„ÄÅÊòéÁ§∫„Åæ„Åü„ÅØÈªôÁ§∫„Çí„Çè„Åö„ÄÅÂïÜÂìÅÊÄß„ÄÅÁâπÂÆöÁõÆÁöÑÈÅ©ÂêàÊÄß„ÄÅÊ®©Âà©Èùû‰æµÂÆ≥„ÄÅTrueÁ¢∫ÊÄß„ÉªÂÆåÂÖ®ÊÄß„ÄÅÂãï‰Ωú„ÉªÂÆâÂÖ®ÊÄßÁ≠â„ÅÑ„Åã„Å™„Çã‰øùË®º„ÇÇ„Åó„Åæ„Åõ„Çì„ÄÇ</li>
            <li>Â§ñÈÉ®„É™„É≥„ÇØ„ÄÅÁ¨¨‰∏âËÄÖ„ÅåÊèê‰æõ„Åô„Çã„Éá„Éº„Çø„Éª„ÉÑ„Éº„É´„Éª„É©„Ç§„Éñ„É©„É™Á≠â„ÅÆContent„ÉªÂèØÁî®ÊÄß„ÉªÂÆâÂÖ®ÊÄß„Å´„Å§„ÅÑ„Å¶„ÄÅAuthor„Åä„Çà„Å≥Êù±ÂåóÂ§ßÂ≠¶„ÅØ‰∏ÄÂàá„ÅÆË≤¨‰ªª„ÇíË≤†„ÅÑ„Åæ„Åõ„Çì„ÄÇ</li>
            <li>Êú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÂà©Áî®„ÉªÂÆüË°å„ÉªËß£Èáà„Å´„Çà„ÇäÁõ¥Êé•ÁöÑ„ÉªÈñìÊé•ÁöÑ„Éª‰ªòÈöèÁöÑ„ÉªÁâπÂà•„ÉªÁµêÊûúÁöÑ„ÉªÊá≤ÁΩ∞ÁöÑÊêçÂÆ≥„ÅåÁîü„Åò„ÅüÂ†¥Âêà„Åß„ÇÇ„ÄÅÈÅ©Áî®Ê≥ï„ÅßË®±ÂÆπ„Åï„Çå„ÇãÊúÄÂ§ßÈôê„ÅÆÁØÑÂõ≤„Åß„ÄÅAuthor„Åä„Çà„Å≥Êù±ÂåóÂ§ßÂ≠¶„ÅØË≤¨‰ªª„ÇíË≤†„ÅÑ„Åæ„Åõ„Çì„ÄÇ</li>
            <li>Êú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆContent„ÅØ„ÄÅ‰∫àÂëä„Å™„ÅèÂ§âÊõ¥„ÉªÊõ¥Êñ∞„ÉªÊèê‰æõÂÅúÊ≠¢„Åï„Çå„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ</li>
            <li>Êú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆËëó‰ΩúÊ®©„ÉªLicense„ÅØÊòéË®ò„Åï„Çå„ÅüÊù°‰ª∂(‰æã: CC BY 4.0)„Å´Âæì„ÅÑ„Åæ„Åô„ÄÇÂΩìË©≤License„ÅØÈÄöÂ∏∏„ÄÅÁÑ°‰øùË®ºÊù°È†Ö„ÇíÂê´„Åø„Åæ„Åô„ÄÇ</li>
        </ul>
    </section>

<footer>
        <p><strong>Author</strong>: AI Terakoya Content Team</p>
        <p><strong>Version</strong>: 1.1 | <strong>Created</strong>: 2025-10-17</p>
        <p><strong>License</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>
</body>
</html>
