<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Chapter 1: Fundamentals of Experimental Data Analysis - AI Terakoya</title>
<link href="../../assets/css/knowledge-base.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
        mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'strict' });
    </script>
<!-- MathJax for LaTeX equation rendering -->
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'mermaid'
            }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="breadcrumb">
<div class="breadcrumb-content">
<a href="/AI-Knowledge-Notes/knowledge/en/index.html">AI Terakoya Top</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/index.html">Materials Informatics</a><span class="breadcrumb-separator">‚Ä∫</span><a href="/AI-Knowledge-Notes/knowledge/en/MI/experimental-data-analysis-introduction/index.html">Experimental Data Analysis</a><span class="breadcrumb-separator">‚Ä∫</span><span class="breadcrumb-current">Chapter 1</span>
</div>
</nav><div class="locale-switcher">
<span class="current-locale">üåê EN</span>
<span class="locale-separator">|</span>
<a class="locale-link" href="../../../jp/MI/experimental-data-analysis-introduction/chapter-1.html">Êó•Êú¨Ë™û</a>
<span class="locale-meta">Last sync: 2025-11-16</span>
</div>

<header>
<div class="header-content">
<h1>Chapter 1: Fundamentals of Experimental Data Analysis</h1>
<p class="subtitle">From Data Preprocessing to Outlier Detection - The First Step to Reliable Analysis</p>
<div class="meta">
<span class="meta-item">üìñ Reading Time: 20-25 min</span>
<span class="meta-item">üìä Difficulty: Beginner</span>
<span class="meta-item">üíª Code Examples: 8</span>
<span class="meta-item">üìù Exercises: 3</span>
</div>
</div>
</header>
<main class="container">
<h1>Chapter 1: Fundamentals of Experimental Data Analysis</h1>
<p class="chapter-description" style="margin: 1.5rem 0; padding: 1rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-left: 4px solid #7b2cbf; border-radius: 8px; font-size: 1.05rem; line-height: 1.8; color: #2d3748;">This chapter organizes major techniques like XRD/SEM/spectroscopy from the perspective of "what can we learn from them," and covers preprocessing fundamentals that often cause challenges in practice.</p>
<p class="chapter-supplement" style="margin: 0.75rem 0 1.5rem 0; padding: 0.75rem 1rem; background: linear-gradient(135deg, #fff8e1 0%, #fff3cd 100%); border-left: 3px solid #f59e0b; border-radius: 6px; font-size: 0.95rem; line-height: 1.7; color: #4a5568;"><strong>üí° Note:</strong> Getting into the habit of thinking "measurement purpose ‚Üí required analysis" will prevent confusion. Automate preprocessing early on.</p>
<p><strong>From Data Preprocessing to Outlier Detection - The First Step to Reliable Analysis</strong></p>
<h2>Learning Objectives</h2>
<p>By reading this chapter, you will be able to:</p>
<ul>
<li>‚úÖ Explain the overall workflow of experimental data analysis</li>
<li>‚úÖ Understand the importance of data preprocessing and the proper use of each technique</li>
<li>‚úÖ Select and apply appropriate noise removal filters</li>
<li>‚úÖ Detect and properly handle outliers</li>
<li>‚úÖ Use standardization and normalization techniques according to purpose</li>
</ul>
<p><strong>Reading Time</strong>: 20-25 minutes
<strong>Code Examples</strong>: 8
<strong>Exercises</strong>: 3</p>
<hr/>
<h2>1.1 Importance and Workflow of Experimental Data Analysis</h2>
<h3>Why Data-Driven Analysis is Necessary</h3>
<p>In materials science research, we use various characterization techniques such as XRD (X-ray Diffraction), XPS (X-ray Photoelectron Spectroscopy), SEM (Scanning Electron Microscopy), and various spectral measurements. Data obtained from these measurements is essential for understanding material structure, composition, and properties.</p>
<p>However, traditional manual analysis has the following challenges:</p>
<p><strong>Limitations of Traditional Manual Analysis</strong>:
1. <strong>Time-consuming</strong>: 30 minutes to 1 hour for peak identification in a single XRD pattern
2. <strong>Subjective</strong>: Results vary depending on the analyst's experience and judgment
3. <strong>Reproducibility issues</strong>: Different people may obtain different results from the same data
4. <strong>Cannot handle large volumes of data</strong>: Cannot keep up with high-throughput measurements (hundreds to thousands of samples per day)</p>
<p><strong>Advantages of Data-Driven Analysis</strong>:
1. <strong>High-speed</strong>: Analysis completed in seconds to minutes (100√ó faster)
2. <strong>Objectivity</strong>: Reproducible results based on clear algorithms
3. <strong>Consistency</strong>: Same code always outputs same results
4. <strong>Scalability</strong>: Same effort whether for 1 sample or 10,000 samples</p>
<h3>Overview of Materials Characterization Techniques</h3>
<p>Major measurement techniques and information obtained:</p>
<table>
<thead>
<tr>
<th>Measurement Technique</th>
<th>Information Obtained</th>
<th>Data Format</th>
<th>Typical Data Size</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XRD</strong></td>
<td>Crystal structure, phase identification, crystallite size</td>
<td>1D spectrum</td>
<td>Thousands of points</td>
</tr>
<tr>
<td><strong>XPS</strong></td>
<td>Elemental composition, chemical state, electronic structure</td>
<td>1D spectrum</td>
<td>Thousands of points</td>
</tr>
<tr>
<td><strong>SEM/TEM</strong></td>
<td>Morphology, particle size, microstructure</td>
<td>2D images</td>
<td>Millions of pixels</td>
</tr>
<tr>
<td><strong>IR/Raman</strong></td>
<td>Molecular vibrations, functional groups, crystallinity</td>
<td>1D spectrum</td>
<td>Thousands of points</td>
</tr>
<tr>
<td><strong>UV-Vis</strong></td>
<td>Light absorption, band gap</td>
<td>1D spectrum</td>
<td>Hundreds to thousands of points</td>
</tr>
<tr>
<td><strong>TGA/DSC</strong></td>
<td>Thermal stability, phase transitions</td>
<td>1D time series</td>
<td>Thousands of points</td>
</tr>
</tbody>
</table>
<h3>Typical Analysis Workflow (5 Steps)</h3>
<p>Experimental data analysis typically proceeds through the following 5 steps:</p>
<div class="mermaid">
flowchart LR
    A[1. Data Loading] --&gt; B[2. Data Preprocessing]
    B --&gt; C[3. Feature Extraction]
    C --&gt; D[4. Statistical Analysis/Machine Learning]
    D --&gt; E[5. Visualization &amp; Reporting]
    E --&gt; F[Result Interpretation]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
</div>
<p><strong>Details of Each Step</strong>:</p>
<ol>
<li><strong>Data Loading</strong>: Load data from CSV, text, or binary formats</li>
<li><strong>Data Preprocessing</strong>: Noise removal, outlier handling, standardization</li>
<li><strong>Feature Extraction</strong>: Peak detection, contour extraction, statistical calculation</li>
<li><strong>Statistical Analysis/Machine Learning</strong>: Regression, classification, clustering</li>
<li><strong>Visualization &amp; Reporting</strong>: Graph creation, report generation</li>
</ol>
<p>This chapter focuses on <strong>Step 2 (Data Preprocessing)</strong>.</p>
<hr/>
<h2>1.2 Data Licensing and Reproducibility</h2>
<h3>Handling Experimental Data and Licensing</h3>
<p>In experimental data analysis, it's important to understand data sources and licenses.</p>
<h4>Public Data Repositories</h4>
<table>
<thead>
<tr>
<th>Repository</th>
<th>Content</th>
<th>Data Format</th>
<th>Access</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Materials Project</strong></td>
<td>DFT calculation results, crystal structures</td>
<td>JSON, CIF</td>
<td>Free, CC BY 4.0</td>
</tr>
<tr>
<td><strong>ICDD PDF</strong></td>
<td>XRD pattern database</td>
<td>Proprietary format</td>
<td>Paid license</td>
</tr>
<tr>
<td><strong>NIST XPS Database</strong></td>
<td>XPS spectra</td>
<td>Text</td>
<td>Free</td>
</tr>
<tr>
<td><strong>Citrination</strong></td>
<td>Materials property data</td>
<td>JSON, CSV</td>
<td>Partially free</td>
</tr>
<tr>
<td><strong>Figshare/Zenodo</strong></td>
<td>Research data</td>
<td>Various</td>
<td>Free, various licenses</td>
</tr>
</tbody>
</table>
<h4>Instrument Data Formats</h4>
<p>Major X-ray diffraction systems and their data formats:</p>
<ul>
<li><strong>Bruker</strong>: <code>.raw</code>, <code>.brml</code> (XML format)</li>
<li><strong>Rigaku</strong>: <code>.asc</code>, <code>.ras</code> (text format)</li>
<li><strong>PANalytical</strong>: <code>.xrdml</code> (XML format)</li>
<li><strong>Generic</strong>: <code>.xy</code>, <code>.csv</code> (2-column text)</li>
</ul>
<h4>Important Considerations When Using Data</h4>
<ol>
<li><strong>License Verification</strong>: Always check terms of use for public data</li>
<li><strong>Citation</strong>: Clearly cite data sources used</li>
<li><strong>Modification</strong>: Respect original licenses even after data processing</li>
<li><strong>Publication</strong>: Specify license when publishing your own data (CC BY 4.0 recommended)</li>
</ol>
<h3>Best Practices for Code Reproducibility</h3>
<h4>Recording Environment Information</h4>
<p>To ensure reproducibility of analysis code, record the following:</p>
<pre><code class="language-python">import sys
import numpy as np
import pandas as pd
import scipy
import matplotlib

print("=== Environment Information ===")
print(f"Python: {sys.version}")
print(f"NumPy: {np.__version__}")
print(f"pandas: {pd.__version__}")
print(f"SciPy: {scipy.__version__}")
print(f"Matplotlib: {matplotlib.__version__}")

# Recommended versions (as of October 2025):
# - Python: 3.10 or higher
# - NumPy: 1.24 or higher
# - pandas: 2.0 or higher
# - SciPy: 1.10 or higher
# - Matplotlib: 3.7 or higher
</code></pre>
<h4>Documenting Parameters</h4>
<p><strong>Bad Example</strong> (non-reproducible):</p>
<pre><code class="language-python">smoothed = savgol_filter(data, 11, 3)  # Parameter meaning unclear
</code></pre>
<p><strong>Good Example</strong> (reproducible):</p>
<pre><code class="language-python"># Savitzky-Golay filter parameters
SG_WINDOW_LENGTH = 11  # Approximately 1.5% of data points
SG_POLYORDER = 3       # 3rd-order polynomial fit
smoothed = savgol_filter(data, SG_WINDOW_LENGTH, SG_POLYORDER)
</code></pre>
<h4>Fixing Random Seeds</h4>
<pre><code class="language-python">import numpy as np

# Fix random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# Use this seed for data generation and sampling
noise = np.random.normal(0, 50, len(data))
</code></pre>
<hr/>
<h2>1.3 Fundamentals of Data Preprocessing</h2>
<h3>Data Loading</h3>
<p>First, let's learn how to load experimental data in various formats.</p>
<p><strong>Code Example 1: Loading CSV Files (XRD Pattern)</strong></p>
<pre><code class="language-python"># Loading XRD pattern data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load CSV file (2Œ∏, intensity)
# Create sample data
np.random.seed(42)
two_theta = np.linspace(10, 80, 700)  # 2Œ∏ range: 10-80 degrees
intensity = (
    1000 * np.exp(-((two_theta - 28) ** 2) / 10) +  # Peak 1
    1500 * np.exp(-((two_theta - 32) ** 2) / 8) +   # Peak 2
    800 * np.exp(-((two_theta - 47) ** 2) / 12) +   # Peak 3
    np.random.normal(0, 50, len(two_theta))          # Noise
)

# Store in DataFrame
df = pd.DataFrame({
    'two_theta': two_theta,
    'intensity': intensity
})

# Check basic statistics
print("=== Data Basic Statistics ===")
print(df.describe())

# Visualize data
plt.figure(figsize=(10, 5))
plt.plot(df['two_theta'], df['intensity'], linewidth=1)
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity (counts)')
plt.title('Raw XRD Pattern')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nNumber of data points: {len(df)}")
print(f"2Œ∏ range: {df['two_theta'].min():.1f} - {df['two_theta'].max():.1f}¬∞")
print(f"Intensity range: {df['intensity'].min():.1f} - {df['intensity'].max():.1f}")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Data Basic Statistics ===
         two_theta    intensity
count   700.000000   700.000000
mean     45.000000   351.893421
std      20.219545   480.523106
min      10.000000  -123.456789
25%      27.500000    38.901234
50%      45.000000   157.345678
75%      62.500000   401.234567
max      80.000000  1523.456789

Number of data points: 700
2Œ∏ range: 10.0 - 80.0¬∞
Intensity range: -123.5 - 1523.5
</code></pre>
<h3>Understanding and Reshaping Data Structures</h3>
<p><strong>Code Example 2: Checking and Reshaping Data Structure</strong></p>
<pre><code class="language-python"># Check data structure
print("=== Data Structure ===")
print(f"Data types:\n{df.dtypes}\n")
print(f"Missing values:\n{df.isnull().sum()}\n")
print(f"Duplicate rows: {df.duplicated().sum()}")

# Example with missing values
df_with_nan = df.copy()
df_with_nan.loc[100:105, 'intensity'] = np.nan  # Intentionally insert missing values

print("\n=== Handling Missing Values ===")
print(f"Number of missing values: {df_with_nan['intensity'].isnull().sum()}")

# Interpolate missing values (linear interpolation)
df_with_nan['intensity_interpolated'] = df_with_nan['intensity'].interpolate(method='linear')

# Check before and after
print("\nBefore and after missing values:")
print(df_with_nan.iloc[98:108][['two_theta', 'intensity', 'intensity_interpolated']])
</code></pre>
<h3>Detecting and Handling Missing Values and Anomalies</h3>
<p><strong>Code Example 3: Detecting Anomalies</strong></p>
<pre><code class="language-python">from scipy import stats

# Negative intensity values are physically impossible (anomalies)
negative_mask = df['intensity'] &lt; 0
print(f"Number of negative intensity values: {negative_mask.sum()} / {len(df)}")

# Replace negative values with 0
df_cleaned = df.copy()
df_cleaned.loc[negative_mask, 'intensity'] = 0

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].plot(df['two_theta'], df['intensity'], label='Raw', alpha=0.7)
axes[0].axhline(y=0, color='r', linestyle='--', label='Zero line')
axes[0].set_xlabel('2Œ∏ (degree)')
axes[0].set_ylabel('Intensity')
axes[0].set_title('Raw Data (with negative values)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(df_cleaned['two_theta'], df_cleaned['intensity'], label='Cleaned', color='green')
axes[1].axhline(y=0, color='r', linestyle='--', label='Zero line')
axes[1].set_xlabel('2Œ∏ (degree)')
axes[1].set_ylabel('Intensity')
axes[1].set_title('Cleaned Data (negatives removed)')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<hr/>
<h2>1.3 Noise Removal Techniques</h2>
<p>Experimental data always contains noise. Noise removal improves the signal-to-noise ratio (S/N ratio) and enhances the accuracy of subsequent analysis.</p>
<h3>Moving Average Filter</h3>
<p>The simplest noise removal technique. Each data point is replaced by the average of its neighbors.</p>
<p><strong>Code Example 4: Moving Average Filter</strong></p>
<pre><code class="language-python">from scipy.ndimage import uniform_filter1d

# Apply moving average filter
window_sizes = [5, 11, 21]

plt.figure(figsize=(12, 8))

# Original data
plt.subplot(2, 2, 1)
plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1)
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('Original Data')
plt.grid(True, alpha=0.3)

# Moving average with different window sizes
for i, window_size in enumerate(window_sizes, start=2):
    smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_size)

    plt.subplot(2, 2, i)
    plt.plot(df_cleaned['two_theta'], smoothed, linewidth=1.5)
    plt.xlabel('2Œ∏ (degree)')
    plt.ylabel('Intensity')
    plt.title(f'Moving Average (window={window_size})')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Quantitative evaluation of noise removal effect
print("=== Noise Removal Effect ===")
original_std = np.std(df_cleaned['intensity'].values)
for window_size in window_sizes:
    smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_size)
    smoothed_std = np.std(smoothed)
    noise_reduction = (1 - smoothed_std / original_std) * 100
    print(f"Window={window_size:2d}: Noise reduction {noise_reduction:.1f}%")
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>=== Noise Removal Effect ===
Window= 5: Noise reduction 15.2%
Window=11: Noise reduction 28.5%
Window=21: Noise reduction 41.3%
</code></pre>
<p><strong>Selection Guide</strong>:
- <strong>Small window (3-5)</strong>: Noise remains but peak shape is preserved
- <strong>Medium window (7-15)</strong>: Good balance, recommended
- <strong>Large window (&gt;20)</strong>: Strong noise removal but peaks broaden</p>
<h3>Savitzky-Golay Filter</h3>
<p>A more advanced technique than moving average that can remove noise while preserving peak shapes.</p>
<p><strong>Code Example 5: Savitzky-Golay Filter</strong></p>
<pre><code class="language-python">from scipy.signal import savgol_filter

# Apply Savitzky-Golay filter
window_length = 11  # Must be odd
polyorder = 3       # Polynomial order

sg_smoothed = savgol_filter(df_cleaned['intensity'].values, window_length, polyorder)

# Compare with moving average
ma_smoothed = uniform_filter1d(df_cleaned['intensity'].values, size=window_length)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'],
         label='Original', alpha=0.5, linewidth=1)
plt.plot(df_cleaned['two_theta'], ma_smoothed,
         label='Moving Average', linewidth=1.5)
plt.plot(df_cleaned['two_theta'], sg_smoothed,
         label='Savitzky-Golay', linewidth=1.5)
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('Comparison of Smoothing Methods')
plt.legend()
plt.grid(True, alpha=0.3)

# Zoom in on peak region
plt.subplot(1, 2, 2)
peak_region = (df_cleaned['two_theta'] &gt; 26) &amp; (df_cleaned['two_theta'] &lt; 34)
plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
         df_cleaned.loc[peak_region, 'intensity'],
         label='Original', alpha=0.5, linewidth=1)
plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
         ma_smoothed[peak_region],
         label='Moving Average', linewidth=1.5)
plt.plot(df_cleaned.loc[peak_region, 'two_theta'],
         sg_smoothed[peak_region],
         label='Savitzky-Golay', linewidth=1.5)
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('Zoomed: Peak Region')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== Savitzky-Golay Parameters ===")
print(f"Window length: {window_length}")
print(f"Polynomial order: {polyorder}")
print(f"\nRecommended settings:")
print("- High noise: window_length=11-21, polyorder=2-3")
print("- Low noise: window_length=5-11, polyorder=2-4")
</code></pre>
<p><strong>Advantages of Savitzky-Golay Filter</strong>:
- More accurately preserves peak height and position
- Does not over-smooth edges (sharp changes)
- Better compatibility with derivative calculations (useful for later peak detection)</p>
<h3>Gaussian Filter</h3>
<p>Widely used in image processing, but also applicable to 1D data.</p>
<p><strong>Code Example 6: Gaussian Filter</strong></p>
<pre><code class="language-python">from scipy.ndimage import gaussian_filter1d

# Apply Gaussian filter
sigma_values = [1, 2, 4]  # Standard deviation

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1)
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('Original Data')
plt.grid(True, alpha=0.3)

for i, sigma in enumerate(sigma_values, start=2):
    gaussian_smoothed = gaussian_filter1d(df_cleaned['intensity'].values, sigma=sigma)

    plt.subplot(2, 2, i)
    plt.plot(df_cleaned['two_theta'], gaussian_smoothed, linewidth=1.5)
    plt.xlabel('2Œ∏ (degree)')
    plt.ylabel('Intensity')
    plt.title(f'Gaussian Filter (œÉ={sigma})')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>Selecting the Appropriate Filter</h3>
<div class="mermaid">
flowchart TD
    Start[Need noise removal] --&gt; Q1{Is peak shape preservation important?}
    Q1 --&gt;|Very important| SG[Savitzky-Golay]
    Q1 --&gt;|Somewhat important| Gauss[Gaussian]
    Q1 --&gt;|Not very important| MA[Moving Average]

    SG --&gt; Param1[window=11-21\npolyorder=2-3]
    Gauss --&gt; Param2[sigma=1-3]
    MA --&gt; Param3[window=7-15]

    style Start fill:#4CAF50,color:#fff
    style SG fill:#2196F3,color:#fff
    style Gauss fill:#FF9800,color:#fff
    style MA fill:#9C27B0,color:#fff
</div>
<hr/>
<h2>1.4 Outlier Detection</h2>
<p>Outliers occur due to measurement errors, equipment malfunctions, or sample contamination. If not properly detected and handled, they can significantly affect analysis results.</p>
<h3>Z-score Method</h3>
<p>Data that is statistically "more than X standard deviations away from the mean" is considered an outlier.</p>
<p><strong>Code Example 7: Outlier Detection Using Z-score</strong></p>
<pre><code class="language-python">from scipy import stats

# Sample data with outliers
data_with_outliers = df_cleaned['intensity'].copy()
# Intentionally add outliers
outlier_indices = [50, 150, 350, 550]
data_with_outliers.iloc[outlier_indices] = [3000, -500, 4000, 3500]

# Calculate Z-score
z_scores = np.abs(stats.zscore(data_with_outliers))
threshold = 3  # Consider data beyond 3œÉ as outliers

outliers = z_scores &gt; threshold

print(f"=== Z-score Outlier Detection ===")
print(f"Number of outliers: {outliers.sum()} / {len(data_with_outliers)}")
print(f"Outlier indices: {np.where(outliers)[0]}")

# Visualize
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(df_cleaned['two_theta'], data_with_outliers, label='Data with outliers')
plt.scatter(df_cleaned['two_theta'][outliers], data_with_outliers[outliers],
            color='red', s=100, zorder=5, label='Detected outliers')
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('Outlier Detection (Z-score method)')
plt.legend()
plt.grid(True, alpha=0.3)

# After outlier removal
data_cleaned = data_with_outliers.copy()
data_cleaned[outliers] = np.nan
data_cleaned = data_cleaned.interpolate(method='linear')

plt.subplot(1, 2, 2)
plt.plot(df_cleaned['two_theta'], data_cleaned, color='green', label='Cleaned data')
plt.xlabel('2Œ∏ (degree)')
plt.ylabel('Intensity')
plt.title('After Outlier Removal')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<h3>IQR (Interquartile Range) Method</h3>
<p>A robust median-based method that can also be applied to non-normally distributed data.</p>
<p><strong>Code Example 8: IQR Method</strong></p>
<pre><code class="language-python"># Outlier detection using IQR method
Q1 = data_with_outliers.quantile(0.25)
Q3 = data_with_outliers.quantile(0.75)
IQR = Q3 - Q1

# Outlier definition: below Q1 - 1.5*IQR or above Q3 + 1.5*IQR
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_iqr = (data_with_outliers &lt; lower_bound) | (data_with_outliers &gt; upper_bound)

print(f"=== IQR Outlier Detection ===")
print(f"Q1: {Q1:.1f}")
print(f"Q3: {Q3:.1f}")
print(f"IQR: {IQR:.1f}")
print(f"Lower bound: {lower_bound:.1f}")
print(f"Upper bound: {upper_bound:.1f}")
print(f"Number of outliers: {outliers_iqr.sum()} / {len(data_with_outliers)}")

# Visualize with box plot
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].boxplot(data_with_outliers.dropna(), vert=True)
axes[0].set_ylabel('Intensity')
axes[0].set_title('Box Plot (outliers visible)')
axes[0].grid(True, alpha=0.3)

# After outlier removal
data_cleaned_iqr = data_with_outliers.copy()
data_cleaned_iqr[outliers_iqr] = np.nan
data_cleaned_iqr = data_cleaned_iqr.interpolate(method='linear')

axes[1].boxplot(data_cleaned_iqr.dropna(), vert=True)
axes[1].set_ylabel('Intensity')
axes[1].set_title('Box Plot (after outlier removal)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>
<p><strong>Z-score vs IQR Selection</strong>:
- <strong>Z-score method</strong>: Effective when data is close to normal distribution, simple to calculate
- <strong>IQR method</strong>: Robust even for non-normal distributions, strong against extreme outliers</p>
<hr/>
<h2>1.5 Standardization and Normalization</h2>
<p>Standardization and normalization are necessary to make data at different scales comparable.</p>
<h3>Min-Max Scaling</h3>
<p>Transforms data into the range [0, 1].</p>
<p>$$
X_{\text{normalized}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
$$</p>
<h3>Z-score Standardization</h3>
<p>Transforms to mean 0 and standard deviation 1.</p>
<p>$$
X_{\text{standardized}} = \frac{X - \mu}{\sigma}
$$</p>
<h3>Baseline Correction</h3>
<p>For spectral data, removes background.</p>
<p><strong>Implementation detailed in Chapter 2</strong></p>
<hr/>
<h2>1.6 Practical Pitfalls and Solutions</h2>
<h3>Common Failure Examples</h3>
<h4>Failure 1: Excessive Noise Removal</h4>
<p><strong>Symptom</strong>: Important peaks disappear or distort after smoothing</p>
<p><strong>Cause</strong>: Window size too large or multiple stages of smoothing</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Bad example: excessive smoothing
data_smooth1 = gaussian_filter1d(data, sigma=5)
data_smooth2 = savgol_filter(data_smooth1, 21, 3)  # Double smoothing
data_smooth3 = uniform_filter1d(data_smooth2, 15)  # More smoothing

# Good example: appropriate smoothing
data_smooth = savgol_filter(data, 11, 3)  # Once only, with appropriate parameters
</code></pre>
<h4>Failure 2: Excessive Outlier Removal</h4>
<p><strong>Symptom</strong>: Important signals (sharp peaks) removed as outliers</p>
<p><strong>Cause</strong>: Z-score threshold too low</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Bad example: threshold too strict
outliers = np.abs(stats.zscore(data)) &gt; 2  # 2œÉ removal ‚Üí normal peaks also removed

# Good example: appropriate threshold with visual confirmation
outliers = np.abs(stats.zscore(data)) &gt; 3  # 3œÉ is standard
# Always visualize and confirm before removal
plt.scatter(range(len(data)), data)
plt.scatter(np.where(outliers)[0], data[outliers], color='red')
plt.show()
</code></pre>
<h4>Failure 3: Misuse of Missing Value Interpolation</h4>
<p><strong>Symptom</strong>: Data continuity lost or unnatural values generated</p>
<p><strong>Cause</strong>: Linear interpolation of large missing regions</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Bad example: interpolate large missing regions
data_interpolated = data.interpolate(method='linear')  # Unconditional interpolation of all regions

# Good example: check missing range
max_gap = 5  # Only interpolate missing values up to 5 points
gaps = data.isnull().astype(int).groupby(data.notnull().astype(int).cumsum()).sum()
if gaps.max() &lt;= max_gap:
    data_interpolated = data.interpolate(method='linear')
else:
    print(f"Warning: Large missing regions exist (max {gaps.max()} points)")
</code></pre>
<h4>Failure 4: Confusing Measurement Noise with Signal</h4>
<p><strong>Symptom</strong>: Misidentifying noise as physical signal</p>
<p><strong>Cause</strong>: Neglecting quantitative noise level assessment</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Quantitative noise level assessment
baseline_region = data[(two_theta &gt; 70) &amp; (two_theta &lt; 80)]  # Region with no signal
noise_std = np.std(baseline_region)
print(f"Noise level (standard deviation): {noise_std:.2f}")

# Calculate signal-to-noise ratio (S/N ratio)
peak_height = data.max() - data.min()
snr = peak_height / noise_std
print(f"S/N ratio: {snr:.1f}")

# If S/N ratio is 3 or below, do not treat as signal
if snr &lt; 3:
    print("Warning: S/N ratio too low. Please re-run measurement.")
</code></pre>
<h3>Importance of Processing Order</h3>
<p>Correct preprocessing pipeline order:</p>
<div class="mermaid">
flowchart LR
    A[1. Physical Anomaly Removal] --&gt; B[2. Statistical Outlier Detection]
    B --&gt; C[3. Missing Value Interpolation]
    C --&gt; D[4. Noise Removal]
    D --&gt; E[5. Baseline Correction]
    E --&gt; F[6. Standardization]

    style A fill:#ffebee
    style B fill:#fff3e0
    style C fill:#e3f2fd
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#fce4ec
</div>
<p><strong>Why This Order is Important</strong>:
1. <strong>Physical anomalies</strong> (negative intensities, etc.) must be removed first or they distort statistics
2. <strong>Statistical outliers</strong> must be removed before noise removal or they spread during smoothing
3. <strong>Noise removal</strong> must be done before baseline correction or noise affects baseline
4. <strong>Baseline correction</strong> must be done before standardization or standardization becomes meaningless</p>
<hr/>
<h2>1.7 Chapter Summary</h2>
<h3>What We Learned</h3>
<ol>
<li>
<p><strong>Data Licensing and Reproducibility</strong>
   - Utilizing public data repositories
   - Documenting environment information and parameters
   - Best practices for code reproducibility</p>
</li>
<li>
<p><strong>Experimental Data Analysis Workflow</strong>
   - Data loading ‚Üí preprocessing ‚Üí feature extraction ‚Üí analysis ‚Üí visualization
   - Importance and impact of preprocessing</p>
</li>
<li>
<p><strong>Noise Removal Techniques</strong>
   - Moving average, Savitzky-Golay, Gaussian filters
   - Criteria for selecting appropriate techniques</p>
</li>
<li>
<p><strong>Outlier Detection</strong>
   - Z-score method, IQR method
   - Physical validity checks</p>
</li>
<li>
<p><strong>Standardization and Normalization</strong>
   - Min-Max scaling, Z-score standardization
   - Principles for selection</p>
</li>
<li>
<p><strong>Practical Pitfalls</strong>
   - Avoiding excessive noise removal
   - Importance of processing order
   - Distinguishing noise from signal</p>
</li>
</ol>
<h3>Key Points</h3>
<ul>
<li>‚úÖ Preprocessing is the most critical step that determines analysis success or failure</li>
<li>‚úÖ Balance between peak shape preservation and noise reduction is important in noise removal</li>
<li>‚úÖ Always verify outliers and check physical validity</li>
<li>‚úÖ Confirm effect of each processing step through visualization</li>
<li>‚úÖ Processing order significantly affects results</li>
<li>‚úÖ Record environment and parameters for code reproducibility</li>
</ul>
<h3>To the Next Chapter</h3>
<p>In Chapter 2, we will learn analysis techniques for spectral data (XRD, XPS, IR, Raman):
- Peak detection algorithms
- Background removal
- Quantitative analysis
- Material identification using machine learning</p>
<p><strong><a href="./chapter-2.html">Chapter 2: Spectral Data Analysis ‚Üí</a></strong></p>
<hr/>
<h2>Experimental Data Preprocessing Checklist</h2>
<h3>Data Loading and Validation</h3>
<ul>
<li>[ ] Verify data format is correct (CSV, text, binary)</li>
<li>[ ] Verify column names are appropriate (two_theta, intensity, etc.)</li>
<li>[ ] Verify sufficient data points (minimum 100 points recommended)</li>
<li>[ ] Check percentage of missing values (caution if &gt;30%)</li>
<li>[ ] Check for duplicate data</li>
</ul>
<h3>Environment and Reproducibility</h3>
<ul>
<li>[ ] Record Python, NumPy, pandas, SciPy, Matplotlib versions</li>
<li>[ ] Fix random seed (if applicable)</li>
<li>[ ] Define parameters as constants (e.g., SG_WINDOW_LENGTH = 11)</li>
<li>[ ] Document data sources and licenses</li>
</ul>
<h3>Physical Validity Checks</h3>
<ul>
<li>[ ] Check for negative intensity values (physically impossible for XRD/XPS)</li>
<li>[ ] Verify measurement range is appropriate (XRD: 10-80¬∞, XPS: 0-1200 eV, etc.)</li>
<li>[ ] Calculate S/N ratio (desirable &gt;3)</li>
<li>[ ] Evaluate noise level in baseline region</li>
</ul>
<h3>Outlier Detection</h3>
<ul>
<li>[ ] Detect outliers using Z-score or IQR method</li>
<li>[ ] <strong>Visualize and confirm before removal</strong> (don't accidentally remove important peaks)</li>
<li>[ ] Record threshold (Z-score: 3œÉ, IQR: 1.5√ó, etc.)</li>
<li>[ ] Record number and position of removed outliers</li>
</ul>
<h3>Missing Value Handling</h3>
<ul>
<li>[ ] Verify number and position of missing values</li>
<li>[ ] Check maximum length of consecutive missing values (desirable ‚â§5 points)</li>
<li>[ ] Select interpolation method (linear, spline, forward/backward fill)</li>
<li>[ ] Visualize and verify data after interpolation</li>
</ul>
<h3>Noise Removal</h3>
<ul>
<li>[ ] Select filter type (moving average, Savitzky-Golay, Gaussian)</li>
<li>[ ] Determine parameters (window_length, polyorder, sigma)</li>
<li>[ ] Compare and visualize data before and after smoothing</li>
<li>[ ] Verify peak shapes are preserved</li>
<li>[ ] <strong>Avoid double smoothing</strong></li>
</ul>
<h3>Verify Processing Order</h3>
<ul>
<li>[ ] 1. Physical anomaly removal</li>
<li>[ ] 2. Statistical outlier detection</li>
<li>[ ] 3. Missing value interpolation</li>
<li>[ ] 4. Noise removal</li>
<li>[ ] 5. Baseline correction (if applicable)</li>
<li>[ ] 6. Standardization/normalization (if applicable)</li>
</ul>
<h3>Visualization and Verification</h3>
<ul>
<li>[ ] Plot original data</li>
<li>[ ] Plot processed data</li>
<li>[ ] Overlay before and after processing</li>
<li>[ ] Verify main peak positions are preserved</li>
<li>[ ] Verify peak intensities have not changed significantly</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>[ ] Record processing parameters as comments or variables</li>
<li>[ ] Record reason for processing (e.g., "Using SG filter due to high noise")</li>
<li>[ ] Record reasons for excluded data</li>
<li>[ ] Record final data quality metrics (S/N ratio, number of data points, etc.)</li>
</ul>
<h3>For Batch Processing</h3>
<ul>
<li>[ ] Implement error handling (try-except)</li>
<li>[ ] Record processing results in log file</li>
<li>[ ] Measure and record processing time</li>
<li>[ ] Save list of failed files</li>
<li>[ ] Calculate and report success rate (e.g., 95/100 files succeeded)</li>
</ul>
<hr/>
<h2>Exercises</h2>
<h3>Problem 1 (Difficulty: Easy)</h3>
<p>Determine whether the following statements are true or false.</p>
<ol>
<li>Increasing the window size of a moving average filter enhances noise removal but broadens peaks</li>
<li>Savitzky-Golay filter preserves peak shapes better than moving average</li>
<li>Z-score method cannot be applied to non-normally distributed data</li>
</ol>
<details>
<summary>Hint</summary>

1. Consider the relationship between window size, noise removal effect, and peak shape
2. Recall the mathematical differences between the two methods
3. Think about the definition of Z-score and its behavior with non-normal distributions

</details>
<details>
<summary>Solution</summary>

**Answer**:
1. **True** - Larger windows average more points, reducing noise but also smoothing peaks
2. **True** - Savitzky-Golay uses polynomial fitting, better preserving sharp changes
3. **False** - Z-score can be calculated but interpretation of 3œÉ rule assumes normal distribution. IQR method is recommended for non-normal distributions

**Explanation**:
Noise removal always involves trade-offs. Attempting to completely remove noise also distorts signals. It's important to select appropriate parameters based on experimental data characteristics (noise level, peak sharpness).

</details>
<hr/>
<h3>Problem 2 (Difficulty: Medium)</h3>
<p>Build an appropriate preprocessing pipeline for the following XRD data (sample).</p>
<pre><code class="language-python"># Sample data
import numpy as np
import pandas as pd

np.random.seed(100)
two_theta = np.linspace(20, 60, 400)
intensity = (
    800 * np.exp(-((two_theta - 30) ** 2) / 8) +
    1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
    np.random.normal(0, 100, len(two_theta))
)
# Add outliers
intensity[50] = 3000
intensity[200] = -500

df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})
</code></pre>
<p><strong>Requirements</strong>:
1. Replace negative intensity values with 0
2. Detect and remove outliers using Z-score (threshold 3œÉ)
3. Remove noise using Savitzky-Golay filter (window=11, polyorder=3)
4. Visualize data before and after processing</p>
<details>
<summary>Hint</summary>

**Processing Flow**:
1. Create mask for negative values ‚Üí replace with 0
2. Detect outliers using `scipy.stats.zscore`
3. Interpolate outliers linearly
4. Smooth using `scipy.signal.savgol_filter`
5. Compare original and processed data using `matplotlib`

**Functions to Use**:
- `df[condition]` for conditional extraction
- `np.abs(stats.zscore())` for Z-score
- `interpolate(method='linear')` for interpolation
- `savgol_filter()` for smoothing

</details>
<details>
<summary>Solution</summary>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.signal import savgol_filter

# Sample data
np.random.seed(100)
two_theta = np.linspace(20, 60, 400)
intensity = (
    800 * np.exp(-((two_theta - 30) ** 2) / 8) +
    1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
    np.random.normal(0, 100, len(two_theta))
)
intensity[50] = 3000
intensity[200] = -500

df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})

# Step 1: Replace negative values with 0
df_cleaned = df.copy()
negative_mask = df_cleaned['intensity'] &lt; 0
df_cleaned.loc[negative_mask, 'intensity'] = 0
print(f"Number of negative values: {negative_mask.sum()}")

# Step 2: Outlier detection (Z-score method)
z_scores = np.abs(stats.zscore(df_cleaned['intensity']))
outliers = z_scores &gt; 3
print(f"Number of outliers: {outliers.sum()}")

# Step 3: Interpolate outliers
df_cleaned.loc[outliers, 'intensity'] = np.nan
df_cleaned['intensity'] = df_cleaned['intensity'].interpolate(method='linear')

# Step 4: Savitzky-Golay filter
intensity_smoothed = savgol_filter(df_cleaned['intensity'].values, window_length=11, polyorder=3)

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Original data
axes[0, 0].plot(df['two_theta'], df['intensity'], linewidth=1)
axes[0, 0].set_title('Original Data (with outliers)')
axes[0, 0].set_xlabel('2Œ∏ (degree)')
axes[0, 0].set_ylabel('Intensity')
axes[0, 0].grid(True, alpha=0.3)

# After outlier removal
axes[0, 1].plot(df_cleaned['two_theta'], df_cleaned['intensity'], linewidth=1, color='orange')
axes[0, 1].set_title('After Outlier Removal')
axes[0, 1].set_xlabel('2Œ∏ (degree)')
axes[0, 1].set_ylabel('Intensity')
axes[0, 1].grid(True, alpha=0.3)

# After smoothing
axes[1, 0].plot(df_cleaned['two_theta'], intensity_smoothed, linewidth=1.5, color='green')
axes[1, 0].set_title('After Savitzky-Golay Smoothing')
axes[1, 0].set_xlabel('2Œ∏ (degree)')
axes[1, 0].set_ylabel('Intensity')
axes[1, 0].grid(True, alpha=0.3)

# Overall comparison
axes[1, 1].plot(df['two_theta'], df['intensity'], label='Original', alpha=0.4, linewidth=1)
axes[1, 1].plot(df_cleaned['two_theta'], intensity_smoothed, label='Processed', linewidth=1.5)
axes[1, 1].set_title('Comparison')
axes[1, 1].set_xlabel('2Œ∏ (degree)')
axes[1, 1].set_ylabel('Intensity')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>


**Output**:

<pre><code>Number of negative values: 1
Number of outliers: 2
</code></pre>


**Explanation**:
This preprocessing pipeline removes physically impossible negative values, detects and interpolates statistical outliers, and finally removes noise. The order of processing is important; performing outlier removal before smoothing minimizes the impact of outliers.

</details>
<hr/>
<h3>Problem 3 (Difficulty: Hard)</h3>
<p>Real materials research scenario: You obtained data from 1000 samples through high-throughput XRD measurement. Build a system that automates the preprocessing pipeline for each sample and saves the processed data to CSV files.</p>
<p><strong>Background</strong>:
An automated XRD system generates measurement data for 100 samples daily. Manual processing is impossible, making automation essential.</p>
<p><strong>Tasks</strong>:
1. Create a function to batch-process multiple samples
2. Error handling (invalid data formats, extreme outliers)
3. Log output of processing results
4. Save processed data</p>
<p><strong>Constraints</strong>:
- Number of data points per sample may vary
- Some samples may have incomplete data due to measurement failure
- Processing must complete within 5 seconds/sample</p>
<details>
<summary>Hint</summary>

**Approach**:
1. Define a function that encapsulates preprocessing
2. Error handling with `try-except`
3. Record processing results in log file
4. Save using `pandas.to_csv()`

**Design Pattern**:

<pre><code class="language-python">def preprocess_xrd(data, params):
    """Preprocess XRD data"""
    # 1. Validation
    # 2. Remove negative values
    # 3. Detect outliers
    # 4. Smoothing
    # 5. Return results
    pass

def batch_process(file_list):
    """Batch process multiple files"""
    for file in file_list:
        try:
            # Load data
            # Execute preprocessing
            # Save
        except Exception as e:
            # Error log
            pass
</code></pre>
</details>
<details>
<summary>Solution</summary>

**Solution Overview**:
Build a robust pipeline to automatically process multiple samples of XRD data, including error handling, log output, and processing time measurement.

**Implementation Code**:


<pre><code class="language-python">import numpy as np
import pandas as pd
from scipy import stats
from scipy.signal import savgol_filter
import time
import logging
from pathlib import Path

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('xrd_processing.log'),
        logging.StreamHandler()
    ]
)

def validate_data(df):
    """Data validity check"""
    if df.empty:
        raise ValueError("Empty DataFrame")

    if 'two_theta' not in df.columns or 'intensity' not in df.columns:
        raise ValueError("Missing required columns")

    if len(df) &lt; 50:
        raise ValueError(f"Insufficient data points: {len(df)}")

    if df['intensity'].isnull().sum() &gt; len(df) * 0.3:
        raise ValueError("Too many missing values (&gt;30%)")

    return True


def preprocess_xrd(df, params=None):
    """
    XRD data preprocessing pipeline

    Parameters:
    -----------
    df : pd.DataFrame
        Columns: 'two_theta', 'intensity'
    params : dict
        Preprocessing parameters
        - z_threshold: Z-score threshold (default: 3)
        - sg_window: Savitzky-Golay window (default: 11)
        - sg_polyorder: Polynomial order (default: 3)

    Returns:
    --------
    df_processed : pd.DataFrame
        Preprocessed data
    """
    # Default parameters
    if params is None:
        params = {
            'z_threshold': 3,
            'sg_window': 11,
            'sg_polyorder': 3
        }

    # Data validation
    validate_data(df)

    df_processed = df.copy()

    # Step 1: Replace negative values with 0
    negative_count = (df_processed['intensity'] &lt; 0).sum()
    df_processed.loc[df_processed['intensity'] &lt; 0, 'intensity'] = 0

    # Step 2: Outlier detection and interpolation
    z_scores = np.abs(stats.zscore(df_processed['intensity']))
    outliers = z_scores &gt; params['z_threshold']
    outlier_count = outliers.sum()

    df_processed.loc[outliers, 'intensity'] = np.nan
    df_processed['intensity'] = df_processed['intensity'].interpolate(method='linear')

    # Step 3: Savitzky-Golay filter
    try:
        intensity_smoothed = savgol_filter(
            df_processed['intensity'].values,
            window_length=params['sg_window'],
            polyorder=params['sg_polyorder']
        )
        df_processed['intensity'] = intensity_smoothed
    except Exception as e:
        logging.warning(f"Savitzky-Golay failed: {e}. Using moving average.")
        from scipy.ndimage import uniform_filter1d
        df_processed['intensity'] = uniform_filter1d(
            df_processed['intensity'].values,
            size=params['sg_window']
        )

    # Processing statistics
    stats_dict = {
        'negative_values': negative_count,
        'outliers': outlier_count,
        'data_points': len(df_processed)
    }

    return df_processed, stats_dict


def batch_process_xrd(input_files, output_dir, params=None):
    """
    Batch process multiple XRD files

    Parameters:
    -----------
    input_files : list
        List of input file paths
    output_dir : str or Path
        Output directory
    params : dict
        Preprocessing parameters

    Returns:
    --------
    results : dict
        Processing result summary
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    results = {
        'total': len(input_files),
        'success': 0,
        'failed': 0,
        'processing_times': []
    }

    logging.info(f"Starting batch processing of {len(input_files)} files")

    for i, file_path in enumerate(input_files, 1):
        file_path = Path(file_path)
        start_time = time.time()

        try:
            # Load data
            df = pd.read_csv(file_path)

            # Execute preprocessing
            df_processed, stats_dict = preprocess_xrd(df, params)

            # Save
            output_file = output_dir / f"processed_{file_path.name}"
            df_processed.to_csv(output_file, index=False)

            # Processing time
            processing_time = time.time() - start_time
            results['processing_times'].append(processing_time)
            results['success'] += 1

            logging.info(
                f"[{i}/{len(input_files)}] SUCCESS: {file_path.name} "
                f"({processing_time:.2f}s) - "
                f"Negatives: {stats_dict['negative_values']}, "
                f"Outliers: {stats_dict['outliers']}"
            )

        except Exception as e:
            results['failed'] += 1
            logging.error(f"[{i}/{len(input_files)}] FAILED: {file_path.name} - {str(e)}")

    # Summary
    avg_time = np.mean(results['processing_times']) if results['processing_times'] else 0
    logging.info(
        f"\n=== Batch Processing Complete ===\n"
        f"Total: {results['total']}\n"
        f"Success: {results['success']}\n"
        f"Failed: {results['failed']}\n"
        f"Average processing time: {avg_time:.2f}s"
    )

    return results


# ==================== Demo Execution ====================

if __name__ == "__main__":
    # Generate sample data (in practice, load experimental data)
    np.random.seed(42)

    # Generate 10 sample datasets
    sample_dir = Path("sample_xrd_data")
    sample_dir.mkdir(exist_ok=True)

    for i in range(10):
        two_theta = np.linspace(20, 60, 400)
        intensity = (
            800 * np.exp(-((two_theta - 30) ** 2) / 8) +
            1200 * np.exp(-((two_theta - 45) ** 2) / 6) +
            np.random.normal(0, 100, len(two_theta))
        )

        # Randomly add outliers
        if np.random.rand() &gt; 0.5:
            outlier_idx = np.random.randint(0, len(intensity), size=2)
            intensity[outlier_idx] = np.random.choice([3000, -500], size=2)

        df = pd.DataFrame({'two_theta': two_theta, 'intensity': intensity})
        df.to_csv(sample_dir / f"sample_{i:03d}.csv", index=False)

    # Execute batch processing
    input_files = list(sample_dir.glob("sample_*.csv"))
    output_dir = Path("processed_xrd_data")

    params = {
        'z_threshold': 3,
        'sg_window': 11,
        'sg_polyorder': 3
    }

    results = batch_process_xrd(input_files, output_dir, params)

    print(f"\nProcessing complete: {results['success']}/{results['total']} files")
</code></pre>


**Results**:

<pre><code>2025-10-17 10:30:15 - INFO - Starting batch processing of 10 files
2025-10-17 10:30:15 - INFO - [1/10] SUCCESS: sample_000.csv (0.15s) - Negatives: 1, Outliers: 2
2025-10-17 10:30:15 - INFO - [2/10] SUCCESS: sample_001.csv (0.12s) - Negatives: 0, Outliers: 1
...
2025-10-17 10:30:16 - INFO -
=== Batch Processing Complete ===
Total: 10
Success: 10
Failed: 0
Average processing time: 0.13s
</code></pre>


**Detailed Explanation**:
1. **Error Handling**: Pre-check with `validate_data()`, capture runtime errors with `try-except`
2. **Logging**: Output processing status to both file and console
3. **Parameterization**: Preprocessing parameters can be specified externally
4. **Performance**: Measure processing time to identify bottlenecks
5. **Scalability**: Works regardless of number of files

**Additional Considerations**:
- Accelerate with parallel processing (`multiprocessing`)
- Save to database (SQLite)
- Visualize processing status with web dashboard
- Integration with cloud storage (S3, GCS)

</details>
<hr/>
<h2>References</h2>
<ol>
<li>
<p>VanderPlas, J. (2016). "Python Data Science Handbook." O'Reilly Media. ISBN: 978-1491912058</p>
</li>
<li>
<p>Savitzky, A., &amp; Golay, M. J. (1964). "Smoothing and Differentiation of Data by Simplified Least Squares Procedures." <em>Analytical Chemistry</em>, 36(8), 1627-1639. DOI: <a href="https://doi.org/10.1021/ac60214a047">10.1021/ac60214a047</a></p>
</li>
<li>
<p>Stein, H. S. et al. (2019). "Progress and prospects for accelerating materials science with automated and autonomous workflows." <em>Chemical Science</em>, 10(42), 9640-9649. DOI: <a href="https://doi.org/10.1039/C9SC03766G">10.1039/C9SC03766G</a></p>
</li>
<li>
<p>SciPy Documentation: Signal Processing. URL: <a href="https://docs.scipy.org/doc/scipy/reference/signal.html">https://docs.scipy.org/doc/scipy/reference/signal.html</a></p>
</li>
<li>
<p>pandas Documentation: Data Cleaning. URL: <a href="https://pandas.pydata.org/docs/user_guide/missing_data.html">https://pandas.pydata.org/docs/user_guide/missing_data.html</a></p>
</li>
</ol>
<hr/>
<h2>Navigation</h2>
<h3>Previous Chapter</h3>
<p>None (Chapter 1)</p>
<h3>Next Chapter</h3>
<p><strong><a href="./chapter-2.html">Chapter 2: Spectral Data Analysis ‚Üí</a></strong></p>
<h3>Series Contents</h3>
<p><strong><a href="./index.html">‚Üê Return to Series Contents</a></strong></p>
<hr/>
<h2>Author Information</h2>
<p><strong>Author</strong>: AI Terakoya Content Team
<strong>Created</strong>: 2025-10-17
<strong>Version</strong>: 1.0</p>
<p><strong>Update History</strong>:
- 2025-10-17: v1.0 Initial release</p>
<p><strong>Feedback</strong>:
- GitHub Issues: [Repository URL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<hr/>
<p><strong>Continue learning in the next chapter!</strong></p><div class="navigation">
<a class="nav-button" href="index.html">Return to Series Contents</a>
<a class="nav-button" href="chapter-2.html">Next Chapter ‚Üí</a>
</div>
</main>
<section class="disclaimer">
<h3>Disclaimer</h3>
<ul>
<li>This content is provided solely for educational, research, and informational purposes and does not constitute professional advice (legal, accounting, technical warranties, etc.).</li>
<li>This content and accompanying code examples are provided "AS IS" without any warranties, express or implied, including merchantability, fitness for a particular purpose, non-infringement, accuracy, completeness, operation, or safety.</li>
<li>The author and Tohoku University assume no responsibility for the content, availability, or safety of external links, third-party data, tools, or libraries.</li>
<li>To the maximum extent permitted by applicable law, the author and Tohoku University shall not be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising from the use, execution, or interpretation of this content.</li>
<li>The content may be changed, updated, or discontinued without notice.</li>
<li>The copyright and license of this content are subject to the specified conditions (e.g., CC BY 4.0). Such licenses typically include no-warranty clauses.</li>
</ul>
</section>
<footer>
<p><strong>Author</strong>: AI Terakoya Content Team</p>
<p><strong>Version</strong>: 1.0 | <strong>Created</strong>: 2025-10-17</p>
<p><strong>License</strong>: Creative Commons BY 4.0</p>
<p>¬© 2025 AI Terakoya. All rights reserved.</p>
</footer>
</body>
</html>
