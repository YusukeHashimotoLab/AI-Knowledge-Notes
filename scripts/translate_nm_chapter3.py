#!/usr/bin/env python3
"""
Translate NM Chapter 3 from Japanese to English
Handles large file by processing in chunks
"""

import re
from pathlib import Path

# File paths
JP_FILE = "/Users/yusukehashimoto/Documents/pycharm/AI_Homepage/wp/knowledge/jp/MI/nm-introduction/chapter3-hands-on.html"
EN_FILE = "/Users/yusukehashimoto/Documents/pycharm/AI_Homepage/wp/knowledge/en/MI/nm-introduction/chapter3-hands-on.html"

# Translation dictionary for NM Chapter 3 content
TRANSLATIONS = {
    # Meta and header
    '<html lang="ja">': '<html lang="en">',
    '<title>Chapter 3: PythonÂÆüË∑µ„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´ - AI Terakoya</title>': '<title>Chapter 3: Hands-On Python Tutorial - AI Terakoya</title>',

    # Breadcrumb
    'AIÂØ∫Â≠êÂ±ã„Éà„ÉÉ„Éó': 'AI Terakoya Top',
    '„Éû„ÉÜ„É™„Ç¢„É´„Ç∫„Éª„Ç§„É≥„Éï„Ç©„Éû„ÉÜ„Ç£„ÇØ„Çπ': 'Materials Informatics',

    # Header content
    'Chapter 3: PythonÂÆüË∑µ„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´': 'Chapter 3: Hands-On Python Tutorial',
    '„Éä„ÉéÊùêÊñô„Éá„Éº„ÇøËß£Êûê„Å®Ê©üÊ¢∞Â≠¶Áøí': 'Nanomaterial Data Analysis and Machine Learning',
    'Ë™≠‰∫ÜÊôÇÈñì: 30-40ÂàÜ': 'Reading Time: 30-40 min',
    'Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö': 'Difficulty: Beginner',
    '„Ç≥„Éº„Éâ‰æã: 0ÂÄã': 'Code Examples: 0',
    'ÊºîÁøíÂïèÈ°å: 0Âïè': 'Exercises: 0',

    # Chapter description
    'Â∞èË¶èÊ®°„Éá„Éº„Çø„Åß„ÇÇÂäπ„ÅèÂõûÂ∏∞„É¢„Éá„É´„Å®„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Åß„ÄÅÂäπÁéá„Çà„ÅèÊù°‰ª∂Êé¢Á¥¢„Åô„ÇãÁ≠ãËÇâ„Çí‰ªò„Åë„Åæ„Åô„ÄÇMD„Éá„Éº„Çø„ÅÆË¶ÅÁÇπÂèØË¶ñÂåñ„Å®SHAP„Å´„Çà„ÇãËß£Èáà„Åæ„Åß‰∏ÄÊ∞ó„Å´ÈÄö„Åó„Åæ„Åô„ÄÇ': 'Build skills for efficiently exploring conditions using regression models effective even with small datasets and Bayesian optimization. Covers essential visualization of MD data and interpretation with SHAP in one go.',
    'üí° Ë£úË∂≥:': 'üí° Supplement:',
    'Â∞ë„Å™„ÅÑË©¶Ë°å„ÅßËâØ„ÅÑÊù°‰ª∂„ÇíË¶ã„Å§„Åë„Çã„ÅÆ„ÅåÁõÆÊ®ô„ÄÇ„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅØ"ÈáëÂ±ûÊé¢Áü•Ê©ü"ÁöÑ„Å´ÂΩì„Åü„Çä„ÇíÂ∞é„Åç„Åæ„Åô„ÄÇ': 'The goal is to find good conditions with minimal trials. Bayesian optimization guides you to hits like a "metal detector".',

    # Learning objectives
    'Êú¨Á´†„ÅÆÂ≠¶ÁøíÁõÆÊ®ô': 'Learning Objectives',
    'Êú¨Á´†„ÇíÂ≠¶Áøí„Åô„Çã„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÅÆ„Çπ„Ç≠„É´„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö': 'By studying this chapter, you will acquire the following skills:',
    '„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÁîüÊàê„ÉªÂèØË¶ñÂåñ„ÉªÂâçÂá¶ÁêÜ„ÅÆÂÆüË∑µ': 'Hands-on nanoparticle data generation, visualization, and preprocessing',
    '5Á®ÆÈ°û„ÅÆÂõûÂ∏∞„É¢„Éá„É´„Å´„Çà„Çã„Éä„ÉéÊùêÊñôÁâ©ÊÄß‰∫àÊ∏¨': 'Prediction of nanomaterial properties using 5 types of regression models',
    '„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å´„Çà„Çã„Éä„ÉéÊùêÊñô„ÅÆÊúÄÈÅ©Ë®≠Ë®à': 'Optimal design of nanomaterials through Bayesian optimization',
    'SHAPÂàÜÊûê„Å´„Çà„ÇãÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„ÅÆËß£Èáà': 'Interpretation of machine learning models using SHAP analysis',
    'Â§öÁõÆÁöÑÊúÄÈÅ©Âåñ„Å´„Çà„Çã„Éà„É¨„Éº„Éâ„Ç™„ÉïÂàÜÊûê': 'Trade-off analysis through multi-objective optimization',
    'TEMÁîªÂÉèËß£Êûê„Å®„Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞': 'TEM image analysis and size distribution fitting',
    'Áï∞Â∏∏Ê§úÁü•„Å´„Çà„ÇãÂìÅË≥™ÁÆ°ÁêÜ„Å∏„ÅÆÂøúÁî®': 'Application to quality control through anomaly detection',

    # Section 3.1
    '3.1 Áí∞Â¢ÉÊßãÁØâ': '3.1 Environment Setup',
    'ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™': 'Required Libraries',
    'Êú¨„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´„Åß‰ΩøÁî®„Åô„Çã‰∏ªË¶Å„Å™Python„É©„Ç§„Éñ„É©„É™Ôºö': 'Main Python libraries used in this tutorial:',
    '# „Éá„Éº„ÇøÂá¶ÁêÜ„ÉªÂèØË¶ñÂåñ': '# Data processing and visualization',
    '# Ê©üÊ¢∞Â≠¶Áøí': '# Machine learning',
    '# ÊúÄÈÅ©Âåñ': '# Optimization',
    '# „É¢„Éá„É´Ëß£Èáà': '# Model interpretation',
    '# Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ': '# Multi-objective optimization (optional)',

    '„Ç§„É≥„Çπ„Éà„Éº„É´ÊñπÊ≥ï': 'Installation Methods',
    'Option 1: AnacondaÁí∞Â¢É': 'Option 1: Anaconda Environment',
    '# Anaconda„ÅßÊñ∞„Åó„ÅÑÁí∞Â¢É„Çí‰ΩúÊàê': '# Create a new environment with Anaconda',
    '# ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´': '# Install required libraries',
    '# Â§öÁõÆÁöÑÊúÄÈÅ©ÂåñÁî®Ôºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ': '# For multi-objective optimization (optional)',

    'Option 2: venv + pipÁí∞Â¢É': 'Option 2: venv + pip Environment',
    '# ‰ªÆÊÉ≥Áí∞Â¢É„Çí‰ΩúÊàê': '# Create virtual environment',
    '# ‰ªÆÊÉ≥Áí∞Â¢É„ÇíÊúâÂäπÂåñ': '# Activate virtual environment',

    'Option 3: Google Colab': 'Option 3: Google Colab',
    'Google Colab„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„Çí„Çª„É´„ÅßÂÆüË°åÔºö': 'When using Google Colab, execute the following code in a cell:',
    '# ËøΩÂä†„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´': '# Install additional packages',
    '# „Ç§„É≥„Éù„Éº„Éà„ÅÆÁ¢∫Ë™ç': '# Verify imports',
    'Áí∞Â¢ÉÊßãÁØâÂÆå‰∫ÜÔºÅ': 'Environment setup complete!',

    # Section 3.2
    '3.2 „Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÊ∫ñÂÇô„Å®ÂèØË¶ñÂåñ': '3.2 Nanoparticle Data Preparation and Visualization',
    '„Äê‰æã1„ÄëÂêàÊàê„Éá„Éº„ÇøÁîüÊàêÔºöÈáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆ„Çµ„Ç§„Ç∫„Å®ÂÖâÂ≠¶ÁâπÊÄß': '[Example 1] Synthetic Data Generation: Size and Optical Properties of Gold Nanoparticles',
    'Èáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆÂ±ÄÂú®Ë°®Èù¢„Éó„É©„Ç∫„É¢„É≥ÂÖ±È≥¥ÔºàLSPRÔºâÊ≥¢Èï∑„ÅØ„ÄÅÁ≤íÂ≠ê„Çµ„Ç§„Ç∫„Å´‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÈñ¢‰øÇ„ÇíÊ®°Êì¨„Éá„Éº„Çø„ÅßË°®Áèæ„Åó„Åæ„Åô„ÄÇ': 'The localized surface plasmon resonance (LSPR) wavelength of gold nanoparticles depends on particle size. This relationship is represented with simulated data.',

    # Code comments
    '# Êó•Êú¨Ë™û„Éï„Ç©„É≥„ÉàË®≠ÂÆöÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Ôºâ': '# Font settings (if needed)',
    '# ‰π±Êï∞„Ç∑„Éº„Éâ„ÅÆË®≠ÂÆöÔºàÂÜçÁèæÊÄß„ÅÆ„Åü„ÇÅÔºâ': '# Set random seed (for reproducibility)',
    '# „Çµ„É≥„Éó„É´Êï∞': '# Number of samples',
    '# Èáë„Éä„ÉéÁ≤íÂ≠ê„ÅÆ„Çµ„Ç§„Ç∫ÔºànmÔºâ: Âπ≥Âùá15 nm„ÄÅÊ®ôÊ∫ñÂÅèÂ∑Æ5 nm': '# Gold nanoparticle size (nm): mean 15 nm, std dev 5 nm',
    '# LSPRÊ≥¢Èï∑ÔºànmÔºâ: MieÁêÜË´ñ„ÅÆÁ∞°ÊòìËøë‰ºº': '# LSPR wavelength (nm): simplified Mie theory approximation',
    '# Âü∫Êú¨Ê≥¢Èï∑520 nm + „Çµ„Ç§„Ç∫‰æùÂ≠òÈ†Ö + „Éé„Ç§„Ç∫': '# Base wavelength 520 nm + size-dependent term + noise',
    '# ÂêàÊàêÊù°‰ª∂': '# Synthesis conditions',
    '# Ê∏©Â∫¶Ôºà‚ÑÉÔºâ': '# Temperature (¬∞C)',
    '# „Éá„Éº„Çø„Éï„É¨„Éº„É†„ÅÆ‰ΩúÊàê': '# Create DataFrame',

    # Output messages
    '"Èáë„Éä„ÉéÁ≤íÂ≠ê„Éá„Éº„Çø„ÅÆÁîüÊàêÂÆå‰∫Ü"': '"Gold nanoparticle data generation complete"',
    '"\\nÂü∫Êú¨Áµ±Ë®àÈáè:"': '"\\nBasic statistics:"',

    '„Äê‰æã2„Äë„Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†': '[Example 2] Size Distribution Histogram',
    '# „Çµ„Ç§„Ç∫ÂàÜÂ∏É„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†': '# Size distribution histogram',
    '# „Éí„Çπ„Éà„Ç∞„É©„É†„Å®KDEÔºà„Ç´„Éº„Éç„É´ÂØÜÂ∫¶Êé®ÂÆöÔºâ': '# Histogram and KDE (kernel density estimation)',
    '# KDE„Éó„É≠„ÉÉ„Éà': '# KDE plot',
    '"Âπ≥Âùá„Çµ„Ç§„Ç∫: {data[\'size_nm\'].mean():.2f} nm"': '"Average size: {data[\'size_nm\'].mean():.2f} nm"',
    '"Ê®ôÊ∫ñÂÅèÂ∑Æ: {data[\'size_nm\'].std():.2f} nm"': '"Standard deviation: {data[\'size_nm\'].std():.2f} nm"',
    '"‰∏≠Â§ÆÂÄ§: {data[\'size_nm\'].median():.2f} nm"': '"Median: {data[\'size_nm\'].median():.2f} nm"',

    '„Äê‰æã3„ÄëÊï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„Çπ': '[Example 3] Scatter Plot Matrix',
    '# „Éö„Ç¢„Éó„É≠„ÉÉ„ÉàÔºàÊï£Â∏ÉÂõ≥„Éû„Éà„É™„ÉÉ„ÇØ„ÇπÔºâ': '# Pairplot (scatter plot matrix)',
    '"ÂêÑÂ§âÊï∞Èñì„ÅÆÈñ¢‰øÇ„ÇíÂèØË¶ñÂåñ„Åó„Åæ„Åó„Åü"': '"Visualized relationships between variables"',

    '„Äê‰æã4„ÄëÁõ∏Èñ¢Ë°åÂàó„ÅÆ„Éí„Éº„Éà„Éû„ÉÉ„Éó': '[Example 4] Correlation Matrix Heatmap',
    '# Áõ∏Èñ¢Ë°åÂàó„ÅÆË®àÁÆó': '# Calculate correlation matrix',
    '# „Éí„Éº„Éà„Éû„ÉÉ„Éó': '# Heatmap',
}

def translate_text(text: str) -> str:
    """Apply translations to text"""
    result = text
    for jp, en in TRANSLATIONS.items():
        result = result.replace(jp, en)
    return result

def main():
    print("Reading Japanese file...")
    with open(JP_FILE, 'r', encoding='utf-8') as f:
        content = f.read()

    print("Applying translations...")
    translated = translate_text(content)

    # Ensure output directory exists
    output_path = Path(EN_FILE)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"Writing to {EN_FILE}...")
    with open(EN_FILE, 'w', encoding='utf-8') as f:
        f.write(translated)

    print("‚úì Translation complete!")

    # Count remaining Japanese characters
    jp_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+')
    jp_matches = jp_pattern.findall(translated)

    if jp_matches:
        print(f"\n‚ö† Warning: {len(jp_matches)} Japanese text segments remain")
        print("First 10 occurrences:")
        for i, match in enumerate(jp_matches[:10], 1):
            print(f"  {i}. {match}")
    else:
        print("\n‚úì No Japanese characters detected!")

    # Count lines
    lines = translated.split('\n')
    print(f"\nTotal lines: {len(lines)}")
    print(f"File size: {len(translated)} characters")

if __name__ == "__main__":
    main()
